{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type='text/css'>\n",
       ".datatable table.frame { margin-bottom: 0; }\n",
       ".datatable table.frame thead { border-bottom: none; }\n",
       ".datatable table.frame tr.coltypes td {  color: #FFFFFF;  line-height: 6px;  padding: 0 0.5em;}\n",
       ".datatable .bool    { background: #DDDD99; }\n",
       ".datatable .object  { background: #565656; }\n",
       ".datatable .int     { background: #5D9E5D; }\n",
       ".datatable .float   { background: #4040CC; }\n",
       ".datatable .str     { background: #CC4040; }\n",
       ".datatable .time    { background: #40CC40; }\n",
       ".datatable .row_index {  background: var(--jp-border-color3);  border-right: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  font-size: 9px;}\n",
       ".datatable .frame tbody td { text-align: left; }\n",
       ".datatable .frame tr.coltypes .row_index {  background: var(--jp-border-color0);}\n",
       ".datatable th:nth-child(2) { padding-left: 12px; }\n",
       ".datatable .hellipsis {  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .vellipsis {  background: var(--jp-layout-color0);  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .na {  color: var(--jp-cell-editor-border-color);  font-size: 80%;}\n",
       ".datatable .sp {  opacity: 0.25;}\n",
       ".datatable .footer { font-size: 9px; }\n",
       ".datatable .frame_dimensions {  background: var(--jp-border-color3);  border-top: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  display: inline-block;  opacity: 0.6;  padding: 1px 10px 1px 5px;}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from time import time\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from collections import Counter\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import NearMiss, TomekLinks\n",
    "import datatable as dt\n",
    "import warnings\n",
    "import json\n",
    "import pickle\n",
    "import datetime\n",
    "from chinese_calendar import is_workday, is_holiday\n",
    "\n",
    "from tools import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 给定预测标签，计算AUC\n",
    "使用OVR的策略计算每个类别的AUC\n",
    "过程：\n",
    "- 选择类别i作为正类，其他类别作为负类\n",
    "- 将真实标签中不等于i的标记为0，等于i的标记为1\n",
    "- 将预测标签中不等于i的标记为0，等于ide标记为1\n",
    "- 计算混淆矩阵\n",
    "- 计算(fpr, tpr)\n",
    "- 计算AUC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 加载数据\n",
    "训练数据加载过程：\n",
    "1. 分别加载处理好的用户特征和视频特征，以及整合的用户历史行为数据；\n",
    "2. 从用户历史行为数据中筛掉在视频特征中没出现过的video_id；\n",
    "3. 将行为数据中的user_id、video_id替换为对应用户/视频的特征\n",
    "4. 根据不同的任务划分为`watch_label`、`is_share`的数据集\n",
    "\n",
    "推断时，类似于上述过程拼接数据。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 读取数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = \"../2021_3_data\"\n",
    "test_data_dir  = os.path.join(base_dir, \"testdata\")\n",
    "train_data_dir = os.path.join(base_dir, \"traindata\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 基础特征与附加特征合并"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_status = dt.fread(os.path.join(train_data_dir, \"video_features_data/video_status.csv\"))\n",
    "user_status = dt.fread(os.path.join(train_data_dir, \"user_features_data/user_status.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "tab_user = dt.fread(os.path.join(train_data_dir, \"user_features_data/user_features.jay\"))\n",
    "tab_video = dt.fread(os.path.join(train_data_dir, \"video_features_data/video_features.jay\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_status.key = 'video_id'\n",
    "video_ws = tab_video[:, :, join(video_status)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_status.key = 'user_id'\n",
    "user_ws = tab_user[:, :, join(user_status)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_ws.to_jay(os.path.join(train_data_dir, \"video_features_data/video_features_with_status.jay\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_ws.to_jay(os.path.join(train_data_dir, \"user_features_data/user_features_with_status.jay\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class='datatable'>\n",
       "  <table class='frame'>\n",
       "  <thead>\n",
       "    <tr class='colnames'><td class='row_index'></td><th>user_id</th><th>age_0</th><th>age_1</th><th>age_2</th><th>age_3</th><th>age_4</th><th>age_5</th><th>age_6</th><th>age_7</th><th>gender_0</th><th class='vellipsis'>&hellip;</th><th>average_watch_label</th><th>sum_watch_times</th><th>sum_comment_times</th><th>sum_collect_times</th><th>sum_share_times</th></tr>\n",
       "    <tr class='coltypes'><td class='row_index'></td><td class='float' title='float64'>&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;</td><td class='float' title='float64'>&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;</td><td class='float' title='float64'>&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;</td><td class='float' title='float64'>&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;</td><td class='float' title='float64'>&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;</td><td class='float' title='float64'>&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;</td><td class='float' title='float64'>&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;</td><td class='float' title='float64'>&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;</td><td class='float' title='float64'>&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;</td><td class='float' title='float64'>&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;</td><td></td><td class='float' title='float64'>&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;</td><td class='float' title='float64'>&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;</td><td class='float' title='float64'>&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;</td><td class='float' title='float64'>&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;</td><td class='float' title='float64'>&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;</td></tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr><td class='row_index'>0</td><td>1.757e+06</td><td>0</td><td>0</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td class=vellipsis>&hellip;</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr>\n",
       "    <tr><td class='row_index'>1</td><td>17938</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td><td class=vellipsis>&hellip;</td><td>0.0967742</td><td>3</td><td>0</td><td>0</td><td>0</td></tr>\n",
       "    <tr><td class='row_index'>2</td><td>4.26352e+06</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td><td class=vellipsis>&hellip;</td><td>0.204545</td><td>2</td><td>0</td><td>0</td><td>0</td></tr>\n",
       "    <tr><td class='row_index'>3</td><td>1.4116e+06</td><td>0</td><td>0</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td><td class=vellipsis>&hellip;</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr>\n",
       "    <tr><td class='row_index'>4</td><td>3.99224e+06</td><td>0</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td><td class=vellipsis>&hellip;</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr>\n",
       "    <tr><td class='row_index'>5</td><td>4.0116e+06</td><td>0</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td><td class=vellipsis>&hellip;</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr>\n",
       "    <tr><td class='row_index'>6</td><td>4.78556e+06</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td><td class=vellipsis>&hellip;</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr>\n",
       "    <tr><td class='row_index'>7</td><td>5.11036e+06</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td><td class=vellipsis>&hellip;</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr>\n",
       "    <tr><td class='row_index'>8</td><td>1.3212e+06</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td><td class=vellipsis>&hellip;</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr>\n",
       "    <tr><td class='row_index'>9</td><td>3.20698e+06</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td class=vellipsis>&hellip;</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr>\n",
       "    <tr><td class='row_index'>10</td><td>5.18172e+06</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td class=vellipsis>&hellip;</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td></tr>\n",
       "    <tr><td class='row_index'>11</td><td>1.878e+06</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td><td class=vellipsis>&hellip;</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td></tr>\n",
       "    <tr><td class='row_index'>12</td><td>3.04464e+06</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td class=vellipsis>&hellip;</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td></tr>\n",
       "    <tr><td class='row_index'>13</td><td>108273</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td class=vellipsis>&hellip;</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr>\n",
       "    <tr><td class='row_index'>14</td><td>5.64838e+06</td><td>0</td><td>0</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td class=vellipsis>&hellip;</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr>\n",
       "    <tr><td class='row_index'>&#x22EE;</td><td class='hellipsis'>&#x22EE;</td><td class='hellipsis'>&#x22EE;</td><td class='hellipsis'>&#x22EE;</td><td class='hellipsis'>&#x22EE;</td><td class='hellipsis'>&#x22EE;</td><td class='hellipsis'>&#x22EE;</td><td class='hellipsis'>&#x22EE;</td><td class='hellipsis'>&#x22EE;</td><td class='hellipsis'>&#x22EE;</td><td class='hellipsis'>&#x22EE;</td><td class='hellipsis'>&#x22F1;</td><td class='hellipsis'>&#x22EE;</td><td class='hellipsis'>&#x22EE;</td><td class='hellipsis'>&#x22EE;</td><td class='hellipsis'>&#x22EE;</td><td class='hellipsis'>&#x22EE;</td></tr>\n",
       "    <tr><td class='row_index'>5,910,795</td><td>3.22343e+06</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td><td>1</td><td class=vellipsis>&hellip;</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr>\n",
       "    <tr><td class='row_index'>5,910,796</td><td>4.70783e+06</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td><td>1</td><td class=vellipsis>&hellip;</td><td>0.142857</td><td>3</td><td>0</td><td>0</td><td>0</td></tr>\n",
       "    <tr><td class='row_index'>5,910,797</td><td>5.90765e+06</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td><td class=vellipsis>&hellip;</td><td>0</td><td>2</td><td>0</td><td>0</td><td>0</td></tr>\n",
       "    <tr><td class='row_index'>5,910,798</td><td>3.63322e+06</td><td>0</td><td>0</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td><td class=vellipsis>&hellip;</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr>\n",
       "    <tr><td class='row_index'>5,910,799</td><td>782537</td><td>0</td><td>0</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td><td class=vellipsis>&hellip;</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr>\n",
       "  </tbody>\n",
       "  </table>\n",
       "  <div class='footer'>\n",
       "    <div class='frame_dimensions'>5,910,800 rows &times; 36 columns</div>\n",
       "  </div>\n",
       "</div>\n"
      ],
      "text/plain": [
       "<Frame#7f315506be10 5910800x36>"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_ws"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 通过 .npz 读取数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 单表读取后合并"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.6 s, sys: 2.4 s, total: 6 s\n",
      "Wall time: 12.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# 单独读取每个文件再进行合并\n",
    "user_df = read_npz_to_df(os.path.join(train_data_dir, \"user_features_data/user_features.npz\"), data_name='features', column_name='columns')\n",
    "video_df = read_npz_to_df(os.path.join(train_data_dir, \"video_features_data/video_features.npz\"), data_name='features')\n",
    "action_df = read_npz_to_df(os.path.join(train_data_dir, \"all_actions.npz\"), data_name='data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 797,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 因为将字符串保存到 .npz时会使dtype为object，重新读回DataFrame时各个列的数据类型均为 object，所以先转换类型\n",
    "dtypes = dict(zip(video_df.columns, [np.float32] * video_df.shape[1]))\n",
    "dtypes.update({'video_name': np.str})\n",
    "video_df = video_df.astype(dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 802,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 29s, sys: 5.88 s, total: 1min 35s\n",
      "Wall time: 40.1 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(7353024, 76)"
      ]
     },
     "execution_count": 802,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# 合并各个表\n",
    "df_train = merge_user_video_action(user_df, video_df, action_df)\n",
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 805,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez(os.path.join(train_data_dir, \"train.npz\"), data=df_train.to_pandas().values, columns=df_train.to_pandas().columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 777,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 626 ms, sys: 0 ns, total: 626 ms\n",
      "Wall time: 721 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "test_df = load_table(os.path.join(test_data_dir, \"test.csv\"), ftype=\"csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 779,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 8s, sys: 12.8 s, total: 3min 20s\n",
      "Wall time: 51.2 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2822180, 73)"
      ]
     },
     "execution_count": 779,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "df_test = merge_user_video_action(user_df, video_df, test_df)\n",
    "df_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 读取合并好的数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# 直接读取保存好的合并后的训练数据\n",
    "path = os.path.join(train_data_dir, \"train.npz\")\n",
    "df_train = read_npz_to_df(path, data_name='data')\n",
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 810,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 37s, sys: 36.9 s, total: 2min 14s\n",
      "Wall time: 5min 49s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2822180, 73)"
      ]
     },
     "execution_count": 810,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# 直接读取保存好的合并后的测试数据\n",
    "path = os.path.join(test_data_dir, \"test.npz\")\n",
    "df_test = read_npz_to_df(path, data_name='data')\n",
    "df_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 通过 .jay 文件读取"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 单表读取后合并"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with_status = False\n",
    "if with_status:\n",
    "    user_features_name = \"user_features_with_status\"\n",
    "    video_features_name = \"video_features_with_status\"\n",
    "else:\n",
    "    user_features_name = \"user_features\"\n",
    "    video_features_name = \"video_features\"\n",
    "    \n",
    "p_user = os.path.join(train_data_dir, f\"user_features_data/{user_features_name}.jay\")\n",
    "p_video = os.path.join(train_data_dir, f\"video_features_data/{video_features_name}.jay\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 26 s, sys: 1.13 s, total: 27.1 s\n",
      "Wall time: 1.42 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(7353024, 134)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "## 使用datatable 加载训练数据\n",
    "p_act = os.path.join(train_data_dir, \"all_actions_with_status.jay\")\n",
    "\n",
    "df_train, others = load_train_test_data(None, pre_merged=False, return_others=True,\n",
    "                           **{\"p_user\": p_user, \"p_video\": p_video, \"p_action\": p_act})\n",
    "user_df = others['user']\n",
    "video_df = others['video']\n",
    "action_df = others['action']\n",
    "df_train.shape"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# 给训练数据生成is_happy_day列\n",
    "tt = dt.fread(os.path.join(train_data_dir, \"all_actions_with_ptd_v1.jay\")).to_pandas()\n",
    "tt['pt_d'] = tt['pt_d'].apply(lambda x: datetime.date(x//10000, x%20210000//100, x%100))\n",
    "dates = set(tt['pt_d'])\n",
    "happy_dict = {d: int((not is_workday(d)) or is_holiday(d)) for d in dates}\n",
    "is_happy_day = tt['pt_d'].apply(lambda x: happy_dict[x])\n",
    "is_happy_day"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# action_date = dt.fread(os.path.join(train_data_dir, f\"all_actions_with_ptd.jay\"))\n",
    "video_status = dt.fread(os.path.join(train_data_dir, f\"video_features_data/video_137days_status.csv\"))\n",
    "user_status = dt.fread(os.path.join(train_data_dir, f\"user_features_data/user_137days_status.csv\"))\n",
    "video_status.key = ('video_id', 'pt_d')\n",
    "user_status.key = ('user_id', 'pt_d')\n",
    "video_status"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "tt = df_train.to_pandas()\n",
    "np.savez(os.path.join(train_data_dir, \"train\"), data=tt.values, columns=tt.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 21.1 s, sys: 540 ms, total: 21.6 s\n",
      "Wall time: 1.16 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2822180, 131)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# p_user = os.path.join(train_data_dir, \"user_features_data/user_features.jay\")\n",
    "# p_video = os.path.join(train_data_dir, \"video_features_data/video_features.jay\")\n",
    "p_act = os.path.join(test_data_dir, \"test_with_status.jay\")\n",
    "\n",
    "#path = os.path.join(test_data_dir, \"test.jay\")\n",
    "kwargs = {\"p_user\": p_user, \"p_video\": p_video, \"p_action\": p_act}\n",
    "\n",
    "df_test, others = load_train_test_data(None, pre_merged=False, return_others=True, **kwargs)\n",
    "test_df = others['action']\n",
    "df_test.shape"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "% 为测试数据加上is_happy_day特征列\n",
    "d = datetime.date(2021,5,3)\n",
    "v = int((not is_workday(d)) or is_holiday(d))\n",
    "test_df['is_happy_day'] = v"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "act = dt.fread(os.path.join(test_data_dir, \"test.csv\"))\n",
    "act['pt_d'] = 20210503\n",
    "# act = dt.fread(os.path.join(train_data_dir, \"all_actions_with_ptd_v1.jay\"))\n",
    "video_status = dt.fread(os.path.join(train_data_dir, f\"video_features_data/video_137days_status.csv\"))\n",
    "user_status = dt.fread(os.path.join(train_data_dir, f\"user_features_data/user_137days_status.csv\"))\n",
    "video_status.key = ('video_id', 'pt_d')\n",
    "user_status.key = ('user_id', 'pt_d')\n",
    "\n",
    "act = act[:, :, dt.join(video_status)]\n",
    "act = act[:, :, dt.join(user_status)]\n",
    "act"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "del act['pt_d']\n",
    "act.to_jay(os.path.join(train_data_dir, \"all_actions_with_status_v1.jay\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_df = action_df.to_pandas()\n",
    "user_df = user_df.to_pandas()\n",
    "video_df = video_df.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = test_df.to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 读取合并好后的数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_test.to_jay(os.path.join(test_data_dir, \"test_with_status.jay\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.to_jay(os.path.join(train_data_dir, \"train_with_status.jay\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 177 ms, total: 177 ms\n",
      "Wall time: 184 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(7353024, 76)"
      ]
     },
     "execution_count": 468,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# 直接读取保存好的合并后的训练数据\n",
    "path = os.path.join(train_data_dir, \"train.jay\")\n",
    "df_train = load_train_test_data(path, pre_merged=True)\n",
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 649 µs, sys: 46 µs, total: 695 µs\n",
      "Wall time: 684 µs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2822180, 72)"
      ]
     },
     "execution_count": 454,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# 直接读取保存好的合并后的测试数据\n",
    "path = os.path.join(test_data_dir, \"test.jay\")\n",
    "df_test = load_train_test_data(path, pre_merged=True)\n",
    "df_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 处理训练数据\n",
    "可在此做一些预处理：\n",
    "- 从用户历史行为数据中筛掉在视频特征中没出现过的video_id\n",
    "- 删除多余的列\n",
    "- 调整列的顺序\n",
    "- 改变列的数据类型\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 节约内存的一个标配函数\n",
    "def reduce_mem(df):\n",
    "    starttime = time()\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if pd.isnull(c_min) or pd.isnull(c_max):\n",
    "                continue\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)\n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('-- Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction),time spend:{:2.2f} min'.format(end_mem,\n",
    "                                                                                                           100*(start_mem-end_mem)/start_mem,\n",
    "                                                                                                           (time()-starttime)/60))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 23.3 s, sys: 3.36 s, total: 26.6 s\n",
      "Wall time: 5.17 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if isinstance(df_train, dt.Frame):\n",
    "    df_train = df_train.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Mem. usage decreased to 1879.32 Mb (48.7% reduction),time spend:0.36 min\n"
     ]
    }
   ],
   "source": [
    "df_train = reduce_mem(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7353655 entries, 0 to 7353654\n",
      "Columns: 176 entries, user_id to da_7\n",
      "dtypes: bool(53), float16(105), float32(6), int16(1), int32(2), int8(4), object(5)\n",
      "memory usage: 2.3+ GB\n"
     ]
    }
   ],
   "source": [
    "df_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 删除 video_name、is_watch 列\n",
    "df_train.drop(['video_name', 'is_watch'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "if 'action_df' not in dir():\n",
    "    action_df = load_table(os.path.join(train_data_dir, \"all_actions.jay\")).to_pandas()\n",
    "if 'video_df' not in dir():\n",
    "    video_df = load_table(os.path.join(train_data_dir, \"video_features_data/video_features.jay\")).to_pandas()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# 从用户历史行为数据中筛掉在视频特征中没出现过的video_id\n",
    "idx1 = pd.Index(action_df['video_id'].unique())\n",
    "idx2 = pd.Index(video_df['video_id'])\n",
    "not_exists = idx1.difference(idx2)\n",
    "not_exists"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "%%time\n",
    "# 将训练数据中未出现的视频剔除\n",
    "n = 0\n",
    "for vid in not_exists:\n",
    "    tn = (df_train['video_id'] == vid).sum()\n",
    "    df_train['video_id'].replace(vid, np.nan, inplace=True)\n",
    "    n += tn\n",
    "\n",
    "if n > 0:\n",
    "    df_train.dropna(axis=0, inplace=True)\n",
    "    df_train.reset_index(drop=True, inplace=True)\n",
    "print(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 删除 video_id、user_id列\n",
    "df_train.drop(['user_id', 'video_id'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7353024, 130)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = df_train\n",
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((7353024,), (7353024,), (7353024, 128))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 准备数据\n",
    "watch_label = dataset.pop('watch_label').astype(np.uint8)\n",
    "is_share = dataset.pop('is_share').astype(np.uint8)\n",
    "watch_label.shape, is_share.shape, dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "del action_df, user_df, video_df, #test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 处理测试数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 10.7 s, sys: 2.71 s, total: 13.4 s\n",
      "Wall time: 4.71 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if isinstance(df_test, dt.Frame):\n",
    "    df_test = df_test.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Mem. usage decreased to 810.12 Mb (65.6% reduction),time spend:0.39 min\n"
     ]
    }
   ],
   "source": [
    "df_test  = reduce_mem(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'test_df' not in dir():\n",
    "    test_df = pd.read_csv(os.path.join(test_data_dir, \"test.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2822180 entries, 0 to 2822179\n",
      "Data columns (total 60 columns):\n",
      " #   Column                 Dtype  \n",
      "---  ------                 -----  \n",
      " 0   user_id                int32  \n",
      " 1   video_id               int32  \n",
      " 2   v_avg_watch_label_1    float64\n",
      " 3   v_sum_watch_times_1    float64\n",
      " 4   v_sum_watch_overs_1    float64\n",
      " 5   v_sum_comment_times_1  float64\n",
      " 6   v_sum_collect_times_1  float64\n",
      " 7   v_sum_share_times_1    float64\n",
      " 8   v_sum_quit_times_1     float64\n",
      " 9   v_sum_skip_times_1     float64\n",
      " 10  v_sum_watch_days_1     float64\n",
      " 11  v_avg_watch_label_3    float64\n",
      " 12  v_sum_watch_times_3    float64\n",
      " 13  v_sum_watch_overs_3    float64\n",
      " 14  v_sum_comment_times_3  float64\n",
      " 15  v_sum_collect_times_3  float64\n",
      " 16  v_sum_share_times_3    float64\n",
      " 17  v_sum_quit_times_3     float64\n",
      " 18  v_sum_skip_times_3     float64\n",
      " 19  v_sum_watch_days_3     float64\n",
      " 20  v_avg_watch_label_7    float64\n",
      " 21  v_sum_watch_times_7    float64\n",
      " 22  v_sum_watch_overs_7    float64\n",
      " 23  v_sum_comment_times_7  float64\n",
      " 24  v_sum_collect_times_7  float64\n",
      " 25  v_sum_share_times_7    float64\n",
      " 26  v_sum_quit_times_7     float64\n",
      " 27  v_sum_skip_times_7     float64\n",
      " 28  v_sum_watch_days_7     float64\n",
      " 29  u_avg_watch_label_1    float64\n",
      " 30  u_sum_watch_times_1    float64\n",
      " 31  u_sum_watch_overs_1    float64\n",
      " 32  u_sum_quit_times_1     float64\n",
      " 33  u_sum_skip_times_1     object \n",
      " 34  u_sum_comment_times_1  float64\n",
      " 35  u_sum_collect_times_1  float64\n",
      " 36  u_sum_share_times_1    float64\n",
      " 37  u_sum_watch_time_1     float64\n",
      " 38  u_sum_watch_days_1     object \n",
      " 39  u_avg_watch_label_3    float64\n",
      " 40  u_sum_watch_times_3    float64\n",
      " 41  u_sum_watch_overs_3    float64\n",
      " 42  u_sum_quit_times_3     float64\n",
      " 43  u_sum_skip_times_3     object \n",
      " 44  u_sum_comment_times_3  float64\n",
      " 45  u_sum_collect_times_3  float64\n",
      " 46  u_sum_share_times_3    float64\n",
      " 47  u_sum_watch_time_3     float64\n",
      " 48  u_sum_watch_days_3     float64\n",
      " 49  u_avg_watch_label_7    float64\n",
      " 50  u_sum_watch_times_7    float64\n",
      " 51  u_sum_watch_overs_7    float64\n",
      " 52  u_sum_quit_times_7     float64\n",
      " 53  u_sum_skip_times_7     object \n",
      " 54  u_sum_comment_times_7  float64\n",
      " 55  u_sum_collect_times_7  float64\n",
      " 56  u_sum_share_times_7    float64\n",
      " 57  u_sum_watch_time_7     float64\n",
      " 58  u_sum_watch_days_7     float64\n",
      " 59  is_happy_day           int32  \n",
      "dtypes: float64(53), int32(3), object(4)\n",
      "memory usage: 1.2+ GB\n"
     ]
    }
   ],
   "source": [
    "test_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7353655 entries, 0 to 7353654\n",
      "Columns: 170 entries, v_avg_watch_label_1 to da_7\n",
      "dtypes: bool(53), float16(105), float32(6), int16(1), int8(1), object(4)\n",
      "memory usage: 2.2+ GB\n"
     ]
    }
   ],
   "source": [
    "df_test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 删除 video_name 列\n",
    "if 'video_name' in df_test.columns:\n",
    "    df_test.drop('video_name', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# 测试数据集中存在video_id没有在视频特征中出现\n",
    "idx1 = pd.Index(test_df['video_id'].unique())\n",
    "idx2 = pd.Index(video_df['video_id'].unique())\n",
    "non_exists = idx1.difference(idx2)\n",
    "non_exists"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "%%time\n",
    "t0 = time()\n",
    "n = 0\n",
    "for vid in not_exists:\n",
    "    tn = (test_df['video_id'] == vid).sum()\n",
    "#     df_test = action_df[action_df['video_id'] != vid]\n",
    "    n += tn\n",
    "\n",
    "print(f\"在视频特征中不存在的video_id在测试数据集中出现的次数 = {n}\\t\\t(cost {time() - t0:.3f}s)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 删除 video_id、user_id 列\n",
    "df_test.drop(['user_id', 'video_id'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2822180, 128)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inference_dataset = df_test\n",
    "inference_dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 判断训练集和测试集的特征列是对齐的\n",
    "all([e1 == e2 for e1, e2 in zip(inference_dataset.columns.to_list(), dataset.columns.to_list())])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# watch_label 预测"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 处理数据不均衡问题"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 5176743), (1, 557421), (2, 314107), (3, 219188), (4, 172404), (5, 143001), (6, 125092), (7, 117749), (8, 138798), (9, 388521)]\n",
      "[[0.         0.70402912]\n",
      " [1.         0.0758084 ]\n",
      " [2.         0.04271807]\n",
      " [3.         0.02980923]\n",
      " [4.         0.02344668]\n",
      " [5.         0.01944792]\n",
      " [6.         0.01701232]\n",
      " [7.         0.01601368]\n",
      " [8.         0.01887632]\n",
      " [9.         0.05283826]]\n"
     ]
    }
   ],
   "source": [
    "items = list(Counter(watch_label).items())\n",
    "items.sort(key=lambda x: x[0])\n",
    "print(items)\n",
    "\n",
    "dist = np.array(items, dtype=np.float)\n",
    "dist[:, 1] = dist[:, 1] / watch_label.shape[0]\n",
    "print(dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "under_ss = np.array(items)\n",
    "under_ss_thresh = under_ss[9, 1]  # 设置每个类别样本数目的上限\n",
    "under_ss[:, 1] = np.clip(under_ss[:, 1], a_min=None, a_max=under_ss_thresh)\n",
    "\n",
    "over_ss = under_ss.copy()\n",
    "over_ss_thresh = under_ss[2, 1]  # 设置每个类别样本数据的下限\n",
    "over_ss[:, 1] = np.clip(over_ss[:, 1], a_min=over_ss_thresh, a_max=None)\n",
    "\n",
    "under_ss = dict(under_ss)\n",
    "over_ss = dict(over_ss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({0: 388521,\n",
       "  1: 388521,\n",
       "  2: 314107,\n",
       "  3: 219188,\n",
       "  4: 172404,\n",
       "  5: 143001,\n",
       "  6: 125092,\n",
       "  7: 117749,\n",
       "  8: 138798,\n",
       "  9: 388521},\n",
       " {0: 388521,\n",
       "  1: 388521,\n",
       "  2: 314107,\n",
       "  3: 314107,\n",
       "  4: 314107,\n",
       "  5: 314107,\n",
       "  6: 314107,\n",
       "  7: 314107,\n",
       "  8: 314107,\n",
       "  9: 388521})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "under_ss, over_ss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({0: 4788222})\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Int64Index([      1,       3,       6,       8,       9,      11,      12,\n",
       "                 13,      14,      15,\n",
       "            ...\n",
       "            7353009, 7353010, 7353014, 7353016, 7353017, 7353018, 7353019,\n",
       "            7353020, 7353022, 7353023],\n",
       "           dtype='int64', length=4788222)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del_idxs = pd.Index([], dtype=int)\n",
    "exclude = [1]\n",
    "for l, n in items:\n",
    "    if n > under_ss_thresh and l not in exclude:\n",
    "        t_idxs = watch_label == l\n",
    "        t_idxs = t_idxs.replace(False, np.nan).dropna().index  # 保留watch_label=l的行索引\n",
    "        t_left_idxs = np.random.choice(t_idxs, under_ss_thresh, replace=False)  # 选择一部分保留，注意replace参数，为True时会重复采样\n",
    "        t_del_idxs = t_idxs.difference(t_left_idxs)\n",
    "        print(Counter(watch_label[t_del_idxs]))\n",
    "                \n",
    "        del_idxs = del_idxs.union(t_del_idxs)\n",
    "del_idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({2: 314107,\n",
       "         0: 5176743,\n",
       "         5: 143001,\n",
       "         4: 172404,\n",
       "         1: 557421,\n",
       "         9: 388521,\n",
       "         3: 219188,\n",
       "         8: 138798,\n",
       "         7: 117749,\n",
       "         6: 125092})"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(watch_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "left_idxs = dataset.index.difference(del_idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2564802, 128), (2564802,))"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 提取出下采样后的数据\n",
    "data = dataset.loc[left_idxs]\n",
    "watch_label_res = watch_label.loc[left_idxs]\n",
    "data.reset_index(inplace=True, drop=True)\n",
    "watch_label_res.reset_index(inplace=True, drop=True)\n",
    "data.shape, watch_label_res.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({2: 314107,\n",
       "         5: 143001,\n",
       "         4: 172404,\n",
       "         1: 557421,\n",
       "         9: 388521,\n",
       "         3: 219188,\n",
       "         0: 388521,\n",
       "         8: 138798,\n",
       "         7: 117749,\n",
       "         6: 125092})"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(watch_label_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_data = data[(watch_label_res==5) | (watch_label_res == 6) | (watch_label_res == 7) | (watch_label_res == 8)]\n",
    "tmp_wl = watch_label_res[(watch_label_res==5) | (watch_label_res == 6) | (watch_label_res == 7) | (watch_label_res == 8)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({5: 143001, 8: 138798, 7: 117749, 6: 125092})"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(tmp_wl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# 对几个数量较少的类别进行上采样\n",
    "smt = SMOTE(sampling_strategy={5: 150000, 6: 130000, 7: 120000, 8: 140000}, n_jobs=8)\n",
    "X_r, y_r = smt.fit_resample(tmp_data, tmp_wl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(tmp_data.index, axis=0, inplace=True)\n",
    "watch_label_res.drop(tmp_wl.index, axis=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2441262, 127), (2441262,))"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data.append(X_r, ignore_index=True)\n",
    "watch_label_res = watch_label_res.append(y_r, ignore_index=True)\n",
    "data.shape, watch_label_res.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1796809,), (394422,))"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_rate = 0.18\n",
    "train_idx, test_idx = train_test_split(data.index, test_size=test_rate, random_state=0, shuffle=True, stratify=watch_label_res)\n",
    "train_idx.shape, test_idx.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = data.iloc[train_idx]\n",
    "X_test  = data.iloc[test_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = watch_label_res.iloc[train_idx]\n",
    "y_test  = watch_label_res.iloc[test_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1964639, 170), (1964639,), (431263, 170), (431263,))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, y_train.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# 为每个样本赋予权重，值为样本类别所占比例\n",
    "t = list(Counter(y_train).items())\n",
    "t.sort(key=lambda x: x[0])\n",
    "t = np.array(t)[:, 1]\n",
    "t = t / sum(t)\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data preparing finished ...\t\t(65.686s)\n"
     ]
    }
   ],
   "source": [
    "t0 = time()\n",
    "# train_weight = [t[i] for i in y_train]\n",
    "# test_weight = [t[i] for i in y_test]\n",
    "xg_train = xgb.DMatrix(X_train.values, label=y_train.values, enable_categorical=True)\n",
    "xg_test = xgb.DMatrix(X_test.values, label=y_test.values, enable_categorical=True)\n",
    "print(f\"Data preparing finished ...\\t\\t({time()-t0:.3f}s)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup parameters for xgboost\n",
    "param = {\n",
    "    'objective': 'multi:softmax',\n",
    "    'eta': 0.1,\n",
    "    'nthread': 8,\n",
    "    'num_class': 10,\n",
    "    'gpu_id': 0,\n",
    "    'tree_method': 'gpu_hist',\n",
    "    'eval_metric': ['mlogloss', 'auc', 'merror'],\n",
    "    'max_depth': 9,\n",
    "    'min_child_weight': 9,\n",
    "    'gamma': 0.2,\n",
    "    'subsample': 0.9,\n",
    "    'colsample_bytree': 0.6,\n",
    "    'reg_alpha': 0\n",
    "}\n",
    "\n",
    "watchlist = [(xg_train, 'train'), (xg_test, 'test')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-mlogloss:2.27472\ttrain-auc:0.62861\ttrain-merror:0.74241\ttest-mlogloss:2.27541\ttest-auc:0.62020\ttest-merror:0.74557\n",
      "[1]\ttrain-mlogloss:2.25122\ttrain-auc:0.63706\ttrain-merror:0.73937\ttest-mlogloss:2.25255\ttest-auc:0.62695\ttest-merror:0.74270\n",
      "[2]\ttrain-mlogloss:2.23127\ttrain-auc:0.64070\ttrain-merror:0.73825\ttest-mlogloss:2.23329\ttest-auc:0.62903\ttest-merror:0.74124\n",
      "[3]\ttrain-mlogloss:2.21338\ttrain-auc:0.64383\ttrain-merror:0.73637\ttest-mlogloss:2.21605\ttest-auc:0.63153\ttest-merror:0.73965\n",
      "[4]\ttrain-mlogloss:2.19776\ttrain-auc:0.64634\ttrain-merror:0.73507\ttest-mlogloss:2.20110\ttest-auc:0.63336\ttest-merror:0.73846\n",
      "[5]\ttrain-mlogloss:2.18415\ttrain-auc:0.64793\ttrain-merror:0.73433\ttest-mlogloss:2.18814\ttest-auc:0.63431\ttest-merror:0.73769\n",
      "[6]\ttrain-mlogloss:2.17215\ttrain-auc:0.64917\ttrain-merror:0.73374\ttest-mlogloss:2.17671\ttest-auc:0.63513\ttest-merror:0.73748\n",
      "[7]\ttrain-mlogloss:2.16136\ttrain-auc:0.65026\ttrain-merror:0.73326\ttest-mlogloss:2.16652\ttest-auc:0.63589\ttest-merror:0.73728\n",
      "[8]\ttrain-mlogloss:2.15181\ttrain-auc:0.65134\ttrain-merror:0.73269\ttest-mlogloss:2.15758\ttest-auc:0.63649\ttest-merror:0.73669\n",
      "[9]\ttrain-mlogloss:2.14339\ttrain-auc:0.65211\ttrain-merror:0.73237\ttest-mlogloss:2.14973\ttest-auc:0.63692\ttest-merror:0.73659\n",
      "[10]\ttrain-mlogloss:2.13575\ttrain-auc:0.65287\ttrain-merror:0.73201\ttest-mlogloss:2.14269\ttest-auc:0.63728\ttest-merror:0.73648\n",
      "[11]\ttrain-mlogloss:2.12880\ttrain-auc:0.65366\ttrain-merror:0.73159\ttest-mlogloss:2.13633\ttest-auc:0.63769\ttest-merror:0.73617\n",
      "[12]\ttrain-mlogloss:2.12249\ttrain-auc:0.65439\ttrain-merror:0.73116\ttest-mlogloss:2.13058\ttest-auc:0.63807\ttest-merror:0.73585\n",
      "[13]\ttrain-mlogloss:2.11670\ttrain-auc:0.65505\ttrain-merror:0.73087\ttest-mlogloss:2.12535\ttest-auc:0.63841\ttest-merror:0.73546\n",
      "[14]\ttrain-mlogloss:2.11153\ttrain-auc:0.65574\ttrain-merror:0.73040\ttest-mlogloss:2.12074\ttest-auc:0.63873\ttest-merror:0.73534\n",
      "[15]\ttrain-mlogloss:2.10678\ttrain-auc:0.65632\ttrain-merror:0.73010\ttest-mlogloss:2.11655\ttest-auc:0.63898\ttest-merror:0.73511\n",
      "[16]\ttrain-mlogloss:2.10248\ttrain-auc:0.65696\ttrain-merror:0.72982\ttest-mlogloss:2.11282\ttest-auc:0.63926\ttest-merror:0.73478\n",
      "[17]\ttrain-mlogloss:2.09854\ttrain-auc:0.65755\ttrain-merror:0.72950\ttest-mlogloss:2.10947\ttest-auc:0.63945\ttest-merror:0.73457\n",
      "[18]\ttrain-mlogloss:2.09495\ttrain-auc:0.65812\ttrain-merror:0.72921\ttest-mlogloss:2.10648\ttest-auc:0.63965\ttest-merror:0.73429\n",
      "[19]\ttrain-mlogloss:2.09164\ttrain-auc:0.65867\ttrain-merror:0.72885\ttest-mlogloss:2.10371\ttest-auc:0.63993\ttest-merror:0.73413\n",
      "[20]\ttrain-mlogloss:2.08856\ttrain-auc:0.65921\ttrain-merror:0.72859\ttest-mlogloss:2.10120\ttest-auc:0.64013\ttest-merror:0.73395\n",
      "[21]\ttrain-mlogloss:2.08574\ttrain-auc:0.65975\ttrain-merror:0.72830\ttest-mlogloss:2.09891\ttest-auc:0.64036\ttest-merror:0.73380\n",
      "[22]\ttrain-mlogloss:2.08311\ttrain-auc:0.66028\ttrain-merror:0.72798\ttest-mlogloss:2.09680\ttest-auc:0.64056\ttest-merror:0.73368\n",
      "[23]\ttrain-mlogloss:2.08066\ttrain-auc:0.66078\ttrain-merror:0.72776\ttest-mlogloss:2.09491\ttest-auc:0.64074\ttest-merror:0.73343\n",
      "[24]\ttrain-mlogloss:2.07834\ttrain-auc:0.66131\ttrain-merror:0.72745\ttest-mlogloss:2.09312\ttest-auc:0.64094\ttest-merror:0.73338\n",
      "[25]\ttrain-mlogloss:2.07615\ttrain-auc:0.66181\ttrain-merror:0.72714\ttest-mlogloss:2.09149\ttest-auc:0.64113\ttest-merror:0.73327\n",
      "[26]\ttrain-mlogloss:2.07414\ttrain-auc:0.66233\ttrain-merror:0.72690\ttest-mlogloss:2.09002\ttest-auc:0.64130\ttest-merror:0.73304\n",
      "[27]\ttrain-mlogloss:2.07226\ttrain-auc:0.66284\ttrain-merror:0.72662\ttest-mlogloss:2.08868\ttest-auc:0.64148\ttest-merror:0.73299\n",
      "[28]\ttrain-mlogloss:2.07041\ttrain-auc:0.66341\ttrain-merror:0.72629\ttest-mlogloss:2.08738\ttest-auc:0.64168\ttest-merror:0.73290\n",
      "[29]\ttrain-mlogloss:2.06872\ttrain-auc:0.66390\ttrain-merror:0.72611\ttest-mlogloss:2.08622\ttest-auc:0.64185\ttest-merror:0.73287\n",
      "[30]\ttrain-mlogloss:2.06712\ttrain-auc:0.66436\ttrain-merror:0.72577\ttest-mlogloss:2.08511\ttest-auc:0.64202\ttest-merror:0.73262\n",
      "[31]\ttrain-mlogloss:2.06560\ttrain-auc:0.66495\ttrain-merror:0.72549\ttest-mlogloss:2.08415\ttest-auc:0.64220\ttest-merror:0.73243\n",
      "[32]\ttrain-mlogloss:2.06414\ttrain-auc:0.66543\ttrain-merror:0.72522\ttest-mlogloss:2.08321\ttest-auc:0.64239\ttest-merror:0.73230\n",
      "[33]\ttrain-mlogloss:2.06274\ttrain-auc:0.66592\ttrain-merror:0.72488\ttest-mlogloss:2.08236\ttest-auc:0.64254\ttest-merror:0.73211\n",
      "[34]\ttrain-mlogloss:2.06142\ttrain-auc:0.66638\ttrain-merror:0.72463\ttest-mlogloss:2.08155\ttest-auc:0.64271\ttest-merror:0.73181\n",
      "[35]\ttrain-mlogloss:2.06018\ttrain-auc:0.66682\ttrain-merror:0.72431\ttest-mlogloss:2.08082\ttest-auc:0.64285\ttest-merror:0.73175\n",
      "[36]\ttrain-mlogloss:2.05903\ttrain-auc:0.66727\ttrain-merror:0.72413\ttest-mlogloss:2.08018\ttest-auc:0.64297\ttest-merror:0.73158\n",
      "[37]\ttrain-mlogloss:2.05788\ttrain-auc:0.66774\ttrain-merror:0.72388\ttest-mlogloss:2.07955\ttest-auc:0.64309\ttest-merror:0.73151\n",
      "[38]\ttrain-mlogloss:2.05674\ttrain-auc:0.66823\ttrain-merror:0.72356\ttest-mlogloss:2.07891\ttest-auc:0.64327\ttest-merror:0.73143\n",
      "[39]\ttrain-mlogloss:2.05570\ttrain-auc:0.66865\ttrain-merror:0.72328\ttest-mlogloss:2.07839\ttest-auc:0.64336\ttest-merror:0.73132\n",
      "[40]\ttrain-mlogloss:2.05469\ttrain-auc:0.66913\ttrain-merror:0.72307\ttest-mlogloss:2.07788\ttest-auc:0.64351\ttest-merror:0.73132\n",
      "[41]\ttrain-mlogloss:2.05367\ttrain-auc:0.66961\ttrain-merror:0.72280\ttest-mlogloss:2.07736\ttest-auc:0.64367\ttest-merror:0.73118\n",
      "[42]\ttrain-mlogloss:2.05270\ttrain-auc:0.67007\ttrain-merror:0.72246\ttest-mlogloss:2.07691\ttest-auc:0.64379\ttest-merror:0.73108\n",
      "[43]\ttrain-mlogloss:2.05175\ttrain-auc:0.67056\ttrain-merror:0.72219\ttest-mlogloss:2.07646\ttest-auc:0.64392\ttest-merror:0.73110\n",
      "[44]\ttrain-mlogloss:2.05084\ttrain-auc:0.67102\ttrain-merror:0.72189\ttest-mlogloss:2.07606\ttest-auc:0.64404\ttest-merror:0.73101\n",
      "[45]\ttrain-mlogloss:2.04992\ttrain-auc:0.67149\ttrain-merror:0.72161\ttest-mlogloss:2.07565\ttest-auc:0.64415\ttest-merror:0.73089\n",
      "[46]\ttrain-mlogloss:2.04901\ttrain-auc:0.67197\ttrain-merror:0.72137\ttest-mlogloss:2.07528\ttest-auc:0.64426\ttest-merror:0.73086\n",
      "[47]\ttrain-mlogloss:2.04818\ttrain-auc:0.67238\ttrain-merror:0.72116\ttest-mlogloss:2.07496\ttest-auc:0.64434\ttest-merror:0.73074\n",
      "[48]\ttrain-mlogloss:2.04741\ttrain-auc:0.67277\ttrain-merror:0.72089\ttest-mlogloss:2.07465\ttest-auc:0.64445\ttest-merror:0.73065\n",
      "[49]\ttrain-mlogloss:2.04661\ttrain-auc:0.67322\ttrain-merror:0.72069\ttest-mlogloss:2.07435\ttest-auc:0.64453\ttest-merror:0.73060\n",
      "[50]\ttrain-mlogloss:2.04581\ttrain-auc:0.67367\ttrain-merror:0.72043\ttest-mlogloss:2.07400\ttest-auc:0.64466\ttest-merror:0.73041\n",
      "[51]\ttrain-mlogloss:2.04505\ttrain-auc:0.67409\ttrain-merror:0.72021\ttest-mlogloss:2.07376\ttest-auc:0.64472\ttest-merror:0.73027\n",
      "[52]\ttrain-mlogloss:2.04425\ttrain-auc:0.67456\ttrain-merror:0.71987\ttest-mlogloss:2.07351\ttest-auc:0.64479\ttest-merror:0.73013\n",
      "[53]\ttrain-mlogloss:2.04349\ttrain-auc:0.67504\ttrain-merror:0.71965\ttest-mlogloss:2.07326\ttest-auc:0.64488\ttest-merror:0.73007\n",
      "[54]\ttrain-mlogloss:2.04276\ttrain-auc:0.67546\ttrain-merror:0.71944\ttest-mlogloss:2.07305\ttest-auc:0.64493\ttest-merror:0.73002\n",
      "[55]\ttrain-mlogloss:2.04205\ttrain-auc:0.67587\ttrain-merror:0.71927\ttest-mlogloss:2.07280\ttest-auc:0.64504\ttest-merror:0.72991\n",
      "[56]\ttrain-mlogloss:2.04131\ttrain-auc:0.67631\ttrain-merror:0.71893\ttest-mlogloss:2.07256\ttest-auc:0.64511\ttest-merror:0.72992\n",
      "[57]\ttrain-mlogloss:2.04056\ttrain-auc:0.67676\ttrain-merror:0.71872\ttest-mlogloss:2.07231\ttest-auc:0.64522\ttest-merror:0.72984\n",
      "[58]\ttrain-mlogloss:2.03990\ttrain-auc:0.67717\ttrain-merror:0.71847\ttest-mlogloss:2.07210\ttest-auc:0.64530\ttest-merror:0.72981\n",
      "[59]\ttrain-mlogloss:2.03919\ttrain-auc:0.67758\ttrain-merror:0.71826\ttest-mlogloss:2.07190\ttest-auc:0.64537\ttest-merror:0.72976\n",
      "[60]\ttrain-mlogloss:2.03850\ttrain-auc:0.67800\ttrain-merror:0.71803\ttest-mlogloss:2.07172\ttest-auc:0.64544\ttest-merror:0.72978\n",
      "[61]\ttrain-mlogloss:2.03783\ttrain-auc:0.67841\ttrain-merror:0.71783\ttest-mlogloss:2.07152\ttest-auc:0.64551\ttest-merror:0.72966\n",
      "[62]\ttrain-mlogloss:2.03716\ttrain-auc:0.67883\ttrain-merror:0.71758\ttest-mlogloss:2.07136\ttest-auc:0.64556\ttest-merror:0.72967\n",
      "[63]\ttrain-mlogloss:2.03655\ttrain-auc:0.67923\ttrain-merror:0.71738\ttest-mlogloss:2.07120\ttest-auc:0.64563\ttest-merror:0.72970\n",
      "[64]\ttrain-mlogloss:2.03598\ttrain-auc:0.67960\ttrain-merror:0.71715\ttest-mlogloss:2.07108\ttest-auc:0.64566\ttest-merror:0.72962\n",
      "[65]\ttrain-mlogloss:2.03545\ttrain-auc:0.67991\ttrain-merror:0.71698\ttest-mlogloss:2.07096\ttest-auc:0.64570\ttest-merror:0.72954\n",
      "[66]\ttrain-mlogloss:2.03479\ttrain-auc:0.68032\ttrain-merror:0.71672\ttest-mlogloss:2.07079\ttest-auc:0.64579\ttest-merror:0.72941\n",
      "[67]\ttrain-mlogloss:2.03412\ttrain-auc:0.68075\ttrain-merror:0.71649\ttest-mlogloss:2.07066\ttest-auc:0.64583\ttest-merror:0.72938\n",
      "[68]\ttrain-mlogloss:2.03362\ttrain-auc:0.68107\ttrain-merror:0.71635\ttest-mlogloss:2.07056\ttest-auc:0.64587\ttest-merror:0.72937\n",
      "[69]\ttrain-mlogloss:2.03306\ttrain-auc:0.68141\ttrain-merror:0.71613\ttest-mlogloss:2.07043\ttest-auc:0.64592\ttest-merror:0.72932\n",
      "[70]\ttrain-mlogloss:2.03250\ttrain-auc:0.68176\ttrain-merror:0.71596\ttest-mlogloss:2.07031\ttest-auc:0.64597\ttest-merror:0.72923\n",
      "[71]\ttrain-mlogloss:2.03197\ttrain-auc:0.68212\ttrain-merror:0.71577\ttest-mlogloss:2.07023\ttest-auc:0.64599\ttest-merror:0.72918\n",
      "[72]\ttrain-mlogloss:2.03143\ttrain-auc:0.68247\ttrain-merror:0.71562\ttest-mlogloss:2.07013\ttest-auc:0.64602\ttest-merror:0.72918\n",
      "[73]\ttrain-mlogloss:2.03083\ttrain-auc:0.68285\ttrain-merror:0.71541\ttest-mlogloss:2.06999\ttest-auc:0.64608\ttest-merror:0.72911\n",
      "[74]\ttrain-mlogloss:2.03026\ttrain-auc:0.68320\ttrain-merror:0.71524\ttest-mlogloss:2.06985\ttest-auc:0.64615\ttest-merror:0.72904\n",
      "[75]\ttrain-mlogloss:2.02974\ttrain-auc:0.68354\ttrain-merror:0.71503\ttest-mlogloss:2.06975\ttest-auc:0.64618\ttest-merror:0.72896\n",
      "[76]\ttrain-mlogloss:2.02923\ttrain-auc:0.68385\ttrain-merror:0.71479\ttest-mlogloss:2.06969\ttest-auc:0.64619\ttest-merror:0.72892\n",
      "[77]\ttrain-mlogloss:2.02871\ttrain-auc:0.68420\ttrain-merror:0.71461\ttest-mlogloss:2.06958\ttest-auc:0.64625\ttest-merror:0.72890\n",
      "[78]\ttrain-mlogloss:2.02825\ttrain-auc:0.68449\ttrain-merror:0.71447\ttest-mlogloss:2.06950\ttest-auc:0.64628\ttest-merror:0.72888\n",
      "[79]\ttrain-mlogloss:2.02776\ttrain-auc:0.68479\ttrain-merror:0.71429\ttest-mlogloss:2.06940\ttest-auc:0.64633\ttest-merror:0.72884\n",
      "[80]\ttrain-mlogloss:2.02722\ttrain-auc:0.68516\ttrain-merror:0.71408\ttest-mlogloss:2.06931\ttest-auc:0.64636\ttest-merror:0.72873\n",
      "[81]\ttrain-mlogloss:2.02667\ttrain-auc:0.68551\ttrain-merror:0.71386\ttest-mlogloss:2.06919\ttest-auc:0.64642\ttest-merror:0.72857\n",
      "[82]\ttrain-mlogloss:2.02615\ttrain-auc:0.68585\ttrain-merror:0.71362\ttest-mlogloss:2.06912\ttest-auc:0.64646\ttest-merror:0.72855\n",
      "[83]\ttrain-mlogloss:2.02565\ttrain-auc:0.68617\ttrain-merror:0.71344\ttest-mlogloss:2.06900\ttest-auc:0.64651\ttest-merror:0.72847\n",
      "[84]\ttrain-mlogloss:2.02515\ttrain-auc:0.68647\ttrain-merror:0.71322\ttest-mlogloss:2.06893\ttest-auc:0.64654\ttest-merror:0.72842\n",
      "[85]\ttrain-mlogloss:2.02468\ttrain-auc:0.68678\ttrain-merror:0.71306\ttest-mlogloss:2.06887\ttest-auc:0.64655\ttest-merror:0.72836\n",
      "[86]\ttrain-mlogloss:2.02428\ttrain-auc:0.68704\ttrain-merror:0.71292\ttest-mlogloss:2.06879\ttest-auc:0.64659\ttest-merror:0.72831\n",
      "[87]\ttrain-mlogloss:2.02382\ttrain-auc:0.68733\ttrain-merror:0.71266\ttest-mlogloss:2.06872\ttest-auc:0.64662\ttest-merror:0.72833\n",
      "[88]\ttrain-mlogloss:2.02335\ttrain-auc:0.68762\ttrain-merror:0.71250\ttest-mlogloss:2.06866\ttest-auc:0.64666\ttest-merror:0.72828\n",
      "[89]\ttrain-mlogloss:2.02287\ttrain-auc:0.68793\ttrain-merror:0.71231\ttest-mlogloss:2.06857\ttest-auc:0.64670\ttest-merror:0.72823\n",
      "[90]\ttrain-mlogloss:2.02240\ttrain-auc:0.68824\ttrain-merror:0.71214\ttest-mlogloss:2.06851\ttest-auc:0.64672\ttest-merror:0.72841\n",
      "[91]\ttrain-mlogloss:2.02192\ttrain-auc:0.68854\ttrain-merror:0.71191\ttest-mlogloss:2.06845\ttest-auc:0.64675\ttest-merror:0.72839\n",
      "[92]\ttrain-mlogloss:2.02152\ttrain-auc:0.68878\ttrain-merror:0.71175\ttest-mlogloss:2.06840\ttest-auc:0.64677\ttest-merror:0.72841\n",
      "[93]\ttrain-mlogloss:2.02108\ttrain-auc:0.68906\ttrain-merror:0.71159\ttest-mlogloss:2.06834\ttest-auc:0.64679\ttest-merror:0.72833\n",
      "[94]\ttrain-mlogloss:2.02057\ttrain-auc:0.68941\ttrain-merror:0.71138\ttest-mlogloss:2.06827\ttest-auc:0.64682\ttest-merror:0.72838\n",
      "[95]\ttrain-mlogloss:2.02008\ttrain-auc:0.68975\ttrain-merror:0.71120\ttest-mlogloss:2.06823\ttest-auc:0.64684\ttest-merror:0.72830\n",
      "[96]\ttrain-mlogloss:2.01964\ttrain-auc:0.69003\ttrain-merror:0.71099\ttest-mlogloss:2.06817\ttest-auc:0.64687\ttest-merror:0.72830\n",
      "[97]\ttrain-mlogloss:2.01918\ttrain-auc:0.69033\ttrain-merror:0.71080\ttest-mlogloss:2.06809\ttest-auc:0.64691\ttest-merror:0.72827\n",
      "[98]\ttrain-mlogloss:2.01877\ttrain-auc:0.69059\ttrain-merror:0.71063\ttest-mlogloss:2.06803\ttest-auc:0.64693\ttest-merror:0.72823\n",
      "[99]\ttrain-mlogloss:2.01834\ttrain-auc:0.69088\ttrain-merror:0.71043\ttest-mlogloss:2.06798\ttest-auc:0.64694\ttest-merror:0.72822\n",
      "[100]\ttrain-mlogloss:2.01788\ttrain-auc:0.69116\ttrain-merror:0.71025\ttest-mlogloss:2.06792\ttest-auc:0.64697\ttest-merror:0.72813\n",
      "[101]\ttrain-mlogloss:2.01747\ttrain-auc:0.69144\ttrain-merror:0.71007\ttest-mlogloss:2.06786\ttest-auc:0.64701\ttest-merror:0.72809\n",
      "[102]\ttrain-mlogloss:2.01707\ttrain-auc:0.69168\ttrain-merror:0.70987\ttest-mlogloss:2.06778\ttest-auc:0.64706\ttest-merror:0.72795\n",
      "[103]\ttrain-mlogloss:2.01665\ttrain-auc:0.69194\ttrain-merror:0.70971\ttest-mlogloss:2.06773\ttest-auc:0.64708\ttest-merror:0.72801\n",
      "[104]\ttrain-mlogloss:2.01623\ttrain-auc:0.69221\ttrain-merror:0.70956\ttest-mlogloss:2.06770\ttest-auc:0.64709\ttest-merror:0.72793\n",
      "[105]\ttrain-mlogloss:2.01580\ttrain-auc:0.69247\ttrain-merror:0.70936\ttest-mlogloss:2.06764\ttest-auc:0.64711\ttest-merror:0.72782\n",
      "[106]\ttrain-mlogloss:2.01532\ttrain-auc:0.69278\ttrain-merror:0.70924\ttest-mlogloss:2.06759\ttest-auc:0.64712\ttest-merror:0.72783\n",
      "[107]\ttrain-mlogloss:2.01488\ttrain-auc:0.69306\ttrain-merror:0.70908\ttest-mlogloss:2.06754\ttest-auc:0.64715\ttest-merror:0.72785\n",
      "[108]\ttrain-mlogloss:2.01448\ttrain-auc:0.69333\ttrain-merror:0.70889\ttest-mlogloss:2.06751\ttest-auc:0.64715\ttest-merror:0.72784\n",
      "[109]\ttrain-mlogloss:2.01404\ttrain-auc:0.69362\ttrain-merror:0.70873\ttest-mlogloss:2.06747\ttest-auc:0.64717\ttest-merror:0.72781\n",
      "[110]\ttrain-mlogloss:2.01363\ttrain-auc:0.69389\ttrain-merror:0.70854\ttest-mlogloss:2.06742\ttest-auc:0.64719\ttest-merror:0.72783\n",
      "[111]\ttrain-mlogloss:2.01324\ttrain-auc:0.69414\ttrain-merror:0.70842\ttest-mlogloss:2.06739\ttest-auc:0.64721\ttest-merror:0.72779\n",
      "[112]\ttrain-mlogloss:2.01282\ttrain-auc:0.69440\ttrain-merror:0.70822\ttest-mlogloss:2.06733\ttest-auc:0.64724\ttest-merror:0.72769\n",
      "[113]\ttrain-mlogloss:2.01242\ttrain-auc:0.69465\ttrain-merror:0.70804\ttest-mlogloss:2.06730\ttest-auc:0.64724\ttest-merror:0.72772\n",
      "[114]\ttrain-mlogloss:2.01201\ttrain-auc:0.69493\ttrain-merror:0.70792\ttest-mlogloss:2.06726\ttest-auc:0.64727\ttest-merror:0.72767\n",
      "[115]\ttrain-mlogloss:2.01157\ttrain-auc:0.69520\ttrain-merror:0.70773\ttest-mlogloss:2.06721\ttest-auc:0.64729\ttest-merror:0.72768\n",
      "[116]\ttrain-mlogloss:2.01123\ttrain-auc:0.69542\ttrain-merror:0.70759\ttest-mlogloss:2.06719\ttest-auc:0.64730\ttest-merror:0.72774\n",
      "[117]\ttrain-mlogloss:2.01081\ttrain-auc:0.69568\ttrain-merror:0.70736\ttest-mlogloss:2.06713\ttest-auc:0.64733\ttest-merror:0.72766\n",
      "[118]\ttrain-mlogloss:2.01043\ttrain-auc:0.69592\ttrain-merror:0.70723\ttest-mlogloss:2.06710\ttest-auc:0.64734\ttest-merror:0.72769\n",
      "[119]\ttrain-mlogloss:2.00998\ttrain-auc:0.69621\ttrain-merror:0.70701\ttest-mlogloss:2.06704\ttest-auc:0.64737\ttest-merror:0.72763\n",
      "[120]\ttrain-mlogloss:2.00955\ttrain-auc:0.69649\ttrain-merror:0.70683\ttest-mlogloss:2.06698\ttest-auc:0.64741\ttest-merror:0.72764\n",
      "[121]\ttrain-mlogloss:2.00913\ttrain-auc:0.69676\ttrain-merror:0.70669\ttest-mlogloss:2.06694\ttest-auc:0.64743\ttest-merror:0.72768\n",
      "[122]\ttrain-mlogloss:2.00874\ttrain-auc:0.69701\ttrain-merror:0.70653\ttest-mlogloss:2.06692\ttest-auc:0.64743\ttest-merror:0.72771\n",
      "[123]\ttrain-mlogloss:2.00835\ttrain-auc:0.69727\ttrain-merror:0.70639\ttest-mlogloss:2.06687\ttest-auc:0.64746\ttest-merror:0.72772\n",
      "[124]\ttrain-mlogloss:2.00799\ttrain-auc:0.69752\ttrain-merror:0.70622\ttest-mlogloss:2.06685\ttest-auc:0.64747\ttest-merror:0.72762\n",
      "[125]\ttrain-mlogloss:2.00765\ttrain-auc:0.69772\ttrain-merror:0.70607\ttest-mlogloss:2.06682\ttest-auc:0.64748\ttest-merror:0.72760\n",
      "[126]\ttrain-mlogloss:2.00728\ttrain-auc:0.69796\ttrain-merror:0.70596\ttest-mlogloss:2.06681\ttest-auc:0.64748\ttest-merror:0.72765\n",
      "[127]\ttrain-mlogloss:2.00687\ttrain-auc:0.69822\ttrain-merror:0.70581\ttest-mlogloss:2.06676\ttest-auc:0.64751\ttest-merror:0.72767\n",
      "[128]\ttrain-mlogloss:2.00654\ttrain-auc:0.69844\ttrain-merror:0.70565\ttest-mlogloss:2.06674\ttest-auc:0.64752\ttest-merror:0.72761\n",
      "[129]\ttrain-mlogloss:2.00616\ttrain-auc:0.69869\ttrain-merror:0.70551\ttest-mlogloss:2.06670\ttest-auc:0.64753\ttest-merror:0.72752\n",
      "[130]\ttrain-mlogloss:2.00578\ttrain-auc:0.69893\ttrain-merror:0.70536\ttest-mlogloss:2.06668\ttest-auc:0.64754\ttest-merror:0.72755\n",
      "[131]\ttrain-mlogloss:2.00537\ttrain-auc:0.69917\ttrain-merror:0.70522\ttest-mlogloss:2.06664\ttest-auc:0.64756\ttest-merror:0.72747\n",
      "[132]\ttrain-mlogloss:2.00506\ttrain-auc:0.69936\ttrain-merror:0.70510\ttest-mlogloss:2.06661\ttest-auc:0.64758\ttest-merror:0.72742\n",
      "[133]\ttrain-mlogloss:2.00473\ttrain-auc:0.69957\ttrain-merror:0.70500\ttest-mlogloss:2.06658\ttest-auc:0.64759\ttest-merror:0.72738\n",
      "[134]\ttrain-mlogloss:2.00432\ttrain-auc:0.69983\ttrain-merror:0.70488\ttest-mlogloss:2.06655\ttest-auc:0.64760\ttest-merror:0.72736\n",
      "[135]\ttrain-mlogloss:2.00395\ttrain-auc:0.70007\ttrain-merror:0.70469\ttest-mlogloss:2.06651\ttest-auc:0.64762\ttest-merror:0.72737\n",
      "[136]\ttrain-mlogloss:2.00360\ttrain-auc:0.70032\ttrain-merror:0.70458\ttest-mlogloss:2.06649\ttest-auc:0.64764\ttest-merror:0.72739\n",
      "[137]\ttrain-mlogloss:2.00328\ttrain-auc:0.70051\ttrain-merror:0.70448\ttest-mlogloss:2.06647\ttest-auc:0.64765\ttest-merror:0.72738\n",
      "[138]\ttrain-mlogloss:2.00290\ttrain-auc:0.70075\ttrain-merror:0.70434\ttest-mlogloss:2.06644\ttest-auc:0.64766\ttest-merror:0.72731\n",
      "[139]\ttrain-mlogloss:2.00254\ttrain-auc:0.70098\ttrain-merror:0.70422\ttest-mlogloss:2.06643\ttest-auc:0.64766\ttest-merror:0.72730\n",
      "[140]\ttrain-mlogloss:2.00218\ttrain-auc:0.70120\ttrain-merror:0.70400\ttest-mlogloss:2.06640\ttest-auc:0.64768\ttest-merror:0.72738\n",
      "[141]\ttrain-mlogloss:2.00177\ttrain-auc:0.70147\ttrain-merror:0.70382\ttest-mlogloss:2.06637\ttest-auc:0.64769\ttest-merror:0.72734\n",
      "[142]\ttrain-mlogloss:2.00142\ttrain-auc:0.70170\ttrain-merror:0.70367\ttest-mlogloss:2.06635\ttest-auc:0.64770\ttest-merror:0.72727\n",
      "[143]\ttrain-mlogloss:2.00106\ttrain-auc:0.70194\ttrain-merror:0.70354\ttest-mlogloss:2.06632\ttest-auc:0.64771\ttest-merror:0.72729\n",
      "[144]\ttrain-mlogloss:2.00071\ttrain-auc:0.70217\ttrain-merror:0.70334\ttest-mlogloss:2.06630\ttest-auc:0.64772\ttest-merror:0.72726\n",
      "[145]\ttrain-mlogloss:2.00035\ttrain-auc:0.70239\ttrain-merror:0.70318\ttest-mlogloss:2.06629\ttest-auc:0.64773\ttest-merror:0.72728\n",
      "[146]\ttrain-mlogloss:1.99998\ttrain-auc:0.70262\ttrain-merror:0.70299\ttest-mlogloss:2.06626\ttest-auc:0.64773\ttest-merror:0.72721\n",
      "[147]\ttrain-mlogloss:1.99963\ttrain-auc:0.70284\ttrain-merror:0.70281\ttest-mlogloss:2.06622\ttest-auc:0.64775\ttest-merror:0.72720\n",
      "[148]\ttrain-mlogloss:1.99926\ttrain-auc:0.70306\ttrain-merror:0.70267\ttest-mlogloss:2.06618\ttest-auc:0.64777\ttest-merror:0.72713\n",
      "[149]\ttrain-mlogloss:1.99902\ttrain-auc:0.70322\ttrain-merror:0.70256\ttest-mlogloss:2.06616\ttest-auc:0.64778\ttest-merror:0.72710\n",
      "[150]\ttrain-mlogloss:1.99866\ttrain-auc:0.70343\ttrain-merror:0.70244\ttest-mlogloss:2.06613\ttest-auc:0.64780\ttest-merror:0.72714\n",
      "[151]\ttrain-mlogloss:1.99837\ttrain-auc:0.70362\ttrain-merror:0.70228\ttest-mlogloss:2.06611\ttest-auc:0.64781\ttest-merror:0.72714\n",
      "[152]\ttrain-mlogloss:1.99805\ttrain-auc:0.70383\ttrain-merror:0.70220\ttest-mlogloss:2.06609\ttest-auc:0.64782\ttest-merror:0.72717\n",
      "[153]\ttrain-mlogloss:1.99772\ttrain-auc:0.70402\ttrain-merror:0.70207\ttest-mlogloss:2.06607\ttest-auc:0.64783\ttest-merror:0.72718\n",
      "[154]\ttrain-mlogloss:1.99743\ttrain-auc:0.70420\ttrain-merror:0.70192\ttest-mlogloss:2.06605\ttest-auc:0.64785\ttest-merror:0.72713\n",
      "[155]\ttrain-mlogloss:1.99708\ttrain-auc:0.70443\ttrain-merror:0.70177\ttest-mlogloss:2.06604\ttest-auc:0.64786\ttest-merror:0.72714\n",
      "[156]\ttrain-mlogloss:1.99680\ttrain-auc:0.70460\ttrain-merror:0.70162\ttest-mlogloss:2.06602\ttest-auc:0.64786\ttest-merror:0.72713\n",
      "[157]\ttrain-mlogloss:1.99653\ttrain-auc:0.70476\ttrain-merror:0.70152\ttest-mlogloss:2.06600\ttest-auc:0.64787\ttest-merror:0.72709\n",
      "[158]\ttrain-mlogloss:1.99616\ttrain-auc:0.70501\ttrain-merror:0.70137\ttest-mlogloss:2.06599\ttest-auc:0.64788\ttest-merror:0.72708\n",
      "[159]\ttrain-mlogloss:1.99575\ttrain-auc:0.70528\ttrain-merror:0.70122\ttest-mlogloss:2.06598\ttest-auc:0.64789\ttest-merror:0.72704\n",
      "[160]\ttrain-mlogloss:1.99539\ttrain-auc:0.70550\ttrain-merror:0.70109\ttest-mlogloss:2.06596\ttest-auc:0.64790\ttest-merror:0.72709\n",
      "[161]\ttrain-mlogloss:1.99504\ttrain-auc:0.70572\ttrain-merror:0.70094\ttest-mlogloss:2.06594\ttest-auc:0.64790\ttest-merror:0.72710\n",
      "[162]\ttrain-mlogloss:1.99472\ttrain-auc:0.70591\ttrain-merror:0.70081\ttest-mlogloss:2.06592\ttest-auc:0.64791\ttest-merror:0.72718\n",
      "[163]\ttrain-mlogloss:1.99440\ttrain-auc:0.70610\ttrain-merror:0.70071\ttest-mlogloss:2.06590\ttest-auc:0.64792\ttest-merror:0.72718\n",
      "[164]\ttrain-mlogloss:1.99406\ttrain-auc:0.70632\ttrain-merror:0.70059\ttest-mlogloss:2.06588\ttest-auc:0.64794\ttest-merror:0.72715\n",
      "[165]\ttrain-mlogloss:1.99369\ttrain-auc:0.70654\ttrain-merror:0.70044\ttest-mlogloss:2.06588\ttest-auc:0.64794\ttest-merror:0.72716\n",
      "[166]\ttrain-mlogloss:1.99338\ttrain-auc:0.70673\ttrain-merror:0.70029\ttest-mlogloss:2.06586\ttest-auc:0.64795\ttest-merror:0.72713\n",
      "[167]\ttrain-mlogloss:1.99302\ttrain-auc:0.70696\ttrain-merror:0.70015\ttest-mlogloss:2.06584\ttest-auc:0.64795\ttest-merror:0.72703\n",
      "[168]\ttrain-mlogloss:1.99269\ttrain-auc:0.70717\ttrain-merror:0.70000\ttest-mlogloss:2.06584\ttest-auc:0.64794\ttest-merror:0.72703\n",
      "[169]\ttrain-mlogloss:1.99235\ttrain-auc:0.70739\ttrain-merror:0.69987\ttest-mlogloss:2.06585\ttest-auc:0.64793\ttest-merror:0.72710\n",
      "[170]\ttrain-mlogloss:1.99200\ttrain-auc:0.70761\ttrain-merror:0.69973\ttest-mlogloss:2.06583\ttest-auc:0.64794\ttest-merror:0.72707\n",
      "[171]\ttrain-mlogloss:1.99168\ttrain-auc:0.70781\ttrain-merror:0.69961\ttest-mlogloss:2.06582\ttest-auc:0.64795\ttest-merror:0.72705\n",
      "[172]\ttrain-mlogloss:1.99136\ttrain-auc:0.70801\ttrain-merror:0.69951\ttest-mlogloss:2.06581\ttest-auc:0.64796\ttest-merror:0.72706\n",
      "[173]\ttrain-mlogloss:1.99107\ttrain-auc:0.70821\ttrain-merror:0.69938\ttest-mlogloss:2.06579\ttest-auc:0.64797\ttest-merror:0.72709\n",
      "[174]\ttrain-mlogloss:1.99073\ttrain-auc:0.70843\ttrain-merror:0.69922\ttest-mlogloss:2.06577\ttest-auc:0.64799\ttest-merror:0.72706\n",
      "[175]\ttrain-mlogloss:1.99044\ttrain-auc:0.70860\ttrain-merror:0.69910\ttest-mlogloss:2.06576\ttest-auc:0.64799\ttest-merror:0.72707\n",
      "[176]\ttrain-mlogloss:1.99017\ttrain-auc:0.70875\ttrain-merror:0.69900\ttest-mlogloss:2.06575\ttest-auc:0.64800\ttest-merror:0.72711\n",
      "[177]\ttrain-mlogloss:1.98981\ttrain-auc:0.70898\ttrain-merror:0.69882\ttest-mlogloss:2.06574\ttest-auc:0.64800\ttest-merror:0.72710\n",
      "[178]\ttrain-mlogloss:1.98946\ttrain-auc:0.70919\ttrain-merror:0.69864\ttest-mlogloss:2.06573\ttest-auc:0.64800\ttest-merror:0.72709\n",
      "[179]\ttrain-mlogloss:1.98921\ttrain-auc:0.70934\ttrain-merror:0.69852\ttest-mlogloss:2.06573\ttest-auc:0.64799\ttest-merror:0.72708\n",
      "[180]\ttrain-mlogloss:1.98891\ttrain-auc:0.70953\ttrain-merror:0.69842\ttest-mlogloss:2.06571\ttest-auc:0.64799\ttest-merror:0.72706\n",
      "[181]\ttrain-mlogloss:1.98860\ttrain-auc:0.70972\ttrain-merror:0.69823\ttest-mlogloss:2.06571\ttest-auc:0.64799\ttest-merror:0.72710\n",
      "[182]\ttrain-mlogloss:1.98829\ttrain-auc:0.70991\ttrain-merror:0.69814\ttest-mlogloss:2.06569\ttest-auc:0.64800\ttest-merror:0.72703\n",
      "[183]\ttrain-mlogloss:1.98799\ttrain-auc:0.71008\ttrain-merror:0.69802\ttest-mlogloss:2.06569\ttest-auc:0.64800\ttest-merror:0.72700\n",
      "[184]\ttrain-mlogloss:1.98766\ttrain-auc:0.71027\ttrain-merror:0.69786\ttest-mlogloss:2.06566\ttest-auc:0.64801\ttest-merror:0.72701\n",
      "[185]\ttrain-mlogloss:1.98738\ttrain-auc:0.71045\ttrain-merror:0.69774\ttest-mlogloss:2.06566\ttest-auc:0.64801\ttest-merror:0.72700\n",
      "[186]\ttrain-mlogloss:1.98709\ttrain-auc:0.71065\ttrain-merror:0.69766\ttest-mlogloss:2.06566\ttest-auc:0.64801\ttest-merror:0.72701\n",
      "[187]\ttrain-mlogloss:1.98671\ttrain-auc:0.71089\ttrain-merror:0.69751\ttest-mlogloss:2.06566\ttest-auc:0.64801\ttest-merror:0.72697\n",
      "[188]\ttrain-mlogloss:1.98638\ttrain-auc:0.71110\ttrain-merror:0.69734\ttest-mlogloss:2.06565\ttest-auc:0.64801\ttest-merror:0.72696\n",
      "[189]\ttrain-mlogloss:1.98606\ttrain-auc:0.71129\ttrain-merror:0.69723\ttest-mlogloss:2.06566\ttest-auc:0.64800\ttest-merror:0.72696\n",
      "[190]\ttrain-mlogloss:1.98577\ttrain-auc:0.71146\ttrain-merror:0.69707\ttest-mlogloss:2.06565\ttest-auc:0.64801\ttest-merror:0.72695\n",
      "[191]\ttrain-mlogloss:1.98548\ttrain-auc:0.71162\ttrain-merror:0.69693\ttest-mlogloss:2.06564\ttest-auc:0.64801\ttest-merror:0.72691\n",
      "[192]\ttrain-mlogloss:1.98523\ttrain-auc:0.71177\ttrain-merror:0.69682\ttest-mlogloss:2.06563\ttest-auc:0.64802\ttest-merror:0.72689\n",
      "[193]\ttrain-mlogloss:1.98494\ttrain-auc:0.71194\ttrain-merror:0.69672\ttest-mlogloss:2.06562\ttest-auc:0.64802\ttest-merror:0.72695\n",
      "[194]\ttrain-mlogloss:1.98462\ttrain-auc:0.71214\ttrain-merror:0.69662\ttest-mlogloss:2.06562\ttest-auc:0.64802\ttest-merror:0.72692\n",
      "[195]\ttrain-mlogloss:1.98428\ttrain-auc:0.71234\ttrain-merror:0.69646\ttest-mlogloss:2.06560\ttest-auc:0.64804\ttest-merror:0.72694\n",
      "[196]\ttrain-mlogloss:1.98397\ttrain-auc:0.71252\ttrain-merror:0.69634\ttest-mlogloss:2.06559\ttest-auc:0.64804\ttest-merror:0.72695\n",
      "[197]\ttrain-mlogloss:1.98371\ttrain-auc:0.71267\ttrain-merror:0.69624\ttest-mlogloss:2.06558\ttest-auc:0.64804\ttest-merror:0.72696\n",
      "[198]\ttrain-mlogloss:1.98339\ttrain-auc:0.71288\ttrain-merror:0.69607\ttest-mlogloss:2.06557\ttest-auc:0.64804\ttest-merror:0.72687\n",
      "[199]\ttrain-mlogloss:1.98305\ttrain-auc:0.71307\ttrain-merror:0.69598\ttest-mlogloss:2.06557\ttest-auc:0.64804\ttest-merror:0.72683\n",
      "[200]\ttrain-mlogloss:1.98273\ttrain-auc:0.71325\ttrain-merror:0.69581\ttest-mlogloss:2.06554\ttest-auc:0.64805\ttest-merror:0.72689\n",
      "[201]\ttrain-mlogloss:1.98239\ttrain-auc:0.71347\ttrain-merror:0.69571\ttest-mlogloss:2.06554\ttest-auc:0.64805\ttest-merror:0.72691\n",
      "[202]\ttrain-mlogloss:1.98212\ttrain-auc:0.71363\ttrain-merror:0.69557\ttest-mlogloss:2.06553\ttest-auc:0.64806\ttest-merror:0.72692\n",
      "[203]\ttrain-mlogloss:1.98180\ttrain-auc:0.71382\ttrain-merror:0.69540\ttest-mlogloss:2.06553\ttest-auc:0.64805\ttest-merror:0.72685\n",
      "[204]\ttrain-mlogloss:1.98154\ttrain-auc:0.71398\ttrain-merror:0.69530\ttest-mlogloss:2.06552\ttest-auc:0.64805\ttest-merror:0.72685\n",
      "[205]\ttrain-mlogloss:1.98121\ttrain-auc:0.71419\ttrain-merror:0.69517\ttest-mlogloss:2.06551\ttest-auc:0.64806\ttest-merror:0.72676\n",
      "[206]\ttrain-mlogloss:1.98093\ttrain-auc:0.71435\ttrain-merror:0.69505\ttest-mlogloss:2.06549\ttest-auc:0.64807\ttest-merror:0.72675\n",
      "[207]\ttrain-mlogloss:1.98063\ttrain-auc:0.71452\ttrain-merror:0.69491\ttest-mlogloss:2.06549\ttest-auc:0.64806\ttest-merror:0.72679\n",
      "[208]\ttrain-mlogloss:1.98030\ttrain-auc:0.71472\ttrain-merror:0.69483\ttest-mlogloss:2.06548\ttest-auc:0.64807\ttest-merror:0.72668\n",
      "[209]\ttrain-mlogloss:1.98002\ttrain-auc:0.71489\ttrain-merror:0.69469\ttest-mlogloss:2.06548\ttest-auc:0.64807\ttest-merror:0.72667\n",
      "[210]\ttrain-mlogloss:1.97975\ttrain-auc:0.71506\ttrain-merror:0.69456\ttest-mlogloss:2.06547\ttest-auc:0.64808\ttest-merror:0.72670\n",
      "[211]\ttrain-mlogloss:1.97948\ttrain-auc:0.71522\ttrain-merror:0.69444\ttest-mlogloss:2.06548\ttest-auc:0.64807\ttest-merror:0.72670\n",
      "[212]\ttrain-mlogloss:1.97916\ttrain-auc:0.71539\ttrain-merror:0.69432\ttest-mlogloss:2.06547\ttest-auc:0.64807\ttest-merror:0.72676\n",
      "[213]\ttrain-mlogloss:1.97887\ttrain-auc:0.71556\ttrain-merror:0.69418\ttest-mlogloss:2.06547\ttest-auc:0.64807\ttest-merror:0.72677\n",
      "[214]\ttrain-mlogloss:1.97855\ttrain-auc:0.71576\ttrain-merror:0.69400\ttest-mlogloss:2.06547\ttest-auc:0.64806\ttest-merror:0.72676\n",
      "[215]\ttrain-mlogloss:1.97826\ttrain-auc:0.71592\ttrain-merror:0.69391\ttest-mlogloss:2.06546\ttest-auc:0.64807\ttest-merror:0.72674\n",
      "[216]\ttrain-mlogloss:1.97796\ttrain-auc:0.71610\ttrain-merror:0.69379\ttest-mlogloss:2.06545\ttest-auc:0.64808\ttest-merror:0.72670\n",
      "[217]\ttrain-mlogloss:1.97765\ttrain-auc:0.71627\ttrain-merror:0.69365\ttest-mlogloss:2.06545\ttest-auc:0.64808\ttest-merror:0.72672\n",
      "[218]\ttrain-mlogloss:1.97734\ttrain-auc:0.71645\ttrain-merror:0.69352\ttest-mlogloss:2.06545\ttest-auc:0.64808\ttest-merror:0.72673\n",
      "[219]\ttrain-mlogloss:1.97697\ttrain-auc:0.71669\ttrain-merror:0.69335\ttest-mlogloss:2.06545\ttest-auc:0.64807\ttest-merror:0.72667\n",
      "[220]\ttrain-mlogloss:1.97661\ttrain-auc:0.71691\ttrain-merror:0.69315\ttest-mlogloss:2.06544\ttest-auc:0.64807\ttest-merror:0.72667\n",
      "[221]\ttrain-mlogloss:1.97631\ttrain-auc:0.71709\ttrain-merror:0.69305\ttest-mlogloss:2.06543\ttest-auc:0.64808\ttest-merror:0.72669\n",
      "[222]\ttrain-mlogloss:1.97596\ttrain-auc:0.71730\ttrain-merror:0.69288\ttest-mlogloss:2.06541\ttest-auc:0.64810\ttest-merror:0.72662\n",
      "[223]\ttrain-mlogloss:1.97569\ttrain-auc:0.71745\ttrain-merror:0.69278\ttest-mlogloss:2.06541\ttest-auc:0.64809\ttest-merror:0.72661\n",
      "[224]\ttrain-mlogloss:1.97536\ttrain-auc:0.71764\ttrain-merror:0.69259\ttest-mlogloss:2.06541\ttest-auc:0.64810\ttest-merror:0.72660\n",
      "[225]\ttrain-mlogloss:1.97507\ttrain-auc:0.71781\ttrain-merror:0.69251\ttest-mlogloss:2.06541\ttest-auc:0.64810\ttest-merror:0.72659\n",
      "[226]\ttrain-mlogloss:1.97476\ttrain-auc:0.71798\ttrain-merror:0.69234\ttest-mlogloss:2.06540\ttest-auc:0.64811\ttest-merror:0.72658\n",
      "[227]\ttrain-mlogloss:1.97443\ttrain-auc:0.71818\ttrain-merror:0.69217\ttest-mlogloss:2.06540\ttest-auc:0.64811\ttest-merror:0.72647\n",
      "[228]\ttrain-mlogloss:1.97422\ttrain-auc:0.71830\ttrain-merror:0.69206\ttest-mlogloss:2.06540\ttest-auc:0.64810\ttest-merror:0.72649\n",
      "[229]\ttrain-mlogloss:1.97399\ttrain-auc:0.71843\ttrain-merror:0.69196\ttest-mlogloss:2.06540\ttest-auc:0.64811\ttest-merror:0.72649\n",
      "[230]\ttrain-mlogloss:1.97368\ttrain-auc:0.71863\ttrain-merror:0.69183\ttest-mlogloss:2.06540\ttest-auc:0.64811\ttest-merror:0.72652\n",
      "[231]\ttrain-mlogloss:1.97339\ttrain-auc:0.71878\ttrain-merror:0.69173\ttest-mlogloss:2.06539\ttest-auc:0.64811\ttest-merror:0.72659\n",
      "[232]\ttrain-mlogloss:1.97308\ttrain-auc:0.71896\ttrain-merror:0.69157\ttest-mlogloss:2.06537\ttest-auc:0.64813\ttest-merror:0.72660\n",
      "[233]\ttrain-mlogloss:1.97277\ttrain-auc:0.71915\ttrain-merror:0.69143\ttest-mlogloss:2.06537\ttest-auc:0.64813\ttest-merror:0.72664\n",
      "[234]\ttrain-mlogloss:1.97249\ttrain-auc:0.71931\ttrain-merror:0.69130\ttest-mlogloss:2.06537\ttest-auc:0.64813\ttest-merror:0.72665\n",
      "[235]\ttrain-mlogloss:1.97223\ttrain-auc:0.71946\ttrain-merror:0.69118\ttest-mlogloss:2.06537\ttest-auc:0.64813\ttest-merror:0.72668\n",
      "[236]\ttrain-mlogloss:1.97194\ttrain-auc:0.71963\ttrain-merror:0.69107\ttest-mlogloss:2.06537\ttest-auc:0.64813\ttest-merror:0.72664\n",
      "[237]\ttrain-mlogloss:1.97163\ttrain-auc:0.71983\ttrain-merror:0.69095\ttest-mlogloss:2.06538\ttest-auc:0.64813\ttest-merror:0.72669\n",
      "[238]\ttrain-mlogloss:1.97130\ttrain-auc:0.72001\ttrain-merror:0.69082\ttest-mlogloss:2.06537\ttest-auc:0.64814\ttest-merror:0.72670\n",
      "[239]\ttrain-mlogloss:1.97100\ttrain-auc:0.72017\ttrain-merror:0.69072\ttest-mlogloss:2.06536\ttest-auc:0.64815\ttest-merror:0.72667\n",
      "[240]\ttrain-mlogloss:1.97071\ttrain-auc:0.72034\ttrain-merror:0.69059\ttest-mlogloss:2.06536\ttest-auc:0.64815\ttest-merror:0.72672\n",
      "[241]\ttrain-mlogloss:1.97037\ttrain-auc:0.72054\ttrain-merror:0.69043\ttest-mlogloss:2.06536\ttest-auc:0.64815\ttest-merror:0.72674\n",
      "[242]\ttrain-mlogloss:1.97005\ttrain-auc:0.72072\ttrain-merror:0.69030\ttest-mlogloss:2.06536\ttest-auc:0.64815\ttest-merror:0.72667\n",
      "[243]\ttrain-mlogloss:1.96978\ttrain-auc:0.72087\ttrain-merror:0.69016\ttest-mlogloss:2.06537\ttest-auc:0.64814\ttest-merror:0.72664\n",
      "[244]\ttrain-mlogloss:1.96946\ttrain-auc:0.72106\ttrain-merror:0.69005\ttest-mlogloss:2.06536\ttest-auc:0.64815\ttest-merror:0.72660\n",
      "[245]\ttrain-mlogloss:1.96920\ttrain-auc:0.72120\ttrain-merror:0.68991\ttest-mlogloss:2.06535\ttest-auc:0.64815\ttest-merror:0.72663\n",
      "[246]\ttrain-mlogloss:1.96887\ttrain-auc:0.72139\ttrain-merror:0.68980\ttest-mlogloss:2.06533\ttest-auc:0.64816\ttest-merror:0.72659\n",
      "[247]\ttrain-mlogloss:1.96862\ttrain-auc:0.72154\ttrain-merror:0.68967\ttest-mlogloss:2.06534\ttest-auc:0.64816\ttest-merror:0.72663\n",
      "[248]\ttrain-mlogloss:1.96827\ttrain-auc:0.72174\ttrain-merror:0.68946\ttest-mlogloss:2.06533\ttest-auc:0.64816\ttest-merror:0.72662\n",
      "[249]\ttrain-mlogloss:1.96798\ttrain-auc:0.72190\ttrain-merror:0.68935\ttest-mlogloss:2.06533\ttest-auc:0.64816\ttest-merror:0.72662\n",
      "250-rounds Training finished ...\t\t(488.307s)\n"
     ]
    }
   ],
   "source": [
    "num_round = 250\n",
    "t0 = time()\n",
    "eval_result = {}\n",
    "def decay_eta(nth):\n",
    "    etas = [.1, .05, .03, .01]\n",
    "    return etas[(nth // 60) % len(etas)]\n",
    "\n",
    "wl_bst_sm = xgb.train(param, xg_train, num_round, watchlist, evals_result=eval_result, early_stopping_rounds=30)\n",
    "#                       callbacks=[xgb.callback.LearningRateScheduler(decay_eta)])\n",
    "print(f\"{num_round}-rounds Training finished ...\\t\\t({time()-t0:.3f}s)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': OrderedDict([('mlogloss',\n",
       "               [2.273391,\n",
       "                2.249271,\n",
       "                2.228938,\n",
       "                2.211438,\n",
       "                2.196323,\n",
       "                2.183143,\n",
       "                2.171514,\n",
       "                2.16128,\n",
       "                2.15201,\n",
       "                2.14378,\n",
       "                2.136391,\n",
       "                2.129838,\n",
       "                2.12384,\n",
       "                2.118412,\n",
       "                2.113522,\n",
       "                2.109029,\n",
       "                2.105014,\n",
       "                2.101309,\n",
       "                2.0979,\n",
       "                2.094819,\n",
       "                2.092041,\n",
       "                2.089248,\n",
       "                2.086848,\n",
       "                2.084486,\n",
       "                2.082175,\n",
       "                2.080239,\n",
       "                2.078318,\n",
       "                2.076628,\n",
       "                2.074873,\n",
       "                2.073293,\n",
       "                2.071798,\n",
       "                2.070462,\n",
       "                2.069056,\n",
       "                2.067713,\n",
       "                2.066565,\n",
       "                2.065228,\n",
       "                2.064006,\n",
       "                2.062921,\n",
       "                2.061723,\n",
       "                2.060604,\n",
       "                2.059523,\n",
       "                2.058563,\n",
       "                2.057618,\n",
       "                2.056739,\n",
       "                2.055834,\n",
       "                2.055038,\n",
       "                2.054273,\n",
       "                2.053414,\n",
       "                2.052641,\n",
       "                2.051878,\n",
       "                2.051152,\n",
       "                2.050514,\n",
       "                2.049806,\n",
       "                2.049126,\n",
       "                2.04851,\n",
       "                2.047696,\n",
       "                2.047043,\n",
       "                2.046361,\n",
       "                2.045739,\n",
       "                2.045126,\n",
       "                2.044484,\n",
       "                2.043822,\n",
       "                2.043189,\n",
       "                2.042589,\n",
       "                2.041953,\n",
       "                2.041371,\n",
       "                2.040852,\n",
       "                2.040214,\n",
       "                2.039633,\n",
       "                2.039045,\n",
       "                2.038459,\n",
       "                2.037893,\n",
       "                2.037293,\n",
       "                2.03677,\n",
       "                2.036205,\n",
       "                2.035654,\n",
       "                2.035119,\n",
       "                2.034608,\n",
       "                2.034042,\n",
       "                2.033438,\n",
       "                2.033002,\n",
       "                2.032466,\n",
       "                2.031891,\n",
       "                2.031407,\n",
       "                2.030922,\n",
       "                2.030501,\n",
       "                2.029937,\n",
       "                2.029474,\n",
       "                2.028983,\n",
       "                2.028555,\n",
       "                2.028012,\n",
       "                2.027574,\n",
       "                2.027126,\n",
       "                2.026658,\n",
       "                2.026212,\n",
       "                2.025816,\n",
       "                2.025496,\n",
       "                2.025096,\n",
       "                2.024753,\n",
       "                2.024403,\n",
       "                2.024139,\n",
       "                2.023821,\n",
       "                2.023507,\n",
       "                2.023234,\n",
       "                2.022821,\n",
       "                2.022518,\n",
       "                2.022208,\n",
       "                2.022001,\n",
       "                2.021776,\n",
       "                2.021562,\n",
       "                2.021295,\n",
       "                2.021012,\n",
       "                2.020731,\n",
       "                2.020529,\n",
       "                2.020328,\n",
       "                2.020141,\n",
       "                2.01988,\n",
       "                2.019671,\n",
       "                2.01945,\n",
       "                2.019143,\n",
       "                2.018958,\n",
       "                2.018705,\n",
       "                2.018505,\n",
       "                2.018282,\n",
       "                2.018093,\n",
       "                2.017933,\n",
       "                2.017817,\n",
       "                2.017704,\n",
       "                2.017591,\n",
       "                2.017471,\n",
       "                2.017397,\n",
       "                2.017253,\n",
       "                2.017146,\n",
       "                2.017033,\n",
       "                2.016887,\n",
       "                2.016766,\n",
       "                2.016657,\n",
       "                2.01646,\n",
       "                2.016353,\n",
       "                2.016211,\n",
       "                2.016126,\n",
       "                2.016019,\n",
       "                2.015876,\n",
       "                2.015805,\n",
       "                2.015669,\n",
       "                2.015573,\n",
       "                2.015421,\n",
       "                2.015299,\n",
       "                2.015163,\n",
       "                2.015013,\n",
       "                2.014934,\n",
       "                2.014853,\n",
       "                2.014726,\n",
       "                2.014677,\n",
       "                2.014621,\n",
       "                2.014551,\n",
       "                2.014474,\n",
       "                2.014271,\n",
       "                2.01423,\n",
       "                2.014152,\n",
       "                2.014093,\n",
       "                2.014045,\n",
       "                2.013975,\n",
       "                2.013928,\n",
       "                2.013889,\n",
       "                2.013806,\n",
       "                2.013786,\n",
       "                2.013756,\n",
       "                2.013702,\n",
       "                2.013679,\n",
       "                2.013618,\n",
       "                2.013508,\n",
       "                2.013389,\n",
       "                2.013267,\n",
       "                2.01323,\n",
       "                2.013192,\n",
       "                2.01314,\n",
       "                2.013098,\n",
       "                2.01299,\n",
       "                2.012877,\n",
       "                2.012784,\n",
       "                2.012739,\n",
       "                2.012705,\n",
       "                2.012635,\n",
       "                2.012612,\n",
       "                2.012561,\n",
       "                2.012506,\n",
       "                2.012436,\n",
       "                2.012393,\n",
       "                2.012362,\n",
       "                2.012309,\n",
       "                2.012259,\n",
       "                2.012213,\n",
       "                2.012111,\n",
       "                2.012066,\n",
       "                2.011981,\n",
       "                2.011952,\n",
       "                2.011911,\n",
       "                2.011884,\n",
       "                2.011832]),\n",
       "              ('auc',\n",
       "               [0.609166,\n",
       "                0.613574,\n",
       "                0.615673,\n",
       "                0.617684,\n",
       "                0.618899,\n",
       "                0.620377,\n",
       "                0.621478,\n",
       "                0.62236,\n",
       "                0.623596,\n",
       "                0.624718,\n",
       "                0.625693,\n",
       "                0.626546,\n",
       "                0.627477,\n",
       "                0.628391,\n",
       "                0.629221,\n",
       "                0.630098,\n",
       "                0.630813,\n",
       "                0.631575,\n",
       "                0.632343,\n",
       "                0.633113,\n",
       "                0.633773,\n",
       "                0.634675,\n",
       "                0.635358,\n",
       "                0.636146,\n",
       "                0.637049,\n",
       "                0.637714,\n",
       "                0.638476,\n",
       "                0.63913,\n",
       "                0.639907,\n",
       "                0.64057,\n",
       "                0.641302,\n",
       "                0.641907,\n",
       "                0.642584,\n",
       "                0.643294,\n",
       "                0.64389,\n",
       "                0.644677,\n",
       "                0.645397,\n",
       "                0.646051,\n",
       "                0.646797,\n",
       "                0.64751,\n",
       "                0.648158,\n",
       "                0.648724,\n",
       "                0.649365,\n",
       "                0.649896,\n",
       "                0.650452,\n",
       "                0.650985,\n",
       "                0.651553,\n",
       "                0.652133,\n",
       "                0.652763,\n",
       "                0.653332,\n",
       "                0.653814,\n",
       "                0.654313,\n",
       "                0.654877,\n",
       "                0.655356,\n",
       "                0.655828,\n",
       "                0.656415,\n",
       "                0.656954,\n",
       "                0.65745,\n",
       "                0.657979,\n",
       "                0.658472,\n",
       "                0.658947,\n",
       "                0.659518,\n",
       "                0.660011,\n",
       "                0.66048,\n",
       "                0.661035,\n",
       "                0.661515,\n",
       "                0.661939,\n",
       "                0.66243,\n",
       "                0.662903,\n",
       "                0.663358,\n",
       "                0.663827,\n",
       "                0.664305,\n",
       "                0.664799,\n",
       "                0.665215,\n",
       "                0.665669,\n",
       "                0.66609,\n",
       "                0.666529,\n",
       "                0.666907,\n",
       "                0.66736,\n",
       "                0.667831,\n",
       "                0.668144,\n",
       "                0.668582,\n",
       "                0.669065,\n",
       "                0.66944,\n",
       "                0.669814,\n",
       "                0.67014,\n",
       "                0.670602,\n",
       "                0.670932,\n",
       "                0.671322,\n",
       "                0.671626,\n",
       "                0.672037,\n",
       "                0.672368,\n",
       "                0.672707,\n",
       "                0.673066,\n",
       "                0.673372,\n",
       "                0.673655,\n",
       "                0.673886,\n",
       "                0.674167,\n",
       "                0.674412,\n",
       "                0.67468,\n",
       "                0.674849,\n",
       "                0.675083,\n",
       "                0.675328,\n",
       "                0.675503,\n",
       "                0.675813,\n",
       "                0.676021,\n",
       "                0.676225,\n",
       "                0.67635,\n",
       "                0.676503,\n",
       "                0.676644,\n",
       "                0.676819,\n",
       "                0.677004,\n",
       "                0.677191,\n",
       "                0.677305,\n",
       "                0.677427,\n",
       "                0.677553,\n",
       "                0.677731,\n",
       "                0.677853,\n",
       "                0.678005,\n",
       "                0.67821,\n",
       "                0.678332,\n",
       "                0.678514,\n",
       "                0.678642,\n",
       "                0.678797,\n",
       "                0.678916,\n",
       "                0.67901,\n",
       "                0.679068,\n",
       "                0.679116,\n",
       "                0.679174,\n",
       "                0.679233,\n",
       "                0.679268,\n",
       "                0.679347,\n",
       "                0.679393,\n",
       "                0.679453,\n",
       "                0.679532,\n",
       "                0.679592,\n",
       "                0.679641,\n",
       "                0.679756,\n",
       "                0.679805,\n",
       "                0.679884,\n",
       "                0.67992,\n",
       "                0.679976,\n",
       "                0.680058,\n",
       "                0.680091,\n",
       "                0.680166,\n",
       "                0.680201,\n",
       "                0.680297,\n",
       "                0.680364,\n",
       "                0.680429,\n",
       "                0.680511,\n",
       "                0.680554,\n",
       "                0.680594,\n",
       "                0.68066,\n",
       "                0.680675,\n",
       "                0.680696,\n",
       "                0.680724,\n",
       "                0.680755,\n",
       "                0.680916,\n",
       "                0.68093,\n",
       "                0.680963,\n",
       "                0.680986,\n",
       "                0.681005,\n",
       "                0.681047,\n",
       "                0.681063,\n",
       "                0.681076,\n",
       "                0.681109,\n",
       "                0.681114,\n",
       "                0.681125,\n",
       "                0.681142,\n",
       "                0.681148,\n",
       "                0.681176,\n",
       "                0.681243,\n",
       "                0.681297,\n",
       "                0.68136,\n",
       "                0.681373,\n",
       "                0.681389,\n",
       "                0.681404,\n",
       "                0.681419,\n",
       "                0.681469,\n",
       "                0.681542,\n",
       "                0.681581,\n",
       "                0.681599,\n",
       "                0.681611,\n",
       "                0.681642,\n",
       "                0.681648,\n",
       "                0.681666,\n",
       "                0.681681,\n",
       "                0.681711,\n",
       "                0.681724,\n",
       "                0.681731,\n",
       "                0.681748,\n",
       "                0.681768,\n",
       "                0.681781,\n",
       "                0.681825,\n",
       "                0.681841,\n",
       "                0.681887,\n",
       "                0.681898,\n",
       "                0.681912,\n",
       "                0.681922,\n",
       "                0.681937]),\n",
       "              ('merror',\n",
       "               [0.736567,\n",
       "                0.734123,\n",
       "                0.733287,\n",
       "                0.732624,\n",
       "                0.732093,\n",
       "                0.731596,\n",
       "                0.731194,\n",
       "                0.730924,\n",
       "                0.730526,\n",
       "                0.730136,\n",
       "                0.729627,\n",
       "                0.729457,\n",
       "                0.729307,\n",
       "                0.729097,\n",
       "                0.72865,\n",
       "                0.728462,\n",
       "                0.728281,\n",
       "                0.728087,\n",
       "                0.727865,\n",
       "                0.72766,\n",
       "                0.727516,\n",
       "                0.727328,\n",
       "                0.727033,\n",
       "                0.72682,\n",
       "                0.726528,\n",
       "                0.726331,\n",
       "                0.726146,\n",
       "                0.725933,\n",
       "                0.725733,\n",
       "                0.725498,\n",
       "                0.725218,\n",
       "                0.725064,\n",
       "                0.724871,\n",
       "                0.724663,\n",
       "                0.724455,\n",
       "                0.724101,\n",
       "                0.723866,\n",
       "                0.723614,\n",
       "                0.723313,\n",
       "                0.723139,\n",
       "                0.722898,\n",
       "                0.722674,\n",
       "                0.72244,\n",
       "                0.722268,\n",
       "                0.722085,\n",
       "                0.721878,\n",
       "                0.721729,\n",
       "                0.721451,\n",
       "                0.721261,\n",
       "                0.721111,\n",
       "                0.720938,\n",
       "                0.720817,\n",
       "                0.72056,\n",
       "                0.72037,\n",
       "                0.720195,\n",
       "                0.719901,\n",
       "                0.719704,\n",
       "                0.719465,\n",
       "                0.719327,\n",
       "                0.719086,\n",
       "                0.71886,\n",
       "                0.718676,\n",
       "                0.718485,\n",
       "                0.718345,\n",
       "                0.718081,\n",
       "                0.717925,\n",
       "                0.717798,\n",
       "                0.717584,\n",
       "                0.717402,\n",
       "                0.717194,\n",
       "                0.716954,\n",
       "                0.716775,\n",
       "                0.716569,\n",
       "                0.71638,\n",
       "                0.716216,\n",
       "                0.716011,\n",
       "                0.715823,\n",
       "                0.715645,\n",
       "                0.715486,\n",
       "                0.715216,\n",
       "                0.71509,\n",
       "                0.714905,\n",
       "                0.714731,\n",
       "                0.714552,\n",
       "                0.714427,\n",
       "                0.714291,\n",
       "                0.714078,\n",
       "                0.71394,\n",
       "                0.71374,\n",
       "                0.713625,\n",
       "                0.713416,\n",
       "                0.71326,\n",
       "                0.713118,\n",
       "                0.71294,\n",
       "                0.712771,\n",
       "                0.712584,\n",
       "                0.712466,\n",
       "                0.712347,\n",
       "                0.712194,\n",
       "                0.712007,\n",
       "                0.711866,\n",
       "                0.711762,\n",
       "                0.711646,\n",
       "                0.711515,\n",
       "                0.711271,\n",
       "                0.71117,\n",
       "                0.711008,\n",
       "                0.710895,\n",
       "                0.710786,\n",
       "                0.710699,\n",
       "                0.710572,\n",
       "                0.710429,\n",
       "                0.710301,\n",
       "                0.710233,\n",
       "                0.710098,\n",
       "                0.709965,\n",
       "                0.709799,\n",
       "                0.709727,\n",
       "                0.709592,\n",
       "                0.709422,\n",
       "                0.709292,\n",
       "                0.709139,\n",
       "                0.708986,\n",
       "                0.708837,\n",
       "                0.708743,\n",
       "                0.70863,\n",
       "                0.708564,\n",
       "                0.708518,\n",
       "                0.708465,\n",
       "                0.708408,\n",
       "                0.708354,\n",
       "                0.708295,\n",
       "                0.708238,\n",
       "                0.708167,\n",
       "                0.708065,\n",
       "                0.708008,\n",
       "                0.70796,\n",
       "                0.707828,\n",
       "                0.707759,\n",
       "                0.707672,\n",
       "                0.707609,\n",
       "                0.707545,\n",
       "                0.707455,\n",
       "                0.707402,\n",
       "                0.707346,\n",
       "                0.707296,\n",
       "                0.707199,\n",
       "                0.707134,\n",
       "                0.707047,\n",
       "                0.706961,\n",
       "                0.706909,\n",
       "                0.706842,\n",
       "                0.706767,\n",
       "                0.706737,\n",
       "                0.706718,\n",
       "                0.706687,\n",
       "                0.706645,\n",
       "                0.70648,\n",
       "                0.706468,\n",
       "                0.706433,\n",
       "                0.706415,\n",
       "                0.706409,\n",
       "                0.706365,\n",
       "                0.706351,\n",
       "                0.706338,\n",
       "                0.706305,\n",
       "                0.706298,\n",
       "                0.706279,\n",
       "                0.706264,\n",
       "                0.706252,\n",
       "                0.706232,\n",
       "                0.706168,\n",
       "                0.706121,\n",
       "                0.706088,\n",
       "                0.70608,\n",
       "                0.706062,\n",
       "                0.706038,\n",
       "                0.70602,\n",
       "                0.705938,\n",
       "                0.705859,\n",
       "                0.705835,\n",
       "                0.705819,\n",
       "                0.705809,\n",
       "                0.705772,\n",
       "                0.705756,\n",
       "                0.705754,\n",
       "                0.705724,\n",
       "                0.705688,\n",
       "                0.705675,\n",
       "                0.705644,\n",
       "                0.705621,\n",
       "                0.705604,\n",
       "                0.705576,\n",
       "                0.705528,\n",
       "                0.705507,\n",
       "                0.705469,\n",
       "                0.705457,\n",
       "                0.705441,\n",
       "                0.705429,\n",
       "                0.705403])]),\n",
       " 'test': OrderedDict([('mlogloss',\n",
       "               [2.273966,\n",
       "                2.250428,\n",
       "                2.230677,\n",
       "                2.213754,\n",
       "                2.199225,\n",
       "                2.186585,\n",
       "                2.175527,\n",
       "                2.165833,\n",
       "                2.157122,\n",
       "                2.149473,\n",
       "                2.142651,\n",
       "                2.136662,\n",
       "                2.131266,\n",
       "                2.126448,\n",
       "                2.122146,\n",
       "                2.118271,\n",
       "                2.114859,\n",
       "                2.111758,\n",
       "                2.108978,\n",
       "                2.106493,\n",
       "                2.104311,\n",
       "                2.102141,\n",
       "                2.100348,\n",
       "                2.098594,\n",
       "                2.096948,\n",
       "                2.095634,\n",
       "                2.094353,\n",
       "                2.093259,\n",
       "                2.092155,\n",
       "                2.091199,\n",
       "                2.090338,\n",
       "                2.089609,\n",
       "                2.088794,\n",
       "                2.088084,\n",
       "                2.087566,\n",
       "                2.086879,\n",
       "                2.086292,\n",
       "                2.085867,\n",
       "                2.085336,\n",
       "                2.08488,\n",
       "                2.084435,\n",
       "                2.084051,\n",
       "                2.083765,\n",
       "                2.083475,\n",
       "                2.083183,\n",
       "                2.083005,\n",
       "                2.08284,\n",
       "                2.082611,\n",
       "                2.082492,\n",
       "                2.082344,\n",
       "                2.082157,\n",
       "                2.08208,\n",
       "                2.082,\n",
       "                2.081881,\n",
       "                2.081804,\n",
       "                2.081594,\n",
       "                2.081518,\n",
       "                2.081397,\n",
       "                2.081355,\n",
       "                2.081302,\n",
       "                2.081173,\n",
       "                2.081138,\n",
       "                2.081072,\n",
       "                2.08103,\n",
       "                2.080996,\n",
       "                2.080967,\n",
       "                2.080953,\n",
       "                2.080883,\n",
       "                2.08084,\n",
       "                2.080801,\n",
       "                2.080752,\n",
       "                2.080732,\n",
       "                2.080695,\n",
       "                2.080673,\n",
       "                2.080658,\n",
       "                2.080602,\n",
       "                2.080593,\n",
       "                2.080565,\n",
       "                2.080539,\n",
       "                2.080512,\n",
       "                2.080488,\n",
       "                2.080473,\n",
       "                2.080469,\n",
       "                2.08046,\n",
       "                2.080437,\n",
       "                2.080427,\n",
       "                2.080432,\n",
       "                2.080418,\n",
       "                2.080394,\n",
       "                2.080377,\n",
       "                2.080375,\n",
       "                2.080362,\n",
       "                2.080354,\n",
       "                2.080343,\n",
       "                2.080349,\n",
       "                2.08034,\n",
       "                2.080347,\n",
       "                2.080342,\n",
       "                2.080329,\n",
       "                2.080311,\n",
       "                2.080294,\n",
       "                2.080276,\n",
       "                2.080289,\n",
       "                2.080271,\n",
       "                2.08027,\n",
       "                2.080249,\n",
       "                2.080216,\n",
       "                2.080196,\n",
       "                2.080184,\n",
       "                2.080182,\n",
       "                2.080157,\n",
       "                2.080133,\n",
       "                2.08011,\n",
       "                2.080096,\n",
       "                2.08008,\n",
       "                2.080082,\n",
       "                2.080074,\n",
       "                2.080047,\n",
       "                2.080029,\n",
       "                2.080013,\n",
       "                2.079996,\n",
       "                2.079986,\n",
       "                2.079977,\n",
       "                2.079978,\n",
       "                2.079966,\n",
       "                2.079957,\n",
       "                2.07994,\n",
       "                2.079918,\n",
       "                2.079911,\n",
       "                2.07991,\n",
       "                2.079897,\n",
       "                2.079887,\n",
       "                2.07987,\n",
       "                2.079848,\n",
       "                2.079819,\n",
       "                2.079782,\n",
       "                2.079779,\n",
       "                2.079745,\n",
       "                2.07973,\n",
       "                2.079714,\n",
       "                2.07969,\n",
       "                2.079686,\n",
       "                2.079694,\n",
       "                2.079683,\n",
       "                2.079683,\n",
       "                2.079665,\n",
       "                2.079645,\n",
       "                2.079633,\n",
       "                2.079602,\n",
       "                2.079573,\n",
       "                2.079575,\n",
       "                2.079566,\n",
       "                2.079545,\n",
       "                2.079539,\n",
       "                2.079537,\n",
       "                2.079527,\n",
       "                2.07952,\n",
       "                2.079513,\n",
       "                2.079507,\n",
       "                2.079505,\n",
       "                2.0795,\n",
       "                2.079493,\n",
       "                2.079496,\n",
       "                2.079484,\n",
       "                2.079475,\n",
       "                2.07947,\n",
       "                2.079468,\n",
       "                2.079465,\n",
       "                2.079451,\n",
       "                2.07945,\n",
       "                2.079444,\n",
       "                2.079444,\n",
       "                2.079428,\n",
       "                2.079413,\n",
       "                2.079404,\n",
       "                2.079399,\n",
       "                2.079379,\n",
       "                2.079372,\n",
       "                2.079349,\n",
       "                2.079342,\n",
       "                2.079335,\n",
       "                2.079337,\n",
       "                2.079336,\n",
       "                2.079337,\n",
       "                2.079331,\n",
       "                2.079327,\n",
       "                2.079313,\n",
       "                2.079311,\n",
       "                2.079301,\n",
       "                2.079287,\n",
       "                2.079275,\n",
       "                2.079274,\n",
       "                2.079258,\n",
       "                2.079248,\n",
       "                2.079233,\n",
       "                2.07923,\n",
       "                2.07923,\n",
       "                2.079229,\n",
       "                2.079225,\n",
       "                2.079212]),\n",
       "              ('auc',\n",
       "               [0.598253,\n",
       "                0.601141,\n",
       "                0.602342,\n",
       "                0.603543,\n",
       "                0.604139,\n",
       "                0.60494,\n",
       "                0.605443,\n",
       "                0.605851,\n",
       "                0.606437,\n",
       "                0.606949,\n",
       "                0.607375,\n",
       "                0.607739,\n",
       "                0.608041,\n",
       "                0.608347,\n",
       "                0.60867,\n",
       "                0.608922,\n",
       "                0.609089,\n",
       "                0.60928,\n",
       "                0.609503,\n",
       "                0.609693,\n",
       "                0.60982,\n",
       "                0.61015,\n",
       "                0.610302,\n",
       "                0.610542,\n",
       "                0.610849,\n",
       "                0.61098,\n",
       "                0.611182,\n",
       "                0.611318,\n",
       "                0.611519,\n",
       "                0.61164,\n",
       "                0.611776,\n",
       "                0.611853,\n",
       "                0.612054,\n",
       "                0.61219,\n",
       "                0.612242,\n",
       "                0.612456,\n",
       "                0.61262,\n",
       "                0.612678,\n",
       "                0.612842,\n",
       "                0.612986,\n",
       "                0.613119,\n",
       "                0.61323,\n",
       "                0.613288,\n",
       "                0.613347,\n",
       "                0.613418,\n",
       "                0.613423,\n",
       "                0.613461,\n",
       "                0.613496,\n",
       "                0.61352,\n",
       "                0.61354,\n",
       "                0.6136,\n",
       "                0.613591,\n",
       "                0.613589,\n",
       "                0.613613,\n",
       "                0.613626,\n",
       "                0.61372,\n",
       "                0.613748,\n",
       "                0.613798,\n",
       "                0.613788,\n",
       "                0.6138,\n",
       "                0.613858,\n",
       "                0.613856,\n",
       "                0.613858,\n",
       "                0.613861,\n",
       "                0.613865,\n",
       "                0.613865,\n",
       "                0.613868,\n",
       "                0.613893,\n",
       "                0.613916,\n",
       "                0.613913,\n",
       "                0.613942,\n",
       "                0.613944,\n",
       "                0.613963,\n",
       "                0.613981,\n",
       "                0.61398,\n",
       "                0.614018,\n",
       "                0.614012,\n",
       "                0.61403,\n",
       "                0.614051,\n",
       "                0.614065,\n",
       "                0.614085,\n",
       "                0.614102,\n",
       "                0.614087,\n",
       "                0.614089,\n",
       "                0.614105,\n",
       "                0.614113,\n",
       "                0.6141,\n",
       "                0.61411,\n",
       "                0.614128,\n",
       "                0.614128,\n",
       "                0.614123,\n",
       "                0.614124,\n",
       "                0.614132,\n",
       "                0.61414,\n",
       "                0.614134,\n",
       "                0.614134,\n",
       "                0.614128,\n",
       "                0.614134,\n",
       "                0.614138,\n",
       "                0.614145,\n",
       "                0.614153,\n",
       "                0.614162,\n",
       "                0.61415,\n",
       "                0.614162,\n",
       "                0.614155,\n",
       "                0.614178,\n",
       "                0.614195,\n",
       "                0.614212,\n",
       "                0.614221,\n",
       "                0.614221,\n",
       "                0.614237,\n",
       "                0.614244,\n",
       "                0.61426,\n",
       "                0.61427,\n",
       "                0.614282,\n",
       "                0.614284,\n",
       "                0.61429,\n",
       "                0.614312,\n",
       "                0.614321,\n",
       "                0.614324,\n",
       "                0.614341,\n",
       "                0.614347,\n",
       "                0.614351,\n",
       "                0.61435,\n",
       "                0.614353,\n",
       "                0.614354,\n",
       "                0.614365,\n",
       "                0.61438,\n",
       "                0.614384,\n",
       "                0.614381,\n",
       "                0.614389,\n",
       "                0.614395,\n",
       "                0.614405,\n",
       "                0.61442,\n",
       "                0.614442,\n",
       "                0.61446,\n",
       "                0.614462,\n",
       "                0.614475,\n",
       "                0.614485,\n",
       "                0.614492,\n",
       "                0.614502,\n",
       "                0.614504,\n",
       "                0.614497,\n",
       "                0.614501,\n",
       "                0.614498,\n",
       "                0.614504,\n",
       "                0.614515,\n",
       "                0.614524,\n",
       "                0.614536,\n",
       "                0.614551,\n",
       "                0.61455,\n",
       "                0.614554,\n",
       "                0.614567,\n",
       "                0.614571,\n",
       "                0.614571,\n",
       "                0.614577,\n",
       "                0.61458,\n",
       "                0.614577,\n",
       "                0.61458,\n",
       "                0.61458,\n",
       "                0.614584,\n",
       "                0.614586,\n",
       "                0.614584,\n",
       "                0.61459,\n",
       "                0.614594,\n",
       "                0.614598,\n",
       "                0.6146,\n",
       "                0.614601,\n",
       "                0.614609,\n",
       "                0.61461,\n",
       "                0.614611,\n",
       "                0.614604,\n",
       "                0.614612,\n",
       "                0.614622,\n",
       "                0.614628,\n",
       "                0.61463,\n",
       "                0.61464,\n",
       "                0.614643,\n",
       "                0.614659,\n",
       "                0.614665,\n",
       "                0.614668,\n",
       "                0.614668,\n",
       "                0.614669,\n",
       "                0.614669,\n",
       "                0.614672,\n",
       "                0.614676,\n",
       "                0.614683,\n",
       "                0.614684,\n",
       "                0.614688,\n",
       "                0.614696,\n",
       "                0.614702,\n",
       "                0.614704,\n",
       "                0.614712,\n",
       "                0.614716,\n",
       "                0.614726,\n",
       "                0.614729,\n",
       "                0.61473,\n",
       "                0.61473,\n",
       "                0.614731,\n",
       "                0.614737]),\n",
       "              ('merror',\n",
       "               [0.73782,\n",
       "                0.736012,\n",
       "                0.734923,\n",
       "                0.734415,\n",
       "                0.734394,\n",
       "                0.733806,\n",
       "                0.733643,\n",
       "                0.733631,\n",
       "                0.73342,\n",
       "                0.732988,\n",
       "                0.732547,\n",
       "                0.732556,\n",
       "                0.732391,\n",
       "                0.732311,\n",
       "                0.732046,\n",
       "                0.73203,\n",
       "                0.731683,\n",
       "                0.731566,\n",
       "                0.731479,\n",
       "                0.731341,\n",
       "                0.73133,\n",
       "                0.731284,\n",
       "                0.731188,\n",
       "                0.731063,\n",
       "                0.730976,\n",
       "                0.730907,\n",
       "                0.730913,\n",
       "                0.730882,\n",
       "                0.730976,\n",
       "                0.730836,\n",
       "                0.730779,\n",
       "                0.730742,\n",
       "                0.730683,\n",
       "                0.730523,\n",
       "                0.730479,\n",
       "                0.730354,\n",
       "                0.730351,\n",
       "                0.730258,\n",
       "                0.730009,\n",
       "                0.729901,\n",
       "                0.729949,\n",
       "                0.729951,\n",
       "                0.729876,\n",
       "                0.729736,\n",
       "                0.729702,\n",
       "                0.72969,\n",
       "                0.729656,\n",
       "                0.729569,\n",
       "                0.729602,\n",
       "                0.729469,\n",
       "                0.7294,\n",
       "                0.729448,\n",
       "                0.729412,\n",
       "                0.729385,\n",
       "                0.729333,\n",
       "                0.729258,\n",
       "                0.729235,\n",
       "                0.729224,\n",
       "                0.729268,\n",
       "                0.729203,\n",
       "                0.729218,\n",
       "                0.72917,\n",
       "                0.729254,\n",
       "                0.729312,\n",
       "                0.729208,\n",
       "                0.729199,\n",
       "                0.729228,\n",
       "                0.729258,\n",
       "                0.72926,\n",
       "                0.72926,\n",
       "                0.729176,\n",
       "                0.729158,\n",
       "                0.729145,\n",
       "                0.729137,\n",
       "                0.729116,\n",
       "                0.729064,\n",
       "                0.729074,\n",
       "                0.728995,\n",
       "                0.728963,\n",
       "                0.728934,\n",
       "                0.728982,\n",
       "                0.728997,\n",
       "                0.729076,\n",
       "                0.729028,\n",
       "                0.729053,\n",
       "                0.729068,\n",
       "                0.729001,\n",
       "                0.729036,\n",
       "                0.728955,\n",
       "                0.72892,\n",
       "                0.728886,\n",
       "                0.72892,\n",
       "                0.728799,\n",
       "                0.728817,\n",
       "                0.728861,\n",
       "                0.728884,\n",
       "                0.728847,\n",
       "                0.728849,\n",
       "                0.728865,\n",
       "                0.728867,\n",
       "                0.728805,\n",
       "                0.728799,\n",
       "                0.728834,\n",
       "                0.72879,\n",
       "                0.728803,\n",
       "                0.728744,\n",
       "                0.728688,\n",
       "                0.728663,\n",
       "                0.728609,\n",
       "                0.72864,\n",
       "                0.728581,\n",
       "                0.728598,\n",
       "                0.728586,\n",
       "                0.72855,\n",
       "                0.728525,\n",
       "                0.728571,\n",
       "                0.728538,\n",
       "                0.728569,\n",
       "                0.728561,\n",
       "                0.728542,\n",
       "                0.728563,\n",
       "                0.728604,\n",
       "                0.728617,\n",
       "                0.728682,\n",
       "                0.728667,\n",
       "                0.728623,\n",
       "                0.728665,\n",
       "                0.728632,\n",
       "                0.728638,\n",
       "                0.728625,\n",
       "                0.728669,\n",
       "                0.728565,\n",
       "                0.728506,\n",
       "                0.728554,\n",
       "                0.728504,\n",
       "                0.728462,\n",
       "                0.728475,\n",
       "                0.72846,\n",
       "                0.72841,\n",
       "                0.728364,\n",
       "                0.728392,\n",
       "                0.728348,\n",
       "                0.728398,\n",
       "                0.728406,\n",
       "                0.7284,\n",
       "                0.728389,\n",
       "                0.728379,\n",
       "                0.728398,\n",
       "                0.728362,\n",
       "                0.728312,\n",
       "                0.728314,\n",
       "                0.728258,\n",
       "                0.72827,\n",
       "                0.72825,\n",
       "                0.728233,\n",
       "                0.728225,\n",
       "                0.728222,\n",
       "                0.728225,\n",
       "                0.728212,\n",
       "                0.728254,\n",
       "                0.728247,\n",
       "                0.728237,\n",
       "                0.72825,\n",
       "                0.728252,\n",
       "                0.728258,\n",
       "                0.728306,\n",
       "                0.728293,\n",
       "                0.728279,\n",
       "                0.728268,\n",
       "                0.728266,\n",
       "                0.728277,\n",
       "                0.728266,\n",
       "                0.728235,\n",
       "                0.72826,\n",
       "                0.728245,\n",
       "                0.728235,\n",
       "                0.72822,\n",
       "                0.728212,\n",
       "                0.728179,\n",
       "                0.728131,\n",
       "                0.728166,\n",
       "                0.728193,\n",
       "                0.728218,\n",
       "                0.728222,\n",
       "                0.728206,\n",
       "                0.728197,\n",
       "                0.728195,\n",
       "                0.728195,\n",
       "                0.728195,\n",
       "                0.728183,\n",
       "                0.728183,\n",
       "                0.728197,\n",
       "                0.728202,\n",
       "                0.728214,\n",
       "                0.728229,\n",
       "                0.728227,\n",
       "                0.728225,\n",
       "                0.728239,\n",
       "                0.728229,\n",
       "                0.728218])])}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test error using softmax = 0.7266146179941242\n"
     ]
    }
   ],
   "source": [
    "# get prediction\n",
    "pred = wl_bst_sm.predict(xg_test)\n",
    "# pred = pred.astype(np.uint8)\n",
    "error_rate = np.sum(pred != y_test) / y_test.shape[0]\n",
    "print('Test error using softmax = {}'.format(error_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[0]\\teval-mlogloss:2.065333\\teval-auc:0.648158\\teval-merror:0.726615'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_s = wl_bst_sm.eval(xg_test)\n",
    "# eval_dict = eval_str_2_dict(eval_s)\n",
    "eval_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.66663234, 0.59726837, 0.5154675 , 0.50434376, 0.50359383,\n",
       "        0.50423387, 0.50620546, 0.50287012, 0.5098358 , 0.63711319]),\n",
       " 2.4046808020465744)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = np.arange(0, 1, 0.1)\n",
    "aucs = auc(y_test.astype(np.uint8), pred.astype(np.uint8), np.arange(param['num_class']))\n",
    "# aucs[aucs == 0.5] = 0\n",
    "w_auc = (aucs * weights).sum()\n",
    "aucs, w_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "report = metrics.classification_report(list(y_test), list(pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.36      0.51      0.42     69934\n",
      "           1       0.26      0.43      0.32     69934\n",
      "           2       0.18      0.10      0.13     56539\n",
      "           3       0.16      0.02      0.03     39454\n",
      "           4       0.17      0.01      0.02     31033\n",
      "           5       0.19      0.01      0.02     25740\n",
      "           6       0.24      0.02      0.03     22516\n",
      "           7       0.18      0.01      0.01     21195\n",
      "           8       0.26      0.02      0.04     24984\n",
      "           9       0.25      0.63      0.36     69934\n",
      "\n",
      "    accuracy                           0.27    431263\n",
      "   macro avg       0.23      0.18      0.14    431263\n",
      "weighted avg       0.24      0.27      0.21    431263\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "del X_train, y_train, X_test, y_test, xg_train, xg_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 调参"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn import svm, datasets\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from collections.abc import Iterable\n",
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "base_param = {  # 基本参数，不需要调参\n",
    "    'objective': 'multi:softmax',\n",
    "    'eta': 0.1,\n",
    "    'nthread': 8,\n",
    "    'num_class': 10,\n",
    "    'gpu_id': 0,\n",
    "    'tree_method': 'gpu_hist',\n",
    "    'eval_metric': ['mlogloss', 'auc', 'merror']\n",
    "} \n",
    "# 需要调参的参数\n",
    "ps1 = {  \n",
    "    'max_depth': list(range(5, 14, 2)),\n",
    "    'min_child_weight': list(range(1, 10, 2)),\n",
    "}\n",
    "\n",
    "ps2 = {\n",
    "    'gamma': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6]\n",
    "}\n",
    "\n",
    "ps3 = {\n",
    "    'subsample':  [i/10.0 for i in range(6,11,1)], \n",
    "    'colsample_bytree':  [i/10.0 for i in range(6,11,1)] \n",
    "}\n",
    "\n",
    "ps4 = {'reg_alpha': [0, 0.1, 0.2, 0.5, 1, 1.5, 2, 4]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 529,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data preparing finished ...\t\t(5.092s)\n"
     ]
    }
   ],
   "source": [
    "t0 = time()\n",
    "xg_train = xgb.DMatrix(X_train.values, label=y_train.values, enable_categorical=True)\n",
    "xg_test = xgb.DMatrix(X_test.values, label=y_test.values, enable_categorical=True)\n",
    "print(f\"Data preparing finished ...\\t\\t({time()-t0:.3f}s)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def myproduct(*iterables):\n",
    "    n = len(iterables)\n",
    "#     print(iterables)\n",
    "    if n == 0:\n",
    "        return None \n",
    "    \n",
    "    ret = []\n",
    "    ret.extend([[e] for e in iterables[0].copy()])\n",
    "    if n == 1:\n",
    "        return ret\n",
    "\n",
    "    # 将需要调参的参数进行组合，即笛卡尔乘积。类似于sklearn中的 ParameterGrid\n",
    "    for k in range(1, n):\n",
    "        v = iterables[k].copy()\n",
    "        l = len(ret)\n",
    "        ret = [ret[i%l].copy() for i in range(len(v) * len(ret))]\n",
    "        for i, e in enumerate(ret):\n",
    "            e.append(v[i // l])\n",
    "    return ret\n",
    "\n",
    "def compose_param_grid(grid, base):\n",
    "    items = list(grid.items())\n",
    "    iterables = [item[1] for item in items]\n",
    "    keys = [item[0] for item in items]\n",
    "\n",
    "    ret = myproduct(*iterables)\n",
    "    com_ps = [dict(zip(keys, e)) for e in ret]\n",
    "\n",
    "\n",
    "    all_params = [base.copy() for _ in range(len(com_ps))] \n",
    "    for i in range(len(com_ps)):\n",
    "        all_params[i].update(com_ps[i])\n",
    "        \n",
    "    return all_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始搜索：colsample_bytree=[0.6, 0.7, 0.8, 0.9, 1.0], subsample=[0.6, 0.7, 0.8, 0.9, 1.0]\n",
      "待搜索的参数组合数量：25\n",
      "1 / 25: 200-rounds Training finished param={'objective': 'multi:softmax', 'eta': 0.1, 'nthread': 8, 'num_class': 10, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['mlogloss', 'auc', 'merror'], 'max_depth': 9, 'min_child_weight': 9, 'gamma': 0.2, 'subsample': 0.6, 'colsample_bytree': 0.6} ...\t\t(820.946s)\n",
      "2 / 25: 200-rounds Training finished param={'objective': 'multi:softmax', 'eta': 0.1, 'nthread': 8, 'num_class': 10, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['mlogloss', 'auc', 'merror'], 'max_depth': 9, 'min_child_weight': 9, 'gamma': 0.2, 'subsample': 0.7, 'colsample_bytree': 0.6} ...\t\t(816.416s)\n",
      "3 / 25: 200-rounds Training finished param={'objective': 'multi:softmax', 'eta': 0.1, 'nthread': 8, 'num_class': 10, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['mlogloss', 'auc', 'merror'], 'max_depth': 9, 'min_child_weight': 9, 'gamma': 0.2, 'subsample': 0.8, 'colsample_bytree': 0.6} ...\t\t(828.957s)\n",
      "4 / 25: 200-rounds Training finished param={'objective': 'multi:softmax', 'eta': 0.1, 'nthread': 8, 'num_class': 10, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['mlogloss', 'auc', 'merror'], 'max_depth': 9, 'min_child_weight': 9, 'gamma': 0.2, 'subsample': 0.9, 'colsample_bytree': 0.6} ...\t\t(838.505s)\n",
      "5 / 25: 200-rounds Training finished param={'objective': 'multi:softmax', 'eta': 0.1, 'nthread': 8, 'num_class': 10, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['mlogloss', 'auc', 'merror'], 'max_depth': 9, 'min_child_weight': 9, 'gamma': 0.2, 'subsample': 1.0, 'colsample_bytree': 0.6} ...\t\t(823.276s)\n",
      "6 / 25: 200-rounds Training finished param={'objective': 'multi:softmax', 'eta': 0.1, 'nthread': 8, 'num_class': 10, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['mlogloss', 'auc', 'merror'], 'max_depth': 9, 'min_child_weight': 9, 'gamma': 0.2, 'subsample': 0.6, 'colsample_bytree': 0.7} ...\t\t(828.692s)\n",
      "7 / 25: 200-rounds Training finished param={'objective': 'multi:softmax', 'eta': 0.1, 'nthread': 8, 'num_class': 10, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['mlogloss', 'auc', 'merror'], 'max_depth': 9, 'min_child_weight': 9, 'gamma': 0.2, 'subsample': 0.7, 'colsample_bytree': 0.7} ...\t\t(848.865s)\n",
      "8 / 25: 200-rounds Training finished param={'objective': 'multi:softmax', 'eta': 0.1, 'nthread': 8, 'num_class': 10, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['mlogloss', 'auc', 'merror'], 'max_depth': 9, 'min_child_weight': 9, 'gamma': 0.2, 'subsample': 0.8, 'colsample_bytree': 0.7} ...\t\t(854.009s)\n",
      "9 / 25: 200-rounds Training finished param={'objective': 'multi:softmax', 'eta': 0.1, 'nthread': 8, 'num_class': 10, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['mlogloss', 'auc', 'merror'], 'max_depth': 9, 'min_child_weight': 9, 'gamma': 0.2, 'subsample': 0.9, 'colsample_bytree': 0.7} ...\t\t(863.061s)\n",
      "10 / 25: 200-rounds Training finished param={'objective': 'multi:softmax', 'eta': 0.1, 'nthread': 8, 'num_class': 10, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['mlogloss', 'auc', 'merror'], 'max_depth': 9, 'min_child_weight': 9, 'gamma': 0.2, 'subsample': 1.0, 'colsample_bytree': 0.7} ...\t\t(854.728s)\n",
      "11 / 25: 200-rounds Training finished param={'objective': 'multi:softmax', 'eta': 0.1, 'nthread': 8, 'num_class': 10, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['mlogloss', 'auc', 'merror'], 'max_depth': 9, 'min_child_weight': 9, 'gamma': 0.2, 'subsample': 0.6, 'colsample_bytree': 0.8} ...\t\t(833.519s)\n",
      "12 / 25: 200-rounds Training finished param={'objective': 'multi:softmax', 'eta': 0.1, 'nthread': 8, 'num_class': 10, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['mlogloss', 'auc', 'merror'], 'max_depth': 9, 'min_child_weight': 9, 'gamma': 0.2, 'subsample': 0.7, 'colsample_bytree': 0.8} ...\t\t(836.241s)\n",
      "13 / 25: 200-rounds Training finished param={'objective': 'multi:softmax', 'eta': 0.1, 'nthread': 8, 'num_class': 10, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['mlogloss', 'auc', 'merror'], 'max_depth': 9, 'min_child_weight': 9, 'gamma': 0.2, 'subsample': 0.8, 'colsample_bytree': 0.8} ...\t\t(852.569s)\n",
      "14 / 25: 200-rounds Training finished param={'objective': 'multi:softmax', 'eta': 0.1, 'nthread': 8, 'num_class': 10, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['mlogloss', 'auc', 'merror'], 'max_depth': 9, 'min_child_weight': 9, 'gamma': 0.2, 'subsample': 0.9, 'colsample_bytree': 0.8} ...\t\t(867.649s)\n",
      "15 / 25: 200-rounds Training finished param={'objective': 'multi:softmax', 'eta': 0.1, 'nthread': 8, 'num_class': 10, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['mlogloss', 'auc', 'merror'], 'max_depth': 9, 'min_child_weight': 9, 'gamma': 0.2, 'subsample': 1.0, 'colsample_bytree': 0.8} ...\t\t(837.901s)\n",
      "16 / 25: 200-rounds Training finished param={'objective': 'multi:softmax', 'eta': 0.1, 'nthread': 8, 'num_class': 10, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['mlogloss', 'auc', 'merror'], 'max_depth': 9, 'min_child_weight': 9, 'gamma': 0.2, 'subsample': 0.6, 'colsample_bytree': 0.9} ...\t\t(827.339s)\n",
      "17 / 25: 200-rounds Training finished param={'objective': 'multi:softmax', 'eta': 0.1, 'nthread': 8, 'num_class': 10, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['mlogloss', 'auc', 'merror'], 'max_depth': 9, 'min_child_weight': 9, 'gamma': 0.2, 'subsample': 0.7, 'colsample_bytree': 0.9} ...\t\t(829.429s)\n",
      "18 / 25: 200-rounds Training finished param={'objective': 'multi:softmax', 'eta': 0.1, 'nthread': 8, 'num_class': 10, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['mlogloss', 'auc', 'merror'], 'max_depth': 9, 'min_child_weight': 9, 'gamma': 0.2, 'subsample': 0.8, 'colsample_bytree': 0.9} ...\t\t(854.584s)\n",
      "19 / 25: 200-rounds Training finished param={'objective': 'multi:softmax', 'eta': 0.1, 'nthread': 8, 'num_class': 10, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['mlogloss', 'auc', 'merror'], 'max_depth': 9, 'min_child_weight': 9, 'gamma': 0.2, 'subsample': 0.9, 'colsample_bytree': 0.9} ...\t\t(865.975s)\n",
      "20 / 25: 200-rounds Training finished param={'objective': 'multi:softmax', 'eta': 0.1, 'nthread': 8, 'num_class': 10, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['mlogloss', 'auc', 'merror'], 'max_depth': 9, 'min_child_weight': 9, 'gamma': 0.2, 'subsample': 1.0, 'colsample_bytree': 0.9} ...\t\t(830.648s)\n",
      "21 / 25: 200-rounds Training finished param={'objective': 'multi:softmax', 'eta': 0.1, 'nthread': 8, 'num_class': 10, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['mlogloss', 'auc', 'merror'], 'max_depth': 9, 'min_child_weight': 9, 'gamma': 0.2, 'subsample': 0.6, 'colsample_bytree': 1.0} ...\t\t(846.784s)\n",
      "22 / 25: 200-rounds Training finished param={'objective': 'multi:softmax', 'eta': 0.1, 'nthread': 8, 'num_class': 10, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['mlogloss', 'auc', 'merror'], 'max_depth': 9, 'min_child_weight': 9, 'gamma': 0.2, 'subsample': 0.7, 'colsample_bytree': 1.0} ...\t\t(852.846s)\n",
      "23 / 25: 200-rounds Training finished param={'objective': 'multi:softmax', 'eta': 0.1, 'nthread': 8, 'num_class': 10, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['mlogloss', 'auc', 'merror'], 'max_depth': 9, 'min_child_weight': 9, 'gamma': 0.2, 'subsample': 0.8, 'colsample_bytree': 1.0} ...\t\t(850.433s)\n",
      "24 / 25: 200-rounds Training finished param={'objective': 'multi:softmax', 'eta': 0.1, 'nthread': 8, 'num_class': 10, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['mlogloss', 'auc', 'merror'], 'max_depth': 9, 'min_child_weight': 9, 'gamma': 0.2, 'subsample': 0.9, 'colsample_bytree': 1.0} ...\t\t(865.453s)\n",
      "25 / 25: 200-rounds Training finished param={'objective': 'multi:softmax', 'eta': 0.1, 'nthread': 8, 'num_class': 10, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['mlogloss', 'auc', 'merror'], 'max_depth': 9, 'min_child_weight': 9, 'gamma': 0.2, 'subsample': 1.0, 'colsample_bytree': 1.0} ...\t\t(833.121s)\n",
      "colsample_bytree=0.6, eta=0.1, eval_metric=['mlogloss', 'auc', 'merror'], gamma=0.2, gpu_id=0, max_depth=9, min_child_weight=9, nthread=8, num_class=10, objective=multi:softmax, subsample=0.9, tree_method=gpu_hist\n",
      "colsample_bytree=1.0, eta=0.1, eval_metric=['mlogloss', 'auc', 'merror'], gamma=0.2, gpu_id=0, max_depth=9, min_child_weight=9, nthread=8, num_class=10, objective=multi:softmax, subsample=0.6, tree_method=gpu_hist\n",
      "colsample_bytree=1.0, eta=0.1, eval_metric=['mlogloss', 'auc', 'merror'], gamma=0.2, gpu_id=0, max_depth=9, min_child_weight=9, nthread=8, num_class=10, objective=multi:softmax, subsample=0.9, tree_method=gpu_hist\n",
      "开始搜索：reg_alpha=[0, 0.1, 0.2, 0.5, 1, 1.5, 2, 4]\n",
      "待搜索的参数组合数量：8\n",
      "1 / 8: 200-rounds Training finished param={'objective': 'multi:softmax', 'eta': 0.1, 'nthread': 8, 'num_class': 10, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['mlogloss', 'auc', 'merror'], 'max_depth': 9, 'min_child_weight': 9, 'gamma': 0.2, 'subsample': 0.9, 'colsample_bytree': 0.6, 'reg_alpha': 0} ...\t\t(864.738s)\n",
      "2 / 8: 200-rounds Training finished param={'objective': 'multi:softmax', 'eta': 0.1, 'nthread': 8, 'num_class': 10, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['mlogloss', 'auc', 'merror'], 'max_depth': 9, 'min_child_weight': 9, 'gamma': 0.2, 'subsample': 0.9, 'colsample_bytree': 0.6, 'reg_alpha': 0.1} ...\t\t(871.016s)\n",
      "3 / 8: 200-rounds Training finished param={'objective': 'multi:softmax', 'eta': 0.1, 'nthread': 8, 'num_class': 10, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['mlogloss', 'auc', 'merror'], 'max_depth': 9, 'min_child_weight': 9, 'gamma': 0.2, 'subsample': 0.9, 'colsample_bytree': 0.6, 'reg_alpha': 0.2} ...\t\t(854.900s)\n",
      "4 / 8: 200-rounds Training finished param={'objective': 'multi:softmax', 'eta': 0.1, 'nthread': 8, 'num_class': 10, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['mlogloss', 'auc', 'merror'], 'max_depth': 9, 'min_child_weight': 9, 'gamma': 0.2, 'subsample': 0.9, 'colsample_bytree': 0.6, 'reg_alpha': 0.5} ...\t\t(846.531s)\n",
      "5 / 8: 200-rounds Training finished param={'objective': 'multi:softmax', 'eta': 0.1, 'nthread': 8, 'num_class': 10, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['mlogloss', 'auc', 'merror'], 'max_depth': 9, 'min_child_weight': 9, 'gamma': 0.2, 'subsample': 0.9, 'colsample_bytree': 0.6, 'reg_alpha': 1} ...\t\t(857.497s)\n",
      "6 / 8: 200-rounds Training finished param={'objective': 'multi:softmax', 'eta': 0.1, 'nthread': 8, 'num_class': 10, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['mlogloss', 'auc', 'merror'], 'max_depth': 9, 'min_child_weight': 9, 'gamma': 0.2, 'subsample': 0.9, 'colsample_bytree': 0.6, 'reg_alpha': 1.5} ...\t\t(874.460s)\n",
      "7 / 8: 200-rounds Training finished param={'objective': 'multi:softmax', 'eta': 0.1, 'nthread': 8, 'num_class': 10, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['mlogloss', 'auc', 'merror'], 'max_depth': 9, 'min_child_weight': 9, 'gamma': 0.2, 'subsample': 0.9, 'colsample_bytree': 0.6, 'reg_alpha': 2} ...\t\t(875.624s)\n",
      "8 / 8: 200-rounds Training finished param={'objective': 'multi:softmax', 'eta': 0.1, 'nthread': 8, 'num_class': 10, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['mlogloss', 'auc', 'merror'], 'max_depth': 9, 'min_child_weight': 9, 'gamma': 0.2, 'subsample': 0.9, 'colsample_bytree': 0.6, 'reg_alpha': 4} ...\t\t(902.339s)\n",
      "colsample_bytree=0.6, eta=0.1, eval_metric=['mlogloss', 'auc', 'merror'], gamma=0.2, gpu_id=0, max_depth=9, min_child_weight=9, nthread=8, num_class=10, objective=multi:softmax, reg_alpha=0, subsample=0.9, tree_method=gpu_hist\n",
      "colsample_bytree=0.6, eta=0.1, eval_metric=['mlogloss', 'auc', 'merror'], gamma=0.2, gpu_id=0, max_depth=9, min_child_weight=9, nthread=8, num_class=10, objective=multi:softmax, reg_alpha=1, subsample=0.9, tree_method=gpu_hist\n",
      "colsample_bytree=0.6, eta=0.1, eval_metric=['mlogloss', 'auc', 'merror'], gamma=0.2, gpu_id=0, max_depth=9, min_child_weight=9, nthread=8, num_class=10, objective=multi:softmax, reg_alpha=4, subsample=0.9, tree_method=gpu_hist\n",
      "找到的最棒的参数是：\n",
      "colsample_bytree=0.6, eta=0.1, eval_metric=['mlogloss', 'auc', 'merror'], gamma=0.2, gpu_id=0, max_depth=9, min_child_weight=9, nthread=8, num_class=10, objective=multi:softmax, reg_alpha=0, subsample=0.9, tree_method=gpu_hist\n"
     ]
    }
   ],
   "source": [
    "base = base_param.copy()\n",
    "base.update({'max_depth': 9, 'min_child_weight': 9})\n",
    "base.update({'gamma': .2})\n",
    "grids = [ps3, ps4]\n",
    "\n",
    "rets = []\n",
    "for grid in grids:\n",
    "    params = compose_param_grid(grid, base)\n",
    "    print(f\"开始搜索：{dict_2_str(grid)}\\n待搜索的参数组合数量：{len(params)}\")\n",
    "    ret = gridsearch_cv_xgb(data.values, watch_label_res.values, params, n_round=200, verbose_eval=False, n_class=10)\n",
    "    arr = np.array([[-e['eval-merror'] for e in ret], \n",
    "                    [-e['eval-mlogloss'] for e in ret],\n",
    "                    [e['eval-auc'] for e in ret], \n",
    "                    [e['w_auc'] for e in ret]], dtype=np.float32)\n",
    "    opt_idxs = list(set(arr.argmax(axis=1)))\n",
    "    for i in opt_idxs:\n",
    "        print(dict_2_str(ret[i]['param']))\n",
    "    opt_idx = opt_idxs[0]\n",
    "    opt_param = ret[opt_idx]['param']\n",
    "    base.update(opt_param)\n",
    "    rets.append(ret)\n",
    "    \n",
    "    ks = list(grid.keys())\n",
    "    ks = '-'.join(ks)\n",
    "    with open(f\"./logs/{ks}-watch_label-{int(time())}.pkl\", \"wb\") as f:\n",
    "        pickle.dump(ret, f)\n",
    "    with open(f\"./logs/{ks}-watch_label-{int(time())}.md\", \"w\") as f:\n",
    "        lines = []\n",
    "        for e in ret:\n",
    "            line = metric_2_str(e)\n",
    "            lines.append(line)\n",
    "            lines.append('\\n')\n",
    "        f.write(\"\\n\".join(lines))\n",
    "\n",
    "print(f\"找到的最棒的参数是：\\n{dict_2_str(base)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'objective': 'multi:softmax',\n",
       " 'eta': 0.1,\n",
       " 'nthread': 8,\n",
       " 'num_class': 10,\n",
       " 'gpu_id': 0,\n",
       " 'tree_method': 'gpu_hist',\n",
       " 'eval_metric': ['mlogloss', 'auc', 'merror'],\n",
       " 'max_depth': 9,\n",
       " 'min_child_weight': 9,\n",
       " 'gamma': 0.2,\n",
       " 'subsample': 0.9,\n",
       " 'colsample_bytree': 0.6,\n",
       " 'reg_alpha': 0}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gridsearch_results = gridsearch_xgb(all_params, xg_train, xg_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_cv_results_sh = gridsearch_cv_xgb(data.values, watch_label_res.values, all_params, n_round=200, verbose_eval=False, n_class=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 584,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "最小误差与最大AUC对应的模型不一致 : [93 19]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-584-646c9ffaa26f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mopt_idxs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mopt_idxs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mopt_idxs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"最小误差与最大AUC对应的模型不一致 : {opt_idxs}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mopt_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopt_idxs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: 最小误差与最大AUC对应的模型不一致 : [93 19]"
     ]
    }
   ],
   "source": [
    "arr = np.array([[-e['test_error'] for e in gridsearch_results], [e['w_auc'] for e in gridsearch_results]], dtype=np.float32)\n",
    "opt_idxs = arr.argmax(axis=1)\n",
    "if opt_idxs[0] != opt_idxs[1]:\n",
    "     warnings.warn(f\"最小误差与最大AUC对应的模型不一致 : {opt_idxs}。选择误差最小的模型 : {opt_idxs[0]}\")\n",
    "\n",
    "opt_idx = opt_idxs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 583,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'test_error': 0.7289070620796754,\n",
       "  'aucs': array([0.57555597, 0.58567653, 0.50347593, 0.50017473, 0.50017414,\n",
       "         0.5000983 , 0.50292636, 0.50040085, 0.50878295, 0.60767447]),\n",
       "  'w_auc': 2.365403849959146,\n",
       "  'report': '              precision    recall  f1-score   support\\n\\n           0       0.33      0.19      0.24     43749\\n           1       0.29      0.68      0.41    111616\\n           2       0.23      0.01      0.03     62829\\n           3       0.13      0.00      0.00     43872\\n           4       0.15      0.00      0.00     34228\\n           5       0.18      0.00      0.00     28926\\n           6       0.33      0.01      0.01     25061\\n           7       0.16      0.00      0.00     23400\\n           8       0.31      0.02      0.04     27750\\n           9       0.24      0.56      0.34     77663\\n\\n    accuracy                           0.27    479094\\n   macro avg       0.23      0.15      0.11    479094\\nweighted avg       0.24      0.27      0.18    479094\\n',\n",
       "  'model': <xgboost.core.Booster at 0x7f35ba8b00d0>},\n",
       " {'test_error': 0.73068750600091,\n",
       "  'aucs': array([0.57882633, 0.58668642, 0.50378179, 0.50067885, 0.50046282,\n",
       "         0.50010955, 0.50312644, 0.50043677, 0.50916784, 0.60848382]),\n",
       "  'w_auc': 2.367019878746503,\n",
       "  'report': '              precision    recall  f1-score   support\\n\\n           0       0.32      0.20      0.25     43749\\n           1       0.29      0.66      0.40    111616\\n           2       0.18      0.03      0.04     62829\\n           3       0.12      0.01      0.01     43872\\n           4       0.12      0.00      0.00     34228\\n           5       0.08      0.00      0.00     28926\\n           6       0.25      0.01      0.01     25061\\n           7       0.11      0.00      0.00     23400\\n           8       0.27      0.02      0.04     27750\\n           9       0.24      0.57      0.34     77663\\n\\n    accuracy                           0.27    479094\\n   macro avg       0.20      0.15      0.11    479094\\nweighted avg       0.22      0.27      0.18    479094\\n',\n",
       "  'model': <xgboost.core.Booster at 0x7f3584f94e50>})"
      ]
     },
     "execution_count": 583,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gridsearch_results[93]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 588,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.33      0.19      0.24     43749\n",
      "           1       0.29      0.68      0.41    111616\n",
      "           2       0.23      0.01      0.03     62829\n",
      "           3       0.13      0.00      0.00     43872\n",
      "           4       0.15      0.00      0.00     34228\n",
      "           5       0.18      0.00      0.00     28926\n",
      "           6       0.33      0.01      0.01     25061\n",
      "           7       0.16      0.00      0.00     23400\n",
      "           8       0.31      0.02      0.04     27750\n",
      "           9       0.24      0.56      0.34     77663\n",
      "\n",
      "    accuracy                           0.27    479094\n",
      "   macro avg       0.23      0.15      0.11    479094\n",
      "weighted avg       0.24      0.27      0.18    479094\n",
      "\n"
     ]
    }
   ],
   "source": [
    "opt_idx = opt_idxs[0]\n",
    "opt_param = all_params[opt_idx]\n",
    "print(gridsearch_results[opt_idx]['report'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 557,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(479094,)"
      ]
     },
     "execution_count": 557,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gridsearch_cv_xgb(data.values, watch_label_res, all_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {}\n",
    "# use softmax multi-class classification\n",
    "param['objective'] = 'multi:softmax'\n",
    "# scale weight of positive examples\n",
    "param['eta'] = 0.1\n",
    "param['max_depth'] = 8\n",
    "param['nthread'] = 8\n",
    "param['num_class'] = 10\n",
    "# param['gpu_id'] = 0\n",
    "# param['tree_method'] = 'gpu_hist'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "cv_res= xgb.cv(param, cv_data, num_boost_round=200,early_stopping_rounds=30,nfold=3, metrics='auc',show_stdv=True)\n",
    "print(cv_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-fold & Bagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1-th split ... \n",
      "2-th split ... \n",
      "3-th split ... \n",
      "4-th split ... \n",
      "5-th split ... \n",
      "CPU times: user 1h 53min 34s, sys: 17.8 s, total: 1h 53min 52s\n",
      "Wall time: 1h 6min 33s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "n_splits = 5\n",
    "is_shuffle = True\n",
    "random_state = 10\n",
    "cv = StratifiedKFold(n_splits, shuffle=is_shuffle, random_state=random_state)\n",
    "X = data\n",
    "y = watch_label_res\n",
    "models = []\n",
    "results = {}\n",
    "\n",
    "n_round = 800\n",
    "verbose_eval = False\n",
    "param = {\n",
    "    'objective': 'multi:softmax',\n",
    "    'eta': 0.01,\n",
    "    'nthread': 8,\n",
    "    'num_class': 10,\n",
    "    'gpu_id': 0,\n",
    "    'tree_method': 'gpu_hist',\n",
    "    'eval_metric': ['mlogloss', 'auc', 'merror'],\n",
    "    'max_depth': 9,\n",
    "    'min_child_weight': 9,\n",
    "    'gamma': 0.2,\n",
    "    'subsample': 0.9,\n",
    "    'colsample_bytree': 0.6,\n",
    "    'reg_alpha': 0\n",
    "}\n",
    "n_class = param['num_class']\n",
    "for i, (train_idxs, test_idxs) in enumerate(cv.split(X, y)):\n",
    "        print(f\"{i+1}-th split ... \")\n",
    "        X_train = X.loc[train_idxs]\n",
    "        X_test  = X.loc[test_idxs]\n",
    "        y_train = y.loc[train_idxs]\n",
    "        y_test  = y.loc[test_idxs]\n",
    "        \n",
    "        xg_train = xgb.DMatrix(X_train, label=y_train, enable_categorical=True)\n",
    "        xg_test = xgb.DMatrix(X_test, label=y_test, enable_categorical=True)\n",
    "        watchlist = [(xg_train, 'train'), (xg_test, 'test')]\n",
    "        \n",
    "        t0 = time()\n",
    "        model = xgb.train(param, xg_train, n_round, watchlist, verbose_eval=verbose_eval)\n",
    "#         print(f\"{n_round}-rounds Training finished ...\\t\\t({time()-t0:.3f}s)\")\n",
    "\n",
    "#         # get prediction\n",
    "        pred = model.predict(xg_test)\n",
    "#         # pred = pred.astype(np.uint8)\n",
    "#         error_rate = np.sum(pred != y_test) / y_test.shape[0]\n",
    "#         print('Test error using softmax = {}'.format(error_rate))\n",
    "        \n",
    "        # eval the test using model\n",
    "        evals = model.eval(xg_test)\n",
    "        eval_dict = eval_str_2_dict(evals)\n",
    "        \n",
    "        aucs = auc(y_test.astype(np.uint8), pred.astype(np.uint8), np.arange(n_class))\n",
    "        # aucs[aucs == 0.5] = 0\n",
    "        if n_class == 2:\n",
    "            w_aucs = 0\n",
    "        else:\n",
    "            weights = np.arange(0, 1, 0.1)\n",
    "            w_aucs = (aucs * weights).sum()\n",
    "\n",
    "\n",
    "        rep = metrics.classification_report(list(y_test), list(pred), output_dict=True, zero_division=0)\n",
    "        rep_df = report_2_df(rep)\n",
    "        \n",
    "        # 处理每一fold的结果，对每个指标进行平均\n",
    "        items = [(e[0], e[1] / n_splits) for e in eval_dict.items()]\n",
    "        eval_dict = dict(items)\n",
    "        if not results:\n",
    "            results = {\n",
    "                'aucs': aucs / n_splits,\n",
    "                'w_auc': w_aucs / n_splits,\n",
    "                'report': rep_df / n_splits,\n",
    "    #             'model': model,\n",
    "    #             'split': i\n",
    "            }\n",
    "            results.update(eval_dict)\n",
    "        else:\n",
    "            results['aucs'] +=  aucs / n_splits\n",
    "            results['w_auc'] += w_aucs / n_splits\n",
    "            results['report'] += rep_df / n_splits\n",
    "            for k, v in eval_dict.items():\n",
    "                results[k] += v\n",
    "        \n",
    "        models.append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "      <th>support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.373191</td>\n",
       "      <td>0.447464</td>\n",
       "      <td>0.406966</td>\n",
       "      <td>77704.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.278011</td>\n",
       "      <td>0.642835</td>\n",
       "      <td>0.388154</td>\n",
       "      <td>111484.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.261718</td>\n",
       "      <td>0.016755</td>\n",
       "      <td>0.031493</td>\n",
       "      <td>62821.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.219837</td>\n",
       "      <td>0.007473</td>\n",
       "      <td>0.014454</td>\n",
       "      <td>43837.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.222941</td>\n",
       "      <td>0.006543</td>\n",
       "      <td>0.012710</td>\n",
       "      <td>34480.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.210198</td>\n",
       "      <td>0.007084</td>\n",
       "      <td>0.013703</td>\n",
       "      <td>28600.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.268990</td>\n",
       "      <td>0.010856</td>\n",
       "      <td>0.020862</td>\n",
       "      <td>25018.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.213552</td>\n",
       "      <td>0.004807</td>\n",
       "      <td>0.009402</td>\n",
       "      <td>23549.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.329894</td>\n",
       "      <td>0.017147</td>\n",
       "      <td>0.032597</td>\n",
       "      <td>27759.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.261190</td>\n",
       "      <td>0.509344</td>\n",
       "      <td>0.345307</td>\n",
       "      <td>77704.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.289853</td>\n",
       "      <td>512960.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>macro avg</th>\n",
       "      <td>0.263952</td>\n",
       "      <td>0.167031</td>\n",
       "      <td>0.127565</td>\n",
       "      <td>512960.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weighted avg</th>\n",
       "      <td>0.274840</td>\n",
       "      <td>0.289853</td>\n",
       "      <td>0.208239</td>\n",
       "      <td>512960.4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              precision    recall  f1-score   support\n",
       "0              0.373191  0.447464  0.406966   77704.2\n",
       "1              0.278011  0.642835  0.388154  111484.2\n",
       "2              0.261718  0.016755  0.031493   62821.4\n",
       "3              0.219837  0.007473  0.014454   43837.6\n",
       "4              0.222941  0.006543  0.012710   34480.8\n",
       "5              0.210198  0.007084  0.013703   28600.2\n",
       "6              0.268990  0.010856  0.020862   25018.4\n",
       "7              0.213552  0.004807  0.009402   23549.8\n",
       "8              0.329894  0.017147  0.032597   27759.6\n",
       "9              0.261190  0.509344  0.345307   77704.2\n",
       "accuracy            NaN       NaN  0.289853  512960.4\n",
       "macro avg      0.263952  0.167031  0.127565  512960.4\n",
       "weighted avg   0.274840  0.289853  0.208239  512960.4"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results['report']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB \n",
    "from mlxtend.classifier import StackingClassifier\n",
    "from sklearn.ensemble import (RandomForestClassifier, AdaBoostClassifier, \n",
    "                              GradientBoostingClassifier, ExtraTreesClassifier)\n",
    "from xgboost import XGBClassifier, XGBRFClassifier\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 第一层模型参数\n",
    "first_layer_params = {\n",
    "#     LogisticRegression : {\n",
    "#         'C' : 10,\n",
    "#         'random_state': 0\n",
    "#     },\n",
    "    XGBRFClassifier : {\n",
    "        'n_jobs': 4,\n",
    "        'n_estimators': 200,\n",
    "         #'max_features': 0.2,\n",
    "        'colsample_bytree': 0.6,\n",
    "        'max_depth': 8,\n",
    "        'learning_rate': .1,\n",
    "        'verbosity': 0,\n",
    "        'gpu_id': 1,\n",
    "    },\n",
    "    ExtraTreesClassifier : {\n",
    "        'n_jobs': -1,\n",
    "        'n_estimators':200,\n",
    "        #'max_features': 0.5,\n",
    "        'max_depth': 8,\n",
    "        'min_samples_leaf': 2,\n",
    "        'verbose': 0\n",
    "    },\n",
    "    AdaBoostClassifier : {\n",
    "        'n_estimators': 200,\n",
    "        'learning_rate' : 0.75\n",
    "    },\n",
    "    XGBClassifier : {\n",
    "        'objective': 'multi:softmax',\n",
    "        'eta': 0.1,\n",
    "        'nthread': 8,\n",
    "        'num_class': 10,\n",
    "        'gpu_id': 0,\n",
    "        'tree_method': 'gpu_hist',\n",
    "        'eval_metric': ['mlogloss', 'auc', 'merror'],\n",
    "        'max_depth': 7,\n",
    "        'min_child_weight': 9,\n",
    "        'gamma': 0.2,\n",
    "        'subsample': 0.9,\n",
    "        'colsample_bytree': 0.6,\n",
    "        'reg_alpha': 0,\n",
    "        'n_estimators': 200,\n",
    "    }\n",
    "#     GradientBoostingClassifier : { # 太慢了\n",
    "#         'n_estimators': 200,\n",
    "#          #'max_features': 0.2,\n",
    "#         'max_depth': 5,\n",
    "#         'min_samples_leaf': 2,\n",
    "#         'verbose': 0\n",
    "#     },\n",
    "#     SVC : { # 太慢了\n",
    "#         'kernel' : 'rbf',\n",
    "#         'C' : 0.025,\n",
    "#         'probability': True\n",
    "#     } \n",
    "}\n",
    "\n",
    "\n",
    "second_layer_params = {\n",
    "    XGBClassifier : {\n",
    "        'objective': 'multi:softmax',\n",
    "        'eta': 0.1,\n",
    "        'nthread': 8,\n",
    "        'num_class': 10,\n",
    "        'gpu_id': 0,\n",
    "        'tree_method': 'gpu_hist',\n",
    "        'eval_metric': ['mlogloss', 'auc', 'merror'],\n",
    "        'max_depth': 9,\n",
    "        'min_child_weight': 9,\n",
    "        'gamma': 0.2,\n",
    "        'subsample': 0.9,\n",
    "        'colsample_bytree': 0.6,\n",
    "        'reg_alpha': 0,\n",
    "        'n_estimators': 200,\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "clfs = []\n",
    "for c, p in first_layer_params.items():\n",
    "    clfs.append(c(**p))\n",
    "\n",
    "meta = XGBClassifier(**second_layer_params[XGBClassifier])\n",
    "sclf = StackingClassifier(classifiers=clfs, \n",
    "                          meta_classifier=meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gzy/.local/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/home/gzy/.local/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "sclf.fit(X_train, y_train)\n",
    "sclf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sclf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_19427/2239363165.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtmp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'sclf' is not defined"
     ]
    }
   ],
   "source": [
    "tmp = sclf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter(tmp), Counter(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gzy/.local/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "clf = XGBRFClassifier(**first_layer_params[XGBRFClassifier])\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.10100084, 0.10091538, 0.10042646, 0.09997695, 0.09962477,\n",
       "        0.09947018, 0.09939709, 0.09926751, 0.09943457, 0.10048625]])"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# is_share 预测"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 处理数据不均衡问题"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 7338705), (1, 14319)]\n",
      "[[0.         0.99805264]\n",
      " [1.         0.00194736]]\n"
     ]
    }
   ],
   "source": [
    "items = list(Counter(is_share).items())\n",
    "items.sort(key=lambda x: x[0])\n",
    "print(items)\n",
    "\n",
    "dist = np.array(items, dtype=np.float)\n",
    "dist[:, 1] = dist[:, 1] / is_share.shape[0]\n",
    "print(dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "under_ss = np.array(items)\n",
    "under_ss_thresh = under_ss[1, 1]\n",
    "under_ss[:, 1] = np.clip(under_ss[:, 1], a_min=None, a_max=under_ss_thresh)\n",
    "\n",
    "over_ss = under_ss.copy()\n",
    "over_ss_thresh = under_ss[1, 1]\n",
    "over_ss[:, 1] = np.clip(over_ss[:, 1], a_min=over_ss_thresh, a_max=None)\n",
    "\n",
    "under_ss = dict(under_ss)\n",
    "over_ss = dict(over_ss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({0: 14319, 1: 14319}, {0: 14319, 1: 14319})"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "under_ss, over_ss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7338705,)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idxs = is_share == 0\n",
    "idxs = idxs.replace(False, np.nan).dropna().index  # 保留watch_label=0的行索引\n",
    "idxs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((7324386,), (14319,))"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "left_idxs = np.random.choice(idxs, under_ss_thresh, replace=False)  # 选择一部分保留\n",
    "del_idxs = idxs.difference(left_idxs)\n",
    "del_idxs.shape, left_idxs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "left_idxs = pd.Index(left_idxs).union(is_share[is_share == 1].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((28638, 128), (28638,))"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 将下采样后的数据提取出来\n",
    "data_sh = dataset.loc[left_idxs]\n",
    "is_share_res = is_share.loc[left_idxs]\n",
    "data_sh.reset_index(inplace=True, drop=True)\n",
    "is_share_res.reset_index(inplace=True, drop=True)\n",
    "data_sh.shape, is_share_res.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9.06 s, sys: 37.7 s, total: 46.8 s\n",
      "Wall time: 4.95 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# 对几个数量较少的类别进行上采样\n",
    "smt_sh = SMOTE(sampling_strategy={1: 15000, 0: 15000}, n_jobs=8)\n",
    "X_sh_r, y_sh_r = smt_sh.fit_resample(data_sh, is_share_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((30000, 128), (30000,))"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_sh = X_sh_r\n",
    "is_share_res = y_sh_r\n",
    "data_sh.shape, is_share_res.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((24518,), (5382,))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_rate = .18\n",
    "train_idx, test_idx = train_test_split(data_sh.index, test_size=test_rate, random_state=1)\n",
    "train_idx.shape, test_idx.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_sh = data_sh.iloc[train_idx]\n",
    "X_test_sh  = data_sh.iloc[test_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_sh = is_share_res.iloc[train_idx]\n",
    "y_test_sh  = is_share_res.iloc[test_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data preparing finished ...\t\t(0.596s)\n"
     ]
    }
   ],
   "source": [
    "t0 = time()\n",
    "xg_train_sh = xgb.DMatrix(X_train_sh.values, label=y_train_sh.values, enable_categorical=True)\n",
    "xg_test_sh = xgb.DMatrix(X_test_sh.values, label=y_test_sh.values, enable_categorical=True)\n",
    "print(f\"Data preparing finished ...\\t\\t({time()-t0:.3f}s)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup parameters for xgboost\n",
    "param_sh = {\n",
    "    'objective': 'binary:hinge',\n",
    "    'eta': 0.1,\n",
    "    'nthread': 8,\n",
    "    'gpu_id': 0,\n",
    "    'tree_method': 'gpu_hist',\n",
    "    'eval_metric': ['logloss', 'auc', 'error'],\n",
    "    'max_depth': 5,\n",
    "    'min_child_weight': 1,\n",
    "    'gamma': 0.1,\n",
    "    'subsample': 0.6,\n",
    "    'colsample_bytree': 0.6,\n",
    "    'reg_alpha': 0\n",
    "}\n",
    "\n",
    "\n",
    "watchlist = [(xg_train_sh, 'train'), (xg_test_sh, 'test')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-logloss:18.44773\ttrain-auc:0.50000\ttrain-error:0.50073\ttest-logloss:18.29747\ttest-auc:0.50000\ttest-error:0.49666\n",
      "[1]\ttrain-logloss:18.44773\ttrain-auc:0.50000\ttrain-error:0.50073\ttest-logloss:18.29747\ttest-auc:0.50000\ttest-error:0.49666\n",
      "[2]\ttrain-logloss:18.44773\ttrain-auc:0.50000\ttrain-error:0.50073\ttest-logloss:18.29747\ttest-auc:0.50000\ttest-error:0.49666\n",
      "[3]\ttrain-logloss:18.44773\ttrain-auc:0.50000\ttrain-error:0.50073\ttest-logloss:18.29747\ttest-auc:0.50000\ttest-error:0.49666\n",
      "[4]\ttrain-logloss:18.44773\ttrain-auc:0.50000\ttrain-error:0.50073\ttest-logloss:18.29747\ttest-auc:0.50000\ttest-error:0.49666\n",
      "[5]\ttrain-logloss:18.44773\ttrain-auc:0.50000\ttrain-error:0.50073\ttest-logloss:18.29747\ttest-auc:0.50000\ttest-error:0.49666\n",
      "[6]\ttrain-logloss:18.44773\ttrain-auc:0.50000\ttrain-error:0.50073\ttest-logloss:18.29747\ttest-auc:0.50000\ttest-error:0.49666\n",
      "[7]\ttrain-logloss:17.56268\ttrain-auc:0.52398\ttrain-error:0.47671\ttest-logloss:17.27067\ttest-auc:0.52810\ttest-error:0.46879\n",
      "[8]\ttrain-logloss:16.90152\ttrain-auc:0.54189\ttrain-error:0.45876\ttest-logloss:16.65459\ttest-auc:0.54498\ttest-error:0.45206\n",
      "[9]\ttrain-logloss:16.28094\ttrain-auc:0.55870\ttrain-error:0.44192\ttest-logloss:16.33287\ttest-auc:0.55386\ttest-error:0.44333\n",
      "[10]\ttrain-logloss:15.99094\ttrain-auc:0.56654\ttrain-error:0.43405\ttest-logloss:16.01114\ttest-auc:0.56273\ttest-error:0.43460\n",
      "[11]\ttrain-logloss:15.82715\ttrain-auc:0.57097\ttrain-error:0.42960\ttest-logloss:15.77840\ttest-auc:0.56913\ttest-error:0.42828\n",
      "[12]\ttrain-logloss:15.63782\ttrain-auc:0.57609\ttrain-error:0.42446\ttest-logloss:15.59357\ttest-auc:0.57422\ttest-error:0.42326\n",
      "[13]\ttrain-logloss:15.47554\ttrain-auc:0.58048\ttrain-error:0.42006\ttest-logloss:15.37453\ttest-auc:0.58024\ttest-error:0.41732\n",
      "[14]\ttrain-logloss:15.37786\ttrain-auc:0.58312\ttrain-error:0.41741\ttest-logloss:15.24446\ttest-auc:0.58381\ttest-error:0.41379\n",
      "[15]\ttrain-logloss:15.25315\ttrain-auc:0.58650\ttrain-error:0.41402\ttest-logloss:15.12125\ttest-auc:0.58721\ttest-error:0.41044\n",
      "[16]\ttrain-logloss:15.20506\ttrain-auc:0.58779\ttrain-error:0.41272\ttest-logloss:15.10071\ttest-auc:0.58781\ttest-error:0.40988\n",
      "[17]\ttrain-logloss:15.10890\ttrain-auc:0.59039\ttrain-error:0.41011\ttest-logloss:14.84744\ttest-auc:0.59475\ttest-error:0.40301\n",
      "[18]\ttrain-logloss:15.05180\ttrain-auc:0.59193\ttrain-error:0.40856\ttest-logloss:14.79268\ttest-auc:0.59626\ttest-error:0.40152\n",
      "[19]\ttrain-logloss:14.95412\ttrain-auc:0.59458\ttrain-error:0.40591\ttest-logloss:14.71053\ttest-auc:0.59852\ttest-error:0.39929\n",
      "[20]\ttrain-logloss:14.76329\ttrain-auc:0.59974\ttrain-error:0.40073\ttest-logloss:14.57363\ttest-auc:0.60228\ttest-error:0.39558\n",
      "[21]\ttrain-logloss:14.73174\ttrain-auc:0.60059\ttrain-error:0.39987\ttest-logloss:14.55994\ttest-auc:0.60267\ttest-error:0.39521\n",
      "[22]\ttrain-logloss:14.67764\ttrain-auc:0.60205\ttrain-error:0.39840\ttest-logloss:14.49148\ttest-auc:0.60457\ttest-error:0.39335\n",
      "[23]\ttrain-logloss:14.63407\ttrain-auc:0.60323\ttrain-error:0.39722\ttest-logloss:14.45726\ttest-auc:0.60553\ttest-error:0.39242\n",
      "[24]\ttrain-logloss:14.57246\ttrain-auc:0.60489\ttrain-error:0.39555\ttest-logloss:14.38196\ttest-auc:0.60760\ttest-error:0.39038\n",
      "[25]\ttrain-logloss:14.49582\ttrain-auc:0.60696\ttrain-error:0.39347\ttest-logloss:14.32035\ttest-auc:0.60930\ttest-error:0.38870\n",
      "[26]\ttrain-logloss:14.47028\ttrain-auc:0.60765\ttrain-error:0.39277\ttest-logloss:14.32035\ttest-auc:0.60933\ttest-error:0.38870\n",
      "[27]\ttrain-logloss:14.42069\ttrain-auc:0.60899\ttrain-error:0.39143\ttest-logloss:14.29297\ttest-auc:0.61011\ttest-error:0.38796\n",
      "[28]\ttrain-logloss:14.36209\ttrain-auc:0.61057\ttrain-error:0.38984\ttest-logloss:14.23136\ttest-auc:0.61182\ttest-error:0.38629\n",
      "[29]\ttrain-logloss:14.27494\ttrain-auc:0.61293\ttrain-error:0.38747\ttest-logloss:14.11499\ttest-auc:0.61501\ttest-error:0.38313\n",
      "[30]\ttrain-logloss:14.20281\ttrain-auc:0.61488\ttrain-error:0.38551\ttest-logloss:14.08761\ttest-auc:0.61579\ttest-error:0.38239\n",
      "[31]\ttrain-logloss:14.14571\ttrain-auc:0.61643\ttrain-error:0.38396\ttest-logloss:14.00547\ttest-auc:0.61806\ttest-error:0.38016\n",
      "[32]\ttrain-logloss:14.10364\ttrain-auc:0.61756\ttrain-error:0.38282\ttest-logloss:13.97809\ttest-auc:0.61884\ttest-error:0.37941\n",
      "[33]\ttrain-logloss:14.01649\ttrain-auc:0.61992\ttrain-error:0.38045\ttest-logloss:13.93017\ttest-auc:0.62017\ttest-error:0.37811\n",
      "[34]\ttrain-logloss:13.99996\ttrain-auc:0.62036\ttrain-error:0.38001\ttest-logloss:13.81380\ttest-auc:0.62337\ttest-error:0.37495\n",
      "[35]\ttrain-logloss:13.93084\ttrain-auc:0.62223\ttrain-error:0.37813\ttest-logloss:13.80695\ttest-auc:0.62360\ttest-error:0.37477\n",
      "[36]\ttrain-logloss:13.89778\ttrain-auc:0.62312\ttrain-error:0.37723\ttest-logloss:13.82065\ttest-auc:0.62324\ttest-error:0.37514\n",
      "[37]\ttrain-logloss:13.86472\ttrain-auc:0.62401\ttrain-error:0.37634\ttest-logloss:13.75219\ttest-auc:0.62514\ttest-error:0.37328\n",
      "[38]\ttrain-logloss:13.85871\ttrain-auc:0.62417\ttrain-error:0.37617\ttest-logloss:13.71112\ttest-auc:0.62627\ttest-error:0.37217\n",
      "[39]\ttrain-logloss:13.83617\ttrain-auc:0.62478\ttrain-error:0.37556\ttest-logloss:13.71112\ttest-auc:0.62629\ttest-error:0.37217\n",
      "[40]\ttrain-logloss:13.78959\ttrain-auc:0.62604\ttrain-error:0.37430\ttest-logloss:13.66320\ttest-auc:0.62761\ttest-error:0.37087\n",
      "[41]\ttrain-logloss:13.74000\ttrain-auc:0.62738\ttrain-error:0.37295\ttest-logloss:13.67005\ttest-auc:0.62744\ttest-error:0.37105\n",
      "[42]\ttrain-logloss:13.71746\ttrain-auc:0.62799\ttrain-error:0.37234\ttest-logloss:13.64951\ttest-auc:0.62801\ttest-error:0.37049\n",
      "[43]\ttrain-logloss:13.69943\ttrain-auc:0.62847\ttrain-error:0.37185\ttest-logloss:13.61529\ttest-auc:0.62896\ttest-error:0.36956\n",
      "[44]\ttrain-logloss:13.66638\ttrain-auc:0.62937\ttrain-error:0.37095\ttest-logloss:13.54683\ttest-auc:0.63084\ttest-error:0.36771\n",
      "[45]\ttrain-logloss:13.61078\ttrain-auc:0.63087\ttrain-error:0.36944\ttest-logloss:13.53999\ttest-auc:0.63104\ttest-error:0.36752\n",
      "[46]\ttrain-logloss:13.59275\ttrain-auc:0.63135\ttrain-error:0.36895\ttest-logloss:13.52630\ttest-auc:0.63143\ttest-error:0.36715\n",
      "[47]\ttrain-logloss:13.53264\ttrain-auc:0.63298\ttrain-error:0.36732\ttest-logloss:13.49892\ttest-auc:0.63221\ttest-error:0.36641\n",
      "[48]\ttrain-logloss:13.48756\ttrain-auc:0.63420\ttrain-error:0.36610\ttest-logloss:13.44415\ttest-auc:0.63372\ttest-error:0.36492\n",
      "[49]\ttrain-logloss:13.47705\ttrain-auc:0.63448\ttrain-error:0.36581\ttest-logloss:13.45784\ttest-auc:0.63335\ttest-error:0.36529\n",
      "[50]\ttrain-logloss:13.43798\ttrain-auc:0.63554\ttrain-error:0.36475\ttest-logloss:13.41677\ttest-auc:0.63449\ttest-error:0.36418\n",
      "[51]\ttrain-logloss:13.41093\ttrain-auc:0.63627\ttrain-error:0.36402\ttest-logloss:13.37570\ttest-auc:0.63562\ttest-error:0.36306\n",
      "[52]\ttrain-logloss:13.37487\ttrain-auc:0.63725\ttrain-error:0.36304\ttest-logloss:13.38255\ttest-auc:0.63546\ttest-error:0.36325\n",
      "[53]\ttrain-logloss:13.37186\ttrain-auc:0.63733\ttrain-error:0.36296\ttest-logloss:13.38939\ttest-auc:0.63528\ttest-error:0.36343\n",
      "[54]\ttrain-logloss:13.32528\ttrain-auc:0.63859\ttrain-error:0.36169\ttest-logloss:13.38255\ttest-auc:0.63547\ttest-error:0.36325\n",
      "[55]\ttrain-logloss:13.28471\ttrain-auc:0.63968\ttrain-error:0.36059\ttest-logloss:13.39624\ttest-auc:0.63511\ttest-error:0.36362\n",
      "[56]\ttrain-logloss:13.26217\ttrain-auc:0.64029\ttrain-error:0.35998\ttest-logloss:13.38255\ttest-auc:0.63549\ttest-error:0.36325\n",
      "[57]\ttrain-logloss:13.26067\ttrain-auc:0.64033\ttrain-error:0.35994\ttest-logloss:13.39624\ttest-auc:0.63513\ttest-error:0.36362\n",
      "[58]\ttrain-logloss:13.23212\ttrain-auc:0.64110\ttrain-error:0.35917\ttest-logloss:13.30725\ttest-auc:0.63757\ttest-error:0.36120\n",
      "[59]\ttrain-logloss:13.20807\ttrain-auc:0.64175\ttrain-error:0.35851\ttest-logloss:13.34832\ttest-auc:0.63646\ttest-error:0.36232\n",
      "[60]\ttrain-logloss:13.20056\ttrain-auc:0.64196\ttrain-error:0.35831\ttest-logloss:13.34147\ttest-auc:0.63667\ttest-error:0.36213\n",
      "[61]\ttrain-logloss:13.17952\ttrain-auc:0.64253\ttrain-error:0.35774\ttest-logloss:13.33463\ttest-auc:0.63686\ttest-error:0.36195\n",
      "[62]\ttrain-logloss:13.13445\ttrain-auc:0.64375\ttrain-error:0.35651\ttest-logloss:13.32778\ttest-auc:0.63706\ttest-error:0.36176\n",
      "[63]\ttrain-logloss:13.10139\ttrain-auc:0.64464\ttrain-error:0.35562\ttest-logloss:13.31409\ttest-auc:0.63745\ttest-error:0.36139\n",
      "[64]\ttrain-logloss:13.08185\ttrain-auc:0.64517\ttrain-error:0.35509\ttest-logloss:13.30725\ttest-auc:0.63764\ttest-error:0.36120\n",
      "[65]\ttrain-logloss:13.05330\ttrain-auc:0.64594\ttrain-error:0.35431\ttest-logloss:13.31409\ttest-auc:0.63746\ttest-error:0.36139\n",
      "[66]\ttrain-logloss:13.02776\ttrain-auc:0.64663\ttrain-error:0.35362\ttest-logloss:13.25933\ttest-auc:0.63897\ttest-error:0.35990\n",
      "[67]\ttrain-logloss:13.00071\ttrain-auc:0.64736\ttrain-error:0.35288\ttest-logloss:13.22510\ttest-auc:0.63992\ttest-error:0.35897\n",
      "[68]\ttrain-logloss:12.95263\ttrain-auc:0.64866\ttrain-error:0.35158\ttest-logloss:13.20457\ttest-auc:0.64049\ttest-error:0.35842\n",
      "[69]\ttrain-logloss:12.91506\ttrain-auc:0.64968\ttrain-error:0.35056\ttest-logloss:13.15665\ttest-auc:0.64180\ttest-error:0.35712\n",
      "[70]\ttrain-logloss:12.92408\ttrain-auc:0.64943\ttrain-error:0.35080\ttest-logloss:13.14981\ttest-auc:0.64200\ttest-error:0.35693\n",
      "[71]\ttrain-logloss:12.91957\ttrain-auc:0.64955\ttrain-error:0.35068\ttest-logloss:13.16350\ttest-auc:0.64163\ttest-error:0.35730\n",
      "[72]\ttrain-logloss:12.91506\ttrain-auc:0.64968\ttrain-error:0.35056\ttest-logloss:13.17719\ttest-auc:0.64127\ttest-error:0.35767\n",
      "[73]\ttrain-logloss:12.86998\ttrain-auc:0.65090\ttrain-error:0.34934\ttest-logloss:13.17719\ttest-auc:0.64128\ttest-error:0.35767\n",
      "[74]\ttrain-logloss:12.85496\ttrain-auc:0.65130\ttrain-error:0.34893\ttest-logloss:13.19088\ttest-auc:0.64092\ttest-error:0.35805\n",
      "[75]\ttrain-logloss:12.83993\ttrain-auc:0.65171\ttrain-error:0.34852\ttest-logloss:13.19772\ttest-auc:0.64074\ttest-error:0.35823\n",
      "[76]\ttrain-logloss:12.82190\ttrain-auc:0.65219\ttrain-error:0.34803\ttest-logloss:13.13612\ttest-auc:0.64242\ttest-error:0.35656\n",
      "[77]\ttrain-logloss:12.80988\ttrain-auc:0.65252\ttrain-error:0.34770\ttest-logloss:13.15665\ttest-auc:0.64188\ttest-error:0.35712\n",
      "[78]\ttrain-logloss:12.78433\ttrain-auc:0.65321\ttrain-error:0.34701\ttest-logloss:13.10873\ttest-auc:0.64319\ttest-error:0.35582\n",
      "[79]\ttrain-logloss:12.76330\ttrain-auc:0.65378\ttrain-error:0.34644\ttest-logloss:13.08135\ttest-auc:0.64395\ttest-error:0.35507\n",
      "[80]\ttrain-logloss:12.73325\ttrain-auc:0.65459\ttrain-error:0.34562\ttest-logloss:13.06766\ttest-auc:0.64433\ttest-error:0.35470\n",
      "[81]\ttrain-logloss:12.72122\ttrain-auc:0.65491\ttrain-error:0.34530\ttest-logloss:13.08135\ttest-auc:0.64396\ttest-error:0.35507\n",
      "[82]\ttrain-logloss:12.70920\ttrain-auc:0.65524\ttrain-error:0.34497\ttest-logloss:13.06766\ttest-auc:0.64433\ttest-error:0.35470\n",
      "[83]\ttrain-logloss:12.70319\ttrain-auc:0.65540\ttrain-error:0.34481\ttest-logloss:13.08135\ttest-auc:0.64397\ttest-error:0.35507\n",
      "[84]\ttrain-logloss:12.70169\ttrain-auc:0.65544\ttrain-error:0.34477\ttest-logloss:13.08820\ttest-auc:0.64379\ttest-error:0.35526\n",
      "[85]\ttrain-logloss:12.67014\ttrain-auc:0.65629\ttrain-error:0.34391\ttest-logloss:13.07451\ttest-auc:0.64417\ttest-error:0.35489\n",
      "[86]\ttrain-logloss:12.64008\ttrain-auc:0.65711\ttrain-error:0.34309\ttest-logloss:13.01975\ttest-auc:0.64568\ttest-error:0.35340\n",
      "[87]\ttrain-logloss:12.62205\ttrain-auc:0.65759\ttrain-error:0.34260\ttest-logloss:13.01290\ttest-auc:0.64587\ttest-error:0.35321\n",
      "[88]\ttrain-logloss:12.60853\ttrain-auc:0.65796\ttrain-error:0.34224\ttest-logloss:13.00605\ttest-auc:0.64607\ttest-error:0.35303\n",
      "[89]\ttrain-logloss:12.57697\ttrain-auc:0.65881\ttrain-error:0.34138\ttest-logloss:12.93760\ttest-auc:0.64795\ttest-error:0.35117\n",
      "[90]\ttrain-logloss:12.56044\ttrain-auc:0.65926\ttrain-error:0.34093\ttest-logloss:12.93076\ttest-auc:0.64813\ttest-error:0.35098\n",
      "[91]\ttrain-logloss:12.54241\ttrain-auc:0.65975\ttrain-error:0.34044\ttest-logloss:12.93076\ttest-auc:0.64815\ttest-error:0.35098\n",
      "[92]\ttrain-logloss:12.53039\ttrain-auc:0.66007\ttrain-error:0.34012\ttest-logloss:12.89653\ttest-auc:0.64909\ttest-error:0.35006\n",
      "[93]\ttrain-logloss:12.50485\ttrain-auc:0.66076\ttrain-error:0.33942\ttest-logloss:12.87599\ttest-auc:0.64965\ttest-error:0.34950\n",
      "[94]\ttrain-logloss:12.48982\ttrain-auc:0.66117\ttrain-error:0.33902\ttest-logloss:12.86915\ttest-auc:0.64984\ttest-error:0.34931\n",
      "[95]\ttrain-logloss:12.48982\ttrain-auc:0.66117\ttrain-error:0.33902\ttest-logloss:12.88969\ttest-auc:0.64929\ttest-error:0.34987\n",
      "[96]\ttrain-logloss:12.47630\ttrain-auc:0.66153\ttrain-error:0.33865\ttest-logloss:12.84861\ttest-auc:0.65041\ttest-error:0.34875\n",
      "[97]\ttrain-logloss:12.45075\ttrain-auc:0.66222\ttrain-error:0.33796\ttest-logloss:12.82808\ttest-auc:0.65098\ttest-error:0.34820\n",
      "[98]\ttrain-logloss:12.41469\ttrain-auc:0.66320\ttrain-error:0.33698\ttest-logloss:12.82123\ttest-auc:0.65117\ttest-error:0.34801\n",
      "[99]\ttrain-logloss:12.39816\ttrain-auc:0.66365\ttrain-error:0.33653\ttest-logloss:12.82123\ttest-auc:0.65118\ttest-error:0.34801\n",
      "[100]\ttrain-logloss:12.39065\ttrain-auc:0.66385\ttrain-error:0.33632\ttest-logloss:12.81439\ttest-auc:0.65137\ttest-error:0.34783\n",
      "[101]\ttrain-logloss:12.39215\ttrain-auc:0.66381\ttrain-error:0.33637\ttest-logloss:12.82808\ttest-auc:0.65100\ttest-error:0.34820\n",
      "[102]\ttrain-logloss:12.36660\ttrain-auc:0.66450\ttrain-error:0.33567\ttest-logloss:12.87599\ttest-auc:0.64971\ttest-error:0.34950\n",
      "[103]\ttrain-logloss:12.36811\ttrain-auc:0.66446\ttrain-error:0.33571\ttest-logloss:12.85546\ttest-auc:0.65027\ttest-error:0.34894\n",
      "[104]\ttrain-logloss:12.35458\ttrain-auc:0.66482\ttrain-error:0.33535\ttest-logloss:12.87599\ttest-auc:0.64972\ttest-error:0.34950\n",
      "[105]\ttrain-logloss:12.33655\ttrain-auc:0.66531\ttrain-error:0.33486\ttest-logloss:12.86915\ttest-auc:0.64991\ttest-error:0.34931\n",
      "[106]\ttrain-logloss:12.32904\ttrain-auc:0.66551\ttrain-error:0.33465\ttest-logloss:12.86230\ttest-auc:0.65010\ttest-error:0.34913\n",
      "[107]\ttrain-logloss:12.32303\ttrain-auc:0.66568\ttrain-error:0.33449\ttest-logloss:12.87599\ttest-auc:0.64974\ttest-error:0.34950\n",
      "[108]\ttrain-logloss:12.31101\ttrain-auc:0.66600\ttrain-error:0.33416\ttest-logloss:12.84177\ttest-auc:0.65067\ttest-error:0.34857\n",
      "[109]\ttrain-logloss:12.29748\ttrain-auc:0.66637\ttrain-error:0.33380\ttest-logloss:12.82123\ttest-auc:0.65124\ttest-error:0.34801\n",
      "[110]\ttrain-logloss:12.27194\ttrain-auc:0.66706\ttrain-error:0.33310\ttest-logloss:12.81439\ttest-auc:0.65142\ttest-error:0.34783\n",
      "[111]\ttrain-logloss:12.26292\ttrain-auc:0.66730\ttrain-error:0.33286\ttest-logloss:12.81439\ttest-auc:0.65143\ttest-error:0.34783\n",
      "[112]\ttrain-logloss:12.25391\ttrain-auc:0.66755\ttrain-error:0.33261\ttest-logloss:12.82808\ttest-auc:0.65106\ttest-error:0.34820\n",
      "[113]\ttrain-logloss:12.22235\ttrain-auc:0.66840\ttrain-error:0.33176\ttest-logloss:12.82808\ttest-auc:0.65106\ttest-error:0.34820\n",
      "[114]\ttrain-logloss:12.22386\ttrain-auc:0.66836\ttrain-error:0.33180\ttest-logloss:12.81439\ttest-auc:0.65143\ttest-error:0.34783\n",
      "[115]\ttrain-logloss:12.19380\ttrain-auc:0.66918\ttrain-error:0.33098\ttest-logloss:12.80754\ttest-auc:0.65162\ttest-error:0.34764\n",
      "[116]\ttrain-logloss:12.18028\ttrain-auc:0.66954\ttrain-error:0.33061\ttest-logloss:12.78016\ttest-auc:0.65236\ttest-error:0.34690\n",
      "[117]\ttrain-logloss:12.15924\ttrain-auc:0.67012\ttrain-error:0.33004\ttest-logloss:12.80754\ttest-auc:0.65163\ttest-error:0.34764\n",
      "[118]\ttrain-logloss:12.15624\ttrain-auc:0.67020\ttrain-error:0.32996\ttest-logloss:12.81439\ttest-auc:0.65145\ttest-error:0.34783\n",
      "[119]\ttrain-logloss:12.13370\ttrain-auc:0.67081\ttrain-error:0.32935\ttest-logloss:12.82808\ttest-auc:0.65108\ttest-error:0.34820\n",
      "[120]\ttrain-logloss:12.13821\ttrain-auc:0.67068\ttrain-error:0.32947\ttest-logloss:12.81439\ttest-auc:0.65146\ttest-error:0.34783\n",
      "[121]\ttrain-logloss:12.13069\ttrain-auc:0.67089\ttrain-error:0.32927\ttest-logloss:12.82123\ttest-auc:0.65127\ttest-error:0.34801\n",
      "[122]\ttrain-logloss:12.11116\ttrain-auc:0.67142\ttrain-error:0.32874\ttest-logloss:12.80754\ttest-auc:0.65165\ttest-error:0.34764\n",
      "[123]\ttrain-logloss:12.10064\ttrain-auc:0.67170\ttrain-error:0.32845\ttest-logloss:12.78701\ttest-auc:0.65221\ttest-error:0.34708\n",
      "[124]\ttrain-logloss:12.07960\ttrain-auc:0.67227\ttrain-error:0.32788\ttest-logloss:12.73224\ttest-auc:0.65371\ttest-error:0.34560\n",
      "[125]\ttrain-logloss:12.05256\ttrain-auc:0.67300\ttrain-error:0.32715\ttest-logloss:12.76647\ttest-auc:0.65280\ttest-error:0.34653\n",
      "[126]\ttrain-logloss:12.03903\ttrain-auc:0.67337\ttrain-error:0.32678\ttest-logloss:12.79385\ttest-auc:0.65206\ttest-error:0.34727\n",
      "[127]\ttrain-logloss:12.03903\ttrain-auc:0.67337\ttrain-error:0.32678\ttest-logloss:12.78701\ttest-auc:0.65225\ttest-error:0.34708\n",
      "[128]\ttrain-logloss:12.03002\ttrain-auc:0.67361\ttrain-error:0.32654\ttest-logloss:12.78701\ttest-auc:0.65224\ttest-error:0.34708\n",
      "[129]\ttrain-logloss:12.01349\ttrain-auc:0.67406\ttrain-error:0.32609\ttest-logloss:12.79385\ttest-auc:0.65207\ttest-error:0.34727\n",
      "[130]\ttrain-logloss:12.00297\ttrain-auc:0.67435\ttrain-error:0.32580\ttest-logloss:12.80069\ttest-auc:0.65189\ttest-error:0.34745\n",
      "[131]\ttrain-logloss:11.99095\ttrain-auc:0.67467\ttrain-error:0.32548\ttest-logloss:12.82808\ttest-auc:0.65115\ttest-error:0.34820\n",
      "[132]\ttrain-logloss:11.96540\ttrain-auc:0.67536\ttrain-error:0.32478\ttest-logloss:12.79385\ttest-auc:0.65209\ttest-error:0.34727\n",
      "[133]\ttrain-logloss:11.95489\ttrain-auc:0.67565\ttrain-error:0.32450\ttest-logloss:12.81439\ttest-auc:0.65153\ttest-error:0.34783\n",
      "[134]\ttrain-logloss:11.95489\ttrain-auc:0.67565\ttrain-error:0.32450\ttest-logloss:12.80069\ttest-auc:0.65190\ttest-error:0.34745\n",
      "[135]\ttrain-logloss:11.94136\ttrain-auc:0.67602\ttrain-error:0.32413\ttest-logloss:12.77332\ttest-auc:0.65265\ttest-error:0.34671\n",
      "[136]\ttrain-logloss:11.92183\ttrain-auc:0.67655\ttrain-error:0.32360\ttest-logloss:12.78701\ttest-auc:0.65228\ttest-error:0.34708\n",
      "[137]\ttrain-logloss:11.92633\ttrain-auc:0.67642\ttrain-error:0.32372\ttest-logloss:12.80069\ttest-auc:0.65191\ttest-error:0.34745\n",
      "[138]\ttrain-logloss:11.91582\ttrain-auc:0.67671\ttrain-error:0.32344\ttest-logloss:12.82123\ttest-auc:0.65136\ttest-error:0.34801\n",
      "[139]\ttrain-logloss:11.90981\ttrain-auc:0.67687\ttrain-error:0.32327\ttest-logloss:12.84177\ttest-auc:0.65080\ttest-error:0.34857\n",
      "[140]\ttrain-logloss:11.88727\ttrain-auc:0.67748\ttrain-error:0.32266\ttest-logloss:12.84861\ttest-auc:0.65062\ttest-error:0.34875\n",
      "[141]\ttrain-logloss:11.87374\ttrain-auc:0.67785\ttrain-error:0.32229\ttest-logloss:12.84177\ttest-auc:0.65081\ttest-error:0.34857\n",
      "[142]\ttrain-logloss:11.86623\ttrain-auc:0.67805\ttrain-error:0.32209\ttest-logloss:12.84177\ttest-auc:0.65080\ttest-error:0.34857\n",
      "[143]\ttrain-logloss:11.86773\ttrain-auc:0.67801\ttrain-error:0.32213\ttest-logloss:12.84177\ttest-auc:0.65081\ttest-error:0.34857\n",
      "[144]\ttrain-logloss:11.85872\ttrain-auc:0.67825\ttrain-error:0.32189\ttest-logloss:12.87599\ttest-auc:0.64988\ttest-error:0.34950\n",
      "[145]\ttrain-logloss:11.83918\ttrain-auc:0.67878\ttrain-error:0.32136\ttest-logloss:12.87599\ttest-auc:0.64988\ttest-error:0.34950\n",
      "[146]\ttrain-logloss:11.84369\ttrain-auc:0.67866\ttrain-error:0.32148\ttest-logloss:12.82123\ttest-auc:0.65136\ttest-error:0.34801\n",
      "[147]\ttrain-logloss:11.83167\ttrain-auc:0.67899\ttrain-error:0.32115\ttest-logloss:12.83492\ttest-auc:0.65100\ttest-error:0.34838\n",
      "[148]\ttrain-logloss:11.82416\ttrain-auc:0.67919\ttrain-error:0.32095\ttest-logloss:12.84861\ttest-auc:0.65063\ttest-error:0.34875\n",
      "[149]\ttrain-logloss:11.79861\ttrain-auc:0.67988\ttrain-error:0.32026\ttest-logloss:12.81439\ttest-auc:0.65157\ttest-error:0.34783\n",
      "[150]\ttrain-logloss:11.80763\ttrain-auc:0.67964\ttrain-error:0.32050\ttest-logloss:12.80069\ttest-auc:0.65194\ttest-error:0.34745\n",
      "[151]\ttrain-logloss:11.78810\ttrain-auc:0.68017\ttrain-error:0.31997\ttest-logloss:12.80069\ttest-auc:0.65195\ttest-error:0.34745\n",
      "[152]\ttrain-logloss:11.78659\ttrain-auc:0.68021\ttrain-error:0.31993\ttest-logloss:12.79385\ttest-auc:0.65214\ttest-error:0.34727\n",
      "[153]\ttrain-logloss:11.77457\ttrain-auc:0.68053\ttrain-error:0.31960\ttest-logloss:12.80754\ttest-auc:0.65177\ttest-error:0.34764\n",
      "[154]\ttrain-logloss:11.76555\ttrain-auc:0.68078\ttrain-error:0.31936\ttest-logloss:12.79385\ttest-auc:0.65214\ttest-error:0.34727\n",
      "[155]\ttrain-logloss:11.77607\ttrain-auc:0.68049\ttrain-error:0.31964\ttest-logloss:12.80754\ttest-auc:0.65177\ttest-error:0.34764\n",
      "[156]\ttrain-logloss:11.77457\ttrain-auc:0.68053\ttrain-error:0.31960\ttest-logloss:12.80069\ttest-auc:0.65196\ttest-error:0.34745\n",
      "[157]\ttrain-logloss:11.76405\ttrain-auc:0.68082\ttrain-error:0.31932\ttest-logloss:12.80754\ttest-auc:0.65177\ttest-error:0.34764\n",
      "[158]\ttrain-logloss:11.76555\ttrain-auc:0.68078\ttrain-error:0.31936\ttest-logloss:12.75962\ttest-auc:0.65308\ttest-error:0.34634\n",
      "[159]\ttrain-logloss:11.75053\ttrain-auc:0.68119\ttrain-error:0.31895\ttest-logloss:12.79385\ttest-auc:0.65215\ttest-error:0.34727\n",
      "[160]\ttrain-logloss:11.73700\ttrain-auc:0.68155\ttrain-error:0.31858\ttest-logloss:12.78016\ttest-auc:0.65252\ttest-error:0.34690\n",
      "[161]\ttrain-logloss:11.71747\ttrain-auc:0.68208\ttrain-error:0.31805\ttest-logloss:12.73909\ttest-auc:0.65364\ttest-error:0.34578\n",
      "[162]\ttrain-logloss:11.70545\ttrain-auc:0.68241\ttrain-error:0.31773\ttest-logloss:12.75962\ttest-auc:0.65309\ttest-error:0.34634\n",
      "[163]\ttrain-logloss:11.69643\ttrain-auc:0.68265\ttrain-error:0.31748\ttest-logloss:12.73224\ttest-auc:0.65384\ttest-error:0.34560\n",
      "[164]\ttrain-logloss:11.69343\ttrain-auc:0.68273\ttrain-error:0.31740\ttest-logloss:12.72540\ttest-auc:0.65403\ttest-error:0.34541\n",
      "[165]\ttrain-logloss:11.65286\ttrain-auc:0.68383\ttrain-error:0.31630\ttest-logloss:12.71171\ttest-auc:0.65439\ttest-error:0.34504\n",
      "[166]\ttrain-logloss:11.62882\ttrain-auc:0.68449\ttrain-error:0.31565\ttest-logloss:12.71171\ttest-auc:0.65440\ttest-error:0.34504\n",
      "[167]\ttrain-logloss:11.63332\ttrain-auc:0.68436\ttrain-error:0.31577\ttest-logloss:12.71171\ttest-auc:0.65440\ttest-error:0.34504\n",
      "[168]\ttrain-logloss:11.60778\ttrain-auc:0.68506\ttrain-error:0.31507\ttest-logloss:12.70486\ttest-auc:0.65459\ttest-error:0.34485\n",
      "[169]\ttrain-logloss:11.60477\ttrain-auc:0.68514\ttrain-error:0.31499\ttest-logloss:12.69802\ttest-auc:0.65478\ttest-error:0.34467\n",
      "[170]\ttrain-logloss:11.59876\ttrain-auc:0.68530\ttrain-error:0.31483\ttest-logloss:12.73909\ttest-auc:0.65366\ttest-error:0.34578\n",
      "[171]\ttrain-logloss:11.58524\ttrain-auc:0.68567\ttrain-error:0.31446\ttest-logloss:12.71855\ttest-auc:0.65422\ttest-error:0.34523\n",
      "[172]\ttrain-logloss:11.57923\ttrain-auc:0.68583\ttrain-error:0.31430\ttest-logloss:12.69117\ttest-auc:0.65497\ttest-error:0.34448\n",
      "[173]\ttrain-logloss:11.56270\ttrain-auc:0.68628\ttrain-error:0.31385\ttest-logloss:12.69802\ttest-auc:0.65478\ttest-error:0.34467\n",
      "[174]\ttrain-logloss:11.56270\ttrain-auc:0.68628\ttrain-error:0.31385\ttest-logloss:12.69117\ttest-auc:0.65497\ttest-error:0.34448\n",
      "[175]\ttrain-logloss:11.55218\ttrain-auc:0.68656\ttrain-error:0.31357\ttest-logloss:12.67064\ttest-auc:0.65553\ttest-error:0.34392\n",
      "[176]\ttrain-logloss:11.54467\ttrain-auc:0.68677\ttrain-error:0.31336\ttest-logloss:12.67064\ttest-auc:0.65553\ttest-error:0.34392\n",
      "[177]\ttrain-logloss:11.54467\ttrain-auc:0.68676\ttrain-error:0.31336\ttest-logloss:12.67748\ttest-auc:0.65534\ttest-error:0.34411\n",
      "[178]\ttrain-logloss:11.54317\ttrain-auc:0.68681\ttrain-error:0.31332\ttest-logloss:12.67064\ttest-auc:0.65553\ttest-error:0.34392\n",
      "[179]\ttrain-logloss:11.50109\ttrain-auc:0.68795\ttrain-error:0.31218\ttest-logloss:12.66379\ttest-auc:0.65571\ttest-error:0.34374\n",
      "[180]\ttrain-logloss:11.51011\ttrain-auc:0.68770\ttrain-error:0.31242\ttest-logloss:12.67064\ttest-auc:0.65553\ttest-error:0.34392\n",
      "[181]\ttrain-logloss:11.51161\ttrain-auc:0.68766\ttrain-error:0.31246\ttest-logloss:12.66379\ttest-auc:0.65571\ttest-error:0.34374\n",
      "[182]\ttrain-logloss:11.49358\ttrain-auc:0.68815\ttrain-error:0.31198\ttest-logloss:12.65010\ttest-auc:0.65609\ttest-error:0.34337\n",
      "[183]\ttrain-logloss:11.48306\ttrain-auc:0.68844\ttrain-error:0.31169\ttest-logloss:12.66379\ttest-auc:0.65572\ttest-error:0.34374\n",
      "[184]\ttrain-logloss:11.48757\ttrain-auc:0.68831\ttrain-error:0.31181\ttest-logloss:12.66379\ttest-auc:0.65572\ttest-error:0.34374\n",
      "[185]\ttrain-logloss:11.47705\ttrain-auc:0.68860\ttrain-error:0.31153\ttest-logloss:12.65010\ttest-auc:0.65609\ttest-error:0.34337\n",
      "[186]\ttrain-logloss:11.46954\ttrain-auc:0.68880\ttrain-error:0.31132\ttest-logloss:12.65010\ttest-auc:0.65609\ttest-error:0.34337\n",
      "[187]\ttrain-logloss:11.46353\ttrain-auc:0.68897\ttrain-error:0.31116\ttest-logloss:12.62272\ttest-auc:0.65684\ttest-error:0.34262\n",
      "[188]\ttrain-logloss:11.45752\ttrain-auc:0.68913\ttrain-error:0.31100\ttest-logloss:12.61587\ttest-auc:0.65703\ttest-error:0.34244\n",
      "[189]\ttrain-logloss:11.45301\ttrain-auc:0.68925\ttrain-error:0.31087\ttest-logloss:12.61587\ttest-auc:0.65703\ttest-error:0.34244\n",
      "[190]\ttrain-logloss:11.43347\ttrain-auc:0.68978\ttrain-error:0.31034\ttest-logloss:12.60903\ttest-auc:0.65722\ttest-error:0.34225\n",
      "[191]\ttrain-logloss:11.41394\ttrain-auc:0.69031\ttrain-error:0.30981\ttest-logloss:12.61587\ttest-auc:0.65704\ttest-error:0.34244\n",
      "[192]\ttrain-logloss:11.40342\ttrain-auc:0.69059\ttrain-error:0.30953\ttest-logloss:12.61587\ttest-auc:0.65704\ttest-error:0.34244\n",
      "[193]\ttrain-logloss:11.39441\ttrain-auc:0.69084\ttrain-error:0.30928\ttest-logloss:12.62272\ttest-auc:0.65684\ttest-error:0.34262\n",
      "[194]\ttrain-logloss:11.36586\ttrain-auc:0.69161\ttrain-error:0.30851\ttest-logloss:12.61587\ttest-auc:0.65704\ttest-error:0.34244\n",
      "[195]\ttrain-logloss:11.36586\ttrain-auc:0.69161\ttrain-error:0.30851\ttest-logloss:12.62956\ttest-auc:0.65667\ttest-error:0.34281\n",
      "[196]\ttrain-logloss:11.34933\ttrain-auc:0.69206\ttrain-error:0.30806\ttest-logloss:12.62956\ttest-auc:0.65667\ttest-error:0.34281\n",
      "[197]\ttrain-logloss:11.33881\ttrain-auc:0.69235\ttrain-error:0.30777\ttest-logloss:12.62956\ttest-auc:0.65667\ttest-error:0.34281\n",
      "[198]\ttrain-logloss:11.33580\ttrain-auc:0.69243\ttrain-error:0.30769\ttest-logloss:12.61587\ttest-auc:0.65704\ttest-error:0.34244\n",
      "[199]\ttrain-logloss:11.32679\ttrain-auc:0.69267\ttrain-error:0.30745\ttest-logloss:12.60218\ttest-auc:0.65741\ttest-error:0.34207\n",
      "[200]\ttrain-logloss:11.33130\ttrain-auc:0.69255\ttrain-error:0.30757\ttest-logloss:12.60903\ttest-auc:0.65722\ttest-error:0.34225\n",
      "[201]\ttrain-logloss:11.32679\ttrain-auc:0.69267\ttrain-error:0.30745\ttest-logloss:12.58165\ttest-auc:0.65797\ttest-error:0.34151\n",
      "[202]\ttrain-logloss:11.31777\ttrain-auc:0.69292\ttrain-error:0.30720\ttest-logloss:12.59534\ttest-auc:0.65760\ttest-error:0.34188\n",
      "[203]\ttrain-logloss:11.31477\ttrain-auc:0.69300\ttrain-error:0.30712\ttest-logloss:12.60218\ttest-auc:0.65741\ttest-error:0.34207\n",
      "[204]\ttrain-logloss:11.30876\ttrain-auc:0.69316\ttrain-error:0.30696\ttest-logloss:12.59534\ttest-auc:0.65760\ttest-error:0.34188\n",
      "[205]\ttrain-logloss:11.31477\ttrain-auc:0.69300\ttrain-error:0.30712\ttest-logloss:12.59534\ttest-auc:0.65760\ttest-error:0.34188\n",
      "[206]\ttrain-logloss:11.29824\ttrain-auc:0.69345\ttrain-error:0.30667\ttest-logloss:12.62272\ttest-auc:0.65685\ttest-error:0.34262\n",
      "[207]\ttrain-logloss:11.28772\ttrain-auc:0.69373\ttrain-error:0.30639\ttest-logloss:12.62956\ttest-auc:0.65667\ttest-error:0.34281\n",
      "[208]\ttrain-logloss:11.28021\ttrain-auc:0.69394\ttrain-error:0.30618\ttest-logloss:12.62956\ttest-auc:0.65667\ttest-error:0.34281\n",
      "[209]\ttrain-logloss:11.25617\ttrain-auc:0.69459\ttrain-error:0.30553\ttest-logloss:12.65010\ttest-auc:0.65611\ttest-error:0.34337\n",
      "[210]\ttrain-logloss:11.25466\ttrain-auc:0.69463\ttrain-error:0.30549\ttest-logloss:12.60903\ttest-auc:0.65725\ttest-error:0.34225\n",
      "[211]\ttrain-logloss:11.24414\ttrain-auc:0.69492\ttrain-error:0.30520\ttest-logloss:12.60903\ttest-auc:0.65724\ttest-error:0.34225\n",
      "[212]\ttrain-logloss:11.21710\ttrain-auc:0.69565\ttrain-error:0.30447\ttest-logloss:12.60218\ttest-auc:0.65742\ttest-error:0.34207\n",
      "[213]\ttrain-logloss:11.19005\ttrain-auc:0.69638\ttrain-error:0.30374\ttest-logloss:12.59534\ttest-auc:0.65761\ttest-error:0.34188\n",
      "[214]\ttrain-logloss:11.17502\ttrain-auc:0.69679\ttrain-error:0.30333\ttest-logloss:12.59534\ttest-auc:0.65761\ttest-error:0.34188\n",
      "[215]\ttrain-logloss:11.16150\ttrain-auc:0.69716\ttrain-error:0.30296\ttest-logloss:12.54058\ttest-auc:0.65910\ttest-error:0.34039\n",
      "[216]\ttrain-logloss:11.16751\ttrain-auc:0.69699\ttrain-error:0.30312\ttest-logloss:12.56111\ttest-auc:0.65854\ttest-error:0.34095\n",
      "[217]\ttrain-logloss:11.13445\ttrain-auc:0.69789\ttrain-error:0.30223\ttest-logloss:12.55426\ttest-auc:0.65873\ttest-error:0.34077\n",
      "[218]\ttrain-logloss:11.13145\ttrain-auc:0.69797\ttrain-error:0.30214\ttest-logloss:12.56111\ttest-auc:0.65855\ttest-error:0.34095\n",
      "[219]\ttrain-logloss:11.12243\ttrain-auc:0.69822\ttrain-error:0.30190\ttest-logloss:12.55426\ttest-auc:0.65874\ttest-error:0.34077\n",
      "[220]\ttrain-logloss:11.12694\ttrain-auc:0.69809\ttrain-error:0.30202\ttest-logloss:12.52689\ttest-auc:0.65948\ttest-error:0.34002\n",
      "[221]\ttrain-logloss:11.11642\ttrain-auc:0.69838\ttrain-error:0.30174\ttest-logloss:12.52004\ttest-auc:0.65967\ttest-error:0.33984\n",
      "[222]\ttrain-logloss:11.11191\ttrain-auc:0.69850\ttrain-error:0.30162\ttest-logloss:12.55426\ttest-auc:0.65874\ttest-error:0.34077\n",
      "[223]\ttrain-logloss:11.10891\ttrain-auc:0.69858\ttrain-error:0.30153\ttest-logloss:12.60218\ttest-auc:0.65744\ttest-error:0.34207\n",
      "[224]\ttrain-logloss:11.09238\ttrain-auc:0.69903\ttrain-error:0.30108\ttest-logloss:12.58849\ttest-auc:0.65781\ttest-error:0.34170\n",
      "[225]\ttrain-logloss:11.08787\ttrain-auc:0.69915\ttrain-error:0.30096\ttest-logloss:12.57480\ttest-auc:0.65818\ttest-error:0.34132\n",
      "[226]\ttrain-logloss:11.07585\ttrain-auc:0.69948\ttrain-error:0.30064\ttest-logloss:12.56796\ttest-auc:0.65836\ttest-error:0.34114\n",
      "[227]\ttrain-logloss:11.08036\ttrain-auc:0.69936\ttrain-error:0.30076\ttest-logloss:12.57480\ttest-auc:0.65818\ttest-error:0.34132\n",
      "[228]\ttrain-logloss:11.08186\ttrain-auc:0.69932\ttrain-error:0.30080\ttest-logloss:12.56111\ttest-auc:0.65856\ttest-error:0.34095\n",
      "[229]\ttrain-logloss:11.07585\ttrain-auc:0.69948\ttrain-error:0.30064\ttest-logloss:12.52689\ttest-auc:0.65949\ttest-error:0.34002\n",
      "[230]\ttrain-logloss:11.03828\ttrain-auc:0.70050\ttrain-error:0.29962\ttest-logloss:12.52004\ttest-auc:0.65968\ttest-error:0.33984\n",
      "[231]\ttrain-logloss:11.01875\ttrain-auc:0.70103\ttrain-error:0.29909\ttest-logloss:12.53373\ttest-auc:0.65930\ttest-error:0.34021\n",
      "[232]\ttrain-logloss:11.01424\ttrain-auc:0.70115\ttrain-error:0.29896\ttest-logloss:12.52004\ttest-auc:0.65968\ttest-error:0.33984\n",
      "[233]\ttrain-logloss:11.01124\ttrain-auc:0.70123\ttrain-error:0.29888\ttest-logloss:12.52689\ttest-auc:0.65949\ttest-error:0.34002\n",
      "[234]\ttrain-logloss:10.99020\ttrain-auc:0.70180\ttrain-error:0.29831\ttest-logloss:12.54742\ttest-auc:0.65893\ttest-error:0.34058\n",
      "[235]\ttrain-logloss:11.00072\ttrain-auc:0.70151\ttrain-error:0.29860\ttest-logloss:12.57480\ttest-auc:0.65820\ttest-error:0.34132\n",
      "[236]\ttrain-logloss:10.98870\ttrain-auc:0.70184\ttrain-error:0.29827\ttest-logloss:12.55426\ttest-auc:0.65875\ttest-error:0.34077\n",
      "[237]\ttrain-logloss:10.97517\ttrain-auc:0.70221\ttrain-error:0.29790\ttest-logloss:12.53373\ttest-auc:0.65931\ttest-error:0.34021\n",
      "[238]\ttrain-logloss:10.96766\ttrain-auc:0.70241\ttrain-error:0.29770\ttest-logloss:12.56796\ttest-auc:0.65839\ttest-error:0.34114\n",
      "[239]\ttrain-logloss:10.96315\ttrain-auc:0.70254\ttrain-error:0.29758\ttest-logloss:12.58165\ttest-auc:0.65802\ttest-error:0.34151\n",
      "[240]\ttrain-logloss:10.96315\ttrain-auc:0.70253\ttrain-error:0.29758\ttest-logloss:12.57480\ttest-auc:0.65820\ttest-error:0.34132\n",
      "[241]\ttrain-logloss:10.94813\ttrain-auc:0.70294\ttrain-error:0.29717\ttest-logloss:12.56796\ttest-auc:0.65839\ttest-error:0.34114\n",
      "[242]\ttrain-logloss:10.93611\ttrain-auc:0.70327\ttrain-error:0.29684\ttest-logloss:12.58849\ttest-auc:0.65783\ttest-error:0.34170\n",
      "[243]\ttrain-logloss:10.92559\ttrain-auc:0.70355\ttrain-error:0.29656\ttest-logloss:12.57480\ttest-auc:0.65822\ttest-error:0.34132\n",
      "[244]\ttrain-logloss:10.90756\ttrain-auc:0.70404\ttrain-error:0.29607\ttest-logloss:12.58849\ttest-auc:0.65785\ttest-error:0.34170\n",
      "[245]\ttrain-logloss:10.89554\ttrain-auc:0.70437\ttrain-error:0.29574\ttest-logloss:12.60903\ttest-auc:0.65729\ttest-error:0.34225\n",
      "[246]\ttrain-logloss:10.88051\ttrain-auc:0.70478\ttrain-error:0.29533\ttest-logloss:12.60218\ttest-auc:0.65747\ttest-error:0.34207\n",
      "[247]\ttrain-logloss:10.87450\ttrain-auc:0.70494\ttrain-error:0.29517\ttest-logloss:12.58165\ttest-auc:0.65803\ttest-error:0.34151\n",
      "[248]\ttrain-logloss:10.85797\ttrain-auc:0.70539\ttrain-error:0.29472\ttest-logloss:12.56111\ttest-auc:0.65859\ttest-error:0.34095\n",
      "[249]\ttrain-logloss:10.85496\ttrain-auc:0.70547\ttrain-error:0.29464\ttest-logloss:12.55426\ttest-auc:0.65877\ttest-error:0.34077\n",
      "250-rounds Training finished ...\t\t(1.344s)\n"
     ]
    }
   ],
   "source": [
    "num_round = 250\n",
    "t0 = time()\n",
    "sh_bst_sm = xgb.train(param_sh, xg_train_sh, num_round, watchlist, early_stopping_rounds=60)\n",
    "print(f\"{num_round}-rounds Training finished ...\\t\\t({time()-t0:.3f}s)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test error using softmax = 0.34076551467855815\n"
     ]
    }
   ],
   "source": [
    "# get prediction\n",
    "pred_sh = sh_bst_sm.predict(xg_test_sh)\n",
    "error_rate = np.sum(pred_sh != y_test_sh) / y_test_sh.shape[0]\n",
    "print('Test error using softmax = {}'.format(error_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Counter({0.0: 2309, 1.0: 3073}), Counter({1: 2709, 0: 2673}))"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(pred_sh), Counter(y_test_sh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.59      0.63      2673\n",
      "           1       0.64      0.73      0.68      2709\n",
      "\n",
      "    accuracy                           0.66      5382\n",
      "   macro avg       0.66      0.66      0.66      5382\n",
      "weighted avg       0.66      0.66      0.66      5382\n",
      "\n"
     ]
    }
   ],
   "source": [
    "report_sh = metrics.classification_report(list(y_test_sh), list(pred_sh))\n",
    "print(report_sh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.65876682, 0.65876682])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aucs_sh = auc(y_test_sh.astype(np.uint8), pred_sh.astype(np.uint8), [0, 1])\n",
    "aucs_sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "del X_train_sh, y_train_sh, X_test_sh, y_test_sh, xg_train_sh, xg_test_sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 调参"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "base_param_sh = {  # 基本参数，不需要调参\n",
    "    'objective': 'binary:hinge',\n",
    "    'eta': 0.1,\n",
    "    'nthread': 8,\n",
    "#     'num_class': 10,\n",
    "    'gpu_id': 0,\n",
    "    'tree_method': 'gpu_hist',\n",
    "    'eval_metric': ['logloss', 'auc', 'error']\n",
    "} \n",
    "# 需要调参的参数\n",
    "ps1 = {  \n",
    "    'max_depth': list(range(5, 10, 1)),\n",
    "    'min_child_weight': list(range(1, 10, 2)),\n",
    "}\n",
    "\n",
    "ps2 = {\n",
    "    'gamma': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6]\n",
    "}\n",
    "\n",
    "ps3 = {\n",
    "    'subsample':  [i/10.0 for i in range(6,11,1)], \n",
    "    'colsample_bytree':  [i/10.0 for i in range(6,11,1)] \n",
    "}\n",
    "\n",
    "ps4 = {'reg_alpha': [0, 0.1, 0.2, 0.5, 1, 1.5, 2, 4]}\n",
    "\n",
    "\n",
    "# com_ps_sh = list(ParameterGrid(ps_sh))\n",
    "\n",
    "\n",
    "# all_params_sh = [base_param_sh.copy() for _ in range(len(com_ps_sh))] \n",
    "# for i in range(len(com_ps_sh)):\n",
    "#     all_params_sh[i].update(com_ps_sh[i])\n",
    "\n",
    "# # print(com_ps_sh)\n",
    "# print(all_params_sh.__len__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 550,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data preparing finished ...\t\t(24.924s)\n"
     ]
    }
   ],
   "source": [
    "t0 = time()\n",
    "xg_train_sh = xgb.DMatrix(X_train_sh.values, label=y_train_sh.values, enable_categorical=True)\n",
    "xg_test_sh = xgb.DMatrix(X_test_sh.values, label=y_test_sh.values, enable_categorical=True)\n",
    "print(f\"Data preparing finished ...\\t\\t({time()-t0:.3f}s)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始搜索：max_depth=[5, 6, 7, 8, 9], min_child_weight=[1, 3, 5, 7, 9]\n",
      "待搜索的参数组合数量：25\n",
      "1 / 25: 150-rounds Training finished param={'objective': 'binary:hinge', 'eta': 0.1, 'nthread': 8, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['logloss', 'auc', 'error'], 'max_depth': 5, 'min_child_weight': 1} ...\t\t(3.035s)\n",
      "2 / 25: 150-rounds Training finished param={'objective': 'binary:hinge', 'eta': 0.1, 'nthread': 8, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['logloss', 'auc', 'error'], 'max_depth': 6, 'min_child_weight': 1} ...\t\t(4.271s)\n",
      "3 / 25: 150-rounds Training finished param={'objective': 'binary:hinge', 'eta': 0.1, 'nthread': 8, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['logloss', 'auc', 'error'], 'max_depth': 7, 'min_child_weight': 1} ...\t\t(6.333s)\n",
      "4 / 25: 150-rounds Training finished param={'objective': 'binary:hinge', 'eta': 0.1, 'nthread': 8, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['logloss', 'auc', 'error'], 'max_depth': 8, 'min_child_weight': 1} ...\t\t(9.294s)\n",
      "5 / 25: 150-rounds Training finished param={'objective': 'binary:hinge', 'eta': 0.1, 'nthread': 8, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['logloss', 'auc', 'error'], 'max_depth': 9, 'min_child_weight': 1} ...\t\t(12.788s)\n",
      "6 / 25: 150-rounds Training finished param={'objective': 'binary:hinge', 'eta': 0.1, 'nthread': 8, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['logloss', 'auc', 'error'], 'max_depth': 5, 'min_child_weight': 3} ...\t\t(2.993s)\n",
      "7 / 25: 150-rounds Training finished param={'objective': 'binary:hinge', 'eta': 0.1, 'nthread': 8, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['logloss', 'auc', 'error'], 'max_depth': 6, 'min_child_weight': 3} ...\t\t(4.183s)\n",
      "8 / 25: 150-rounds Training finished param={'objective': 'binary:hinge', 'eta': 0.1, 'nthread': 8, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['logloss', 'auc', 'error'], 'max_depth': 7, 'min_child_weight': 3} ...\t\t(6.127s)\n",
      "9 / 25: 150-rounds Training finished param={'objective': 'binary:hinge', 'eta': 0.1, 'nthread': 8, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['logloss', 'auc', 'error'], 'max_depth': 8, 'min_child_weight': 3} ...\t\t(9.131s)\n",
      "10 / 25: 150-rounds Training finished param={'objective': 'binary:hinge', 'eta': 0.1, 'nthread': 8, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['logloss', 'auc', 'error'], 'max_depth': 9, 'min_child_weight': 3} ...\t\t(12.637s)\n",
      "11 / 25: 150-rounds Training finished param={'objective': 'binary:hinge', 'eta': 0.1, 'nthread': 8, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['logloss', 'auc', 'error'], 'max_depth': 5, 'min_child_weight': 5} ...\t\t(2.996s)\n",
      "12 / 25: 150-rounds Training finished param={'objective': 'binary:hinge', 'eta': 0.1, 'nthread': 8, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['logloss', 'auc', 'error'], 'max_depth': 6, 'min_child_weight': 5} ...\t\t(4.244s)\n",
      "13 / 25: 150-rounds Training finished param={'objective': 'binary:hinge', 'eta': 0.1, 'nthread': 8, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['logloss', 'auc', 'error'], 'max_depth': 7, 'min_child_weight': 5} ...\t\t(6.028s)\n",
      "14 / 25: 150-rounds Training finished param={'objective': 'binary:hinge', 'eta': 0.1, 'nthread': 8, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['logloss', 'auc', 'error'], 'max_depth': 8, 'min_child_weight': 5} ...\t\t(8.756s)\n",
      "15 / 25: 150-rounds Training finished param={'objective': 'binary:hinge', 'eta': 0.1, 'nthread': 8, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['logloss', 'auc', 'error'], 'max_depth': 9, 'min_child_weight': 5} ...\t\t(12.128s)\n",
      "16 / 25: 150-rounds Training finished param={'objective': 'binary:hinge', 'eta': 0.1, 'nthread': 8, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['logloss', 'auc', 'error'], 'max_depth': 5, 'min_child_weight': 7} ...\t\t(2.959s)\n",
      "17 / 25: 150-rounds Training finished param={'objective': 'binary:hinge', 'eta': 0.1, 'nthread': 8, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['logloss', 'auc', 'error'], 'max_depth': 6, 'min_child_weight': 7} ...\t\t(4.105s)\n",
      "18 / 25: 150-rounds Training finished param={'objective': 'binary:hinge', 'eta': 0.1, 'nthread': 8, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['logloss', 'auc', 'error'], 'max_depth': 7, 'min_child_weight': 7} ...\t\t(5.897s)\n",
      "19 / 25: 150-rounds Training finished param={'objective': 'binary:hinge', 'eta': 0.1, 'nthread': 8, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['logloss', 'auc', 'error'], 'max_depth': 8, 'min_child_weight': 7} ...\t\t(8.433s)\n",
      "20 / 25: 150-rounds Training finished param={'objective': 'binary:hinge', 'eta': 0.1, 'nthread': 8, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['logloss', 'auc', 'error'], 'max_depth': 9, 'min_child_weight': 7} ...\t\t(11.471s)\n",
      "21 / 25: 150-rounds Training finished param={'objective': 'binary:hinge', 'eta': 0.1, 'nthread': 8, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['logloss', 'auc', 'error'], 'max_depth': 5, 'min_child_weight': 9} ...\t\t(2.935s)\n",
      "22 / 25: 150-rounds Training finished param={'objective': 'binary:hinge', 'eta': 0.1, 'nthread': 8, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['logloss', 'auc', 'error'], 'max_depth': 6, 'min_child_weight': 9} ...\t\t(4.050s)\n",
      "23 / 25: 150-rounds Training finished param={'objective': 'binary:hinge', 'eta': 0.1, 'nthread': 8, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['logloss', 'auc', 'error'], 'max_depth': 7, 'min_child_weight': 9} ...\t\t(5.802s)\n",
      "24 / 25: 150-rounds Training finished param={'objective': 'binary:hinge', 'eta': 0.1, 'nthread': 8, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['logloss', 'auc', 'error'], 'max_depth': 8, 'min_child_weight': 9} ...\t\t(8.152s)\n",
      "25 / 25: 150-rounds Training finished param={'objective': 'binary:hinge', 'eta': 0.1, 'nthread': 8, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['logloss', 'auc', 'error'], 'max_depth': 9, 'min_child_weight': 9} ...\t\t(11.286s)\n",
      "eta=0.1, eval_metric=['logloss', 'auc', 'error'], gpu_id=0, max_depth=5, min_child_weight=1, nthread=8, objective=binary:hinge, tree_method=gpu_hist\n",
      "eta=0.1, eval_metric=['logloss', 'auc', 'error'], gpu_id=0, max_depth=7, min_child_weight=9, nthread=8, objective=binary:hinge, tree_method=gpu_hist\n",
      "开始搜索：gamma=[0.1, 0.2, 0.3, 0.4, 0.5, 0.6]\n",
      "待搜索的参数组合数量：6\n",
      "1 / 6: 150-rounds Training finished param={'objective': 'binary:hinge', 'eta': 0.1, 'nthread': 8, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['logloss', 'auc', 'error'], 'max_depth': 5, 'min_child_weight': 1, 'gamma': 0.1} ...\t\t(3.033s)\n",
      "2 / 6: 150-rounds Training finished param={'objective': 'binary:hinge', 'eta': 0.1, 'nthread': 8, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['logloss', 'auc', 'error'], 'max_depth': 5, 'min_child_weight': 1, 'gamma': 0.2} ...\t\t(3.015s)\n",
      "3 / 6: 150-rounds Training finished param={'objective': 'binary:hinge', 'eta': 0.1, 'nthread': 8, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['logloss', 'auc', 'error'], 'max_depth': 5, 'min_child_weight': 1, 'gamma': 0.3} ...\t\t(3.015s)\n",
      "4 / 6: 150-rounds Training finished param={'objective': 'binary:hinge', 'eta': 0.1, 'nthread': 8, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['logloss', 'auc', 'error'], 'max_depth': 5, 'min_child_weight': 1, 'gamma': 0.4} ...\t\t(3.057s)\n",
      "5 / 6: 150-rounds Training finished param={'objective': 'binary:hinge', 'eta': 0.1, 'nthread': 8, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['logloss', 'auc', 'error'], 'max_depth': 5, 'min_child_weight': 1, 'gamma': 0.5} ...\t\t(3.028s)\n",
      "6 / 6: 150-rounds Training finished param={'objective': 'binary:hinge', 'eta': 0.1, 'nthread': 8, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['logloss', 'auc', 'error'], 'max_depth': 5, 'min_child_weight': 1, 'gamma': 0.6} ...\t\t(3.047s)\n",
      "eta=0.1, eval_metric=['logloss', 'auc', 'error'], gamma=0.1, gpu_id=0, max_depth=5, min_child_weight=1, nthread=8, objective=binary:hinge, tree_method=gpu_hist\n",
      "eta=0.1, eval_metric=['logloss', 'auc', 'error'], gamma=0.3, gpu_id=0, max_depth=5, min_child_weight=1, nthread=8, objective=binary:hinge, tree_method=gpu_hist\n",
      "开始搜索：colsample_bytree=[0.6, 0.7, 0.8, 0.9, 1.0], subsample=[0.6, 0.7, 0.8, 0.9, 1.0]\n",
      "待搜索的参数组合数量：25\n",
      "1 / 25: 150-rounds Training finished param={'objective': 'binary:hinge', 'eta': 0.1, 'nthread': 8, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['logloss', 'auc', 'error'], 'max_depth': 5, 'min_child_weight': 1, 'gamma': 0.1, 'subsample': 0.6, 'colsample_bytree': 0.6} ...\t\t(2.884s)\n",
      "2 / 25: 150-rounds Training finished param={'objective': 'binary:hinge', 'eta': 0.1, 'nthread': 8, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['logloss', 'auc', 'error'], 'max_depth': 5, 'min_child_weight': 1, 'gamma': 0.1, 'subsample': 0.7, 'colsample_bytree': 0.6} ...\t\t(2.922s)\n",
      "3 / 25: 150-rounds Training finished param={'objective': 'binary:hinge', 'eta': 0.1, 'nthread': 8, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['logloss', 'auc', 'error'], 'max_depth': 5, 'min_child_weight': 1, 'gamma': 0.1, 'subsample': 0.8, 'colsample_bytree': 0.6} ...\t\t(2.947s)\n",
      "4 / 25: 150-rounds Training finished param={'objective': 'binary:hinge', 'eta': 0.1, 'nthread': 8, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['logloss', 'auc', 'error'], 'max_depth': 5, 'min_child_weight': 1, 'gamma': 0.1, 'subsample': 0.9, 'colsample_bytree': 0.6} ...\t\t(2.897s)\n",
      "5 / 25: 150-rounds Training finished param={'objective': 'binary:hinge', 'eta': 0.1, 'nthread': 8, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['logloss', 'auc', 'error'], 'max_depth': 5, 'min_child_weight': 1, 'gamma': 0.1, 'subsample': 1.0, 'colsample_bytree': 0.6} ...\t\t(2.930s)\n",
      "6 / 25: 150-rounds Training finished param={'objective': 'binary:hinge', 'eta': 0.1, 'nthread': 8, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['logloss', 'auc', 'error'], 'max_depth': 5, 'min_child_weight': 1, 'gamma': 0.1, 'subsample': 0.6, 'colsample_bytree': 0.7} ...\t\t(2.918s)\n",
      "7 / 25: 150-rounds Training finished param={'objective': 'binary:hinge', 'eta': 0.1, 'nthread': 8, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['logloss', 'auc', 'error'], 'max_depth': 5, 'min_child_weight': 1, 'gamma': 0.1, 'subsample': 0.7, 'colsample_bytree': 0.7} ...\t\t(2.911s)\n",
      "8 / 25: 150-rounds Training finished param={'objective': 'binary:hinge', 'eta': 0.1, 'nthread': 8, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['logloss', 'auc', 'error'], 'max_depth': 5, 'min_child_weight': 1, 'gamma': 0.1, 'subsample': 0.8, 'colsample_bytree': 0.7} ...\t\t(2.913s)\n",
      "9 / 25: 150-rounds Training finished param={'objective': 'binary:hinge', 'eta': 0.1, 'nthread': 8, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['logloss', 'auc', 'error'], 'max_depth': 5, 'min_child_weight': 1, 'gamma': 0.1, 'subsample': 0.9, 'colsample_bytree': 0.7} ...\t\t(2.894s)\n",
      "10 / 25: 150-rounds Training finished param={'objective': 'binary:hinge', 'eta': 0.1, 'nthread': 8, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['logloss', 'auc', 'error'], 'max_depth': 5, 'min_child_weight': 1, 'gamma': 0.1, 'subsample': 1.0, 'colsample_bytree': 0.7} ...\t\t(2.918s)\n",
      "11 / 25: 150-rounds Training finished param={'objective': 'binary:hinge', 'eta': 0.1, 'nthread': 8, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['logloss', 'auc', 'error'], 'max_depth': 5, 'min_child_weight': 1, 'gamma': 0.1, 'subsample': 0.6, 'colsample_bytree': 0.8} ...\t\t(2.878s)\n",
      "12 / 25: 150-rounds Training finished param={'objective': 'binary:hinge', 'eta': 0.1, 'nthread': 8, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['logloss', 'auc', 'error'], 'max_depth': 5, 'min_child_weight': 1, 'gamma': 0.1, 'subsample': 0.7, 'colsample_bytree': 0.8} ...\t\t(2.942s)\n",
      "13 / 25: 150-rounds Training finished param={'objective': 'binary:hinge', 'eta': 0.1, 'nthread': 8, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['logloss', 'auc', 'error'], 'max_depth': 5, 'min_child_weight': 1, 'gamma': 0.1, 'subsample': 0.8, 'colsample_bytree': 0.8} ...\t\t(2.914s)\n",
      "14 / 25: 150-rounds Training finished param={'objective': 'binary:hinge', 'eta': 0.1, 'nthread': 8, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['logloss', 'auc', 'error'], 'max_depth': 5, 'min_child_weight': 1, 'gamma': 0.1, 'subsample': 0.9, 'colsample_bytree': 0.8} ...\t\t(2.922s)\n",
      "15 / 25: 150-rounds Training finished param={'objective': 'binary:hinge', 'eta': 0.1, 'nthread': 8, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['logloss', 'auc', 'error'], 'max_depth': 5, 'min_child_weight': 1, 'gamma': 0.1, 'subsample': 1.0, 'colsample_bytree': 0.8} ...\t\t(2.936s)\n",
      "16 / 25: 150-rounds Training finished param={'objective': 'binary:hinge', 'eta': 0.1, 'nthread': 8, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['logloss', 'auc', 'error'], 'max_depth': 5, 'min_child_weight': 1, 'gamma': 0.1, 'subsample': 0.6, 'colsample_bytree': 0.9} ...\t\t(2.921s)\n",
      "17 / 25: 150-rounds Training finished param={'objective': 'binary:hinge', 'eta': 0.1, 'nthread': 8, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['logloss', 'auc', 'error'], 'max_depth': 5, 'min_child_weight': 1, 'gamma': 0.1, 'subsample': 0.7, 'colsample_bytree': 0.9} ...\t\t(2.938s)\n",
      "18 / 25: 150-rounds Training finished param={'objective': 'binary:hinge', 'eta': 0.1, 'nthread': 8, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['logloss', 'auc', 'error'], 'max_depth': 5, 'min_child_weight': 1, 'gamma': 0.1, 'subsample': 0.8, 'colsample_bytree': 0.9} ...\t\t(2.965s)\n",
      "19 / 25: 150-rounds Training finished param={'objective': 'binary:hinge', 'eta': 0.1, 'nthread': 8, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['logloss', 'auc', 'error'], 'max_depth': 5, 'min_child_weight': 1, 'gamma': 0.1, 'subsample': 0.9, 'colsample_bytree': 0.9} ...\t\t(3.075s)\n",
      "20 / 25: 150-rounds Training finished param={'objective': 'binary:hinge', 'eta': 0.1, 'nthread': 8, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['logloss', 'auc', 'error'], 'max_depth': 5, 'min_child_weight': 1, 'gamma': 0.1, 'subsample': 1.0, 'colsample_bytree': 0.9} ...\t\t(2.903s)\n",
      "21 / 25: 150-rounds Training finished param={'objective': 'binary:hinge', 'eta': 0.1, 'nthread': 8, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['logloss', 'auc', 'error'], 'max_depth': 5, 'min_child_weight': 1, 'gamma': 0.1, 'subsample': 0.6, 'colsample_bytree': 1.0} ...\t\t(2.902s)\n",
      "22 / 25: 150-rounds Training finished param={'objective': 'binary:hinge', 'eta': 0.1, 'nthread': 8, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['logloss', 'auc', 'error'], 'max_depth': 5, 'min_child_weight': 1, 'gamma': 0.1, 'subsample': 0.7, 'colsample_bytree': 1.0} ...\t\t(2.910s)\n",
      "23 / 25: 150-rounds Training finished param={'objective': 'binary:hinge', 'eta': 0.1, 'nthread': 8, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['logloss', 'auc', 'error'], 'max_depth': 5, 'min_child_weight': 1, 'gamma': 0.1, 'subsample': 0.8, 'colsample_bytree': 1.0} ...\t\t(2.951s)\n",
      "24 / 25: 150-rounds Training finished param={'objective': 'binary:hinge', 'eta': 0.1, 'nthread': 8, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['logloss', 'auc', 'error'], 'max_depth': 5, 'min_child_weight': 1, 'gamma': 0.1, 'subsample': 0.9, 'colsample_bytree': 1.0} ...\t\t(2.914s)\n",
      "25 / 25: 150-rounds Training finished param={'objective': 'binary:hinge', 'eta': 0.1, 'nthread': 8, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['logloss', 'auc', 'error'], 'max_depth': 5, 'min_child_weight': 1, 'gamma': 0.1, 'subsample': 1.0, 'colsample_bytree': 1.0} ...\t\t(2.935s)\n",
      "colsample_bytree=0.6, eta=0.1, eval_metric=['logloss', 'auc', 'error'], gamma=0.1, gpu_id=0, max_depth=5, min_child_weight=1, nthread=8, objective=binary:hinge, subsample=0.6, tree_method=gpu_hist\n",
      "colsample_bytree=0.6, eta=0.1, eval_metric=['logloss', 'auc', 'error'], gamma=0.1, gpu_id=0, max_depth=5, min_child_weight=1, nthread=8, objective=binary:hinge, subsample=0.8, tree_method=gpu_hist\n",
      "开始搜索：reg_alpha=[0, 0.1, 0.2, 0.5, 1, 1.5, 2, 4]\n",
      "待搜索的参数组合数量：8\n",
      "1 / 8: 150-rounds Training finished param={'objective': 'binary:hinge', 'eta': 0.1, 'nthread': 8, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['logloss', 'auc', 'error'], 'max_depth': 5, 'min_child_weight': 1, 'gamma': 0.1, 'subsample': 0.6, 'colsample_bytree': 0.6, 'reg_alpha': 0} ...\t\t(2.778s)\n",
      "2 / 8: 150-rounds Training finished param={'objective': 'binary:hinge', 'eta': 0.1, 'nthread': 8, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['logloss', 'auc', 'error'], 'max_depth': 5, 'min_child_weight': 1, 'gamma': 0.1, 'subsample': 0.6, 'colsample_bytree': 0.6, 'reg_alpha': 0.1} ...\t\t(2.773s)\n",
      "3 / 8: 150-rounds Training finished param={'objective': 'binary:hinge', 'eta': 0.1, 'nthread': 8, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['logloss', 'auc', 'error'], 'max_depth': 5, 'min_child_weight': 1, 'gamma': 0.1, 'subsample': 0.6, 'colsample_bytree': 0.6, 'reg_alpha': 0.2} ...\t\t(2.782s)\n",
      "4 / 8: 150-rounds Training finished param={'objective': 'binary:hinge', 'eta': 0.1, 'nthread': 8, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['logloss', 'auc', 'error'], 'max_depth': 5, 'min_child_weight': 1, 'gamma': 0.1, 'subsample': 0.6, 'colsample_bytree': 0.6, 'reg_alpha': 0.5} ...\t\t(2.783s)\n",
      "5 / 8: 150-rounds Training finished param={'objective': 'binary:hinge', 'eta': 0.1, 'nthread': 8, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['logloss', 'auc', 'error'], 'max_depth': 5, 'min_child_weight': 1, 'gamma': 0.1, 'subsample': 0.6, 'colsample_bytree': 0.6, 'reg_alpha': 1} ...\t\t(2.803s)\n",
      "6 / 8: 150-rounds Training finished param={'objective': 'binary:hinge', 'eta': 0.1, 'nthread': 8, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['logloss', 'auc', 'error'], 'max_depth': 5, 'min_child_weight': 1, 'gamma': 0.1, 'subsample': 0.6, 'colsample_bytree': 0.6, 'reg_alpha': 1.5} ...\t\t(2.803s)\n",
      "7 / 8: 150-rounds Training finished param={'objective': 'binary:hinge', 'eta': 0.1, 'nthread': 8, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['logloss', 'auc', 'error'], 'max_depth': 5, 'min_child_weight': 1, 'gamma': 0.1, 'subsample': 0.6, 'colsample_bytree': 0.6, 'reg_alpha': 2} ...\t\t(2.817s)\n",
      "8 / 8: 150-rounds Training finished param={'objective': 'binary:hinge', 'eta': 0.1, 'nthread': 8, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['logloss', 'auc', 'error'], 'max_depth': 5, 'min_child_weight': 1, 'gamma': 0.1, 'subsample': 0.6, 'colsample_bytree': 0.6, 'reg_alpha': 4} ...\t\t(2.815s)\n",
      "colsample_bytree=0.6, eta=0.1, eval_metric=['logloss', 'auc', 'error'], gamma=0.1, gpu_id=0, max_depth=5, min_child_weight=1, nthread=8, objective=binary:hinge, reg_alpha=0, subsample=0.6, tree_method=gpu_hist\n",
      "colsample_bytree=0.6, eta=0.1, eval_metric=['logloss', 'auc', 'error'], gamma=0.1, gpu_id=0, max_depth=5, min_child_weight=1, nthread=8, objective=binary:hinge, reg_alpha=0.5, subsample=0.6, tree_method=gpu_hist\n",
      "找到的最棒的参数是：\n",
      "colsample_bytree=0.6, eta=0.1, eval_metric=['logloss', 'auc', 'error'], gamma=0.1, gpu_id=0, max_depth=5, min_child_weight=1, nthread=8, objective=binary:hinge, reg_alpha=0, subsample=0.6, tree_method=gpu_hist\n"
     ]
    }
   ],
   "source": [
    "base_sh = base_param_sh.copy()\n",
    "grids_sh = [ps1, ps2, ps3, ps4]\n",
    "\n",
    "rets_sh = []\n",
    "for grid in grids_sh:\n",
    "    params = compose_param_grid(grid, base_sh)\n",
    "    print(f\"开始搜索：{dict_2_str(grid)}\\n待搜索的参数组合数量：{len(params)}\")\n",
    "    ret = gridsearch_cv_xgb(data_sh.values, is_share_res.values, params, n_round=150, verbose_eval=False, n_class=2)\n",
    "    arr = np.array([[-e['eval-error'] for e in ret], \n",
    "                    [-e['eval-logloss'] for e in ret],\n",
    "                    [e['eval-auc'] for e in ret], \n",
    "                    [e['w_auc'] for e in ret]], dtype=np.float32)\n",
    "    opt_idxs = list(set(arr.argmax(axis=1)))\n",
    "    for i in opt_idxs:\n",
    "        print(dict_2_str(ret[i]['param']))\n",
    "    opt_idx = opt_idxs[0]\n",
    "    opt_param = ret[opt_idx]['param']\n",
    "    base_sh.update(opt_param)\n",
    "    rets_sh.append(ret)\n",
    "    \n",
    "    ks = list(grid.keys())\n",
    "    ks = '-'.join(ks)\n",
    "    with open(f\"./logs/{ks}-is_share-{int(time())}.pkl\", \"wb\") as f:\n",
    "        pickle.dump(ret, f)\n",
    "    with open(f\"./logs/{ks}-is_share-{int(time())}.md\", \"w\") as f:\n",
    "        lines = []\n",
    "        for e in ret:\n",
    "            line = metric_2_str(e)\n",
    "            lines.append(line)\n",
    "            lines.append('\\n')\n",
    "        f.write(\"\\n\".join(lines))\n",
    "\n",
    "print(f\"找到的最棒的参数是：\\n{dict_2_str(base_sh)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'objective': 'binary:hinge',\n",
       " 'eta': 0.1,\n",
       " 'nthread': 8,\n",
       " 'gpu_id': 0,\n",
       " 'tree_method': 'gpu_hist',\n",
       " 'eval_metric': ['logloss', 'auc', 'error'],\n",
       " 'max_depth': 5,\n",
       " 'min_child_weight': 1,\n",
       " 'gamma': 0.1,\n",
       " 'subsample': 0.6,\n",
       " 'colsample_bytree': 0.6,\n",
       " 'reg_alpha': 0}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gridsearch_results_sh = gridsearch_xgb(all_params_sh, xg_train_sh, xg_test_sh, num_round=150, n_class=2, verbose_eval=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_cv_results_sh = gridsearch_cv_xgb(data_sh.values, is_share_res.values, all_params_sh, n_round=150, verbose_eval=False, n_class=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 753,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('greadsearch-cv-is_share.md', 'w') as f:\n",
    "    for ret in performance:\n",
    "        f.write(f\"# {', '.join([f'{k}={v}' for k, v in ret[0].items()])}\\n\")\n",
    "        for k, v in ret[1].items():\n",
    "            is_break = '\\n' if '\\n' in str(df) else ''\n",
    "            f.write(f\"- {k} :{is_break} {v}\\n\\n\")\n",
    "        f.write(f\"{'-'*50}\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 765,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48"
      ]
     },
     "execution_count": 765,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_test_error = np.array([e[1]['mean_test_error'] for e in performance])\n",
    "mean_test_error.argmin()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 617,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'objective': 'binary:hinge',\n",
       " 'eta': 0.1,\n",
       " 'nthread': 8,\n",
       " 'gpu_id': 0,\n",
       " 'tree_method': 'gpu_hist',\n",
       " 'gamma': 0.3,\n",
       " 'max_depth': 11,\n",
       " 'min_child_weight': 9}"
      ]
     },
     "execution_count": 617,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr = np.array([[-e['test_error'] for e in gridsearch_results_sh], [e['aucs'][1] for e in gridsearch_results_sh]], dtype=np.float32)\n",
    "opt_idxs_sh = arr.argmax(axis=1)\n",
    "if opt_idxs_sh[0] != opt_idxs_sh[1]:\n",
    "    warnings.warn(f\"最小误差与最大AUC对应的模型不一致 : {opt_idxs_sh}。选择误差最小的模型 : {opt_idxs_sh[0]}\")\n",
    "\n",
    "opt_idx_sh = opt_idxs_sh[0]\n",
    "all_params_sh[opt_idx_sh]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 603,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "146"
      ]
     },
     "execution_count": 603,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opt_idx_sh = opt_idxs_sh[0]\n",
    "opt_idx_sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 619,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.54      0.44      0.49      3053\n",
      "         1.0       0.50      0.60      0.54      2835\n",
      "\n",
      "    accuracy                           0.52      5888\n",
      "   macro avg       0.52      0.52      0.51      5888\n",
      "weighted avg       0.52      0.52      0.51      5888\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(gridsearch_results_sh[opt_idx_sh]['report'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 620,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>test_error</th>\n",
       "      <th>aucs</th>\n",
       "      <th>w_auc</th>\n",
       "      <th>report</th>\n",
       "      <th>model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.503057</td>\n",
       "      <td>[0.50379590202715, 0.50379590202715]</td>\n",
       "      <td>None</td>\n",
       "      <td>precision    recall  f1-score   ...</td>\n",
       "      <td>&lt;xgboost.core.Booster object at 0x7f35ba907130&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.496943</td>\n",
       "      <td>[0.508734635779073, 0.508734635779073]</td>\n",
       "      <td>None</td>\n",
       "      <td>precision    recall  f1-score   ...</td>\n",
       "      <td>&lt;xgboost.core.Booster object at 0x7f32e4009580&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.501189</td>\n",
       "      <td>[0.5048543919272165, 0.5048543919272165]</td>\n",
       "      <td>None</td>\n",
       "      <td>precision    recall  f1-score   ...</td>\n",
       "      <td>&lt;xgboost.core.Booster object at 0x7f3583f4f4f0&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.502717</td>\n",
       "      <td>[0.502360357955947, 0.502360357955947]</td>\n",
       "      <td>None</td>\n",
       "      <td>precision    recall  f1-score   ...</td>\n",
       "      <td>&lt;xgboost.core.Booster object at 0x7f3574279550&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.499321</td>\n",
       "      <td>[0.5061017844072763, 0.5061017844072763]</td>\n",
       "      <td>None</td>\n",
       "      <td>precision    recall  f1-score   ...</td>\n",
       "      <td>&lt;xgboost.core.Booster object at 0x7f32e4082730&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>0.498132</td>\n",
       "      <td>[0.5057747576472328, 0.5057747576472328]</td>\n",
       "      <td>None</td>\n",
       "      <td>precision    recall  f1-score   ...</td>\n",
       "      <td>&lt;xgboost.core.Booster object at 0x7f32468a4640&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>0.497962</td>\n",
       "      <td>[0.5046665869463117, 0.5046665869463118]</td>\n",
       "      <td>None</td>\n",
       "      <td>precision    recall  f1-score   ...</td>\n",
       "      <td>&lt;xgboost.core.Booster object at 0x7f32468a4850&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>0.499490</td>\n",
       "      <td>[0.5026636996830249, 0.5026636996830249]</td>\n",
       "      <td>None</td>\n",
       "      <td>precision    recall  f1-score   ...</td>\n",
       "      <td>&lt;xgboost.core.Booster object at 0x7f32468a46a0&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>0.491678</td>\n",
       "      <td>[0.5107513874519006, 0.5107513874519005]</td>\n",
       "      <td>None</td>\n",
       "      <td>precision    recall  f1-score   ...</td>\n",
       "      <td>&lt;xgboost.core.Booster object at 0x7f32468a4670&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>0.499151</td>\n",
       "      <td>[0.5034068320344114, 0.5034068320344114]</td>\n",
       "      <td>None</td>\n",
       "      <td>precision    recall  f1-score   ...</td>\n",
       "      <td>&lt;xgboost.core.Booster object at 0x7f32468a4820&gt;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>120 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     test_error                                      aucs w_auc  \\\n",
       "0      0.503057      [0.50379590202715, 0.50379590202715]  None   \n",
       "1      0.496943    [0.508734635779073, 0.508734635779073]  None   \n",
       "2      0.501189  [0.5048543919272165, 0.5048543919272165]  None   \n",
       "3      0.502717    [0.502360357955947, 0.502360357955947]  None   \n",
       "4      0.499321  [0.5061017844072763, 0.5061017844072763]  None   \n",
       "..          ...                                       ...   ...   \n",
       "115    0.498132  [0.5057747576472328, 0.5057747576472328]  None   \n",
       "116    0.497962  [0.5046665869463117, 0.5046665869463118]  None   \n",
       "117    0.499490  [0.5026636996830249, 0.5026636996830249]  None   \n",
       "118    0.491678  [0.5107513874519006, 0.5107513874519005]  None   \n",
       "119    0.499151  [0.5034068320344114, 0.5034068320344114]  None   \n",
       "\n",
       "                                                report  \\\n",
       "0                  precision    recall  f1-score   ...   \n",
       "1                  precision    recall  f1-score   ...   \n",
       "2                  precision    recall  f1-score   ...   \n",
       "3                  precision    recall  f1-score   ...   \n",
       "4                  precision    recall  f1-score   ...   \n",
       "..                                                 ...   \n",
       "115                precision    recall  f1-score   ...   \n",
       "116                precision    recall  f1-score   ...   \n",
       "117                precision    recall  f1-score   ...   \n",
       "118                precision    recall  f1-score   ...   \n",
       "119                precision    recall  f1-score   ...   \n",
       "\n",
       "                                               model  \n",
       "0    <xgboost.core.Booster object at 0x7f35ba907130>  \n",
       "1    <xgboost.core.Booster object at 0x7f32e4009580>  \n",
       "2    <xgboost.core.Booster object at 0x7f3583f4f4f0>  \n",
       "3    <xgboost.core.Booster object at 0x7f3574279550>  \n",
       "4    <xgboost.core.Booster object at 0x7f32e4082730>  \n",
       "..                                               ...  \n",
       "115  <xgboost.core.Booster object at 0x7f32468a4640>  \n",
       "116  <xgboost.core.Booster object at 0x7f32468a4850>  \n",
       "117  <xgboost.core.Booster object at 0x7f32468a46a0>  \n",
       "118  <xgboost.core.Booster object at 0x7f32468a4670>  \n",
       "119  <xgboost.core.Booster object at 0x7f32468a4820>  \n",
       "\n",
       "[120 rows x 5 columns]"
      ]
     },
     "execution_count": 620,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(gridsearch_results_sh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-fold & Bagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1-th split ... \n",
      "2-th split ... \n",
      "3-th split ... \n",
      "4-th split ... \n",
      "5-th split ... \n",
      "CPU times: user 17.8 s, sys: 240 ms, total: 18 s\n",
      "Wall time: 17.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "n_splits = 5\n",
    "is_shuffle = True\n",
    "random_state = 10\n",
    "cv = StratifiedKFold(n_splits, shuffle=is_shuffle, random_state=random_state)\n",
    "X = data_sh\n",
    "y = is_share_res\n",
    "models_sh = []\n",
    "results_sh = {}\n",
    "\n",
    "n_round = 800\n",
    "verbose_eval = False\n",
    "param_sh = {\n",
    "    'objective': 'binary:hinge',\n",
    "    'eta': 0.005,\n",
    "    'nthread': 8,\n",
    "    'gpu_id': 0,\n",
    "    'tree_method': 'gpu_hist',\n",
    "    'eval_metric': ['logloss', 'auc', 'error'],\n",
    "    'max_depth': 5,\n",
    "    'min_child_weight': 1,\n",
    "    'gamma': 0.1,\n",
    "    'subsample': 0.6,\n",
    "    'colsample_bytree': 0.6,\n",
    "    'reg_alpha': 0\n",
    "}\n",
    "\n",
    "n_class = 2\n",
    "for i, (train_idxs, test_idxs) in enumerate(cv.split(X, y)):\n",
    "        print(f\"{i+1}-th split ... \")\n",
    "        X_train = X.loc[train_idxs]\n",
    "        X_test  = X.loc[test_idxs]\n",
    "        y_train = y.loc[train_idxs]\n",
    "        y_test  = y.loc[test_idxs]\n",
    "        \n",
    "        xg_train = xgb.DMatrix(X_train, label=y_train, enable_categorical=True)\n",
    "        xg_test = xgb.DMatrix(X_test, label=y_test, enable_categorical=True)\n",
    "        watchlist = [(xg_train, 'train'), (xg_test, 'test')]\n",
    "        \n",
    "        t0 = time()\n",
    "        model = xgb.train(param_sh, xg_train, n_round, watchlist, verbose_eval=verbose_eval)\n",
    "#         print(f\"{n_round}-rounds Training finished ...\\t\\t({time()-t0:.3f}s)\")\n",
    "\n",
    "#         # get prediction\n",
    "        pred = model.predict(xg_test)\n",
    "#         # pred = pred.astype(np.uint8)\n",
    "#         error_rate = np.sum(pred != y_test) / y_test.shape[0]\n",
    "#         print('Test error using softmax = {}'.format(error_rate))\n",
    "        \n",
    "        # eval the test using model\n",
    "        evals = model.eval(xg_test)\n",
    "        eval_dict = eval_str_2_dict(evals)\n",
    "        \n",
    "        aucs = auc(y_test.astype(np.uint8), pred.astype(np.uint8), np.arange(n_class))\n",
    "        # aucs[aucs == 0.5] = 0\n",
    "        if n_class == 2:\n",
    "            w_aucs = 0\n",
    "        else:\n",
    "            weights = np.arange(0, 1, 0.1)\n",
    "            w_aucs = (aucs * weights).sum()\n",
    "\n",
    "\n",
    "        rep = metrics.classification_report(list(y_test), list(pred), output_dict=True)\n",
    "        rep_df = report_2_df(rep)\n",
    "        \n",
    "        # 处理每一fold的结果，对每个指标进行平均\n",
    "        items = [(e[0], e[1] / n_splits) for e in eval_dict.items()]\n",
    "        eval_dict = dict(items)\n",
    "        if not results_sh:\n",
    "            results_sh = {\n",
    "                'aucs': aucs / n_splits,\n",
    "                'w_auc': w_aucs / n_splits,\n",
    "                'report': rep_df / n_splits,\n",
    "    #             'model': model,\n",
    "    #             'split': i\n",
    "            }\n",
    "            results_sh.update(eval_dict)\n",
    "        else:\n",
    "            results_sh['aucs'] +=  aucs / n_splits\n",
    "            results_sh['w_auc'] += w_aucs / n_splits\n",
    "            results_sh['report'] += rep_df / n_splits\n",
    "            for k, v in eval_dict.items():\n",
    "                results_sh[k] += v\n",
    "        \n",
    "        models_sh.append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "      <th>support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.738168</td>\n",
       "      <td>0.368111</td>\n",
       "      <td>0.491119</td>\n",
       "      <td>2863.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.579109</td>\n",
       "      <td>0.869334</td>\n",
       "      <td>0.695125</td>\n",
       "      <td>2863.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.618723</td>\n",
       "      <td>5727.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>macro avg</th>\n",
       "      <td>0.658639</td>\n",
       "      <td>0.618723</td>\n",
       "      <td>0.593122</td>\n",
       "      <td>5727.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weighted avg</th>\n",
       "      <td>0.658638</td>\n",
       "      <td>0.618723</td>\n",
       "      <td>0.593122</td>\n",
       "      <td>5727.6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              precision    recall  f1-score  support\n",
       "0              0.738168  0.368111  0.491119   2863.8\n",
       "1              0.579109  0.869334  0.695125   2863.8\n",
       "accuracy            NaN       NaN  0.618723   5727.6\n",
       "macro avg      0.658639  0.618723  0.593122   5727.6\n",
       "weighted avg   0.658638  0.618723  0.593122   5727.6"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_sh['report']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_sh = X_train_sh.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gzy/.local/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=10, random_state=0)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sh_clf = LogisticRegression(penalty='l2', C=10, random_state=0, )\n",
    "sh_clf.fit(X_train_sh, y_train_sh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_test_sh' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_19100/2335965782.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX_test_sh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_test_sh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfillna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0msh_clf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test_sh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test_sh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_test_sh' is not defined"
     ]
    }
   ],
   "source": [
    "X_test_sh = X_test_sh.fillna(0)\n",
    "sh_clf.score(X_test_sh, y_test_sh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB \n",
    "from mlxtend.classifier import StackingClassifier\n",
    "from sklearn.ensemble import (RandomForestClassifier, AdaBoostClassifier, \n",
    "                              GradientBoostingClassifier, ExtraTreesClassifier)\n",
    "from xgboost import XGBClassifier, XGBRFClassifier\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 第一层模型参数\n",
    "first_layer_params = {\n",
    "    LogisticRegression : {\n",
    "        'C' : 10,\n",
    "        'random_state': 0\n",
    "    },\n",
    "    RandomForestClassifier : {\n",
    "        'n_jobs': -1,\n",
    "        'n_estimators': 200,\n",
    "         'warm_start': True, \n",
    "         #'max_features': 0.2,\n",
    "        'max_depth': 8,\n",
    "        'min_samples_leaf': 2,\n",
    "        'max_features' : 'sqrt',\n",
    "        'verbose': 0\n",
    "    },\n",
    "    ExtraTreesClassifier : {\n",
    "        'n_jobs': -1,\n",
    "        'n_estimators':200,\n",
    "        #'max_features': 0.5,\n",
    "        'max_depth': 8,\n",
    "        'min_samples_leaf': 2,\n",
    "        'verbose': 0\n",
    "    },\n",
    "    AdaBoostClassifier : {\n",
    "        'n_estimators': 200,\n",
    "        'learning_rate' : 0.75\n",
    "    },\n",
    "#     GradientBoostingClassifier : { # 太慢了\n",
    "#         'n_estimators': 200,\n",
    "#          #'max_features': 0.2,\n",
    "#         'max_depth': 5,\n",
    "#         'min_samples_leaf': 2,\n",
    "#         'verbose': 0\n",
    "#     },\n",
    "#     SVC : { # 太慢了\n",
    "#         'kernel' : 'rbf',\n",
    "#         'C' : 0.025,\n",
    "#         'probability': True\n",
    "#     } \n",
    "}\n",
    "\n",
    "\n",
    "second_layer_params = {\n",
    "    XGBClassifier : {\n",
    "        'objective': 'multi:softmax',\n",
    "        'eta': 0.1,\n",
    "        'nthread': 8,\n",
    "        'num_class': 10,\n",
    "        'gpu_id': 0,\n",
    "        'tree_method': 'gpu_hist',\n",
    "        'eval_metric': ['mlogloss', 'auc', 'merror'],\n",
    "        'max_depth': 9,\n",
    "        'min_child_weight': 9,\n",
    "        'gamma': 0.2,\n",
    "        'subsample': 0.9,\n",
    "        'colsample_bytree': 0.6,\n",
    "        'reg_alpha': 0,\n",
    "        'n_estimators': 200,\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "clfs = []\n",
    "for c, p in first_layer_params.items():\n",
    "    clfs.append(c(**p))\n",
    "\n",
    "meta = XGBClassifier(**second_layer_params[XGBClassifier])\n",
    "sclf = StackingClassifier(classifiers=clfs, \n",
    "                          meta_classifier=meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gzy/.local/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "StackingClassifier(classifiers=[RandomForestClassifier(max_depth=8,\n",
       "                                                       max_features='sqrt',\n",
       "                                                       min_samples_leaf=2,\n",
       "                                                       n_estimators=200,\n",
       "                                                       n_jobs=-1,\n",
       "                                                       warm_start=True),\n",
       "                                ExtraTreesClassifier(max_depth=8,\n",
       "                                                     min_samples_leaf=2,\n",
       "                                                     n_estimators=200,\n",
       "                                                     n_jobs=-1),\n",
       "                                AdaBoostClassifier(learning_rate=0.75,\n",
       "                                                   n_estimators=200),\n",
       "                                GradientBoostingClassifier(max_depth=5,\n",
       "                                                           min_samples_leaf=2,\n",
       "                                                           n_esti...\n",
       "                                                 interaction_constraints=None,\n",
       "                                                 learning_rate=None,\n",
       "                                                 max_delta_step=None,\n",
       "                                                 max_depth=9,\n",
       "                                                 min_child_weight=9,\n",
       "                                                 missing=nan,\n",
       "                                                 monotone_constraints=None,\n",
       "                                                 n_estimators=200, n_jobs=None,\n",
       "                                                 nthread=8, num_class=10,\n",
       "                                                 num_parallel_tree=None,\n",
       "                                                 objective='multi:softmax',\n",
       "                                                 random_state=None, reg_alpha=0,\n",
       "                                                 reg_lambda=None,\n",
       "                                                 scale_pos_weight=None,\n",
       "                                                 subsample=0.9,\n",
       "                                                 tree_method='gpu_hist',\n",
       "                                                 validate_parameters=None, ...))"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sclf.fit(X_train_sh, y_train_sh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6674231843575419"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sclf.score(X_test_sh, y_test_sh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 13.1 s, sys: 112 ms, total: 13.2 s\n",
      "Wall time: 987 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(max_depth=8, max_features='sqrt', min_samples_leaf=2,\n",
       "                       n_estimators=200, n_jobs=-1, warm_start=True)"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "clf = RandomForestClassifier(**first_layer_params[RandomForestClassifier])\n",
    "clf.fit(X_train_sh, y_train_sh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.61      0.63      2845\n",
      "           1       0.64      0.69      0.66      2883\n",
      "\n",
      "    accuracy                           0.65      5728\n",
      "   macro avg       0.65      0.65      0.65      5728\n",
      "weighted avg       0.65      0.65      0.65      5728\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tmp = clf.predict(X_test_sh)\n",
    "print(metrics.classification_report(list(y_test_sh), list(tmp)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>v_avg_watch_label_1</th>\n",
       "      <th>v_sum_watch_times_1</th>\n",
       "      <th>v_sum_watch_overs_1</th>\n",
       "      <th>v_sum_comment_times_1</th>\n",
       "      <th>v_sum_collect_times_1</th>\n",
       "      <th>v_sum_share_times_1</th>\n",
       "      <th>v_sum_quit_times_1</th>\n",
       "      <th>v_sum_skip_times_1</th>\n",
       "      <th>v_sum_watch_days_1</th>\n",
       "      <th>v_avg_watch_label_3</th>\n",
       "      <th>...</th>\n",
       "      <th>class_6</th>\n",
       "      <th>class_7</th>\n",
       "      <th>class_8</th>\n",
       "      <th>class_9</th>\n",
       "      <th>da_0</th>\n",
       "      <th>da_1</th>\n",
       "      <th>da_2</th>\n",
       "      <th>da_3</th>\n",
       "      <th>da_4</th>\n",
       "      <th>watch_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>17211</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.958333</td>\n",
       "      <td>...</td>\n",
       "      <td>0.041922</td>\n",
       "      <td>0.041925</td>\n",
       "      <td>0.041925</td>\n",
       "      <td>0.041923</td>\n",
       "      <td>0.084701</td>\n",
       "      <td>0.084811</td>\n",
       "      <td>0.661892</td>\n",
       "      <td>0.084126</td>\n",
       "      <td>0.084470</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12479</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.306212</td>\n",
       "      <td>0.037078</td>\n",
       "      <td>0.037070</td>\n",
       "      <td>0.037070</td>\n",
       "      <td>0.074563</td>\n",
       "      <td>0.075237</td>\n",
       "      <td>0.316654</td>\n",
       "      <td>0.074140</td>\n",
       "      <td>0.459407</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2532</th>\n",
       "      <td>0.208611</td>\n",
       "      <td>1951.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1866.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.213834</td>\n",
       "      <td>...</td>\n",
       "      <td>0.350094</td>\n",
       "      <td>0.041778</td>\n",
       "      <td>0.041778</td>\n",
       "      <td>0.041771</td>\n",
       "      <td>0.324958</td>\n",
       "      <td>0.083541</td>\n",
       "      <td>0.083541</td>\n",
       "      <td>0.083541</td>\n",
       "      <td>0.424418</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28572</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.041922</td>\n",
       "      <td>0.041925</td>\n",
       "      <td>0.041925</td>\n",
       "      <td>0.041923</td>\n",
       "      <td>0.084701</td>\n",
       "      <td>0.084811</td>\n",
       "      <td>0.661892</td>\n",
       "      <td>0.084126</td>\n",
       "      <td>0.084470</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19280</th>\n",
       "      <td>0.509397</td>\n",
       "      <td>3565.0</td>\n",
       "      <td>106.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>3082.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.548684</td>\n",
       "      <td>...</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050003</td>\n",
       "      <td>0.050009</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11302</th>\n",
       "      <td>1.333333</td>\n",
       "      <td>123.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.124863</td>\n",
       "      <td>...</td>\n",
       "      <td>0.270403</td>\n",
       "      <td>0.036901</td>\n",
       "      <td>0.036893</td>\n",
       "      <td>0.271428</td>\n",
       "      <td>0.073786</td>\n",
       "      <td>0.074915</td>\n",
       "      <td>0.078284</td>\n",
       "      <td>0.073786</td>\n",
       "      <td>0.699228</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4584</th>\n",
       "      <td>0.902047</td>\n",
       "      <td>684.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>531.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.040830</td>\n",
       "      <td>...</td>\n",
       "      <td>0.212644</td>\n",
       "      <td>0.031771</td>\n",
       "      <td>0.031764</td>\n",
       "      <td>0.409137</td>\n",
       "      <td>0.065150</td>\n",
       "      <td>0.328750</td>\n",
       "      <td>0.068145</td>\n",
       "      <td>0.064165</td>\n",
       "      <td>0.473790</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26296</th>\n",
       "      <td>0.674253</td>\n",
       "      <td>32513.0</td>\n",
       "      <td>388.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>245.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>22996.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.725334</td>\n",
       "      <td>...</td>\n",
       "      <td>0.037483</td>\n",
       "      <td>0.037487</td>\n",
       "      <td>0.395917</td>\n",
       "      <td>0.037484</td>\n",
       "      <td>0.075851</td>\n",
       "      <td>0.075966</td>\n",
       "      <td>0.388781</td>\n",
       "      <td>0.383735</td>\n",
       "      <td>0.075666</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9816</th>\n",
       "      <td>1.617686</td>\n",
       "      <td>12903.0</td>\n",
       "      <td>1328.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>174.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>8865.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.619248</td>\n",
       "      <td>...</td>\n",
       "      <td>0.248009</td>\n",
       "      <td>0.036880</td>\n",
       "      <td>0.036881</td>\n",
       "      <td>0.270608</td>\n",
       "      <td>0.238563</td>\n",
       "      <td>0.073745</td>\n",
       "      <td>0.073745</td>\n",
       "      <td>0.073745</td>\n",
       "      <td>0.540203</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10186</th>\n",
       "      <td>1.143805</td>\n",
       "      <td>1356.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1044.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.158740</td>\n",
       "      <td>...</td>\n",
       "      <td>0.036693</td>\n",
       "      <td>0.528539</td>\n",
       "      <td>0.036671</td>\n",
       "      <td>0.178063</td>\n",
       "      <td>0.074926</td>\n",
       "      <td>0.074954</td>\n",
       "      <td>0.124523</td>\n",
       "      <td>0.075064</td>\n",
       "      <td>0.650534</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5728 rows × 128 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       v_avg_watch_label_1  v_sum_watch_times_1  v_sum_watch_overs_1  \\\n",
       "17211             0.000000                  4.0                  0.0   \n",
       "12479             0.000000                  0.0                  0.0   \n",
       "2532              0.208611               1951.0                 27.0   \n",
       "28572             0.000000                  0.0                  0.0   \n",
       "19280             0.509397               3565.0                106.0   \n",
       "...                    ...                  ...                  ...   \n",
       "11302             1.333333                123.0                 10.0   \n",
       "4584              0.902047                684.0                 30.0   \n",
       "26296             0.674253              32513.0                388.0   \n",
       "9816              1.617686              12903.0               1328.0   \n",
       "10186             1.143805               1356.0                100.0   \n",
       "\n",
       "       v_sum_comment_times_1  v_sum_collect_times_1  v_sum_share_times_1  \\\n",
       "17211                    0.0                    0.0                  0.0   \n",
       "12479                    0.0                    0.0                  0.0   \n",
       "2532                     2.0                   24.0                  4.0   \n",
       "28572                    0.0                    0.0                  0.0   \n",
       "19280                    2.0                   55.0                 35.0   \n",
       "...                      ...                    ...                  ...   \n",
       "11302                    0.0                    0.0                  1.0   \n",
       "4584                     1.0                    5.0                  1.0   \n",
       "26296                   11.0                  245.0                 55.0   \n",
       "9816                    34.0                  174.0                 46.0   \n",
       "10186                    7.0                   22.0                  1.0   \n",
       "\n",
       "       v_sum_quit_times_1  v_sum_skip_times_1  v_sum_watch_days_1  \\\n",
       "17211                 4.0                 0.0                 1.0   \n",
       "12479                 0.0                 0.0                 0.0   \n",
       "2532               1866.0                 0.0                 1.0   \n",
       "28572                 0.0                 0.0                 0.0   \n",
       "19280              3082.0                 0.0                 1.0   \n",
       "...                   ...                 ...                 ...   \n",
       "11302                88.0                 0.0                 1.0   \n",
       "4584                531.0                 0.0                 1.0   \n",
       "26296             22996.0                 0.0                 1.0   \n",
       "9816               8865.0                 0.0                 1.0   \n",
       "10186              1044.0                 0.0                 1.0   \n",
       "\n",
       "       v_avg_watch_label_3  ...   class_6   class_7   class_8   class_9  \\\n",
       "17211             0.958333  ...  0.041922  0.041925  0.041925  0.041923   \n",
       "12479             0.000000  ...  0.306212  0.037078  0.037070  0.037070   \n",
       "2532              0.213834  ...  0.350094  0.041778  0.041778  0.041771   \n",
       "28572             0.000000  ...  0.041922  0.041925  0.041925  0.041923   \n",
       "19280             0.548684  ...  0.050000  0.050003  0.050009  0.050000   \n",
       "...                    ...  ...       ...       ...       ...       ...   \n",
       "11302             1.124863  ...  0.270403  0.036901  0.036893  0.271428   \n",
       "4584              1.040830  ...  0.212644  0.031771  0.031764  0.409137   \n",
       "26296             0.725334  ...  0.037483  0.037487  0.395917  0.037484   \n",
       "9816              1.619248  ...  0.248009  0.036880  0.036881  0.270608   \n",
       "10186             1.158740  ...  0.036693  0.528539  0.036671  0.178063   \n",
       "\n",
       "           da_0      da_1      da_2      da_3      da_4  watch_label  \n",
       "17211  0.084701  0.084811  0.661892  0.084126  0.084470          2.0  \n",
       "12479  0.074563  0.075237  0.316654  0.074140  0.459407          0.0  \n",
       "2532   0.324958  0.083541  0.083541  0.083541  0.424418          0.0  \n",
       "28572  0.084701  0.084811  0.661892  0.084126  0.084470          0.0  \n",
       "19280  0.600000  0.100000  0.100000  0.100000  0.100000          0.0  \n",
       "...         ...       ...       ...       ...       ...          ...  \n",
       "11302  0.073786  0.074915  0.078284  0.073786  0.699228          9.0  \n",
       "4584   0.065150  0.328750  0.068145  0.064165  0.473790          3.0  \n",
       "26296  0.075851  0.075966  0.388781  0.383735  0.075666          0.0  \n",
       "9816   0.238563  0.073745  0.073745  0.073745  0.540203          0.0  \n",
       "10186  0.074926  0.074954  0.124523  0.075064  0.650534          0.0  \n",
       "\n",
       "[5728 rows x 128 columns]"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "del dataset, data, data_sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['u_sum_skip_times_1', 'u_sum_watch_days_1', 'u_sum_skip_times_3', 'u_sum_skip_times_7']\n",
    "for c in cols:\n",
    "    test[c] = test[c].fillna(False).astype('int8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = inference_dataset\n",
    "# test_sh = inference_dataset.copy()\n",
    "test = xgb.DMatrix(test, enable_categorical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2822180, 128), 128)"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inference_dataset.shape, test.num_col()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "bst_wl = wl_bst_sm  # gridsearch_results[opt_idx]['model']  \n",
    "bst_sh = sh_bst_sm  # gridsearch_results_sh[opt_idx_sh]['model']  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "bst_wl = xgb.Booster()\n",
    "bst_sh = xgb.Booster()\n",
    "bst_wl.load_model('wl_model_v22')\n",
    "bst_sh.load_model('sh_model_v22')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Counter({0.0: 1560782,\n",
       "          9.0: 681586,\n",
       "          1.0: 393379,\n",
       "          2.0: 143113,\n",
       "          3.0: 19147,\n",
       "          5.0: 4003,\n",
       "          4.0: 6093,\n",
       "          6.0: 3337,\n",
       "          8.0: 9392,\n",
       "          7.0: 1348}),\n",
       " Counter({1.0: 2759429, 0.0: 62751}))"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wl = bst_wl.predict(test)\n",
    "# test_sh['watch_label'] = wl\n",
    "# test_sh = xgb.DMatrix(test_sh.values, enable_categorical=True)\n",
    "sh = bst_sh.predict(test)\n",
    "Counter(wl), Counter(Counter(sh))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "wl = None\n",
    "sh = None\n",
    "wl_enc = OneHotEncoder(categories=[list(range(10))])\n",
    "sh_enc = OneHotEncoder(categories=[[0, 1]])\n",
    "for i, (wl_m, sh_m) in enumerate(zip(models, models_sh)):\n",
    "    tmp_wl = wl_m.predict(test).reshape(-1, 1)\n",
    "    tmp_sh = sh_m.predict(test).reshape(-1, 1)\n",
    "    \n",
    "    if wl is None:\n",
    "        wl = wl_enc.fit_transform(tmp_wl)\n",
    "    else:\n",
    "        wl += wl_enc.fit_transform(tmp_wl) \n",
    "    \n",
    "    if sh is None:\n",
    "        sh = sh_enc.fit_transform(tmp_sh)\n",
    "    else:\n",
    "        sh += sh_enc.fit_transform(tmp_sh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Counter({1: 1135158,\n",
       "          0: 777931,\n",
       "          9: 880178,\n",
       "          2: 12289,\n",
       "          6: 1854,\n",
       "          3: 3973,\n",
       "          8: 7119,\n",
       "          4: 1684,\n",
       "          5: 1703,\n",
       "          7: 291}),\n",
       " Counter({1: 1556470, 0: 1265710}))"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ensembled_wl = wl.toarray().argmax(axis=1)\n",
    "ensembled_sh = sh.toarray().argmax(axis=1)\n",
    "Counter(ensembled_wl), Counter(ensembled_sh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2822180, 62)"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df['watch_label'] = ensembled_wl.astype(np.uint8)\n",
    "test_df['is_share'] = ensembled_sh.astype(np.uint8)\n",
    "test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>video_id</th>\n",
       "      <th>watch_label</th>\n",
       "      <th>is_share</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1688013</td>\n",
       "      <td>32645</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4502598</td>\n",
       "      <td>41270</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5585629</td>\n",
       "      <td>16345</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1635520</td>\n",
       "      <td>28149</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4160191</td>\n",
       "      <td>40554</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2822175</th>\n",
       "      <td>5019057</td>\n",
       "      <td>18766</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2822176</th>\n",
       "      <td>5019057</td>\n",
       "      <td>12968</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2822177</th>\n",
       "      <td>4255762</td>\n",
       "      <td>21794</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2822178</th>\n",
       "      <td>171497</td>\n",
       "      <td>21578</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2822179</th>\n",
       "      <td>5642580</td>\n",
       "      <td>28914</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2822180 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         user_id  video_id  watch_label  is_share\n",
       "0        1688013     32645            1         1\n",
       "1        4502598     41270            0         1\n",
       "2        5585629     16345            9         0\n",
       "3        1635520     28149            1         1\n",
       "4        4160191     40554            1         1\n",
       "...          ...       ...          ...       ...\n",
       "2822175  5019057     18766            1         0\n",
       "2822176  5019057     12968            0         1\n",
       "2822177  4255762     21794            1         0\n",
       "2822178   171497     21578            1         0\n",
       "2822179  5642580     28914            9         1\n",
       "\n",
       "[2822180 rows x 4 columns]"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission = pd.DataFrame(test_df[['user_id', 'video_id', 'watch_label', 'is_share']])\n",
    "submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new submission saved to ../submission-1630346741.csv\n"
     ]
    }
   ],
   "source": [
    "fn = f'../submission-{int(time())}.csv'\n",
    "submission.to_csv(fn, index=False, sep=\",\")\n",
    "print(f\"new submission saved to {fn}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 保存模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "version = 24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "wl_model_name = f'wl_model_v{version}'\n",
    "sh_model_name = f'sh_model_v{version}'\n",
    "bst_wl.save_model(wl_model_name)\n",
    "bst_sh.save_model(sh_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_log(log_name, info, log_path=\"./\"):\n",
    "    import datetime\n",
    "    with open(os.path.join(log_path, log_name), 'w') as log:\n",
    "        log.write(f\"# {datetime.datetime.now().__str__()}\\n\")\n",
    "        if info.get('comment', False):\n",
    "            log.write(f\"\\n## Comment: \\n\")\n",
    "            log.write(f\"{info['comment']}\\n\")\n",
    "            \n",
    "        log.write(f\"\\n## model name: {info['model_name']}\\n\")\n",
    "        log.write(f\"- model save path : {info['model_save_path']}\\n\")\n",
    "        \n",
    "        try:\n",
    "            log.write(f\"\\n## Data setup\\n\")\n",
    "            log.write(f\"- dataset.shape : {dataset.shape}\\n\")\n",
    "            log.write(f\"- dataset.columns : {dataset.columns}\\n\")\n",
    "            log.write(f\"- is resample : {info['is_resample']}\\n\")\n",
    "            log.write(f\"- Traing_Data.shape (watch_label)  : {X_train.shape}\\n\")\n",
    "            log.write(f\"- Testing_Data.shape (watch_label) : {X_test.shape}\\n\")\n",
    "            log.write(f\"- Traing_Data.shape (is_share)  : {X_train_sh.shape}\\n\")\n",
    "            log.write(f\"- Testing_Data.shape (is_share) : {X_test_sh.shape}\\n\")\n",
    "        except Exception as exp:\n",
    "            pass\n",
    "        \n",
    "        if info.get('is_resample', False):\n",
    "            log.write(f\"- Resampled class distribution (watch_label): \\n{Counter(watch_label_res)}\\n\")\n",
    "            log.write(f\"- Resampled class distribution (is_share): \\n{Counter(is_share_res)}\\n\")\n",
    "            \n",
    "        log.write(f\"\\n## Model Params\\n\")\n",
    "        log.write(f\"- model params (watch_label) : \\n{info['param_wl']}\\n\")\n",
    "        log.write(f\"- model params (is_share) : \\n{info['param_sh']}\\n\")\n",
    "        \n",
    "        log.write(f\"\\n## Model's Performance\\n\")\n",
    "        log.write(f\"- Aucs (watch_label) : {info['aucs']}\\n\")\n",
    "        log.write(f\"- Weighted Aucs (watch_label) : {info['w_auc']}\\n\")\n",
    "        log.write(f\"- Aucs (is_share) : {info['aucs_sh']}\\n\")\n",
    "        \n",
    "        log.write(f\"- Classification Report (watch_label) : \\n\\n{info['report']}\\n\")\n",
    "        log.write(f\"- Classification Report (is_share) : \\n\\n{info['report_sh']}\\n\")\n",
    "        \n",
    "        log.flush()\n",
    "        \n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'aucs': array([0.65664533, 0.58962866, 0.50507785, 0.50249548, 0.50244899,\n",
       "        0.50275637, 0.5046687 , 0.50197719, 0.50757588, 0.62606703]),\n",
       " 'w_auc': 2.3867911395958608,\n",
       " 'report':               precision    recall  f1-score   support\n",
       " 0              0.373191  0.447464  0.406966   77704.2\n",
       " 1              0.278011  0.642835  0.388154  111484.2\n",
       " 2              0.261718  0.016755  0.031493   62821.4\n",
       " 3              0.219837  0.007473  0.014454   43837.6\n",
       " 4              0.222941  0.006543  0.012710   34480.8\n",
       " 5              0.210198  0.007084  0.013703   28600.2\n",
       " 6              0.268990  0.010856  0.020862   25018.4\n",
       " 7              0.213552  0.004807  0.009402   23549.8\n",
       " 8              0.329894  0.017147  0.032597   27759.6\n",
       " 9              0.261190  0.509344  0.345307   77704.2\n",
       " accuracy            NaN       NaN  0.289853  512960.4\n",
       " macro avg      0.263952  0.167031  0.127565  512960.4\n",
       " weighted avg   0.274840  0.289853  0.208239  512960.4,\n",
       " 'eval-mlogloss': 2.0348056,\n",
       " 'eval-auc': 0.6477464000000002,\n",
       " 'eval-merror': 0.710147}"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_wl = param  # all_params[opt_idx]\n",
    "param_sh = param_sh  # all_params[opt_idx_sh]\n",
    "\n",
    "aucs = results['aucs']  # gridsearch_results[opt_idx]['aucs']\n",
    "w_auc = results['w_auc']  # gridsearch_results[opt_idx]['w_auc']\n",
    "aucs_sh = results['aucs']  # gridsearch_results_sh[opt_idx]['aucs']\n",
    "\n",
    "report = results['report']  # gridsearch_results[opt_idx]['report']\n",
    "report_sh = results_sh['report']  # gridsearch_results_sh[opt_idx]['report']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_name = f\"log_v{version}.md\"\n",
    "info = {'is_resample': True, 'model_name': [\"\", \"\"], 'model_save_path': os.getcwd(),\n",
    "        'comment': f\"特征：v1版基础特征+用户和视频的统计量特征，添加了is_happy_day特征，表示这天是否为周末或节假日。\\n数据集划分：watch_label的测试集为.18，is_share的测试集为.18。\\nwatch_label训练250rounds，早停=30，is_share训练250rounds，早停=60。\\n。数据集划分时进行了shuffle和stratified。\\n此次生成的提交是：{fn}。官方测评得分：xxx😐\",\n",
    "        'param_wl': param_wl, 'param_sh': param_sh, 'aucs': aucs, 'w_auc': w_auc, 'aucs_sh': aucs_sh, \n",
    "        'report': report, 'report_sh': report_sh}\n",
    "write_log(log_name, info, log_path=\"./logs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 服务器间同步文件"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 推向Digix服务器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ssh: connect to host 49.123.120.71 port 22: No route to host\n",
      "lost connection\n"
     ]
    }
   ],
   "source": [
    "!scp ./models.ipynb digix@49.123.120.71:/home/digix/digix/Models/models.ipynb "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "digix@49.123.120.71's password: "
     ]
    }
   ],
   "source": [
    "!scp ./ensemble.ipynb digix@49.123.120.71:/home/digix/digix/Models/ensemble_from_gzy.ipynb "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 599,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ssh: connect to host 49.123.120.71 port 22: No route to host\n",
      "lost connection\n"
     ]
    }
   ],
   "source": [
    "!scp ./log_*.md digix@49.123.120.71:/home/digix/digix/Models/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "explore-data.ipynb                            100%  306KB  10.6MB/s   00:00    \n"
     ]
    }
   ],
   "source": [
    "!scp ../explore-data.ipynb digix@49.123.120.71:/home/digix/digix/explore-data.ipynb "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 650,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "video_features.jay                            100% 9035KB  11.1MB/s   00:00    \n"
     ]
    }
   ],
   "source": [
    "!scp ../2021_3_data/traindata/video_features_data/video_features.jay digix@49.123.120.71:/home/digix/digix/dataset/new_video_features.jay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 从Digix服务器拉数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "digix@49.123.120.71's password: \n"
     ]
    }
   ],
   "source": [
    "!scp  digix@49.123.120.71:/home/digix/digix/Models/LightGBM.ipynb ./LightGBM.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scp: /home/digix/digix/Models/feature_engineering.ipynb: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!scp  digix@49.123.120.71:/home/digix/digix/Models/feature_engineering.ipynb ./feature_engineering.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "utils.py                                      100% 3860     2.6MB/s   00:00    \n",
      "data_analysis.ipynb                           100% 6566KB  11.2MB/s   00:00    \n",
      "__init__.py                                   100%    0     0.0KB/s   00:00    \n",
      "__init__.cpython-36.pyc                       100%  139   128.3KB/s   00:00    \n",
      "utils.cpython-36.pyc                          100% 4120     2.6MB/s   00:00    \n",
      "video_data.ipynb                              100%   55KB   1.7MB/s   00:00    \n",
      "user_data-checkpoint.ipynb                    100%  202KB  10.1MB/s   00:00    \n",
      "data_analysis-checkpoint.ipynb                100% 6554KB  11.0MB/s   00:00    \n",
      "utils-checkpoint.py                           100% 3860     2.4MB/s   00:00    \n",
      "video_data-checkpoint.ipynb                   100%   17KB   1.4MB/s   00:00    \n",
      "user_data.ipynb                               100%  202KB  10.3MB/s   00:00    \n"
     ]
    }
   ],
   "source": [
    "!scp -r digix@49.123.120.71:/home/digix/digix/Models/Feature_Engineering/  ./"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "video_status.csv                              100% 2008KB   9.1MB/s   00:00    \n",
      "user_status.csv                               100%  138MB   9.3MB/s   00:14    \n"
     ]
    }
   ],
   "source": [
    "!scp -r digix@49.123.120.71:/home/digix/digix/dataset/traindata/video_features_data/video_status.csv ../2021_3_data/traindata/video_features_data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user_status.csv                               100%  168MB  11.2MB/s   00:14    \n"
     ]
    }
   ],
   "source": [
    "!scp -r digix@49.123.120.71:/home/digix/digix/dataset/traindata/user_features_data/user_status.csv ../2021_3_data/traindata/user_features_data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "digix@49.123.120.71's password: \n"
     ]
    }
   ],
   "source": [
    "!scp -r digix@49.123.120.71:/home/digix/digix/Models/MMoE/MMoe_DouLoss.ipynb  ./MMoe_DouLoss.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
