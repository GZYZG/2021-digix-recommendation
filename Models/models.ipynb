{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type='text/css'>\n",
       ".datatable table.frame { margin-bottom: 0; }\n",
       ".datatable table.frame thead { border-bottom: none; }\n",
       ".datatable table.frame tr.coltypes td {  color: #FFFFFF;  line-height: 6px;  padding: 0 0.5em;}\n",
       ".datatable .bool    { background: #DDDD99; }\n",
       ".datatable .object  { background: #565656; }\n",
       ".datatable .int     { background: #5D9E5D; }\n",
       ".datatable .float   { background: #4040CC; }\n",
       ".datatable .str     { background: #CC4040; }\n",
       ".datatable .time    { background: #40CC40; }\n",
       ".datatable .row_index {  background: var(--jp-border-color3);  border-right: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  font-size: 9px;}\n",
       ".datatable .frame tbody td { text-align: left; }\n",
       ".datatable .frame tr.coltypes .row_index {  background: var(--jp-border-color0);}\n",
       ".datatable th:nth-child(2) { padding-left: 12px; }\n",
       ".datatable .hellipsis {  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .vellipsis {  background: var(--jp-layout-color0);  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .na {  color: var(--jp-cell-editor-border-color);  font-size: 80%;}\n",
       ".datatable .sp {  opacity: 0.25;}\n",
       ".datatable .footer { font-size: 9px; }\n",
       ".datatable .frame_dimensions {  background: var(--jp-border-color3);  border-top: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  display: inline-block;  opacity: 0.6;  padding: 1px 10px 1px 5px;}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from time import time\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from collections import Counter\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import NearMiss, TomekLinks\n",
    "import datatable as dt\n",
    "import warnings\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "from tools import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 给定预测标签，计算AUC\n",
    "使用OVR的策略计算每个类别的AUC\n",
    "过程：\n",
    "- 选择类别i作为正类，其他类别作为负类\n",
    "- 将真实标签中不等于i的标记为0，等于i的标记为1\n",
    "- 将预测标签中不等于i的标记为0，等于ide标记为1\n",
    "- 计算混淆矩阵\n",
    "- 计算(fpr, tpr)\n",
    "- 计算AUC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 加载数据\n",
    "训练数据加载过程：\n",
    "1. 分别加载处理好的用户特征和视频特征，以及整合的用户历史行为数据；\n",
    "2. 从用户历史行为数据中筛掉在视频特征中没出现过的video_id；\n",
    "3. 将行为数据中的user_id、video_id替换为对应用户/视频的特征\n",
    "4. 根据不同的任务划分为`watch_label`、`is_share`的数据集\n",
    "\n",
    "推断时，类似于上述过程拼接数据。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 读取数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = \"../2021_3_data\"\n",
    "test_data_dir  = os.path.join(base_dir, \"testdata\")\n",
    "train_data_dir = os.path.join(base_dir, \"traindata\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 基础特征与附加特征合并"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_status = dt.fread(os.path.join(train_data_dir, \"video_features_data/video_status.csv\"))\n",
    "user_status = dt.fread(os.path.join(train_data_dir, \"user_features_data/user_status.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "tab_user = dt.fread(os.path.join(train_data_dir, \"user_features_data/user_features.jay\"))\n",
    "tab_video = dt.fread(os.path.join(train_data_dir, \"video_features_data/video_features.jay\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_status.key = 'video_id'\n",
    "video_ws = tab_video[:, :, join(video_status)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_status.key = 'user_id'\n",
    "user_ws = tab_user[:, :, join(user_status)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_ws.to_jay(os.path.join(train_data_dir, \"video_features_data/video_features_with_status.jay\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_ws.to_jay(os.path.join(train_data_dir, \"user_features_data/user_features_with_status.jay\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class='datatable'>\n",
       "  <table class='frame'>\n",
       "  <thead>\n",
       "    <tr class='colnames'><td class='row_index'></td><th>user_id</th><th>age_0</th><th>age_1</th><th>age_2</th><th>age_3</th><th>age_4</th><th>age_5</th><th>age_6</th><th>age_7</th><th>gender_0</th><th class='vellipsis'>&hellip;</th><th>average_watch_label</th><th>sum_watch_times</th><th>sum_comment_times</th><th>sum_collect_times</th><th>sum_share_times</th></tr>\n",
       "    <tr class='coltypes'><td class='row_index'></td><td class='float' title='float64'>&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;</td><td class='float' title='float64'>&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;</td><td class='float' title='float64'>&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;</td><td class='float' title='float64'>&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;</td><td class='float' title='float64'>&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;</td><td class='float' title='float64'>&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;</td><td class='float' title='float64'>&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;</td><td class='float' title='float64'>&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;</td><td class='float' title='float64'>&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;</td><td class='float' title='float64'>&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;</td><td></td><td class='float' title='float64'>&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;</td><td class='float' title='float64'>&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;</td><td class='float' title='float64'>&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;</td><td class='float' title='float64'>&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;</td><td class='float' title='float64'>&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;</td></tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr><td class='row_index'>0</td><td>1.757e+06</td><td>0</td><td>0</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td class=vellipsis>&hellip;</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr>\n",
       "    <tr><td class='row_index'>1</td><td>17938</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td><td class=vellipsis>&hellip;</td><td>0.0967742</td><td>3</td><td>0</td><td>0</td><td>0</td></tr>\n",
       "    <tr><td class='row_index'>2</td><td>4.26352e+06</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td><td class=vellipsis>&hellip;</td><td>0.204545</td><td>2</td><td>0</td><td>0</td><td>0</td></tr>\n",
       "    <tr><td class='row_index'>3</td><td>1.4116e+06</td><td>0</td><td>0</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td><td class=vellipsis>&hellip;</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr>\n",
       "    <tr><td class='row_index'>4</td><td>3.99224e+06</td><td>0</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td><td class=vellipsis>&hellip;</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr>\n",
       "    <tr><td class='row_index'>5</td><td>4.0116e+06</td><td>0</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td><td class=vellipsis>&hellip;</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr>\n",
       "    <tr><td class='row_index'>6</td><td>4.78556e+06</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td><td class=vellipsis>&hellip;</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr>\n",
       "    <tr><td class='row_index'>7</td><td>5.11036e+06</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td><td class=vellipsis>&hellip;</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr>\n",
       "    <tr><td class='row_index'>8</td><td>1.3212e+06</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td><td class=vellipsis>&hellip;</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr>\n",
       "    <tr><td class='row_index'>9</td><td>3.20698e+06</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td class=vellipsis>&hellip;</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr>\n",
       "    <tr><td class='row_index'>10</td><td>5.18172e+06</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td class=vellipsis>&hellip;</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td></tr>\n",
       "    <tr><td class='row_index'>11</td><td>1.878e+06</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td><td class=vellipsis>&hellip;</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td></tr>\n",
       "    <tr><td class='row_index'>12</td><td>3.04464e+06</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td class=vellipsis>&hellip;</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td></tr>\n",
       "    <tr><td class='row_index'>13</td><td>108273</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td class=vellipsis>&hellip;</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr>\n",
       "    <tr><td class='row_index'>14</td><td>5.64838e+06</td><td>0</td><td>0</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td class=vellipsis>&hellip;</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr>\n",
       "    <tr><td class='row_index'>&#x22EE;</td><td class='hellipsis'>&#x22EE;</td><td class='hellipsis'>&#x22EE;</td><td class='hellipsis'>&#x22EE;</td><td class='hellipsis'>&#x22EE;</td><td class='hellipsis'>&#x22EE;</td><td class='hellipsis'>&#x22EE;</td><td class='hellipsis'>&#x22EE;</td><td class='hellipsis'>&#x22EE;</td><td class='hellipsis'>&#x22EE;</td><td class='hellipsis'>&#x22EE;</td><td class='hellipsis'>&#x22F1;</td><td class='hellipsis'>&#x22EE;</td><td class='hellipsis'>&#x22EE;</td><td class='hellipsis'>&#x22EE;</td><td class='hellipsis'>&#x22EE;</td><td class='hellipsis'>&#x22EE;</td></tr>\n",
       "    <tr><td class='row_index'>5,910,795</td><td>3.22343e+06</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td><td>1</td><td class=vellipsis>&hellip;</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr>\n",
       "    <tr><td class='row_index'>5,910,796</td><td>4.70783e+06</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td><td>1</td><td class=vellipsis>&hellip;</td><td>0.142857</td><td>3</td><td>0</td><td>0</td><td>0</td></tr>\n",
       "    <tr><td class='row_index'>5,910,797</td><td>5.90765e+06</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td><td class=vellipsis>&hellip;</td><td>0</td><td>2</td><td>0</td><td>0</td><td>0</td></tr>\n",
       "    <tr><td class='row_index'>5,910,798</td><td>3.63322e+06</td><td>0</td><td>0</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td><td class=vellipsis>&hellip;</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr>\n",
       "    <tr><td class='row_index'>5,910,799</td><td>782537</td><td>0</td><td>0</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td><td class=vellipsis>&hellip;</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr>\n",
       "  </tbody>\n",
       "  </table>\n",
       "  <div class='footer'>\n",
       "    <div class='frame_dimensions'>5,910,800 rows &times; 36 columns</div>\n",
       "  </div>\n",
       "</div>\n"
      ],
      "text/plain": [
       "<Frame#7f315506be10 5910800x36>"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_ws"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 通过 .npz 读取数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 单表读取后合并"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.6 s, sys: 2.4 s, total: 6 s\n",
      "Wall time: 12.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# 单独读取每个文件再进行合并\n",
    "user_df = read_npz_to_df(os.path.join(train_data_dir, \"user_features_data/user_features.npz\"), data_name='features', column_name='columns')\n",
    "video_df = read_npz_to_df(os.path.join(train_data_dir, \"video_features_data/video_features.npz\"), data_name='features')\n",
    "action_df = read_npz_to_df(os.path.join(train_data_dir, \"all_actions.npz\"), data_name='data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 797,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 因为将字符串保存到 .npz时会使dtype为object，重新读回DataFrame时各个列的数据类型均为 object，所以先转换类型\n",
    "dtypes = dict(zip(video_df.columns, [np.float32] * video_df.shape[1]))\n",
    "dtypes.update({'video_name': np.str})\n",
    "video_df = video_df.astype(dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 802,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 29s, sys: 5.88 s, total: 1min 35s\n",
      "Wall time: 40.1 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(7353024, 76)"
      ]
     },
     "execution_count": 802,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# 合并各个表\n",
    "df_train = merge_user_video_action(user_df, video_df, action_df)\n",
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 805,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez(os.path.join(train_data_dir, \"train.npz\"), data=df_train.to_pandas().values, columns=df_train.to_pandas().columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 777,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 626 ms, sys: 0 ns, total: 626 ms\n",
      "Wall time: 721 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "test_df = load_table(os.path.join(test_data_dir, \"test.csv\"), ftype=\"csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 779,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 8s, sys: 12.8 s, total: 3min 20s\n",
      "Wall time: 51.2 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2822180, 73)"
      ]
     },
     "execution_count": 779,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "df_test = merge_user_video_action(user_df, video_df, test_df)\n",
    "df_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 读取合并好的数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 806,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 2s, sys: 38.4 s, total: 3min 40s\n",
      "Wall time: 3min 41s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(7353024, 76)"
      ]
     },
     "execution_count": 806,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# 直接读取保存好的合并后的训练数据\n",
    "path = os.path.join(train_data_dir, \"train.npz\")\n",
    "df_train = read_npz_to_df(path, data_name='data')\n",
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 810,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 37s, sys: 36.9 s, total: 2min 14s\n",
      "Wall time: 5min 49s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2822180, 73)"
      ]
     },
     "execution_count": 810,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# 直接读取保存好的合并后的测试数据\n",
    "path = os.path.join(test_data_dir, \"test.npz\")\n",
    "df_test = read_npz_to_df(path, data_name='data')\n",
    "df_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 通过 .jay 文件读取"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 单表读取后合并"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with_status = False\n",
    "if with_status:\n",
    "    user_features_name = \"user_features_with_status\"\n",
    "    video_features_name = \"video_features_with_status\"\n",
    "else:\n",
    "    user_features_name = \"user_features\"\n",
    "    video_features_name = \"video_features\"\n",
    "    \n",
    "p_user = os.path.join(train_data_dir, f\"user_features_data/{user_features_name}.jay\")\n",
    "p_video = os.path.join(train_data_dir, f\"video_features_data/{video_features_name}.jay\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 26.2 s, sys: 1.3 s, total: 27.5 s\n",
      "Wall time: 1.42 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(7353024, 133)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "## 使用datatable 加载训练数据\n",
    "p_act = os.path.join(train_data_dir, \"all_actions_with_status.jay\")\n",
    "\n",
    "df_train, others = load_train_test_data(None, pre_merged=False, return_others=True,\n",
    "                           **{\"p_user\": p_user, \"p_video\": p_video, \"p_action\": p_act})\n",
    "user_df = others['user']\n",
    "video_df = others['video']\n",
    "action_df = others['action']\n",
    "df_train.shape"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "action_date = dt.fread(os.path.join(train_data_dir, f\"all_actions_with_ptd.jay\"))\n",
    "video_status = dt.fread(os.path.join(train_data_dir, f\"video_features_data/video_137days_status.csv\"))\n",
    "user_status = dt.fread(os.path.join(train_data_dir, f\"user_features_data/user_137days_status.csv\"))\n",
    "video_status.key = ('video_id', 'pt_d')\n",
    "user_status.key = ('user_id', 'pt_d')\n",
    "video_status"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "tt = df_train.to_pandas()\n",
    "np.savez(os.path.join(train_data_dir, \"train\"), data=tt.values, columns=tt.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 21.1 s, sys: 1.24 s, total: 22.4 s\n",
      "Wall time: 1.16 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2822180, 130)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# p_user = os.path.join(train_data_dir, \"user_features_data/user_features.jay\")\n",
    "# p_video = os.path.join(train_data_dir, \"video_features_data/video_features.jay\")\n",
    "p_act = os.path.join(test_data_dir, \"test_with_status.jay\")\n",
    "\n",
    "#path = os.path.join(test_data_dir, \"test.jay\")\n",
    "kwargs = {\"p_user\": p_user, \"p_video\": p_video, \"p_action\": p_act}\n",
    "\n",
    "df_test, others = load_train_test_data(None, pre_merged=False, return_others=True, **kwargs)\n",
    "test_df = others['action']\n",
    "df_test.shape"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "test_df = dt.fread(os.path.join(test_data_dir, \"test.csv\"))\n",
    "video_status = dt.fread(os.path.join(train_data_dir, f\"video_features_data/video_137days_status.csv\"))\n",
    "user_status = dt.fread(os.path.join(train_data_dir, f\"user_features_data/user_137days_status.csv\"))\n",
    "video_status.key = ('video_id', 'pt_d')\n",
    "user_status.key = ('user_id', 'pt_d')\n",
    "test_df['pt_d'] = 20210503\n",
    "test_df = test_df[:, :, dt.join(video_status)]\n",
    "test_df = test_df[:, :, dt.join(user_status)]\n",
    "test_df"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "del test_df['pt_d']\n",
    "test_df.to_jay(os.path.join(test_data_dir, \"test_with_status.jay\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_df = action_df.to_pandas()\n",
    "user_df = user_df.to_pandas()\n",
    "video_df = video_df.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = test_df.to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 读取合并好后的数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_test.to_jay(os.path.join(test_data_dir, \"test_with_status.jay\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.to_jay(os.path.join(train_data_dir, \"train_with_status.jay\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 177 ms, total: 177 ms\n",
      "Wall time: 184 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(7353024, 76)"
      ]
     },
     "execution_count": 468,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# 直接读取保存好的合并后的训练数据\n",
    "path = os.path.join(train_data_dir, \"train.jay\")\n",
    "df_train = load_train_test_data(path, pre_merged=True)\n",
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 649 µs, sys: 46 µs, total: 695 µs\n",
      "Wall time: 684 µs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2822180, 72)"
      ]
     },
     "execution_count": 454,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# 直接读取保存好的合并后的测试数据\n",
    "path = os.path.join(test_data_dir, \"test.jay\")\n",
    "df_test = load_train_test_data(path, pre_merged=True)\n",
    "df_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 处理训练数据\n",
    "可在此做一些预处理：\n",
    "- 从用户历史行为数据中筛掉在视频特征中没出现过的video_id\n",
    "- 删除多余的列\n",
    "- 调整列的顺序\n",
    "- 改变列的数据类型\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 26.8 s, sys: 3.54 s, total: 30.4 s\n",
      "Wall time: 7.53 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if isinstance(df_train, dt.Frame):\n",
    "    df_train = df_train.to_pandas()\n",
    "if isinstance(df_test, dt.Frame):\n",
    "    df_test = df_test.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7353024 entries, 0 to 7353023\n",
      "Columns: 133 entries, user_id to da_4\n",
      "dtypes: float32(40), float64(63), int32(20), int64(9), object(1)\n",
      "memory usage: 5.6+ GB\n"
     ]
    }
   ],
   "source": [
    "df_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 删除 video_name、is_watch 列\n",
    "df_train.drop(['video_name', 'is_watch'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'action_df' not in dir():\n",
    "    action_df = load_table(os.path.join(train_data_dir, \"all_actions.jay\")).to_pandas()\n",
    "if 'video_df' not in dir():\n",
    "    video_df = load_table(os.path.join(train_data_dir, \"video_features_data/video_features.jay\")).to_pandas()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# 从用户历史行为数据中筛掉在视频特征中没出现过的video_id\n",
    "idx1 = pd.Index(action_df['video_id'].unique())\n",
    "idx2 = pd.Index(video_df['video_id'])\n",
    "not_exists = idx1.difference(idx2)\n",
    "not_exists"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "%%time\n",
    "# 将训练数据中未出现的视频剔除\n",
    "n = 0\n",
    "for vid in not_exists:\n",
    "    tn = (df_train['video_id'] == vid).sum()\n",
    "    df_train['video_id'].replace(vid, np.nan, inplace=True)\n",
    "    n += tn\n",
    "\n",
    "if n > 0:\n",
    "    df_train.dropna(axis=0, inplace=True)\n",
    "    df_train.reset_index(drop=True, inplace=True)\n",
    "print(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 删除 video_id、user_id列\n",
    "df_train.drop(['user_id', 'video_id'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7353024, 129)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = df_train\n",
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((7353024,), (7353024,), (7353024, 127))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 准备数据\n",
    "watch_label = dataset.pop('watch_label').astype(np.uint8)\n",
    "is_share = dataset.pop('is_share').astype(np.uint8)\n",
    "watch_label.shape, is_share.shape, dataset.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 处理测试数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'test_df' not in dir():\n",
    "    test_df = pd.read_csv(os.path.join(test_data_dir, \"test.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2822180 entries, 0 to 2822179\n",
      "Data columns (total 59 columns):\n",
      " #   Column                 Dtype  \n",
      "---  ------                 -----  \n",
      " 0   user_id                int32  \n",
      " 1   video_id               int32  \n",
      " 2   v_avg_watch_label_1    float64\n",
      " 3   v_sum_watch_times_1    float64\n",
      " 4   v_sum_watch_overs_1    float64\n",
      " 5   v_sum_comment_times_1  float64\n",
      " 6   v_sum_collect_times_1  float64\n",
      " 7   v_sum_share_times_1    float64\n",
      " 8   v_sum_quit_times_1     float64\n",
      " 9   v_sum_skip_times_1     float64\n",
      " 10  v_sum_watch_days_1     float64\n",
      " 11  v_avg_watch_label_3    float64\n",
      " 12  v_sum_watch_times_3    float64\n",
      " 13  v_sum_watch_overs_3    float64\n",
      " 14  v_sum_comment_times_3  float64\n",
      " 15  v_sum_collect_times_3  float64\n",
      " 16  v_sum_share_times_3    float64\n",
      " 17  v_sum_quit_times_3     float64\n",
      " 18  v_sum_skip_times_3     float64\n",
      " 19  v_sum_watch_days_3     float64\n",
      " 20  v_avg_watch_label_7    float64\n",
      " 21  v_sum_watch_times_7    float64\n",
      " 22  v_sum_watch_overs_7    float64\n",
      " 23  v_sum_comment_times_7  float64\n",
      " 24  v_sum_collect_times_7  float64\n",
      " 25  v_sum_share_times_7    float64\n",
      " 26  v_sum_quit_times_7     float64\n",
      " 27  v_sum_skip_times_7     float64\n",
      " 28  v_sum_watch_days_7     float64\n",
      " 29  u_avg_watch_label_1    float64\n",
      " 30  u_sum_watch_times_1    float64\n",
      " 31  u_sum_watch_overs_1    float64\n",
      " 32  u_sum_quit_times_1     float64\n",
      " 33  u_sum_skip_times_1     object \n",
      " 34  u_sum_comment_times_1  float64\n",
      " 35  u_sum_collect_times_1  float64\n",
      " 36  u_sum_share_times_1    float64\n",
      " 37  u_sum_watch_time_1     float64\n",
      " 38  u_sum_watch_days_1     object \n",
      " 39  u_avg_watch_label_3    float64\n",
      " 40  u_sum_watch_times_3    float64\n",
      " 41  u_sum_watch_overs_3    float64\n",
      " 42  u_sum_quit_times_3     float64\n",
      " 43  u_sum_skip_times_3     object \n",
      " 44  u_sum_comment_times_3  float64\n",
      " 45  u_sum_collect_times_3  float64\n",
      " 46  u_sum_share_times_3    float64\n",
      " 47  u_sum_watch_time_3     float64\n",
      " 48  u_sum_watch_days_3     float64\n",
      " 49  u_avg_watch_label_7    float64\n",
      " 50  u_sum_watch_times_7    float64\n",
      " 51  u_sum_watch_overs_7    float64\n",
      " 52  u_sum_quit_times_7     float64\n",
      " 53  u_sum_skip_times_7     object \n",
      " 54  u_sum_comment_times_7  float64\n",
      " 55  u_sum_collect_times_7  float64\n",
      " 56  u_sum_share_times_7    float64\n",
      " 57  u_sum_watch_time_7     float64\n",
      " 58  u_sum_watch_days_7     float64\n",
      "dtypes: float64(53), int32(2), object(4)\n",
      "memory usage: 1.2+ GB\n"
     ]
    }
   ],
   "source": [
    "test_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2822180 entries, 0 to 2822179\n",
      "Columns: 130 entries, user_id to da_4\n",
      "dtypes: float32(40), float64(83), int32(2), object(5)\n",
      "memory usage: 2.3+ GB\n"
     ]
    }
   ],
   "source": [
    "df_test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 删除 video_name 列\n",
    "if 'video_name' in df_test.columns:\n",
    "    df_test.drop('video_name', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# 测试数据集中存在video_id没有在视频特征中出现\n",
    "idx1 = pd.Index(test_df['video_id'].unique())\n",
    "idx2 = pd.Index(video_df['video_id'].unique())\n",
    "non_exists = idx1.difference(idx2)\n",
    "non_exists"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "%%time\n",
    "t0 = time()\n",
    "n = 0\n",
    "for vid in not_exists:\n",
    "    tn = (test_df['video_id'] == vid).sum()\n",
    "#     df_test = action_df[action_df['video_id'] != vid]\n",
    "    n += tn\n",
    "\n",
    "print(f\"在视频特征中不存在的video_id在测试数据集中出现的次数 = {n}\\t\\t(cost {time() - t0:.3f}s)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 删除 video_id、user_id 列\n",
    "df_test.drop(['user_id', 'video_id'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2822180, 127)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inference_dataset = df_test\n",
    "inference_dataset.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# watch_label 预测"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 处理数据不均衡问题"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 5176743), (1, 557421), (2, 314107), (3, 219188), (4, 172404), (5, 143001), (6, 125092), (7, 117749), (8, 138798), (9, 388521)]\n",
      "[[0.         0.70402912]\n",
      " [1.         0.0758084 ]\n",
      " [2.         0.04271807]\n",
      " [3.         0.02980923]\n",
      " [4.         0.02344668]\n",
      " [5.         0.01944792]\n",
      " [6.         0.01701232]\n",
      " [7.         0.01601368]\n",
      " [8.         0.01887632]\n",
      " [9.         0.05283826]]\n"
     ]
    }
   ],
   "source": [
    "items = list(Counter(watch_label).items())\n",
    "items.sort(key=lambda x: x[0])\n",
    "print(items)\n",
    "\n",
    "dist = np.array(items, dtype=np.float)\n",
    "dist[:, 1] = dist[:, 1] / watch_label.shape[0]\n",
    "print(dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "under_ss = np.array(items)\n",
    "under_ss_thresh = under_ss[9, 1]  # 设置每个类别样本数目的上限\n",
    "under_ss[:, 1] = np.clip(under_ss[:, 1], a_min=None, a_max=under_ss_thresh)\n",
    "\n",
    "over_ss = under_ss.copy()\n",
    "over_ss_thresh = under_ss[2, 1]  # 设置每个类别样本数据的下限\n",
    "over_ss[:, 1] = np.clip(over_ss[:, 1], a_min=over_ss_thresh, a_max=None)\n",
    "\n",
    "under_ss = dict(under_ss)\n",
    "over_ss = dict(over_ss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({0: 388521,\n",
       "  1: 388521,\n",
       "  2: 314107,\n",
       "  3: 219188,\n",
       "  4: 172404,\n",
       "  5: 143001,\n",
       "  6: 125092,\n",
       "  7: 117749,\n",
       "  8: 138798,\n",
       "  9: 388521},\n",
       " {0: 388521,\n",
       "  1: 388521,\n",
       "  2: 314107,\n",
       "  3: 314107,\n",
       "  4: 314107,\n",
       "  5: 314107,\n",
       "  6: 314107,\n",
       "  7: 314107,\n",
       "  8: 314107,\n",
       "  9: 388521})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "under_ss, over_ss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({0: 4788222})\n",
      "Counter({1: 168900})\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Int64Index([      1,       3,       6,       8,       9,      10,      11,\n",
       "                 12,      13,      14,\n",
       "            ...\n",
       "            7353008, 7353009, 7353010, 7353014, 7353016, 7353018, 7353019,\n",
       "            7353020, 7353022, 7353023],\n",
       "           dtype='int64', length=4957122)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del_idxs = pd.Index([], dtype=int)\n",
    "for l, n in items:\n",
    "    if n > under_ss_thresh:\n",
    "        t_idxs = watch_label == l\n",
    "        t_idxs = t_idxs.replace(False, np.nan).dropna().index  # 保留watch_label=l的行索引\n",
    "        t_left_idxs = np.random.choice(t_idxs, under_ss_thresh, replace=False)  # 选择一部分保留，注意replace参数，为True时会重复采样\n",
    "        t_del_idxs = t_idxs.difference(t_left_idxs)\n",
    "        print(Counter(watch_label[t_del_idxs]))\n",
    "                \n",
    "        del_idxs = del_idxs.union(t_del_idxs)\n",
    "del_idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({2: 314107,\n",
       "         0: 5176743,\n",
       "         5: 143001,\n",
       "         4: 172404,\n",
       "         1: 557421,\n",
       "         9: 388521,\n",
       "         3: 219188,\n",
       "         8: 138798,\n",
       "         7: 117749,\n",
       "         6: 125092})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(watch_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2395902, 127), (2395902,))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resampled_data = np.delete(dataset.values, del_idxs, axis=0)\n",
    "resampled_wl = np.delete(watch_label.values, del_idxs, axis=0)\n",
    "resampled_data.shape, resampled_wl.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({2: 314107,\n",
       "         5: 143001,\n",
       "         4: 172404,\n",
       "         0: 388521,\n",
       "         9: 388521,\n",
       "         3: 219188,\n",
       "         1: 388521,\n",
       "         8: 138798,\n",
       "         7: 117749,\n",
       "         6: 125092})"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(resampled_wl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 速度太慢，难以忍受！\n",
    "nm  = TomekLinks()\n",
    "smt = SMOTE(sampling_strategy=over_ss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nm' is not defined"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X_r, y_r = nm.fit_resample(dataset, watch_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'smt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'smt' is not defined"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X_r, y_r = smt.fit_resample(resampled_data, resampled_wl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2395902, 127), (2395902,))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 将采样后的数据重装回 DataFrame\n",
    "data = pd.DataFrame(resampled_data, columns=dataset.columns)\n",
    "watch_label_res = pd.Series(resampled_wl)\n",
    "data.shape, watch_label_res.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2156311,), (239591,))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_rate = 0.1\n",
    "train_idx, test_idx = train_test_split(data.index, test_size=test_rate, random_state=0)\n",
    "train_idx.shape, test_idx.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = data.iloc[train_idx]\n",
    "X_test  = data.iloc[test_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = watch_label_res.iloc[train_idx]\n",
    "y_test  = watch_label_res.iloc[test_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2156311, 127), (2156311,), (239591, 127), (239591,))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, y_train.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data preparing finished ...\t\t(1.862s)\n"
     ]
    }
   ],
   "source": [
    "t0 = time()\n",
    "xg_train = xgb.DMatrix(X_train.values, label=y_train.values, enable_categorical=True)\n",
    "xg_test = xgb.DMatrix(X_test.values, label=y_test.values, enable_categorical=True)\n",
    "print(f\"Data preparing finished ...\\t\\t({time()-t0:.3f}s)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup parameters for xgboost\n",
    "param = {\n",
    "    'objective': 'multi:softmax',\n",
    "    'eta': 0.1,\n",
    "    'nthread': 8,\n",
    "    'num_class': 10,\n",
    "    'gpu_id': 0,\n",
    "    'tree_method': 'gpu_hist',\n",
    "    'eval_metric': ['mlogloss', 'auc', 'merror'],\n",
    "    'max_depth': 9,\n",
    "    'min_child_weight': 9,\n",
    "    'gamma': 0.2,\n",
    "    'subsample': 0.9,\n",
    "    'colsample_bytree': 0.6,\n",
    "    'reg_alpha': 0\n",
    "}\n",
    "\n",
    "watchlist = [(xg_train, 'train'), (xg_test, 'test')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-mlogloss:2.27349\ttrain-auc:0.63110\ttrain-merror:0.74005\ttest-mlogloss:2.27406\ttest-auc:0.62286\ttest-merror:0.74236\n",
      "[1]\ttrain-mlogloss:2.24971\ttrain-auc:0.63740\ttrain-merror:0.73767\ttest-mlogloss:2.25090\ttest-auc:0.62761\ttest-merror:0.74009\n",
      "[2]\ttrain-mlogloss:2.22949\ttrain-auc:0.64162\ttrain-merror:0.73592\ttest-mlogloss:2.23126\ttest-auc:0.63085\ttest-merror:0.73847\n",
      "[3]\ttrain-mlogloss:2.21226\ttrain-auc:0.64404\ttrain-merror:0.73495\ttest-mlogloss:2.21455\ttest-auc:0.63272\ttest-merror:0.73769\n",
      "[4]\ttrain-mlogloss:2.19778\ttrain-auc:0.64600\ttrain-merror:0.73447\ttest-mlogloss:2.20066\ttest-auc:0.63390\ttest-merror:0.73762\n",
      "[5]\ttrain-mlogloss:2.18421\ttrain-auc:0.64724\ttrain-merror:0.73389\ttest-mlogloss:2.18763\ttest-auc:0.63480\ttest-merror:0.73690\n",
      "[6]\ttrain-mlogloss:2.17215\ttrain-auc:0.64844\ttrain-merror:0.73333\ttest-mlogloss:2.17613\ttest-auc:0.63561\ttest-merror:0.73695\n",
      "[7]\ttrain-mlogloss:2.16158\ttrain-auc:0.64937\ttrain-merror:0.73314\ttest-mlogloss:2.16615\ttest-auc:0.63602\ttest-merror:0.73659\n",
      "[8]\ttrain-mlogloss:2.15198\ttrain-auc:0.65034\ttrain-merror:0.73271\ttest-mlogloss:2.15713\ttest-auc:0.63659\ttest-merror:0.73640\n",
      "[9]\ttrain-mlogloss:2.14334\ttrain-auc:0.65109\ttrain-merror:0.73221\ttest-mlogloss:2.14907\ttest-auc:0.63694\ttest-merror:0.73592\n",
      "[10]\ttrain-mlogloss:2.13574\ttrain-auc:0.65172\ttrain-merror:0.73195\ttest-mlogloss:2.14202\ttest-auc:0.63730\ttest-merror:0.73601\n",
      "[11]\ttrain-mlogloss:2.12895\ttrain-auc:0.65237\ttrain-merror:0.73161\ttest-mlogloss:2.13575\ttest-auc:0.63764\ttest-merror:0.73543\n",
      "[12]\ttrain-mlogloss:2.12271\ttrain-auc:0.65310\ttrain-merror:0.73128\ttest-mlogloss:2.13006\ttest-auc:0.63799\ttest-merror:0.73520\n",
      "[13]\ttrain-mlogloss:2.11689\ttrain-auc:0.65380\ttrain-merror:0.73095\ttest-mlogloss:2.12479\ttest-auc:0.63834\ttest-merror:0.73481\n",
      "[14]\ttrain-mlogloss:2.11165\ttrain-auc:0.65451\ttrain-merror:0.73061\ttest-mlogloss:2.12015\ttest-auc:0.63868\ttest-merror:0.73480\n",
      "[15]\ttrain-mlogloss:2.10689\ttrain-auc:0.65509\ttrain-merror:0.73038\ttest-mlogloss:2.11595\ttest-auc:0.63893\ttest-merror:0.73462\n",
      "[16]\ttrain-mlogloss:2.10265\ttrain-auc:0.65570\ttrain-merror:0.73002\ttest-mlogloss:2.11226\ttest-auc:0.63921\ttest-merror:0.73422\n",
      "[17]\ttrain-mlogloss:2.09861\ttrain-auc:0.65624\ttrain-merror:0.72977\ttest-mlogloss:2.10873\ttest-auc:0.63953\ttest-merror:0.73403\n",
      "[18]\ttrain-mlogloss:2.09503\ttrain-auc:0.65680\ttrain-merror:0.72947\ttest-mlogloss:2.10569\ttest-auc:0.63975\ttest-merror:0.73395\n",
      "[19]\ttrain-mlogloss:2.09180\ttrain-auc:0.65736\ttrain-merror:0.72912\ttest-mlogloss:2.10298\ttest-auc:0.63999\ttest-merror:0.73367\n",
      "[20]\ttrain-mlogloss:2.08870\ttrain-auc:0.65792\ttrain-merror:0.72884\ttest-mlogloss:2.10041\ttest-auc:0.64024\ttest-merror:0.73338\n",
      "[21]\ttrain-mlogloss:2.08587\ttrain-auc:0.65851\ttrain-merror:0.72864\ttest-mlogloss:2.09812\ttest-auc:0.64049\ttest-merror:0.73330\n",
      "[22]\ttrain-mlogloss:2.08325\ttrain-auc:0.65903\ttrain-merror:0.72835\ttest-mlogloss:2.09605\ttest-auc:0.64066\ttest-merror:0.73333\n",
      "[23]\ttrain-mlogloss:2.08079\ttrain-auc:0.65951\ttrain-merror:0.72812\ttest-mlogloss:2.09410\ttest-auc:0.64089\ttest-merror:0.73316\n",
      "[24]\ttrain-mlogloss:2.07842\ttrain-auc:0.66012\ttrain-merror:0.72779\ttest-mlogloss:2.09232\ttest-auc:0.64111\ttest-merror:0.73294\n",
      "[25]\ttrain-mlogloss:2.07633\ttrain-auc:0.66063\ttrain-merror:0.72748\ttest-mlogloss:2.09072\ttest-auc:0.64136\ttest-merror:0.73267\n",
      "[26]\ttrain-mlogloss:2.07436\ttrain-auc:0.66114\ttrain-merror:0.72720\ttest-mlogloss:2.08924\ttest-auc:0.64159\ttest-merror:0.73259\n",
      "[27]\ttrain-mlogloss:2.07248\ttrain-auc:0.66163\ttrain-merror:0.72692\ttest-mlogloss:2.08788\ttest-auc:0.64179\ttest-merror:0.73241\n",
      "[28]\ttrain-mlogloss:2.07073\ttrain-auc:0.66213\ttrain-merror:0.72664\ttest-mlogloss:2.08665\ttest-auc:0.64198\ttest-merror:0.73228\n",
      "[29]\ttrain-mlogloss:2.06903\ttrain-auc:0.66266\ttrain-merror:0.72630\ttest-mlogloss:2.08549\ttest-auc:0.64217\ttest-merror:0.73217\n",
      "[30]\ttrain-mlogloss:2.06746\ttrain-auc:0.66312\ttrain-merror:0.72601\ttest-mlogloss:2.08439\ttest-auc:0.64237\ttest-merror:0.73203\n",
      "[31]\ttrain-mlogloss:2.06597\ttrain-auc:0.66357\ttrain-merror:0.72574\ttest-mlogloss:2.08340\ttest-auc:0.64253\ttest-merror:0.73190\n",
      "[32]\ttrain-mlogloss:2.06449\ttrain-auc:0.66408\ttrain-merror:0.72544\ttest-mlogloss:2.08247\ttest-auc:0.64268\ttest-merror:0.73182\n",
      "[33]\ttrain-mlogloss:2.06320\ttrain-auc:0.66451\ttrain-merror:0.72524\ttest-mlogloss:2.08166\ttest-auc:0.64283\ttest-merror:0.73160\n",
      "[34]\ttrain-mlogloss:2.06190\ttrain-auc:0.66501\ttrain-merror:0.72490\ttest-mlogloss:2.08087\ttest-auc:0.64300\ttest-merror:0.73130\n",
      "[35]\ttrain-mlogloss:2.06068\ttrain-auc:0.66546\ttrain-merror:0.72460\ttest-mlogloss:2.08013\ttest-auc:0.64314\ttest-merror:0.73123\n",
      "[36]\ttrain-mlogloss:2.05955\ttrain-auc:0.66589\ttrain-merror:0.72428\ttest-mlogloss:2.07947\ttest-auc:0.64328\ttest-merror:0.73117\n",
      "[37]\ttrain-mlogloss:2.05845\ttrain-auc:0.66636\ttrain-merror:0.72402\ttest-mlogloss:2.07884\ttest-auc:0.64346\ttest-merror:0.73095\n",
      "[38]\ttrain-mlogloss:2.05736\ttrain-auc:0.66682\ttrain-merror:0.72378\ttest-mlogloss:2.07828\ttest-auc:0.64358\ttest-merror:0.73076\n",
      "[39]\ttrain-mlogloss:2.05636\ttrain-auc:0.66719\ttrain-merror:0.72354\ttest-mlogloss:2.07772\ttest-auc:0.64370\ttest-merror:0.73072\n",
      "[40]\ttrain-mlogloss:2.05538\ttrain-auc:0.66763\ttrain-merror:0.72325\ttest-mlogloss:2.07723\ttest-auc:0.64379\ttest-merror:0.73055\n",
      "[41]\ttrain-mlogloss:2.05440\ttrain-auc:0.66802\ttrain-merror:0.72303\ttest-mlogloss:2.07671\ttest-auc:0.64394\ttest-merror:0.73056\n",
      "[42]\ttrain-mlogloss:2.05349\ttrain-auc:0.66844\ttrain-merror:0.72281\ttest-mlogloss:2.07626\ttest-auc:0.64404\ttest-merror:0.73035\n",
      "[43]\ttrain-mlogloss:2.05261\ttrain-auc:0.66886\ttrain-merror:0.72257\ttest-mlogloss:2.07588\ttest-auc:0.64414\ttest-merror:0.73014\n",
      "[44]\ttrain-mlogloss:2.05169\ttrain-auc:0.66934\ttrain-merror:0.72237\ttest-mlogloss:2.07549\ttest-auc:0.64422\ttest-merror:0.73007\n",
      "[45]\ttrain-mlogloss:2.05083\ttrain-auc:0.66978\ttrain-merror:0.72209\ttest-mlogloss:2.07513\ttest-auc:0.64432\ttest-merror:0.72999\n",
      "[46]\ttrain-mlogloss:2.05004\ttrain-auc:0.67017\ttrain-merror:0.72188\ttest-mlogloss:2.07477\ttest-auc:0.64444\ttest-merror:0.72964\n",
      "[47]\ttrain-mlogloss:2.04928\ttrain-auc:0.67054\ttrain-merror:0.72167\ttest-mlogloss:2.07444\ttest-auc:0.64455\ttest-merror:0.72973\n",
      "[48]\ttrain-mlogloss:2.04852\ttrain-auc:0.67094\ttrain-merror:0.72145\ttest-mlogloss:2.07416\ttest-auc:0.64464\ttest-merror:0.72949\n",
      "[49]\ttrain-mlogloss:2.04771\ttrain-auc:0.67140\ttrain-merror:0.72118\ttest-mlogloss:2.07385\ttest-auc:0.64473\ttest-merror:0.72947\n",
      "[50]\ttrain-mlogloss:2.04697\ttrain-auc:0.67181\ttrain-merror:0.72096\ttest-mlogloss:2.07357\ttest-auc:0.64482\ttest-merror:0.72949\n",
      "[51]\ttrain-mlogloss:2.04624\ttrain-auc:0.67221\ttrain-merror:0.72074\ttest-mlogloss:2.07327\ttest-auc:0.64494\ttest-merror:0.72942\n",
      "[52]\ttrain-mlogloss:2.04547\ttrain-auc:0.67265\ttrain-merror:0.72052\ttest-mlogloss:2.07300\ttest-auc:0.64505\ttest-merror:0.72933\n",
      "[53]\ttrain-mlogloss:2.04478\ttrain-auc:0.67300\ttrain-merror:0.72036\ttest-mlogloss:2.07273\ttest-auc:0.64514\ttest-merror:0.72922\n",
      "[54]\ttrain-mlogloss:2.04407\ttrain-auc:0.67342\ttrain-merror:0.72010\ttest-mlogloss:2.07251\ttest-auc:0.64521\ttest-merror:0.72912\n",
      "[55]\ttrain-mlogloss:2.04342\ttrain-auc:0.67377\ttrain-merror:0.71987\ttest-mlogloss:2.07230\ttest-auc:0.64528\ttest-merror:0.72905\n",
      "[56]\ttrain-mlogloss:2.04273\ttrain-auc:0.67417\ttrain-merror:0.71961\ttest-mlogloss:2.07209\ttest-auc:0.64534\ttest-merror:0.72903\n",
      "[57]\ttrain-mlogloss:2.04210\ttrain-auc:0.67454\ttrain-merror:0.71945\ttest-mlogloss:2.07190\ttest-auc:0.64542\ttest-merror:0.72888\n",
      "[58]\ttrain-mlogloss:2.04147\ttrain-auc:0.67491\ttrain-merror:0.71927\ttest-mlogloss:2.07170\ttest-auc:0.64552\ttest-merror:0.72884\n",
      "[59]\ttrain-mlogloss:2.04082\ttrain-auc:0.67529\ttrain-merror:0.71901\ttest-mlogloss:2.07151\ttest-auc:0.64558\ttest-merror:0.72890\n",
      "[60]\ttrain-mlogloss:2.04021\ttrain-auc:0.67567\ttrain-merror:0.71880\ttest-mlogloss:2.07133\ttest-auc:0.64565\ttest-merror:0.72876\n",
      "[61]\ttrain-mlogloss:2.03963\ttrain-auc:0.67604\ttrain-merror:0.71860\ttest-mlogloss:2.07117\ttest-auc:0.64572\ttest-merror:0.72862\n",
      "[62]\ttrain-mlogloss:2.03905\ttrain-auc:0.67640\ttrain-merror:0.71840\ttest-mlogloss:2.07103\ttest-auc:0.64577\ttest-merror:0.72856\n",
      "[63]\ttrain-mlogloss:2.03851\ttrain-auc:0.67672\ttrain-merror:0.71824\ttest-mlogloss:2.07090\ttest-auc:0.64581\ttest-merror:0.72857\n",
      "[64]\ttrain-mlogloss:2.03795\ttrain-auc:0.67707\ttrain-merror:0.71805\ttest-mlogloss:2.07075\ttest-auc:0.64587\ttest-merror:0.72860\n",
      "[65]\ttrain-mlogloss:2.03731\ttrain-auc:0.67748\ttrain-merror:0.71783\ttest-mlogloss:2.07056\ttest-auc:0.64597\ttest-merror:0.72847\n",
      "[66]\ttrain-mlogloss:2.03672\ttrain-auc:0.67783\ttrain-merror:0.71761\ttest-mlogloss:2.07040\ttest-auc:0.64604\ttest-merror:0.72844\n",
      "[67]\ttrain-mlogloss:2.03619\ttrain-auc:0.67817\ttrain-merror:0.71739\ttest-mlogloss:2.07024\ttest-auc:0.64612\ttest-merror:0.72846\n",
      "[68]\ttrain-mlogloss:2.03566\ttrain-auc:0.67849\ttrain-merror:0.71719\ttest-mlogloss:2.07012\ttest-auc:0.64617\ttest-merror:0.72848\n",
      "[69]\ttrain-mlogloss:2.03508\ttrain-auc:0.67887\ttrain-merror:0.71700\ttest-mlogloss:2.07001\ttest-auc:0.64621\ttest-merror:0.72847\n",
      "[70]\ttrain-mlogloss:2.03460\ttrain-auc:0.67916\ttrain-merror:0.71681\ttest-mlogloss:2.06989\ttest-auc:0.64626\ttest-merror:0.72845\n",
      "[71]\ttrain-mlogloss:2.03408\ttrain-auc:0.67945\ttrain-merror:0.71665\ttest-mlogloss:2.06974\ttest-auc:0.64633\ttest-merror:0.72835\n",
      "[72]\ttrain-mlogloss:2.03360\ttrain-auc:0.67976\ttrain-merror:0.71654\ttest-mlogloss:2.06965\ttest-auc:0.64636\ttest-merror:0.72826\n",
      "[73]\ttrain-mlogloss:2.03308\ttrain-auc:0.68009\ttrain-merror:0.71633\ttest-mlogloss:2.06957\ttest-auc:0.64638\ttest-merror:0.72811\n",
      "[74]\ttrain-mlogloss:2.03262\ttrain-auc:0.68037\ttrain-merror:0.71617\ttest-mlogloss:2.06951\ttest-auc:0.64639\ttest-merror:0.72819\n",
      "[75]\ttrain-mlogloss:2.03209\ttrain-auc:0.68070\ttrain-merror:0.71591\ttest-mlogloss:2.06939\ttest-auc:0.64645\ttest-merror:0.72832\n",
      "[76]\ttrain-mlogloss:2.03158\ttrain-auc:0.68101\ttrain-merror:0.71579\ttest-mlogloss:2.06932\ttest-auc:0.64648\ttest-merror:0.72830\n",
      "[77]\ttrain-mlogloss:2.03105\ttrain-auc:0.68136\ttrain-merror:0.71555\ttest-mlogloss:2.06921\ttest-auc:0.64653\ttest-merror:0.72837\n",
      "[78]\ttrain-mlogloss:2.03056\ttrain-auc:0.68168\ttrain-merror:0.71537\ttest-mlogloss:2.06912\ttest-auc:0.64656\ttest-merror:0.72837\n",
      "[79]\ttrain-mlogloss:2.03011\ttrain-auc:0.68196\ttrain-merror:0.71521\ttest-mlogloss:2.06907\ttest-auc:0.64655\ttest-merror:0.72830\n",
      "[80]\ttrain-mlogloss:2.02957\ttrain-auc:0.68231\ttrain-merror:0.71497\ttest-mlogloss:2.06896\ttest-auc:0.64661\ttest-merror:0.72820\n",
      "[81]\ttrain-mlogloss:2.02908\ttrain-auc:0.68261\ttrain-merror:0.71478\ttest-mlogloss:2.06885\ttest-auc:0.64666\ttest-merror:0.72816\n",
      "[82]\ttrain-mlogloss:2.02862\ttrain-auc:0.68291\ttrain-merror:0.71462\ttest-mlogloss:2.06877\ttest-auc:0.64670\ttest-merror:0.72803\n",
      "[83]\ttrain-mlogloss:2.02816\ttrain-auc:0.68321\ttrain-merror:0.71443\ttest-mlogloss:2.06871\ttest-auc:0.64673\ttest-merror:0.72805\n",
      "[84]\ttrain-mlogloss:2.02765\ttrain-auc:0.68352\ttrain-merror:0.71420\ttest-mlogloss:2.06859\ttest-auc:0.64680\ttest-merror:0.72794\n",
      "[85]\ttrain-mlogloss:2.02721\ttrain-auc:0.68380\ttrain-merror:0.71404\ttest-mlogloss:2.06849\ttest-auc:0.64686\ttest-merror:0.72800\n",
      "[86]\ttrain-mlogloss:2.02670\ttrain-auc:0.68413\ttrain-merror:0.71384\ttest-mlogloss:2.06841\ttest-auc:0.64689\ttest-merror:0.72802\n",
      "[87]\ttrain-mlogloss:2.02620\ttrain-auc:0.68444\ttrain-merror:0.71364\ttest-mlogloss:2.06832\ttest-auc:0.64692\ttest-merror:0.72785\n",
      "[88]\ttrain-mlogloss:2.02571\ttrain-auc:0.68477\ttrain-merror:0.71344\ttest-mlogloss:2.06825\ttest-auc:0.64695\ttest-merror:0.72784\n",
      "[89]\ttrain-mlogloss:2.02523\ttrain-auc:0.68508\ttrain-merror:0.71324\ttest-mlogloss:2.06819\ttest-auc:0.64698\ttest-merror:0.72786\n",
      "[90]\ttrain-mlogloss:2.02477\ttrain-auc:0.68536\ttrain-merror:0.71308\ttest-mlogloss:2.06810\ttest-auc:0.64702\ttest-merror:0.72776\n",
      "[91]\ttrain-mlogloss:2.02439\ttrain-auc:0.68560\ttrain-merror:0.71290\ttest-mlogloss:2.06804\ttest-auc:0.64705\ttest-merror:0.72786\n",
      "[92]\ttrain-mlogloss:2.02394\ttrain-auc:0.68587\ttrain-merror:0.71273\ttest-mlogloss:2.06797\ttest-auc:0.64709\ttest-merror:0.72784\n",
      "[93]\ttrain-mlogloss:2.02353\ttrain-auc:0.68613\ttrain-merror:0.71250\ttest-mlogloss:2.06794\ttest-auc:0.64710\ttest-merror:0.72784\n",
      "[94]\ttrain-mlogloss:2.02306\ttrain-auc:0.68645\ttrain-merror:0.71229\ttest-mlogloss:2.06788\ttest-auc:0.64712\ttest-merror:0.72786\n",
      "[95]\ttrain-mlogloss:2.02260\ttrain-auc:0.68674\ttrain-merror:0.71215\ttest-mlogloss:2.06780\ttest-auc:0.64717\ttest-merror:0.72786\n",
      "[96]\ttrain-mlogloss:2.02217\ttrain-auc:0.68703\ttrain-merror:0.71195\ttest-mlogloss:2.06776\ttest-auc:0.64719\ttest-merror:0.72779\n",
      "[97]\ttrain-mlogloss:2.02177\ttrain-auc:0.68728\ttrain-merror:0.71180\ttest-mlogloss:2.06769\ttest-auc:0.64722\ttest-merror:0.72774\n",
      "[98]\ttrain-mlogloss:2.02137\ttrain-auc:0.68754\ttrain-merror:0.71165\ttest-mlogloss:2.06762\ttest-auc:0.64726\ttest-merror:0.72773\n",
      "[99]\ttrain-mlogloss:2.02092\ttrain-auc:0.68785\ttrain-merror:0.71147\ttest-mlogloss:2.06758\ttest-auc:0.64727\ttest-merror:0.72764\n",
      "[100]\ttrain-mlogloss:2.02053\ttrain-auc:0.68809\ttrain-merror:0.71133\ttest-mlogloss:2.06753\ttest-auc:0.64728\ttest-merror:0.72762\n",
      "[101]\ttrain-mlogloss:2.02020\ttrain-auc:0.68829\ttrain-merror:0.71120\ttest-mlogloss:2.06748\ttest-auc:0.64731\ttest-merror:0.72764\n",
      "[102]\ttrain-mlogloss:2.01980\ttrain-auc:0.68853\ttrain-merror:0.71104\ttest-mlogloss:2.06742\ttest-auc:0.64734\ttest-merror:0.72759\n",
      "[103]\ttrain-mlogloss:2.01940\ttrain-auc:0.68879\ttrain-merror:0.71087\ttest-mlogloss:2.06735\ttest-auc:0.64738\ttest-merror:0.72758\n",
      "[104]\ttrain-mlogloss:2.01905\ttrain-auc:0.68900\ttrain-merror:0.71074\ttest-mlogloss:2.06729\ttest-auc:0.64740\ttest-merror:0.72758\n",
      "[105]\ttrain-mlogloss:2.01866\ttrain-auc:0.68925\ttrain-merror:0.71059\ttest-mlogloss:2.06724\ttest-auc:0.64744\ttest-merror:0.72757\n",
      "[106]\ttrain-mlogloss:2.01827\ttrain-auc:0.68949\ttrain-merror:0.71044\ttest-mlogloss:2.06718\ttest-auc:0.64748\ttest-merror:0.72754\n",
      "[107]\ttrain-mlogloss:2.01785\ttrain-auc:0.68977\ttrain-merror:0.71029\ttest-mlogloss:2.06710\ttest-auc:0.64752\ttest-merror:0.72743\n",
      "[108]\ttrain-mlogloss:2.01751\ttrain-auc:0.68998\ttrain-merror:0.71012\ttest-mlogloss:2.06706\ttest-auc:0.64755\ttest-merror:0.72737\n",
      "[109]\ttrain-mlogloss:2.01710\ttrain-auc:0.69024\ttrain-merror:0.70997\ttest-mlogloss:2.06701\ttest-auc:0.64757\ttest-merror:0.72738\n",
      "[110]\ttrain-mlogloss:2.01674\ttrain-auc:0.69045\ttrain-merror:0.70987\ttest-mlogloss:2.06697\ttest-auc:0.64760\ttest-merror:0.72749\n",
      "[111]\ttrain-mlogloss:2.01633\ttrain-auc:0.69072\ttrain-merror:0.70968\ttest-mlogloss:2.06693\ttest-auc:0.64761\ttest-merror:0.72752\n",
      "[112]\ttrain-mlogloss:2.01591\ttrain-auc:0.69098\ttrain-merror:0.70954\ttest-mlogloss:2.06689\ttest-auc:0.64762\ttest-merror:0.72750\n",
      "[113]\ttrain-mlogloss:2.01554\ttrain-auc:0.69123\ttrain-merror:0.70938\ttest-mlogloss:2.06686\ttest-auc:0.64763\ttest-merror:0.72742\n",
      "[114]\ttrain-mlogloss:2.01513\ttrain-auc:0.69148\ttrain-merror:0.70924\ttest-mlogloss:2.06684\ttest-auc:0.64764\ttest-merror:0.72741\n",
      "[115]\ttrain-mlogloss:2.01472\ttrain-auc:0.69174\ttrain-merror:0.70905\ttest-mlogloss:2.06679\ttest-auc:0.64766\ttest-merror:0.72751\n",
      "[116]\ttrain-mlogloss:2.01433\ttrain-auc:0.69199\ttrain-merror:0.70889\ttest-mlogloss:2.06675\ttest-auc:0.64768\ttest-merror:0.72747\n",
      "[117]\ttrain-mlogloss:2.01391\ttrain-auc:0.69227\ttrain-merror:0.70874\ttest-mlogloss:2.06671\ttest-auc:0.64771\ttest-merror:0.72743\n",
      "[118]\ttrain-mlogloss:2.01350\ttrain-auc:0.69252\ttrain-merror:0.70854\ttest-mlogloss:2.06667\ttest-auc:0.64775\ttest-merror:0.72741\n",
      "[119]\ttrain-mlogloss:2.01306\ttrain-auc:0.69280\ttrain-merror:0.70840\ttest-mlogloss:2.06662\ttest-auc:0.64777\ttest-merror:0.72742\n",
      "[120]\ttrain-mlogloss:2.01269\ttrain-auc:0.69303\ttrain-merror:0.70823\ttest-mlogloss:2.06659\ttest-auc:0.64778\ttest-merror:0.72738\n",
      "[121]\ttrain-mlogloss:2.01233\ttrain-auc:0.69326\ttrain-merror:0.70811\ttest-mlogloss:2.06654\ttest-auc:0.64781\ttest-merror:0.72732\n",
      "[122]\ttrain-mlogloss:2.01193\ttrain-auc:0.69352\ttrain-merror:0.70794\ttest-mlogloss:2.06648\ttest-auc:0.64785\ttest-merror:0.72732\n",
      "[123]\ttrain-mlogloss:2.01156\ttrain-auc:0.69375\ttrain-merror:0.70778\ttest-mlogloss:2.06645\ttest-auc:0.64786\ttest-merror:0.72739\n",
      "[124]\ttrain-mlogloss:2.01120\ttrain-auc:0.69397\ttrain-merror:0.70768\ttest-mlogloss:2.06641\ttest-auc:0.64789\ttest-merror:0.72742\n",
      "[125]\ttrain-mlogloss:2.01080\ttrain-auc:0.69423\ttrain-merror:0.70744\ttest-mlogloss:2.06638\ttest-auc:0.64791\ttest-merror:0.72731\n",
      "[126]\ttrain-mlogloss:2.01041\ttrain-auc:0.69447\ttrain-merror:0.70730\ttest-mlogloss:2.06634\ttest-auc:0.64793\ttest-merror:0.72734\n",
      "[127]\ttrain-mlogloss:2.01007\ttrain-auc:0.69468\ttrain-merror:0.70711\ttest-mlogloss:2.06630\ttest-auc:0.64795\ttest-merror:0.72738\n",
      "[128]\ttrain-mlogloss:2.00973\ttrain-auc:0.69490\ttrain-merror:0.70697\ttest-mlogloss:2.06627\ttest-auc:0.64796\ttest-merror:0.72731\n",
      "[129]\ttrain-mlogloss:2.00934\ttrain-auc:0.69515\ttrain-merror:0.70679\ttest-mlogloss:2.06622\ttest-auc:0.64799\ttest-merror:0.72721\n",
      "[130]\ttrain-mlogloss:2.00903\ttrain-auc:0.69535\ttrain-merror:0.70665\ttest-mlogloss:2.06618\ttest-auc:0.64801\ttest-merror:0.72721\n",
      "[131]\ttrain-mlogloss:2.00866\ttrain-auc:0.69558\ttrain-merror:0.70652\ttest-mlogloss:2.06616\ttest-auc:0.64801\ttest-merror:0.72723\n",
      "[132]\ttrain-mlogloss:2.00828\ttrain-auc:0.69583\ttrain-merror:0.70634\ttest-mlogloss:2.06615\ttest-auc:0.64802\ttest-merror:0.72717\n",
      "[133]\ttrain-mlogloss:2.00795\ttrain-auc:0.69603\ttrain-merror:0.70621\ttest-mlogloss:2.06611\ttest-auc:0.64805\ttest-merror:0.72713\n",
      "[134]\ttrain-mlogloss:2.00758\ttrain-auc:0.69627\ttrain-merror:0.70603\ttest-mlogloss:2.06608\ttest-auc:0.64806\ttest-merror:0.72703\n",
      "[135]\ttrain-mlogloss:2.00725\ttrain-auc:0.69648\ttrain-merror:0.70589\ttest-mlogloss:2.06607\ttest-auc:0.64806\ttest-merror:0.72713\n",
      "[136]\ttrain-mlogloss:2.00693\ttrain-auc:0.69669\ttrain-merror:0.70579\ttest-mlogloss:2.06605\ttest-auc:0.64807\ttest-merror:0.72709\n",
      "[137]\ttrain-mlogloss:2.00658\ttrain-auc:0.69689\ttrain-merror:0.70566\ttest-mlogloss:2.06601\ttest-auc:0.64810\ttest-merror:0.72697\n",
      "[138]\ttrain-mlogloss:2.00625\ttrain-auc:0.69711\ttrain-merror:0.70550\ttest-mlogloss:2.06599\ttest-auc:0.64811\ttest-merror:0.72703\n",
      "[139]\ttrain-mlogloss:2.00589\ttrain-auc:0.69732\ttrain-merror:0.70535\ttest-mlogloss:2.06596\ttest-auc:0.64811\ttest-merror:0.72702\n",
      "[140]\ttrain-mlogloss:2.00550\ttrain-auc:0.69757\ttrain-merror:0.70519\ttest-mlogloss:2.06593\ttest-auc:0.64814\ttest-merror:0.72702\n",
      "[141]\ttrain-mlogloss:2.00519\ttrain-auc:0.69777\ttrain-merror:0.70506\ttest-mlogloss:2.06591\ttest-auc:0.64814\ttest-merror:0.72696\n",
      "[142]\ttrain-mlogloss:2.00493\ttrain-auc:0.69793\ttrain-merror:0.70496\ttest-mlogloss:2.06589\ttest-auc:0.64816\ttest-merror:0.72702\n",
      "[143]\ttrain-mlogloss:2.00460\ttrain-auc:0.69814\ttrain-merror:0.70480\ttest-mlogloss:2.06585\ttest-auc:0.64818\ttest-merror:0.72681\n",
      "[144]\ttrain-mlogloss:2.00426\ttrain-auc:0.69837\ttrain-merror:0.70468\ttest-mlogloss:2.06583\ttest-auc:0.64820\ttest-merror:0.72682\n",
      "[145]\ttrain-mlogloss:2.00394\ttrain-auc:0.69856\ttrain-merror:0.70457\ttest-mlogloss:2.06583\ttest-auc:0.64819\ttest-merror:0.72679\n",
      "[146]\ttrain-mlogloss:2.00360\ttrain-auc:0.69878\ttrain-merror:0.70439\ttest-mlogloss:2.06577\ttest-auc:0.64822\ttest-merror:0.72679\n",
      "[147]\ttrain-mlogloss:2.00332\ttrain-auc:0.69896\ttrain-merror:0.70430\ttest-mlogloss:2.06576\ttest-auc:0.64823\ttest-merror:0.72679\n",
      "[148]\ttrain-mlogloss:2.00297\ttrain-auc:0.69917\ttrain-merror:0.70414\ttest-mlogloss:2.06574\ttest-auc:0.64824\ttest-merror:0.72674\n",
      "[149]\ttrain-mlogloss:2.00264\ttrain-auc:0.69939\ttrain-merror:0.70403\ttest-mlogloss:2.06573\ttest-auc:0.64823\ttest-merror:0.72663\n",
      "[150]\ttrain-mlogloss:2.00238\ttrain-auc:0.69955\ttrain-merror:0.70388\ttest-mlogloss:2.06570\ttest-auc:0.64825\ttest-merror:0.72662\n",
      "[151]\ttrain-mlogloss:2.00207\ttrain-auc:0.69973\ttrain-merror:0.70376\ttest-mlogloss:2.06570\ttest-auc:0.64824\ttest-merror:0.72658\n",
      "[152]\ttrain-mlogloss:2.00177\ttrain-auc:0.69992\ttrain-merror:0.70364\ttest-mlogloss:2.06567\ttest-auc:0.64826\ttest-merror:0.72651\n",
      "[153]\ttrain-mlogloss:2.00144\ttrain-auc:0.70013\ttrain-merror:0.70353\ttest-mlogloss:2.06567\ttest-auc:0.64825\ttest-merror:0.72653\n",
      "[154]\ttrain-mlogloss:2.00108\ttrain-auc:0.70034\ttrain-merror:0.70339\ttest-mlogloss:2.06565\ttest-auc:0.64826\ttest-merror:0.72655\n",
      "[155]\ttrain-mlogloss:2.00074\ttrain-auc:0.70056\ttrain-merror:0.70328\ttest-mlogloss:2.06562\ttest-auc:0.64827\ttest-merror:0.72652\n",
      "[156]\ttrain-mlogloss:2.00036\ttrain-auc:0.70080\ttrain-merror:0.70309\ttest-mlogloss:2.06561\ttest-auc:0.64826\ttest-merror:0.72658\n",
      "[157]\ttrain-mlogloss:2.00005\ttrain-auc:0.70099\ttrain-merror:0.70295\ttest-mlogloss:2.06558\ttest-auc:0.64828\ttest-merror:0.72657\n",
      "[158]\ttrain-mlogloss:1.99978\ttrain-auc:0.70116\ttrain-merror:0.70283\ttest-mlogloss:2.06557\ttest-auc:0.64828\ttest-merror:0.72650\n",
      "[159]\ttrain-mlogloss:1.99944\ttrain-auc:0.70137\ttrain-merror:0.70271\ttest-mlogloss:2.06554\ttest-auc:0.64830\ttest-merror:0.72641\n",
      "[160]\ttrain-mlogloss:1.99916\ttrain-auc:0.70155\ttrain-merror:0.70261\ttest-mlogloss:2.06554\ttest-auc:0.64830\ttest-merror:0.72634\n",
      "[161]\ttrain-mlogloss:1.99886\ttrain-auc:0.70173\ttrain-merror:0.70247\ttest-mlogloss:2.06553\ttest-auc:0.64830\ttest-merror:0.72632\n",
      "[162]\ttrain-mlogloss:1.99853\ttrain-auc:0.70193\ttrain-merror:0.70234\ttest-mlogloss:2.06550\ttest-auc:0.64831\ttest-merror:0.72642\n",
      "[163]\ttrain-mlogloss:1.99822\ttrain-auc:0.70212\ttrain-merror:0.70218\ttest-mlogloss:2.06548\ttest-auc:0.64832\ttest-merror:0.72645\n",
      "[164]\ttrain-mlogloss:1.99788\ttrain-auc:0.70234\ttrain-merror:0.70200\ttest-mlogloss:2.06547\ttest-auc:0.64832\ttest-merror:0.72656\n",
      "[165]\ttrain-mlogloss:1.99755\ttrain-auc:0.70255\ttrain-merror:0.70185\ttest-mlogloss:2.06545\ttest-auc:0.64833\ttest-merror:0.72653\n",
      "[166]\ttrain-mlogloss:1.99728\ttrain-auc:0.70271\ttrain-merror:0.70171\ttest-mlogloss:2.06544\ttest-auc:0.64834\ttest-merror:0.72657\n",
      "[167]\ttrain-mlogloss:1.99701\ttrain-auc:0.70288\ttrain-merror:0.70163\ttest-mlogloss:2.06544\ttest-auc:0.64833\ttest-merror:0.72651\n",
      "[168]\ttrain-mlogloss:1.99668\ttrain-auc:0.70307\ttrain-merror:0.70148\ttest-mlogloss:2.06543\ttest-auc:0.64834\ttest-merror:0.72653\n",
      "[169]\ttrain-mlogloss:1.99634\ttrain-auc:0.70327\ttrain-merror:0.70135\ttest-mlogloss:2.06541\ttest-auc:0.64835\ttest-merror:0.72645\n",
      "[170]\ttrain-mlogloss:1.99602\ttrain-auc:0.70347\ttrain-merror:0.70122\ttest-mlogloss:2.06538\ttest-auc:0.64837\ttest-merror:0.72647\n",
      "[171]\ttrain-mlogloss:1.99574\ttrain-auc:0.70363\ttrain-merror:0.70108\ttest-mlogloss:2.06534\ttest-auc:0.64839\ttest-merror:0.72653\n",
      "[172]\ttrain-mlogloss:1.99540\ttrain-auc:0.70384\ttrain-merror:0.70093\ttest-mlogloss:2.06533\ttest-auc:0.64839\ttest-merror:0.72653\n",
      "[173]\ttrain-mlogloss:1.99505\ttrain-auc:0.70406\ttrain-merror:0.70078\ttest-mlogloss:2.06532\ttest-auc:0.64840\ttest-merror:0.72651\n",
      "[174]\ttrain-mlogloss:1.99480\ttrain-auc:0.70420\ttrain-merror:0.70065\ttest-mlogloss:2.06531\ttest-auc:0.64841\ttest-merror:0.72642\n",
      "[175]\ttrain-mlogloss:1.99449\ttrain-auc:0.70439\ttrain-merror:0.70054\ttest-mlogloss:2.06528\ttest-auc:0.64842\ttest-merror:0.72650\n",
      "[176]\ttrain-mlogloss:1.99424\ttrain-auc:0.70454\ttrain-merror:0.70044\ttest-mlogloss:2.06526\ttest-auc:0.64843\ttest-merror:0.72649\n",
      "[177]\ttrain-mlogloss:1.99400\ttrain-auc:0.70467\ttrain-merror:0.70032\ttest-mlogloss:2.06525\ttest-auc:0.64843\ttest-merror:0.72650\n",
      "[178]\ttrain-mlogloss:1.99368\ttrain-auc:0.70487\ttrain-merror:0.70017\ttest-mlogloss:2.06522\ttest-auc:0.64845\ttest-merror:0.72641\n",
      "[179]\ttrain-mlogloss:1.99337\ttrain-auc:0.70508\ttrain-merror:0.70004\ttest-mlogloss:2.06521\ttest-auc:0.64846\ttest-merror:0.72643\n",
      "[180]\ttrain-mlogloss:1.99302\ttrain-auc:0.70529\ttrain-merror:0.69988\ttest-mlogloss:2.06517\ttest-auc:0.64849\ttest-merror:0.72641\n",
      "[181]\ttrain-mlogloss:1.99269\ttrain-auc:0.70548\ttrain-merror:0.69977\ttest-mlogloss:2.06516\ttest-auc:0.64850\ttest-merror:0.72639\n",
      "[182]\ttrain-mlogloss:1.99236\ttrain-auc:0.70566\ttrain-merror:0.69962\ttest-mlogloss:2.06515\ttest-auc:0.64851\ttest-merror:0.72635\n",
      "[183]\ttrain-mlogloss:1.99202\ttrain-auc:0.70588\ttrain-merror:0.69948\ttest-mlogloss:2.06514\ttest-auc:0.64850\ttest-merror:0.72636\n",
      "[184]\ttrain-mlogloss:1.99174\ttrain-auc:0.70604\ttrain-merror:0.69938\ttest-mlogloss:2.06513\ttest-auc:0.64851\ttest-merror:0.72635\n",
      "[185]\ttrain-mlogloss:1.99147\ttrain-auc:0.70621\ttrain-merror:0.69928\ttest-mlogloss:2.06512\ttest-auc:0.64852\ttest-merror:0.72635\n",
      "[186]\ttrain-mlogloss:1.99119\ttrain-auc:0.70639\ttrain-merror:0.69918\ttest-mlogloss:2.06512\ttest-auc:0.64850\ttest-merror:0.72639\n",
      "[187]\ttrain-mlogloss:1.99092\ttrain-auc:0.70654\ttrain-merror:0.69908\ttest-mlogloss:2.06511\ttest-auc:0.64851\ttest-merror:0.72623\n",
      "[188]\ttrain-mlogloss:1.99065\ttrain-auc:0.70670\ttrain-merror:0.69896\ttest-mlogloss:2.06510\ttest-auc:0.64850\ttest-merror:0.72617\n",
      "[189]\ttrain-mlogloss:1.99034\ttrain-auc:0.70689\ttrain-merror:0.69881\ttest-mlogloss:2.06509\ttest-auc:0.64851\ttest-merror:0.72620\n",
      "[190]\ttrain-mlogloss:1.99004\ttrain-auc:0.70708\ttrain-merror:0.69865\ttest-mlogloss:2.06507\ttest-auc:0.64853\ttest-merror:0.72618\n",
      "[191]\ttrain-mlogloss:1.98977\ttrain-auc:0.70725\ttrain-merror:0.69852\ttest-mlogloss:2.06508\ttest-auc:0.64853\ttest-merror:0.72618\n",
      "[192]\ttrain-mlogloss:1.98954\ttrain-auc:0.70739\ttrain-merror:0.69839\ttest-mlogloss:2.06506\ttest-auc:0.64854\ttest-merror:0.72619\n",
      "[193]\ttrain-mlogloss:1.98929\ttrain-auc:0.70754\ttrain-merror:0.69828\ttest-mlogloss:2.06504\ttest-auc:0.64854\ttest-merror:0.72615\n",
      "[194]\ttrain-mlogloss:1.98895\ttrain-auc:0.70775\ttrain-merror:0.69811\ttest-mlogloss:2.06503\ttest-auc:0.64855\ttest-merror:0.72618\n",
      "[195]\ttrain-mlogloss:1.98866\ttrain-auc:0.70793\ttrain-merror:0.69800\ttest-mlogloss:2.06503\ttest-auc:0.64854\ttest-merror:0.72617\n",
      "[196]\ttrain-mlogloss:1.98835\ttrain-auc:0.70811\ttrain-merror:0.69788\ttest-mlogloss:2.06501\ttest-auc:0.64856\ttest-merror:0.72614\n",
      "[197]\ttrain-mlogloss:1.98805\ttrain-auc:0.70831\ttrain-merror:0.69776\ttest-mlogloss:2.06502\ttest-auc:0.64856\ttest-merror:0.72618\n",
      "[198]\ttrain-mlogloss:1.98779\ttrain-auc:0.70845\ttrain-merror:0.69764\ttest-mlogloss:2.06500\ttest-auc:0.64857\ttest-merror:0.72616\n",
      "[199]\ttrain-mlogloss:1.98756\ttrain-auc:0.70859\ttrain-merror:0.69753\ttest-mlogloss:2.06498\ttest-auc:0.64858\ttest-merror:0.72628\n",
      "[200]\ttrain-mlogloss:1.98724\ttrain-auc:0.70879\ttrain-merror:0.69735\ttest-mlogloss:2.06498\ttest-auc:0.64858\ttest-merror:0.72620\n",
      "[201]\ttrain-mlogloss:1.98700\ttrain-auc:0.70893\ttrain-merror:0.69727\ttest-mlogloss:2.06496\ttest-auc:0.64859\ttest-merror:0.72615\n",
      "[202]\ttrain-mlogloss:1.98670\ttrain-auc:0.70910\ttrain-merror:0.69712\ttest-mlogloss:2.06496\ttest-auc:0.64860\ttest-merror:0.72613\n",
      "[203]\ttrain-mlogloss:1.98640\ttrain-auc:0.70929\ttrain-merror:0.69698\ttest-mlogloss:2.06494\ttest-auc:0.64860\ttest-merror:0.72603\n",
      "[204]\ttrain-mlogloss:1.98613\ttrain-auc:0.70945\ttrain-merror:0.69687\ttest-mlogloss:2.06494\ttest-auc:0.64859\ttest-merror:0.72611\n",
      "[205]\ttrain-mlogloss:1.98591\ttrain-auc:0.70958\ttrain-merror:0.69678\ttest-mlogloss:2.06493\ttest-auc:0.64860\ttest-merror:0.72611\n",
      "[206]\ttrain-mlogloss:1.98563\ttrain-auc:0.70974\ttrain-merror:0.69667\ttest-mlogloss:2.06490\ttest-auc:0.64862\ttest-merror:0.72612\n",
      "[207]\ttrain-mlogloss:1.98536\ttrain-auc:0.70990\ttrain-merror:0.69655\ttest-mlogloss:2.06489\ttest-auc:0.64862\ttest-merror:0.72610\n",
      "[208]\ttrain-mlogloss:1.98510\ttrain-auc:0.71005\ttrain-merror:0.69643\ttest-mlogloss:2.06490\ttest-auc:0.64862\ttest-merror:0.72605\n",
      "[209]\ttrain-mlogloss:1.98480\ttrain-auc:0.71023\ttrain-merror:0.69633\ttest-mlogloss:2.06490\ttest-auc:0.64862\ttest-merror:0.72613\n",
      "[210]\ttrain-mlogloss:1.98449\ttrain-auc:0.71041\ttrain-merror:0.69622\ttest-mlogloss:2.06486\ttest-auc:0.64865\ttest-merror:0.72604\n",
      "[211]\ttrain-mlogloss:1.98422\ttrain-auc:0.71056\ttrain-merror:0.69607\ttest-mlogloss:2.06487\ttest-auc:0.64864\ttest-merror:0.72600\n",
      "[212]\ttrain-mlogloss:1.98395\ttrain-auc:0.71071\ttrain-merror:0.69595\ttest-mlogloss:2.06487\ttest-auc:0.64864\ttest-merror:0.72598\n",
      "[213]\ttrain-mlogloss:1.98368\ttrain-auc:0.71087\ttrain-merror:0.69582\ttest-mlogloss:2.06486\ttest-auc:0.64864\ttest-merror:0.72598\n",
      "[214]\ttrain-mlogloss:1.98337\ttrain-auc:0.71105\ttrain-merror:0.69571\ttest-mlogloss:2.06486\ttest-auc:0.64864\ttest-merror:0.72600\n",
      "[215]\ttrain-mlogloss:1.98309\ttrain-auc:0.71121\ttrain-merror:0.69559\ttest-mlogloss:2.06483\ttest-auc:0.64865\ttest-merror:0.72597\n",
      "[216]\ttrain-mlogloss:1.98283\ttrain-auc:0.71136\ttrain-merror:0.69547\ttest-mlogloss:2.06481\ttest-auc:0.64867\ttest-merror:0.72585\n",
      "[217]\ttrain-mlogloss:1.98254\ttrain-auc:0.71152\ttrain-merror:0.69527\ttest-mlogloss:2.06480\ttest-auc:0.64868\ttest-merror:0.72589\n",
      "[218]\ttrain-mlogloss:1.98226\ttrain-auc:0.71168\ttrain-merror:0.69517\ttest-mlogloss:2.06479\ttest-auc:0.64868\ttest-merror:0.72587\n",
      "[219]\ttrain-mlogloss:1.98199\ttrain-auc:0.71183\ttrain-merror:0.69509\ttest-mlogloss:2.06479\ttest-auc:0.64868\ttest-merror:0.72587\n",
      "[220]\ttrain-mlogloss:1.98173\ttrain-auc:0.71198\ttrain-merror:0.69494\ttest-mlogloss:2.06477\ttest-auc:0.64868\ttest-merror:0.72595\n",
      "[221]\ttrain-mlogloss:1.98145\ttrain-auc:0.71214\ttrain-merror:0.69483\ttest-mlogloss:2.06476\ttest-auc:0.64869\ttest-merror:0.72596\n",
      "[222]\ttrain-mlogloss:1.98118\ttrain-auc:0.71230\ttrain-merror:0.69473\ttest-mlogloss:2.06476\ttest-auc:0.64869\ttest-merror:0.72593\n",
      "[223]\ttrain-mlogloss:1.98094\ttrain-auc:0.71242\ttrain-merror:0.69464\ttest-mlogloss:2.06474\ttest-auc:0.64870\ttest-merror:0.72587\n",
      "[224]\ttrain-mlogloss:1.98064\ttrain-auc:0.71260\ttrain-merror:0.69454\ttest-mlogloss:2.06474\ttest-auc:0.64871\ttest-merror:0.72585\n",
      "[225]\ttrain-mlogloss:1.98040\ttrain-auc:0.71274\ttrain-merror:0.69442\ttest-mlogloss:2.06473\ttest-auc:0.64872\ttest-merror:0.72586\n",
      "[226]\ttrain-mlogloss:1.98011\ttrain-auc:0.71291\ttrain-merror:0.69426\ttest-mlogloss:2.06473\ttest-auc:0.64873\ttest-merror:0.72591\n",
      "[227]\ttrain-mlogloss:1.97982\ttrain-auc:0.71305\ttrain-merror:0.69416\ttest-mlogloss:2.06473\ttest-auc:0.64873\ttest-merror:0.72588\n",
      "[228]\ttrain-mlogloss:1.97948\ttrain-auc:0.71324\ttrain-merror:0.69402\ttest-mlogloss:2.06472\ttest-auc:0.64873\ttest-merror:0.72587\n",
      "[229]\ttrain-mlogloss:1.97921\ttrain-auc:0.71339\ttrain-merror:0.69386\ttest-mlogloss:2.06470\ttest-auc:0.64874\ttest-merror:0.72596\n",
      "[230]\ttrain-mlogloss:1.97898\ttrain-auc:0.71352\ttrain-merror:0.69378\ttest-mlogloss:2.06471\ttest-auc:0.64873\ttest-merror:0.72591\n",
      "[231]\ttrain-mlogloss:1.97868\ttrain-auc:0.71369\ttrain-merror:0.69362\ttest-mlogloss:2.06470\ttest-auc:0.64875\ttest-merror:0.72587\n",
      "[232]\ttrain-mlogloss:1.97843\ttrain-auc:0.71383\ttrain-merror:0.69351\ttest-mlogloss:2.06468\ttest-auc:0.64876\ttest-merror:0.72578\n",
      "[233]\ttrain-mlogloss:1.97816\ttrain-auc:0.71399\ttrain-merror:0.69337\ttest-mlogloss:2.06468\ttest-auc:0.64876\ttest-merror:0.72575\n",
      "[234]\ttrain-mlogloss:1.97792\ttrain-auc:0.71413\ttrain-merror:0.69327\ttest-mlogloss:2.06468\ttest-auc:0.64876\ttest-merror:0.72569\n",
      "[235]\ttrain-mlogloss:1.97764\ttrain-auc:0.71430\ttrain-merror:0.69313\ttest-mlogloss:2.06468\ttest-auc:0.64876\ttest-merror:0.72575\n",
      "[236]\ttrain-mlogloss:1.97736\ttrain-auc:0.71449\ttrain-merror:0.69300\ttest-mlogloss:2.06469\ttest-auc:0.64877\ttest-merror:0.72572\n",
      "[237]\ttrain-mlogloss:1.97706\ttrain-auc:0.71466\ttrain-merror:0.69287\ttest-mlogloss:2.06469\ttest-auc:0.64877\ttest-merror:0.72569\n",
      "[238]\ttrain-mlogloss:1.97675\ttrain-auc:0.71484\ttrain-merror:0.69277\ttest-mlogloss:2.06469\ttest-auc:0.64877\ttest-merror:0.72562\n",
      "[239]\ttrain-mlogloss:1.97650\ttrain-auc:0.71499\ttrain-merror:0.69268\ttest-mlogloss:2.06468\ttest-auc:0.64878\ttest-merror:0.72558\n",
      "[240]\ttrain-mlogloss:1.97622\ttrain-auc:0.71516\ttrain-merror:0.69256\ttest-mlogloss:2.06470\ttest-auc:0.64877\ttest-merror:0.72569\n",
      "[241]\ttrain-mlogloss:1.97595\ttrain-auc:0.71531\ttrain-merror:0.69244\ttest-mlogloss:2.06469\ttest-auc:0.64877\ttest-merror:0.72565\n",
      "[242]\ttrain-mlogloss:1.97569\ttrain-auc:0.71545\ttrain-merror:0.69229\ttest-mlogloss:2.06469\ttest-auc:0.64877\ttest-merror:0.72563\n",
      "[243]\ttrain-mlogloss:1.97539\ttrain-auc:0.71561\ttrain-merror:0.69218\ttest-mlogloss:2.06468\ttest-auc:0.64877\ttest-merror:0.72560\n",
      "[244]\ttrain-mlogloss:1.97511\ttrain-auc:0.71578\ttrain-merror:0.69205\ttest-mlogloss:2.06467\ttest-auc:0.64877\ttest-merror:0.72557\n",
      "[245]\ttrain-mlogloss:1.97484\ttrain-auc:0.71593\ttrain-merror:0.69195\ttest-mlogloss:2.06467\ttest-auc:0.64877\ttest-merror:0.72557\n",
      "[246]\ttrain-mlogloss:1.97461\ttrain-auc:0.71607\ttrain-merror:0.69185\ttest-mlogloss:2.06466\ttest-auc:0.64877\ttest-merror:0.72557\n",
      "[247]\ttrain-mlogloss:1.97432\ttrain-auc:0.71623\ttrain-merror:0.69171\ttest-mlogloss:2.06465\ttest-auc:0.64877\ttest-merror:0.72553\n",
      "[248]\ttrain-mlogloss:1.97408\ttrain-auc:0.71636\ttrain-merror:0.69163\ttest-mlogloss:2.06466\ttest-auc:0.64877\ttest-merror:0.72554\n",
      "[249]\ttrain-mlogloss:1.97386\ttrain-auc:0.71649\ttrain-merror:0.69156\ttest-mlogloss:2.06466\ttest-auc:0.64877\ttest-merror:0.72557\n",
      "[250]\ttrain-mlogloss:1.97357\ttrain-auc:0.71665\ttrain-merror:0.69142\ttest-mlogloss:2.06465\ttest-auc:0.64877\ttest-merror:0.72557\n",
      "[251]\ttrain-mlogloss:1.97331\ttrain-auc:0.71680\ttrain-merror:0.69131\ttest-mlogloss:2.06465\ttest-auc:0.64877\ttest-merror:0.72552\n",
      "[252]\ttrain-mlogloss:1.97303\ttrain-auc:0.71696\ttrain-merror:0.69118\ttest-mlogloss:2.06465\ttest-auc:0.64877\ttest-merror:0.72553\n",
      "[253]\ttrain-mlogloss:1.97270\ttrain-auc:0.71715\ttrain-merror:0.69103\ttest-mlogloss:2.06465\ttest-auc:0.64877\ttest-merror:0.72555\n",
      "[254]\ttrain-mlogloss:1.97240\ttrain-auc:0.71731\ttrain-merror:0.69088\ttest-mlogloss:2.06464\ttest-auc:0.64877\ttest-merror:0.72554\n",
      "[255]\ttrain-mlogloss:1.97215\ttrain-auc:0.71746\ttrain-merror:0.69077\ttest-mlogloss:2.06463\ttest-auc:0.64877\ttest-merror:0.72554\n",
      "[256]\ttrain-mlogloss:1.97193\ttrain-auc:0.71758\ttrain-merror:0.69068\ttest-mlogloss:2.06463\ttest-auc:0.64878\ttest-merror:0.72551\n",
      "[257]\ttrain-mlogloss:1.97171\ttrain-auc:0.71771\ttrain-merror:0.69061\ttest-mlogloss:2.06464\ttest-auc:0.64877\ttest-merror:0.72552\n",
      "[258]\ttrain-mlogloss:1.97138\ttrain-auc:0.71790\ttrain-merror:0.69046\ttest-mlogloss:2.06463\ttest-auc:0.64877\ttest-merror:0.72558\n",
      "[259]\ttrain-mlogloss:1.97109\ttrain-auc:0.71807\ttrain-merror:0.69031\ttest-mlogloss:2.06465\ttest-auc:0.64876\ttest-merror:0.72556\n",
      "[260]\ttrain-mlogloss:1.97082\ttrain-auc:0.71822\ttrain-merror:0.69019\ttest-mlogloss:2.06464\ttest-auc:0.64877\ttest-merror:0.72555\n",
      "[261]\ttrain-mlogloss:1.97054\ttrain-auc:0.71838\ttrain-merror:0.69006\ttest-mlogloss:2.06466\ttest-auc:0.64875\ttest-merror:0.72555\n",
      "[262]\ttrain-mlogloss:1.97030\ttrain-auc:0.71850\ttrain-merror:0.68996\ttest-mlogloss:2.06466\ttest-auc:0.64875\ttest-merror:0.72550\n",
      "[263]\ttrain-mlogloss:1.96999\ttrain-auc:0.71869\ttrain-merror:0.68983\ttest-mlogloss:2.06467\ttest-auc:0.64875\ttest-merror:0.72554\n",
      "[264]\ttrain-mlogloss:1.96970\ttrain-auc:0.71886\ttrain-merror:0.68969\ttest-mlogloss:2.06466\ttest-auc:0.64876\ttest-merror:0.72549\n",
      "[265]\ttrain-mlogloss:1.96945\ttrain-auc:0.71899\ttrain-merror:0.68959\ttest-mlogloss:2.06465\ttest-auc:0.64876\ttest-merror:0.72537\n",
      "[266]\ttrain-mlogloss:1.96915\ttrain-auc:0.71915\ttrain-merror:0.68945\ttest-mlogloss:2.06465\ttest-auc:0.64876\ttest-merror:0.72546\n",
      "[267]\ttrain-mlogloss:1.96887\ttrain-auc:0.71930\ttrain-merror:0.68932\ttest-mlogloss:2.06464\ttest-auc:0.64877\ttest-merror:0.72543\n",
      "[268]\ttrain-mlogloss:1.96860\ttrain-auc:0.71945\ttrain-merror:0.68923\ttest-mlogloss:2.06466\ttest-auc:0.64876\ttest-merror:0.72536\n",
      "[269]\ttrain-mlogloss:1.96835\ttrain-auc:0.71959\ttrain-merror:0.68912\ttest-mlogloss:2.06465\ttest-auc:0.64876\ttest-merror:0.72537\n",
      "[270]\ttrain-mlogloss:1.96810\ttrain-auc:0.71972\ttrain-merror:0.68904\ttest-mlogloss:2.06465\ttest-auc:0.64877\ttest-merror:0.72534\n",
      "[271]\ttrain-mlogloss:1.96784\ttrain-auc:0.71986\ttrain-merror:0.68892\ttest-mlogloss:2.06463\ttest-auc:0.64877\ttest-merror:0.72529\n",
      "[272]\ttrain-mlogloss:1.96758\ttrain-auc:0.72001\ttrain-merror:0.68880\ttest-mlogloss:2.06464\ttest-auc:0.64877\ttest-merror:0.72538\n",
      "[273]\ttrain-mlogloss:1.96727\ttrain-auc:0.72018\ttrain-merror:0.68862\ttest-mlogloss:2.06462\ttest-auc:0.64878\ttest-merror:0.72541\n",
      "[274]\ttrain-mlogloss:1.96705\ttrain-auc:0.72030\ttrain-merror:0.68853\ttest-mlogloss:2.06463\ttest-auc:0.64878\ttest-merror:0.72547\n",
      "[275]\ttrain-mlogloss:1.96681\ttrain-auc:0.72044\ttrain-merror:0.68839\ttest-mlogloss:2.06465\ttest-auc:0.64877\ttest-merror:0.72540\n",
      "[276]\ttrain-mlogloss:1.96654\ttrain-auc:0.72059\ttrain-merror:0.68830\ttest-mlogloss:2.06465\ttest-auc:0.64876\ttest-merror:0.72540\n",
      "[277]\ttrain-mlogloss:1.96632\ttrain-auc:0.72071\ttrain-merror:0.68819\ttest-mlogloss:2.06466\ttest-auc:0.64876\ttest-merror:0.72529\n",
      "[278]\ttrain-mlogloss:1.96607\ttrain-auc:0.72083\ttrain-merror:0.68808\ttest-mlogloss:2.06464\ttest-auc:0.64876\ttest-merror:0.72534\n",
      "[279]\ttrain-mlogloss:1.96581\ttrain-auc:0.72098\ttrain-merror:0.68795\ttest-mlogloss:2.06465\ttest-auc:0.64875\ttest-merror:0.72540\n",
      "[280]\ttrain-mlogloss:1.96558\ttrain-auc:0.72111\ttrain-merror:0.68787\ttest-mlogloss:2.06466\ttest-auc:0.64875\ttest-merror:0.72537\n",
      "[281]\ttrain-mlogloss:1.96532\ttrain-auc:0.72126\ttrain-merror:0.68771\ttest-mlogloss:2.06466\ttest-auc:0.64875\ttest-merror:0.72534\n",
      "[282]\ttrain-mlogloss:1.96513\ttrain-auc:0.72136\ttrain-merror:0.68765\ttest-mlogloss:2.06465\ttest-auc:0.64876\ttest-merror:0.72535\n",
      "[283]\ttrain-mlogloss:1.96488\ttrain-auc:0.72150\ttrain-merror:0.68752\ttest-mlogloss:2.06466\ttest-auc:0.64875\ttest-merror:0.72531\n",
      "[284]\ttrain-mlogloss:1.96464\ttrain-auc:0.72164\ttrain-merror:0.68741\ttest-mlogloss:2.06466\ttest-auc:0.64876\ttest-merror:0.72532\n",
      "[285]\ttrain-mlogloss:1.96437\ttrain-auc:0.72179\ttrain-merror:0.68729\ttest-mlogloss:2.06467\ttest-auc:0.64875\ttest-merror:0.72532\n",
      "[286]\ttrain-mlogloss:1.96408\ttrain-auc:0.72195\ttrain-merror:0.68715\ttest-mlogloss:2.06465\ttest-auc:0.64875\ttest-merror:0.72535\n",
      "[287]\ttrain-mlogloss:1.96383\ttrain-auc:0.72209\ttrain-merror:0.68706\ttest-mlogloss:2.06465\ttest-auc:0.64875\ttest-merror:0.72534\n",
      "[288]\ttrain-mlogloss:1.96363\ttrain-auc:0.72219\ttrain-merror:0.68695\ttest-mlogloss:2.06467\ttest-auc:0.64874\ttest-merror:0.72534\n",
      "[289]\ttrain-mlogloss:1.96338\ttrain-auc:0.72233\ttrain-merror:0.68686\ttest-mlogloss:2.06468\ttest-auc:0.64873\ttest-merror:0.72537\n",
      "[290]\ttrain-mlogloss:1.96311\ttrain-auc:0.72247\ttrain-merror:0.68675\ttest-mlogloss:2.06469\ttest-auc:0.64873\ttest-merror:0.72542\n",
      "[291]\ttrain-mlogloss:1.96290\ttrain-auc:0.72259\ttrain-merror:0.68666\ttest-mlogloss:2.06469\ttest-auc:0.64873\ttest-merror:0.72538\n",
      "[292]\ttrain-mlogloss:1.96263\ttrain-auc:0.72274\ttrain-merror:0.68653\ttest-mlogloss:2.06470\ttest-auc:0.64872\ttest-merror:0.72547\n",
      "[293]\ttrain-mlogloss:1.96242\ttrain-auc:0.72285\ttrain-merror:0.68645\ttest-mlogloss:2.06470\ttest-auc:0.64873\ttest-merror:0.72545\n",
      "[294]\ttrain-mlogloss:1.96214\ttrain-auc:0.72302\ttrain-merror:0.68634\ttest-mlogloss:2.06471\ttest-auc:0.64872\ttest-merror:0.72538\n",
      "[295]\ttrain-mlogloss:1.96191\ttrain-auc:0.72315\ttrain-merror:0.68626\ttest-mlogloss:2.06471\ttest-auc:0.64872\ttest-merror:0.72543\n",
      "[296]\ttrain-mlogloss:1.96166\ttrain-auc:0.72329\ttrain-merror:0.68613\ttest-mlogloss:2.06473\ttest-auc:0.64871\ttest-merror:0.72539\n",
      "[297]\ttrain-mlogloss:1.96136\ttrain-auc:0.72345\ttrain-merror:0.68606\ttest-mlogloss:2.06471\ttest-auc:0.64872\ttest-merror:0.72527\n",
      "[298]\ttrain-mlogloss:1.96110\ttrain-auc:0.72361\ttrain-merror:0.68592\ttest-mlogloss:2.06472\ttest-auc:0.64872\ttest-merror:0.72527\n",
      "[299]\ttrain-mlogloss:1.96083\ttrain-auc:0.72375\ttrain-merror:0.68581\ttest-mlogloss:2.06472\ttest-auc:0.64871\ttest-merror:0.72529\n",
      "300-rounds Training finished ...\t\t(247.225s)\n"
     ]
    }
   ],
   "source": [
    "num_round = 300\n",
    "t0 = time()\n",
    "eval_result = {}\n",
    "def decay_eta(nth):\n",
    "    etas = [.1, .05, .03, .01]\n",
    "    return etas[(nth // 60) % len(etas)]\n",
    "\n",
    "wl_bst_sm = xgb.train(param, xg_train, num_round, watchlist, evals_result=eval_result, )\n",
    "#                       callbacks=[xgb.callback.LearningRateScheduler(decay_eta)])\n",
    "print(f\"{num_round}-rounds Training finished ...\\t\\t({time()-t0:.3f}s)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': OrderedDict([('mlogloss',\n",
       "               [2.273391,\n",
       "                2.249271,\n",
       "                2.228938,\n",
       "                2.211438,\n",
       "                2.196323,\n",
       "                2.183143,\n",
       "                2.171514,\n",
       "                2.16128,\n",
       "                2.15201,\n",
       "                2.14378,\n",
       "                2.136391,\n",
       "                2.129838,\n",
       "                2.12384,\n",
       "                2.118412,\n",
       "                2.113522,\n",
       "                2.109029,\n",
       "                2.105014,\n",
       "                2.101309,\n",
       "                2.0979,\n",
       "                2.094819,\n",
       "                2.092041,\n",
       "                2.089248,\n",
       "                2.086848,\n",
       "                2.084486,\n",
       "                2.082175,\n",
       "                2.080239,\n",
       "                2.078318,\n",
       "                2.076628,\n",
       "                2.074873,\n",
       "                2.073293,\n",
       "                2.071798,\n",
       "                2.070462,\n",
       "                2.069056,\n",
       "                2.067713,\n",
       "                2.066565,\n",
       "                2.065228,\n",
       "                2.064006,\n",
       "                2.062921,\n",
       "                2.061723,\n",
       "                2.060604,\n",
       "                2.059523,\n",
       "                2.058563,\n",
       "                2.057618,\n",
       "                2.056739,\n",
       "                2.055834,\n",
       "                2.055038,\n",
       "                2.054273,\n",
       "                2.053414,\n",
       "                2.052641,\n",
       "                2.051878,\n",
       "                2.051152,\n",
       "                2.050514,\n",
       "                2.049806,\n",
       "                2.049126,\n",
       "                2.04851,\n",
       "                2.047696,\n",
       "                2.047043,\n",
       "                2.046361,\n",
       "                2.045739,\n",
       "                2.045126,\n",
       "                2.044484,\n",
       "                2.043822,\n",
       "                2.043189,\n",
       "                2.042589,\n",
       "                2.041953,\n",
       "                2.041371,\n",
       "                2.040852,\n",
       "                2.040214,\n",
       "                2.039633,\n",
       "                2.039045,\n",
       "                2.038459,\n",
       "                2.037893,\n",
       "                2.037293,\n",
       "                2.03677,\n",
       "                2.036205,\n",
       "                2.035654,\n",
       "                2.035119,\n",
       "                2.034608,\n",
       "                2.034042,\n",
       "                2.033438,\n",
       "                2.033002,\n",
       "                2.032466,\n",
       "                2.031891,\n",
       "                2.031407,\n",
       "                2.030922,\n",
       "                2.030501,\n",
       "                2.029937,\n",
       "                2.029474,\n",
       "                2.028983,\n",
       "                2.028555,\n",
       "                2.028012,\n",
       "                2.027574,\n",
       "                2.027126,\n",
       "                2.026658,\n",
       "                2.026212,\n",
       "                2.025816,\n",
       "                2.025496,\n",
       "                2.025096,\n",
       "                2.024753,\n",
       "                2.024403,\n",
       "                2.024139,\n",
       "                2.023821,\n",
       "                2.023507,\n",
       "                2.023234,\n",
       "                2.022821,\n",
       "                2.022518,\n",
       "                2.022208,\n",
       "                2.022001,\n",
       "                2.021776,\n",
       "                2.021562,\n",
       "                2.021295,\n",
       "                2.021012,\n",
       "                2.020731,\n",
       "                2.020529,\n",
       "                2.020328,\n",
       "                2.020141,\n",
       "                2.01988,\n",
       "                2.019671,\n",
       "                2.01945,\n",
       "                2.019143,\n",
       "                2.018958,\n",
       "                2.018705,\n",
       "                2.018505,\n",
       "                2.018282,\n",
       "                2.018093,\n",
       "                2.017933,\n",
       "                2.017817,\n",
       "                2.017704,\n",
       "                2.017591,\n",
       "                2.017471,\n",
       "                2.017397,\n",
       "                2.017253,\n",
       "                2.017146,\n",
       "                2.017033,\n",
       "                2.016887,\n",
       "                2.016766,\n",
       "                2.016657,\n",
       "                2.01646,\n",
       "                2.016353,\n",
       "                2.016211,\n",
       "                2.016126,\n",
       "                2.016019,\n",
       "                2.015876,\n",
       "                2.015805,\n",
       "                2.015669,\n",
       "                2.015573,\n",
       "                2.015421,\n",
       "                2.015299,\n",
       "                2.015163,\n",
       "                2.015013,\n",
       "                2.014934,\n",
       "                2.014853,\n",
       "                2.014726,\n",
       "                2.014677,\n",
       "                2.014621,\n",
       "                2.014551,\n",
       "                2.014474,\n",
       "                2.014271,\n",
       "                2.01423,\n",
       "                2.014152,\n",
       "                2.014093,\n",
       "                2.014045,\n",
       "                2.013975,\n",
       "                2.013928,\n",
       "                2.013889,\n",
       "                2.013806,\n",
       "                2.013786,\n",
       "                2.013756,\n",
       "                2.013702,\n",
       "                2.013679,\n",
       "                2.013618,\n",
       "                2.013508,\n",
       "                2.013389,\n",
       "                2.013267,\n",
       "                2.01323,\n",
       "                2.013192,\n",
       "                2.01314,\n",
       "                2.013098,\n",
       "                2.01299,\n",
       "                2.012877,\n",
       "                2.012784,\n",
       "                2.012739,\n",
       "                2.012705,\n",
       "                2.012635,\n",
       "                2.012612,\n",
       "                2.012561,\n",
       "                2.012506,\n",
       "                2.012436,\n",
       "                2.012393,\n",
       "                2.012362,\n",
       "                2.012309,\n",
       "                2.012259,\n",
       "                2.012213,\n",
       "                2.012111,\n",
       "                2.012066,\n",
       "                2.011981,\n",
       "                2.011952,\n",
       "                2.011911,\n",
       "                2.011884,\n",
       "                2.011832]),\n",
       "              ('auc',\n",
       "               [0.609166,\n",
       "                0.613574,\n",
       "                0.615673,\n",
       "                0.617684,\n",
       "                0.618899,\n",
       "                0.620377,\n",
       "                0.621478,\n",
       "                0.62236,\n",
       "                0.623596,\n",
       "                0.624718,\n",
       "                0.625693,\n",
       "                0.626546,\n",
       "                0.627477,\n",
       "                0.628391,\n",
       "                0.629221,\n",
       "                0.630098,\n",
       "                0.630813,\n",
       "                0.631575,\n",
       "                0.632343,\n",
       "                0.633113,\n",
       "                0.633773,\n",
       "                0.634675,\n",
       "                0.635358,\n",
       "                0.636146,\n",
       "                0.637049,\n",
       "                0.637714,\n",
       "                0.638476,\n",
       "                0.63913,\n",
       "                0.639907,\n",
       "                0.64057,\n",
       "                0.641302,\n",
       "                0.641907,\n",
       "                0.642584,\n",
       "                0.643294,\n",
       "                0.64389,\n",
       "                0.644677,\n",
       "                0.645397,\n",
       "                0.646051,\n",
       "                0.646797,\n",
       "                0.64751,\n",
       "                0.648158,\n",
       "                0.648724,\n",
       "                0.649365,\n",
       "                0.649896,\n",
       "                0.650452,\n",
       "                0.650985,\n",
       "                0.651553,\n",
       "                0.652133,\n",
       "                0.652763,\n",
       "                0.653332,\n",
       "                0.653814,\n",
       "                0.654313,\n",
       "                0.654877,\n",
       "                0.655356,\n",
       "                0.655828,\n",
       "                0.656415,\n",
       "                0.656954,\n",
       "                0.65745,\n",
       "                0.657979,\n",
       "                0.658472,\n",
       "                0.658947,\n",
       "                0.659518,\n",
       "                0.660011,\n",
       "                0.66048,\n",
       "                0.661035,\n",
       "                0.661515,\n",
       "                0.661939,\n",
       "                0.66243,\n",
       "                0.662903,\n",
       "                0.663358,\n",
       "                0.663827,\n",
       "                0.664305,\n",
       "                0.664799,\n",
       "                0.665215,\n",
       "                0.665669,\n",
       "                0.66609,\n",
       "                0.666529,\n",
       "                0.666907,\n",
       "                0.66736,\n",
       "                0.667831,\n",
       "                0.668144,\n",
       "                0.668582,\n",
       "                0.669065,\n",
       "                0.66944,\n",
       "                0.669814,\n",
       "                0.67014,\n",
       "                0.670602,\n",
       "                0.670932,\n",
       "                0.671322,\n",
       "                0.671626,\n",
       "                0.672037,\n",
       "                0.672368,\n",
       "                0.672707,\n",
       "                0.673066,\n",
       "                0.673372,\n",
       "                0.673655,\n",
       "                0.673886,\n",
       "                0.674167,\n",
       "                0.674412,\n",
       "                0.67468,\n",
       "                0.674849,\n",
       "                0.675083,\n",
       "                0.675328,\n",
       "                0.675503,\n",
       "                0.675813,\n",
       "                0.676021,\n",
       "                0.676225,\n",
       "                0.67635,\n",
       "                0.676503,\n",
       "                0.676644,\n",
       "                0.676819,\n",
       "                0.677004,\n",
       "                0.677191,\n",
       "                0.677305,\n",
       "                0.677427,\n",
       "                0.677553,\n",
       "                0.677731,\n",
       "                0.677853,\n",
       "                0.678005,\n",
       "                0.67821,\n",
       "                0.678332,\n",
       "                0.678514,\n",
       "                0.678642,\n",
       "                0.678797,\n",
       "                0.678916,\n",
       "                0.67901,\n",
       "                0.679068,\n",
       "                0.679116,\n",
       "                0.679174,\n",
       "                0.679233,\n",
       "                0.679268,\n",
       "                0.679347,\n",
       "                0.679393,\n",
       "                0.679453,\n",
       "                0.679532,\n",
       "                0.679592,\n",
       "                0.679641,\n",
       "                0.679756,\n",
       "                0.679805,\n",
       "                0.679884,\n",
       "                0.67992,\n",
       "                0.679976,\n",
       "                0.680058,\n",
       "                0.680091,\n",
       "                0.680166,\n",
       "                0.680201,\n",
       "                0.680297,\n",
       "                0.680364,\n",
       "                0.680429,\n",
       "                0.680511,\n",
       "                0.680554,\n",
       "                0.680594,\n",
       "                0.68066,\n",
       "                0.680675,\n",
       "                0.680696,\n",
       "                0.680724,\n",
       "                0.680755,\n",
       "                0.680916,\n",
       "                0.68093,\n",
       "                0.680963,\n",
       "                0.680986,\n",
       "                0.681005,\n",
       "                0.681047,\n",
       "                0.681063,\n",
       "                0.681076,\n",
       "                0.681109,\n",
       "                0.681114,\n",
       "                0.681125,\n",
       "                0.681142,\n",
       "                0.681148,\n",
       "                0.681176,\n",
       "                0.681243,\n",
       "                0.681297,\n",
       "                0.68136,\n",
       "                0.681373,\n",
       "                0.681389,\n",
       "                0.681404,\n",
       "                0.681419,\n",
       "                0.681469,\n",
       "                0.681542,\n",
       "                0.681581,\n",
       "                0.681599,\n",
       "                0.681611,\n",
       "                0.681642,\n",
       "                0.681648,\n",
       "                0.681666,\n",
       "                0.681681,\n",
       "                0.681711,\n",
       "                0.681724,\n",
       "                0.681731,\n",
       "                0.681748,\n",
       "                0.681768,\n",
       "                0.681781,\n",
       "                0.681825,\n",
       "                0.681841,\n",
       "                0.681887,\n",
       "                0.681898,\n",
       "                0.681912,\n",
       "                0.681922,\n",
       "                0.681937]),\n",
       "              ('merror',\n",
       "               [0.736567,\n",
       "                0.734123,\n",
       "                0.733287,\n",
       "                0.732624,\n",
       "                0.732093,\n",
       "                0.731596,\n",
       "                0.731194,\n",
       "                0.730924,\n",
       "                0.730526,\n",
       "                0.730136,\n",
       "                0.729627,\n",
       "                0.729457,\n",
       "                0.729307,\n",
       "                0.729097,\n",
       "                0.72865,\n",
       "                0.728462,\n",
       "                0.728281,\n",
       "                0.728087,\n",
       "                0.727865,\n",
       "                0.72766,\n",
       "                0.727516,\n",
       "                0.727328,\n",
       "                0.727033,\n",
       "                0.72682,\n",
       "                0.726528,\n",
       "                0.726331,\n",
       "                0.726146,\n",
       "                0.725933,\n",
       "                0.725733,\n",
       "                0.725498,\n",
       "                0.725218,\n",
       "                0.725064,\n",
       "                0.724871,\n",
       "                0.724663,\n",
       "                0.724455,\n",
       "                0.724101,\n",
       "                0.723866,\n",
       "                0.723614,\n",
       "                0.723313,\n",
       "                0.723139,\n",
       "                0.722898,\n",
       "                0.722674,\n",
       "                0.72244,\n",
       "                0.722268,\n",
       "                0.722085,\n",
       "                0.721878,\n",
       "                0.721729,\n",
       "                0.721451,\n",
       "                0.721261,\n",
       "                0.721111,\n",
       "                0.720938,\n",
       "                0.720817,\n",
       "                0.72056,\n",
       "                0.72037,\n",
       "                0.720195,\n",
       "                0.719901,\n",
       "                0.719704,\n",
       "                0.719465,\n",
       "                0.719327,\n",
       "                0.719086,\n",
       "                0.71886,\n",
       "                0.718676,\n",
       "                0.718485,\n",
       "                0.718345,\n",
       "                0.718081,\n",
       "                0.717925,\n",
       "                0.717798,\n",
       "                0.717584,\n",
       "                0.717402,\n",
       "                0.717194,\n",
       "                0.716954,\n",
       "                0.716775,\n",
       "                0.716569,\n",
       "                0.71638,\n",
       "                0.716216,\n",
       "                0.716011,\n",
       "                0.715823,\n",
       "                0.715645,\n",
       "                0.715486,\n",
       "                0.715216,\n",
       "                0.71509,\n",
       "                0.714905,\n",
       "                0.714731,\n",
       "                0.714552,\n",
       "                0.714427,\n",
       "                0.714291,\n",
       "                0.714078,\n",
       "                0.71394,\n",
       "                0.71374,\n",
       "                0.713625,\n",
       "                0.713416,\n",
       "                0.71326,\n",
       "                0.713118,\n",
       "                0.71294,\n",
       "                0.712771,\n",
       "                0.712584,\n",
       "                0.712466,\n",
       "                0.712347,\n",
       "                0.712194,\n",
       "                0.712007,\n",
       "                0.711866,\n",
       "                0.711762,\n",
       "                0.711646,\n",
       "                0.711515,\n",
       "                0.711271,\n",
       "                0.71117,\n",
       "                0.711008,\n",
       "                0.710895,\n",
       "                0.710786,\n",
       "                0.710699,\n",
       "                0.710572,\n",
       "                0.710429,\n",
       "                0.710301,\n",
       "                0.710233,\n",
       "                0.710098,\n",
       "                0.709965,\n",
       "                0.709799,\n",
       "                0.709727,\n",
       "                0.709592,\n",
       "                0.709422,\n",
       "                0.709292,\n",
       "                0.709139,\n",
       "                0.708986,\n",
       "                0.708837,\n",
       "                0.708743,\n",
       "                0.70863,\n",
       "                0.708564,\n",
       "                0.708518,\n",
       "                0.708465,\n",
       "                0.708408,\n",
       "                0.708354,\n",
       "                0.708295,\n",
       "                0.708238,\n",
       "                0.708167,\n",
       "                0.708065,\n",
       "                0.708008,\n",
       "                0.70796,\n",
       "                0.707828,\n",
       "                0.707759,\n",
       "                0.707672,\n",
       "                0.707609,\n",
       "                0.707545,\n",
       "                0.707455,\n",
       "                0.707402,\n",
       "                0.707346,\n",
       "                0.707296,\n",
       "                0.707199,\n",
       "                0.707134,\n",
       "                0.707047,\n",
       "                0.706961,\n",
       "                0.706909,\n",
       "                0.706842,\n",
       "                0.706767,\n",
       "                0.706737,\n",
       "                0.706718,\n",
       "                0.706687,\n",
       "                0.706645,\n",
       "                0.70648,\n",
       "                0.706468,\n",
       "                0.706433,\n",
       "                0.706415,\n",
       "                0.706409,\n",
       "                0.706365,\n",
       "                0.706351,\n",
       "                0.706338,\n",
       "                0.706305,\n",
       "                0.706298,\n",
       "                0.706279,\n",
       "                0.706264,\n",
       "                0.706252,\n",
       "                0.706232,\n",
       "                0.706168,\n",
       "                0.706121,\n",
       "                0.706088,\n",
       "                0.70608,\n",
       "                0.706062,\n",
       "                0.706038,\n",
       "                0.70602,\n",
       "                0.705938,\n",
       "                0.705859,\n",
       "                0.705835,\n",
       "                0.705819,\n",
       "                0.705809,\n",
       "                0.705772,\n",
       "                0.705756,\n",
       "                0.705754,\n",
       "                0.705724,\n",
       "                0.705688,\n",
       "                0.705675,\n",
       "                0.705644,\n",
       "                0.705621,\n",
       "                0.705604,\n",
       "                0.705576,\n",
       "                0.705528,\n",
       "                0.705507,\n",
       "                0.705469,\n",
       "                0.705457,\n",
       "                0.705441,\n",
       "                0.705429,\n",
       "                0.705403])]),\n",
       " 'test': OrderedDict([('mlogloss',\n",
       "               [2.273966,\n",
       "                2.250428,\n",
       "                2.230677,\n",
       "                2.213754,\n",
       "                2.199225,\n",
       "                2.186585,\n",
       "                2.175527,\n",
       "                2.165833,\n",
       "                2.157122,\n",
       "                2.149473,\n",
       "                2.142651,\n",
       "                2.136662,\n",
       "                2.131266,\n",
       "                2.126448,\n",
       "                2.122146,\n",
       "                2.118271,\n",
       "                2.114859,\n",
       "                2.111758,\n",
       "                2.108978,\n",
       "                2.106493,\n",
       "                2.104311,\n",
       "                2.102141,\n",
       "                2.100348,\n",
       "                2.098594,\n",
       "                2.096948,\n",
       "                2.095634,\n",
       "                2.094353,\n",
       "                2.093259,\n",
       "                2.092155,\n",
       "                2.091199,\n",
       "                2.090338,\n",
       "                2.089609,\n",
       "                2.088794,\n",
       "                2.088084,\n",
       "                2.087566,\n",
       "                2.086879,\n",
       "                2.086292,\n",
       "                2.085867,\n",
       "                2.085336,\n",
       "                2.08488,\n",
       "                2.084435,\n",
       "                2.084051,\n",
       "                2.083765,\n",
       "                2.083475,\n",
       "                2.083183,\n",
       "                2.083005,\n",
       "                2.08284,\n",
       "                2.082611,\n",
       "                2.082492,\n",
       "                2.082344,\n",
       "                2.082157,\n",
       "                2.08208,\n",
       "                2.082,\n",
       "                2.081881,\n",
       "                2.081804,\n",
       "                2.081594,\n",
       "                2.081518,\n",
       "                2.081397,\n",
       "                2.081355,\n",
       "                2.081302,\n",
       "                2.081173,\n",
       "                2.081138,\n",
       "                2.081072,\n",
       "                2.08103,\n",
       "                2.080996,\n",
       "                2.080967,\n",
       "                2.080953,\n",
       "                2.080883,\n",
       "                2.08084,\n",
       "                2.080801,\n",
       "                2.080752,\n",
       "                2.080732,\n",
       "                2.080695,\n",
       "                2.080673,\n",
       "                2.080658,\n",
       "                2.080602,\n",
       "                2.080593,\n",
       "                2.080565,\n",
       "                2.080539,\n",
       "                2.080512,\n",
       "                2.080488,\n",
       "                2.080473,\n",
       "                2.080469,\n",
       "                2.08046,\n",
       "                2.080437,\n",
       "                2.080427,\n",
       "                2.080432,\n",
       "                2.080418,\n",
       "                2.080394,\n",
       "                2.080377,\n",
       "                2.080375,\n",
       "                2.080362,\n",
       "                2.080354,\n",
       "                2.080343,\n",
       "                2.080349,\n",
       "                2.08034,\n",
       "                2.080347,\n",
       "                2.080342,\n",
       "                2.080329,\n",
       "                2.080311,\n",
       "                2.080294,\n",
       "                2.080276,\n",
       "                2.080289,\n",
       "                2.080271,\n",
       "                2.08027,\n",
       "                2.080249,\n",
       "                2.080216,\n",
       "                2.080196,\n",
       "                2.080184,\n",
       "                2.080182,\n",
       "                2.080157,\n",
       "                2.080133,\n",
       "                2.08011,\n",
       "                2.080096,\n",
       "                2.08008,\n",
       "                2.080082,\n",
       "                2.080074,\n",
       "                2.080047,\n",
       "                2.080029,\n",
       "                2.080013,\n",
       "                2.079996,\n",
       "                2.079986,\n",
       "                2.079977,\n",
       "                2.079978,\n",
       "                2.079966,\n",
       "                2.079957,\n",
       "                2.07994,\n",
       "                2.079918,\n",
       "                2.079911,\n",
       "                2.07991,\n",
       "                2.079897,\n",
       "                2.079887,\n",
       "                2.07987,\n",
       "                2.079848,\n",
       "                2.079819,\n",
       "                2.079782,\n",
       "                2.079779,\n",
       "                2.079745,\n",
       "                2.07973,\n",
       "                2.079714,\n",
       "                2.07969,\n",
       "                2.079686,\n",
       "                2.079694,\n",
       "                2.079683,\n",
       "                2.079683,\n",
       "                2.079665,\n",
       "                2.079645,\n",
       "                2.079633,\n",
       "                2.079602,\n",
       "                2.079573,\n",
       "                2.079575,\n",
       "                2.079566,\n",
       "                2.079545,\n",
       "                2.079539,\n",
       "                2.079537,\n",
       "                2.079527,\n",
       "                2.07952,\n",
       "                2.079513,\n",
       "                2.079507,\n",
       "                2.079505,\n",
       "                2.0795,\n",
       "                2.079493,\n",
       "                2.079496,\n",
       "                2.079484,\n",
       "                2.079475,\n",
       "                2.07947,\n",
       "                2.079468,\n",
       "                2.079465,\n",
       "                2.079451,\n",
       "                2.07945,\n",
       "                2.079444,\n",
       "                2.079444,\n",
       "                2.079428,\n",
       "                2.079413,\n",
       "                2.079404,\n",
       "                2.079399,\n",
       "                2.079379,\n",
       "                2.079372,\n",
       "                2.079349,\n",
       "                2.079342,\n",
       "                2.079335,\n",
       "                2.079337,\n",
       "                2.079336,\n",
       "                2.079337,\n",
       "                2.079331,\n",
       "                2.079327,\n",
       "                2.079313,\n",
       "                2.079311,\n",
       "                2.079301,\n",
       "                2.079287,\n",
       "                2.079275,\n",
       "                2.079274,\n",
       "                2.079258,\n",
       "                2.079248,\n",
       "                2.079233,\n",
       "                2.07923,\n",
       "                2.07923,\n",
       "                2.079229,\n",
       "                2.079225,\n",
       "                2.079212]),\n",
       "              ('auc',\n",
       "               [0.598253,\n",
       "                0.601141,\n",
       "                0.602342,\n",
       "                0.603543,\n",
       "                0.604139,\n",
       "                0.60494,\n",
       "                0.605443,\n",
       "                0.605851,\n",
       "                0.606437,\n",
       "                0.606949,\n",
       "                0.607375,\n",
       "                0.607739,\n",
       "                0.608041,\n",
       "                0.608347,\n",
       "                0.60867,\n",
       "                0.608922,\n",
       "                0.609089,\n",
       "                0.60928,\n",
       "                0.609503,\n",
       "                0.609693,\n",
       "                0.60982,\n",
       "                0.61015,\n",
       "                0.610302,\n",
       "                0.610542,\n",
       "                0.610849,\n",
       "                0.61098,\n",
       "                0.611182,\n",
       "                0.611318,\n",
       "                0.611519,\n",
       "                0.61164,\n",
       "                0.611776,\n",
       "                0.611853,\n",
       "                0.612054,\n",
       "                0.61219,\n",
       "                0.612242,\n",
       "                0.612456,\n",
       "                0.61262,\n",
       "                0.612678,\n",
       "                0.612842,\n",
       "                0.612986,\n",
       "                0.613119,\n",
       "                0.61323,\n",
       "                0.613288,\n",
       "                0.613347,\n",
       "                0.613418,\n",
       "                0.613423,\n",
       "                0.613461,\n",
       "                0.613496,\n",
       "                0.61352,\n",
       "                0.61354,\n",
       "                0.6136,\n",
       "                0.613591,\n",
       "                0.613589,\n",
       "                0.613613,\n",
       "                0.613626,\n",
       "                0.61372,\n",
       "                0.613748,\n",
       "                0.613798,\n",
       "                0.613788,\n",
       "                0.6138,\n",
       "                0.613858,\n",
       "                0.613856,\n",
       "                0.613858,\n",
       "                0.613861,\n",
       "                0.613865,\n",
       "                0.613865,\n",
       "                0.613868,\n",
       "                0.613893,\n",
       "                0.613916,\n",
       "                0.613913,\n",
       "                0.613942,\n",
       "                0.613944,\n",
       "                0.613963,\n",
       "                0.613981,\n",
       "                0.61398,\n",
       "                0.614018,\n",
       "                0.614012,\n",
       "                0.61403,\n",
       "                0.614051,\n",
       "                0.614065,\n",
       "                0.614085,\n",
       "                0.614102,\n",
       "                0.614087,\n",
       "                0.614089,\n",
       "                0.614105,\n",
       "                0.614113,\n",
       "                0.6141,\n",
       "                0.61411,\n",
       "                0.614128,\n",
       "                0.614128,\n",
       "                0.614123,\n",
       "                0.614124,\n",
       "                0.614132,\n",
       "                0.61414,\n",
       "                0.614134,\n",
       "                0.614134,\n",
       "                0.614128,\n",
       "                0.614134,\n",
       "                0.614138,\n",
       "                0.614145,\n",
       "                0.614153,\n",
       "                0.614162,\n",
       "                0.61415,\n",
       "                0.614162,\n",
       "                0.614155,\n",
       "                0.614178,\n",
       "                0.614195,\n",
       "                0.614212,\n",
       "                0.614221,\n",
       "                0.614221,\n",
       "                0.614237,\n",
       "                0.614244,\n",
       "                0.61426,\n",
       "                0.61427,\n",
       "                0.614282,\n",
       "                0.614284,\n",
       "                0.61429,\n",
       "                0.614312,\n",
       "                0.614321,\n",
       "                0.614324,\n",
       "                0.614341,\n",
       "                0.614347,\n",
       "                0.614351,\n",
       "                0.61435,\n",
       "                0.614353,\n",
       "                0.614354,\n",
       "                0.614365,\n",
       "                0.61438,\n",
       "                0.614384,\n",
       "                0.614381,\n",
       "                0.614389,\n",
       "                0.614395,\n",
       "                0.614405,\n",
       "                0.61442,\n",
       "                0.614442,\n",
       "                0.61446,\n",
       "                0.614462,\n",
       "                0.614475,\n",
       "                0.614485,\n",
       "                0.614492,\n",
       "                0.614502,\n",
       "                0.614504,\n",
       "                0.614497,\n",
       "                0.614501,\n",
       "                0.614498,\n",
       "                0.614504,\n",
       "                0.614515,\n",
       "                0.614524,\n",
       "                0.614536,\n",
       "                0.614551,\n",
       "                0.61455,\n",
       "                0.614554,\n",
       "                0.614567,\n",
       "                0.614571,\n",
       "                0.614571,\n",
       "                0.614577,\n",
       "                0.61458,\n",
       "                0.614577,\n",
       "                0.61458,\n",
       "                0.61458,\n",
       "                0.614584,\n",
       "                0.614586,\n",
       "                0.614584,\n",
       "                0.61459,\n",
       "                0.614594,\n",
       "                0.614598,\n",
       "                0.6146,\n",
       "                0.614601,\n",
       "                0.614609,\n",
       "                0.61461,\n",
       "                0.614611,\n",
       "                0.614604,\n",
       "                0.614612,\n",
       "                0.614622,\n",
       "                0.614628,\n",
       "                0.61463,\n",
       "                0.61464,\n",
       "                0.614643,\n",
       "                0.614659,\n",
       "                0.614665,\n",
       "                0.614668,\n",
       "                0.614668,\n",
       "                0.614669,\n",
       "                0.614669,\n",
       "                0.614672,\n",
       "                0.614676,\n",
       "                0.614683,\n",
       "                0.614684,\n",
       "                0.614688,\n",
       "                0.614696,\n",
       "                0.614702,\n",
       "                0.614704,\n",
       "                0.614712,\n",
       "                0.614716,\n",
       "                0.614726,\n",
       "                0.614729,\n",
       "                0.61473,\n",
       "                0.61473,\n",
       "                0.614731,\n",
       "                0.614737]),\n",
       "              ('merror',\n",
       "               [0.73782,\n",
       "                0.736012,\n",
       "                0.734923,\n",
       "                0.734415,\n",
       "                0.734394,\n",
       "                0.733806,\n",
       "                0.733643,\n",
       "                0.733631,\n",
       "                0.73342,\n",
       "                0.732988,\n",
       "                0.732547,\n",
       "                0.732556,\n",
       "                0.732391,\n",
       "                0.732311,\n",
       "                0.732046,\n",
       "                0.73203,\n",
       "                0.731683,\n",
       "                0.731566,\n",
       "                0.731479,\n",
       "                0.731341,\n",
       "                0.73133,\n",
       "                0.731284,\n",
       "                0.731188,\n",
       "                0.731063,\n",
       "                0.730976,\n",
       "                0.730907,\n",
       "                0.730913,\n",
       "                0.730882,\n",
       "                0.730976,\n",
       "                0.730836,\n",
       "                0.730779,\n",
       "                0.730742,\n",
       "                0.730683,\n",
       "                0.730523,\n",
       "                0.730479,\n",
       "                0.730354,\n",
       "                0.730351,\n",
       "                0.730258,\n",
       "                0.730009,\n",
       "                0.729901,\n",
       "                0.729949,\n",
       "                0.729951,\n",
       "                0.729876,\n",
       "                0.729736,\n",
       "                0.729702,\n",
       "                0.72969,\n",
       "                0.729656,\n",
       "                0.729569,\n",
       "                0.729602,\n",
       "                0.729469,\n",
       "                0.7294,\n",
       "                0.729448,\n",
       "                0.729412,\n",
       "                0.729385,\n",
       "                0.729333,\n",
       "                0.729258,\n",
       "                0.729235,\n",
       "                0.729224,\n",
       "                0.729268,\n",
       "                0.729203,\n",
       "                0.729218,\n",
       "                0.72917,\n",
       "                0.729254,\n",
       "                0.729312,\n",
       "                0.729208,\n",
       "                0.729199,\n",
       "                0.729228,\n",
       "                0.729258,\n",
       "                0.72926,\n",
       "                0.72926,\n",
       "                0.729176,\n",
       "                0.729158,\n",
       "                0.729145,\n",
       "                0.729137,\n",
       "                0.729116,\n",
       "                0.729064,\n",
       "                0.729074,\n",
       "                0.728995,\n",
       "                0.728963,\n",
       "                0.728934,\n",
       "                0.728982,\n",
       "                0.728997,\n",
       "                0.729076,\n",
       "                0.729028,\n",
       "                0.729053,\n",
       "                0.729068,\n",
       "                0.729001,\n",
       "                0.729036,\n",
       "                0.728955,\n",
       "                0.72892,\n",
       "                0.728886,\n",
       "                0.72892,\n",
       "                0.728799,\n",
       "                0.728817,\n",
       "                0.728861,\n",
       "                0.728884,\n",
       "                0.728847,\n",
       "                0.728849,\n",
       "                0.728865,\n",
       "                0.728867,\n",
       "                0.728805,\n",
       "                0.728799,\n",
       "                0.728834,\n",
       "                0.72879,\n",
       "                0.728803,\n",
       "                0.728744,\n",
       "                0.728688,\n",
       "                0.728663,\n",
       "                0.728609,\n",
       "                0.72864,\n",
       "                0.728581,\n",
       "                0.728598,\n",
       "                0.728586,\n",
       "                0.72855,\n",
       "                0.728525,\n",
       "                0.728571,\n",
       "                0.728538,\n",
       "                0.728569,\n",
       "                0.728561,\n",
       "                0.728542,\n",
       "                0.728563,\n",
       "                0.728604,\n",
       "                0.728617,\n",
       "                0.728682,\n",
       "                0.728667,\n",
       "                0.728623,\n",
       "                0.728665,\n",
       "                0.728632,\n",
       "                0.728638,\n",
       "                0.728625,\n",
       "                0.728669,\n",
       "                0.728565,\n",
       "                0.728506,\n",
       "                0.728554,\n",
       "                0.728504,\n",
       "                0.728462,\n",
       "                0.728475,\n",
       "                0.72846,\n",
       "                0.72841,\n",
       "                0.728364,\n",
       "                0.728392,\n",
       "                0.728348,\n",
       "                0.728398,\n",
       "                0.728406,\n",
       "                0.7284,\n",
       "                0.728389,\n",
       "                0.728379,\n",
       "                0.728398,\n",
       "                0.728362,\n",
       "                0.728312,\n",
       "                0.728314,\n",
       "                0.728258,\n",
       "                0.72827,\n",
       "                0.72825,\n",
       "                0.728233,\n",
       "                0.728225,\n",
       "                0.728222,\n",
       "                0.728225,\n",
       "                0.728212,\n",
       "                0.728254,\n",
       "                0.728247,\n",
       "                0.728237,\n",
       "                0.72825,\n",
       "                0.728252,\n",
       "                0.728258,\n",
       "                0.728306,\n",
       "                0.728293,\n",
       "                0.728279,\n",
       "                0.728268,\n",
       "                0.728266,\n",
       "                0.728277,\n",
       "                0.728266,\n",
       "                0.728235,\n",
       "                0.72826,\n",
       "                0.728245,\n",
       "                0.728235,\n",
       "                0.72822,\n",
       "                0.728212,\n",
       "                0.728179,\n",
       "                0.728131,\n",
       "                0.728166,\n",
       "                0.728193,\n",
       "                0.728218,\n",
       "                0.728222,\n",
       "                0.728206,\n",
       "                0.728197,\n",
       "                0.728195,\n",
       "                0.728195,\n",
       "                0.728195,\n",
       "                0.728183,\n",
       "                0.728183,\n",
       "                0.728197,\n",
       "                0.728202,\n",
       "                0.728214,\n",
       "                0.728229,\n",
       "                0.728227,\n",
       "                0.728225,\n",
       "                0.728239,\n",
       "                0.728229,\n",
       "                0.728218])])}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test error using softmax = 0.7252901820185232\n"
     ]
    }
   ],
   "source": [
    "# get prediction\n",
    "pred = wl_bst_sm.predict(xg_test)\n",
    "# pred = pred.astype(np.uint8)\n",
    "error_rate = np.sum(pred != y_test) / y_test.shape[0]\n",
    "print('Test error using softmax = {}'.format(error_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:05:31] WARNING: ../src/metric/auc.cc:307: Dataset contains only positive or negative samples.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'[0]\\teval-mlogloss:1.518223\\teval-auc:nan\\teval-merror:0.000000'"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_s = wl_bst_sm.eval(xg_test)\n",
    "# eval_dict = eval_str_2_dict(eval_s)\n",
    "eval_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.66923905, 0.59721769, 0.51477513, 0.50586486, 0.50401369,\n",
       "        0.50401172, 0.50597849, 0.50252654, 0.51054198, 0.63821021]),\n",
       " 2.4062260256445)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = np.arange(0, 1, 0.1)\n",
    "aucs = auc(y_test.astype(np.uint8), pred.astype(np.uint8), np.arange(param['num_class']))\n",
    "# aucs[aucs == 0.5] = 0\n",
    "w_auc = (aucs * weights).sum()\n",
    "aucs, w_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "report = metrics.classification_report(list(y_test), list(pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.37      0.51      0.43     38973\n",
      "           1       0.26      0.43      0.32     38841\n",
      "           2       0.18      0.10      0.13     31297\n",
      "           3       0.18      0.02      0.04     21689\n",
      "           4       0.18      0.01      0.02     17005\n",
      "           5       0.18      0.01      0.02     14485\n",
      "           6       0.21      0.02      0.03     12694\n",
      "           7       0.18      0.01      0.01     11846\n",
      "           8       0.25      0.03      0.05     13820\n",
      "           9       0.26      0.63      0.36     38941\n",
      "\n",
      "    accuracy                           0.27    239591\n",
      "   macro avg       0.22      0.18      0.14    239591\n",
      "weighted avg       0.24      0.27      0.21    239591\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 调参"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn import svm, datasets\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from collections.abc import Iterable\n",
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "base_param = {  # 基本参数，不需要调参\n",
    "    'objective': 'multi:softmax',\n",
    "    'eta': 0.1,\n",
    "    'nthread': 8,\n",
    "    'num_class': 10,\n",
    "    'gpu_id': 0,\n",
    "    'tree_method': 'gpu_hist',\n",
    "    'eval_metric': ['mlogloss', 'auc', 'merror']\n",
    "} \n",
    "# 需要调参的参数\n",
    "ps1 = {  \n",
    "    'max_depth': list(range(5, 14, 2)),\n",
    "    'min_child_weight': list(range(1, 10, 2)),\n",
    "}\n",
    "\n",
    "ps2 = {\n",
    "    'gamma': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6]\n",
    "}\n",
    "\n",
    "ps3 = {\n",
    "    'subsample':  [i/10.0 for i in range(6,11,1)], \n",
    "    'colsample_bytree':  [i/10.0 for i in range(6,11,1)] \n",
    "}\n",
    "\n",
    "ps4 = {'reg_alpha': [0, 0.1, 0.2, 0.5, 1, 1.5, 2, 4]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 529,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data preparing finished ...\t\t(5.092s)\n"
     ]
    }
   ],
   "source": [
    "t0 = time()\n",
    "xg_train = xgb.DMatrix(X_train.values, label=y_train.values, enable_categorical=True)\n",
    "xg_test = xgb.DMatrix(X_test.values, label=y_test.values, enable_categorical=True)\n",
    "print(f\"Data preparing finished ...\\t\\t({time()-t0:.3f}s)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def myproduct(*iterables):\n",
    "    n = len(iterables)\n",
    "#     print(iterables)\n",
    "    if n == 0:\n",
    "        return None \n",
    "    \n",
    "    ret = []\n",
    "    ret.extend([[e] for e in iterables[0].copy()])\n",
    "    if n == 1:\n",
    "        return ret\n",
    "\n",
    "    # 将需要调参的参数进行组合，即笛卡尔乘积。类似于sklearn中的 ParameterGrid\n",
    "    for k in range(1, n):\n",
    "        v = iterables[k].copy()\n",
    "        l = len(ret)\n",
    "        ret = [ret[i%l].copy() for i in range(len(v) * len(ret))]\n",
    "        for i, e in enumerate(ret):\n",
    "            e.append(v[i // l])\n",
    "    return ret\n",
    "\n",
    "def compose_param_grid(grid, base):\n",
    "    items = list(grid.items())\n",
    "    iterables = [item[1] for item in items]\n",
    "    keys = [item[0] for item in items]\n",
    "\n",
    "    ret = myproduct(*iterables)\n",
    "    com_ps = [dict(zip(keys, e)) for e in ret]\n",
    "\n",
    "\n",
    "    all_params = [base.copy() for _ in range(len(com_ps))] \n",
    "    for i in range(len(com_ps)):\n",
    "        all_params[i].update(com_ps[i])\n",
    "        \n",
    "    return all_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始搜索：colsample_bytree=[0.6, 0.7, 0.8, 0.9, 1.0], subsample=[0.6, 0.7, 0.8, 0.9, 1.0]\n",
      "待搜索的参数组合数量：25\n",
      "1 / 25: 200-rounds Training finished param={'objective': 'multi:softmax', 'eta': 0.1, 'nthread': 8, 'num_class': 10, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['mlogloss', 'auc', 'merror'], 'max_depth': 9, 'min_child_weight': 9, 'gamma': 0.2, 'subsample': 0.6, 'colsample_bytree': 0.6} ...\t\t(820.946s)\n",
      "2 / 25: 200-rounds Training finished param={'objective': 'multi:softmax', 'eta': 0.1, 'nthread': 8, 'num_class': 10, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['mlogloss', 'auc', 'merror'], 'max_depth': 9, 'min_child_weight': 9, 'gamma': 0.2, 'subsample': 0.7, 'colsample_bytree': 0.6} ...\t\t(816.416s)\n",
      "3 / 25: 200-rounds Training finished param={'objective': 'multi:softmax', 'eta': 0.1, 'nthread': 8, 'num_class': 10, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['mlogloss', 'auc', 'merror'], 'max_depth': 9, 'min_child_weight': 9, 'gamma': 0.2, 'subsample': 0.8, 'colsample_bytree': 0.6} ...\t\t(828.957s)\n",
      "4 / 25: 200-rounds Training finished param={'objective': 'multi:softmax', 'eta': 0.1, 'nthread': 8, 'num_class': 10, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['mlogloss', 'auc', 'merror'], 'max_depth': 9, 'min_child_weight': 9, 'gamma': 0.2, 'subsample': 0.9, 'colsample_bytree': 0.6} ...\t\t(838.505s)\n",
      "5 / 25: 200-rounds Training finished param={'objective': 'multi:softmax', 'eta': 0.1, 'nthread': 8, 'num_class': 10, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['mlogloss', 'auc', 'merror'], 'max_depth': 9, 'min_child_weight': 9, 'gamma': 0.2, 'subsample': 1.0, 'colsample_bytree': 0.6} ...\t\t(823.276s)\n",
      "6 / 25: 200-rounds Training finished param={'objective': 'multi:softmax', 'eta': 0.1, 'nthread': 8, 'num_class': 10, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['mlogloss', 'auc', 'merror'], 'max_depth': 9, 'min_child_weight': 9, 'gamma': 0.2, 'subsample': 0.6, 'colsample_bytree': 0.7} ...\t\t(828.692s)\n",
      "7 / 25: 200-rounds Training finished param={'objective': 'multi:softmax', 'eta': 0.1, 'nthread': 8, 'num_class': 10, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['mlogloss', 'auc', 'merror'], 'max_depth': 9, 'min_child_weight': 9, 'gamma': 0.2, 'subsample': 0.7, 'colsample_bytree': 0.7} ...\t\t(848.865s)\n",
      "8 / 25: 200-rounds Training finished param={'objective': 'multi:softmax', 'eta': 0.1, 'nthread': 8, 'num_class': 10, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['mlogloss', 'auc', 'merror'], 'max_depth': 9, 'min_child_weight': 9, 'gamma': 0.2, 'subsample': 0.8, 'colsample_bytree': 0.7} ...\t\t(854.009s)\n",
      "9 / 25: 200-rounds Training finished param={'objective': 'multi:softmax', 'eta': 0.1, 'nthread': 8, 'num_class': 10, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['mlogloss', 'auc', 'merror'], 'max_depth': 9, 'min_child_weight': 9, 'gamma': 0.2, 'subsample': 0.9, 'colsample_bytree': 0.7} ...\t\t(863.061s)\n",
      "10 / 25: 200-rounds Training finished param={'objective': 'multi:softmax', 'eta': 0.1, 'nthread': 8, 'num_class': 10, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['mlogloss', 'auc', 'merror'], 'max_depth': 9, 'min_child_weight': 9, 'gamma': 0.2, 'subsample': 1.0, 'colsample_bytree': 0.7} ...\t\t(854.728s)\n",
      "11 / 25: 200-rounds Training finished param={'objective': 'multi:softmax', 'eta': 0.1, 'nthread': 8, 'num_class': 10, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['mlogloss', 'auc', 'merror'], 'max_depth': 9, 'min_child_weight': 9, 'gamma': 0.2, 'subsample': 0.6, 'colsample_bytree': 0.8} ...\t\t(833.519s)\n",
      "12 / 25: 200-rounds Training finished param={'objective': 'multi:softmax', 'eta': 0.1, 'nthread': 8, 'num_class': 10, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['mlogloss', 'auc', 'merror'], 'max_depth': 9, 'min_child_weight': 9, 'gamma': 0.2, 'subsample': 0.7, 'colsample_bytree': 0.8} ...\t\t(836.241s)\n",
      "13 / 25: 200-rounds Training finished param={'objective': 'multi:softmax', 'eta': 0.1, 'nthread': 8, 'num_class': 10, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['mlogloss', 'auc', 'merror'], 'max_depth': 9, 'min_child_weight': 9, 'gamma': 0.2, 'subsample': 0.8, 'colsample_bytree': 0.8} ...\t\t(852.569s)\n",
      "14 / 25: 200-rounds Training finished param={'objective': 'multi:softmax', 'eta': 0.1, 'nthread': 8, 'num_class': 10, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['mlogloss', 'auc', 'merror'], 'max_depth': 9, 'min_child_weight': 9, 'gamma': 0.2, 'subsample': 0.9, 'colsample_bytree': 0.8} ...\t\t(867.649s)\n",
      "15 / 25: 200-rounds Training finished param={'objective': 'multi:softmax', 'eta': 0.1, 'nthread': 8, 'num_class': 10, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['mlogloss', 'auc', 'merror'], 'max_depth': 9, 'min_child_weight': 9, 'gamma': 0.2, 'subsample': 1.0, 'colsample_bytree': 0.8} ...\t\t(837.901s)\n",
      "16 / 25: 200-rounds Training finished param={'objective': 'multi:softmax', 'eta': 0.1, 'nthread': 8, 'num_class': 10, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['mlogloss', 'auc', 'merror'], 'max_depth': 9, 'min_child_weight': 9, 'gamma': 0.2, 'subsample': 0.6, 'colsample_bytree': 0.9} ...\t\t(827.339s)\n",
      "17 / 25: 200-rounds Training finished param={'objective': 'multi:softmax', 'eta': 0.1, 'nthread': 8, 'num_class': 10, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['mlogloss', 'auc', 'merror'], 'max_depth': 9, 'min_child_weight': 9, 'gamma': 0.2, 'subsample': 0.7, 'colsample_bytree': 0.9} ...\t\t(829.429s)\n",
      "18 / 25: 200-rounds Training finished param={'objective': 'multi:softmax', 'eta': 0.1, 'nthread': 8, 'num_class': 10, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['mlogloss', 'auc', 'merror'], 'max_depth': 9, 'min_child_weight': 9, 'gamma': 0.2, 'subsample': 0.8, 'colsample_bytree': 0.9} ...\t\t(854.584s)\n",
      "19 / 25: 200-rounds Training finished param={'objective': 'multi:softmax', 'eta': 0.1, 'nthread': 8, 'num_class': 10, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['mlogloss', 'auc', 'merror'], 'max_depth': 9, 'min_child_weight': 9, 'gamma': 0.2, 'subsample': 0.9, 'colsample_bytree': 0.9} ...\t\t(865.975s)\n",
      "20 / 25: 200-rounds Training finished param={'objective': 'multi:softmax', 'eta': 0.1, 'nthread': 8, 'num_class': 10, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['mlogloss', 'auc', 'merror'], 'max_depth': 9, 'min_child_weight': 9, 'gamma': 0.2, 'subsample': 1.0, 'colsample_bytree': 0.9} ...\t\t(830.648s)\n",
      "21 / 25: 200-rounds Training finished param={'objective': 'multi:softmax', 'eta': 0.1, 'nthread': 8, 'num_class': 10, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['mlogloss', 'auc', 'merror'], 'max_depth': 9, 'min_child_weight': 9, 'gamma': 0.2, 'subsample': 0.6, 'colsample_bytree': 1.0} ...\t\t(846.784s)\n",
      "22 / 25: 200-rounds Training finished param={'objective': 'multi:softmax', 'eta': 0.1, 'nthread': 8, 'num_class': 10, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['mlogloss', 'auc', 'merror'], 'max_depth': 9, 'min_child_weight': 9, 'gamma': 0.2, 'subsample': 0.7, 'colsample_bytree': 1.0} ...\t\t(852.846s)\n",
      "23 / 25: 200-rounds Training finished param={'objective': 'multi:softmax', 'eta': 0.1, 'nthread': 8, 'num_class': 10, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['mlogloss', 'auc', 'merror'], 'max_depth': 9, 'min_child_weight': 9, 'gamma': 0.2, 'subsample': 0.8, 'colsample_bytree': 1.0} ...\t\t(850.433s)\n",
      "24 / 25: 200-rounds Training finished param={'objective': 'multi:softmax', 'eta': 0.1, 'nthread': 8, 'num_class': 10, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['mlogloss', 'auc', 'merror'], 'max_depth': 9, 'min_child_weight': 9, 'gamma': 0.2, 'subsample': 0.9, 'colsample_bytree': 1.0} ...\t\t(865.453s)\n",
      "25 / 25: 200-rounds Training finished param={'objective': 'multi:softmax', 'eta': 0.1, 'nthread': 8, 'num_class': 10, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['mlogloss', 'auc', 'merror'], 'max_depth': 9, 'min_child_weight': 9, 'gamma': 0.2, 'subsample': 1.0, 'colsample_bytree': 1.0} ...\t\t(833.121s)\n",
      "colsample_bytree=0.6, eta=0.1, eval_metric=['mlogloss', 'auc', 'merror'], gamma=0.2, gpu_id=0, max_depth=9, min_child_weight=9, nthread=8, num_class=10, objective=multi:softmax, subsample=0.9, tree_method=gpu_hist\n",
      "colsample_bytree=1.0, eta=0.1, eval_metric=['mlogloss', 'auc', 'merror'], gamma=0.2, gpu_id=0, max_depth=9, min_child_weight=9, nthread=8, num_class=10, objective=multi:softmax, subsample=0.6, tree_method=gpu_hist\n",
      "colsample_bytree=1.0, eta=0.1, eval_metric=['mlogloss', 'auc', 'merror'], gamma=0.2, gpu_id=0, max_depth=9, min_child_weight=9, nthread=8, num_class=10, objective=multi:softmax, subsample=0.9, tree_method=gpu_hist\n",
      "开始搜索：reg_alpha=[0, 0.1, 0.2, 0.5, 1, 1.5, 2, 4]\n",
      "待搜索的参数组合数量：8\n",
      "1 / 8: 200-rounds Training finished param={'objective': 'multi:softmax', 'eta': 0.1, 'nthread': 8, 'num_class': 10, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['mlogloss', 'auc', 'merror'], 'max_depth': 9, 'min_child_weight': 9, 'gamma': 0.2, 'subsample': 0.9, 'colsample_bytree': 0.6, 'reg_alpha': 0} ...\t\t(864.738s)\n",
      "2 / 8: 200-rounds Training finished param={'objective': 'multi:softmax', 'eta': 0.1, 'nthread': 8, 'num_class': 10, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['mlogloss', 'auc', 'merror'], 'max_depth': 9, 'min_child_weight': 9, 'gamma': 0.2, 'subsample': 0.9, 'colsample_bytree': 0.6, 'reg_alpha': 0.1} ...\t\t(871.016s)\n",
      "3 / 8: 200-rounds Training finished param={'objective': 'multi:softmax', 'eta': 0.1, 'nthread': 8, 'num_class': 10, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['mlogloss', 'auc', 'merror'], 'max_depth': 9, 'min_child_weight': 9, 'gamma': 0.2, 'subsample': 0.9, 'colsample_bytree': 0.6, 'reg_alpha': 0.2} ...\t\t(854.900s)\n",
      "4 / 8: 200-rounds Training finished param={'objective': 'multi:softmax', 'eta': 0.1, 'nthread': 8, 'num_class': 10, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['mlogloss', 'auc', 'merror'], 'max_depth': 9, 'min_child_weight': 9, 'gamma': 0.2, 'subsample': 0.9, 'colsample_bytree': 0.6, 'reg_alpha': 0.5} ...\t\t(846.531s)\n",
      "5 / 8: 200-rounds Training finished param={'objective': 'multi:softmax', 'eta': 0.1, 'nthread': 8, 'num_class': 10, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['mlogloss', 'auc', 'merror'], 'max_depth': 9, 'min_child_weight': 9, 'gamma': 0.2, 'subsample': 0.9, 'colsample_bytree': 0.6, 'reg_alpha': 1} ...\t\t(857.497s)\n",
      "6 / 8: 200-rounds Training finished param={'objective': 'multi:softmax', 'eta': 0.1, 'nthread': 8, 'num_class': 10, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['mlogloss', 'auc', 'merror'], 'max_depth': 9, 'min_child_weight': 9, 'gamma': 0.2, 'subsample': 0.9, 'colsample_bytree': 0.6, 'reg_alpha': 1.5} ...\t\t(874.460s)\n",
      "7 / 8: 200-rounds Training finished param={'objective': 'multi:softmax', 'eta': 0.1, 'nthread': 8, 'num_class': 10, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['mlogloss', 'auc', 'merror'], 'max_depth': 9, 'min_child_weight': 9, 'gamma': 0.2, 'subsample': 0.9, 'colsample_bytree': 0.6, 'reg_alpha': 2} ...\t\t(875.624s)\n",
      "8 / 8: 200-rounds Training finished param={'objective': 'multi:softmax', 'eta': 0.1, 'nthread': 8, 'num_class': 10, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['mlogloss', 'auc', 'merror'], 'max_depth': 9, 'min_child_weight': 9, 'gamma': 0.2, 'subsample': 0.9, 'colsample_bytree': 0.6, 'reg_alpha': 4} ...\t\t(902.339s)\n",
      "colsample_bytree=0.6, eta=0.1, eval_metric=['mlogloss', 'auc', 'merror'], gamma=0.2, gpu_id=0, max_depth=9, min_child_weight=9, nthread=8, num_class=10, objective=multi:softmax, reg_alpha=0, subsample=0.9, tree_method=gpu_hist\n",
      "colsample_bytree=0.6, eta=0.1, eval_metric=['mlogloss', 'auc', 'merror'], gamma=0.2, gpu_id=0, max_depth=9, min_child_weight=9, nthread=8, num_class=10, objective=multi:softmax, reg_alpha=1, subsample=0.9, tree_method=gpu_hist\n",
      "colsample_bytree=0.6, eta=0.1, eval_metric=['mlogloss', 'auc', 'merror'], gamma=0.2, gpu_id=0, max_depth=9, min_child_weight=9, nthread=8, num_class=10, objective=multi:softmax, reg_alpha=4, subsample=0.9, tree_method=gpu_hist\n",
      "找到的最棒的参数是：\n",
      "colsample_bytree=0.6, eta=0.1, eval_metric=['mlogloss', 'auc', 'merror'], gamma=0.2, gpu_id=0, max_depth=9, min_child_weight=9, nthread=8, num_class=10, objective=multi:softmax, reg_alpha=0, subsample=0.9, tree_method=gpu_hist\n"
     ]
    }
   ],
   "source": [
    "base = base_param.copy()\n",
    "base.update({'max_depth': 9, 'min_child_weight': 9})\n",
    "base.update({'gamma': .2})\n",
    "grids = [ps3, ps4]\n",
    "\n",
    "rets = []\n",
    "for grid in grids:\n",
    "    params = compose_param_grid(grid, base)\n",
    "    print(f\"开始搜索：{dict_2_str(grid)}\\n待搜索的参数组合数量：{len(params)}\")\n",
    "    ret = gridsearch_cv_xgb(data.values, watch_label_res.values, params, n_round=200, verbose_eval=False, n_class=10)\n",
    "    arr = np.array([[-e['eval-merror'] for e in ret], \n",
    "                    [-e['eval-mlogloss'] for e in ret],\n",
    "                    [e['eval-auc'] for e in ret], \n",
    "                    [e['w_auc'] for e in ret]], dtype=np.float32)\n",
    "    opt_idxs = list(set(arr.argmax(axis=1)))\n",
    "    for i in opt_idxs:\n",
    "        print(dict_2_str(ret[i]['param']))\n",
    "    opt_idx = opt_idxs[0]\n",
    "    opt_param = ret[opt_idx]['param']\n",
    "    base.update(opt_param)\n",
    "    rets.append(ret)\n",
    "    \n",
    "    ks = list(grid.keys())\n",
    "    ks = '-'.join(ks)\n",
    "    with open(f\"./logs/{ks}-watch_label-{int(time())}.pkl\", \"wb\") as f:\n",
    "        pickle.dump(ret, f)\n",
    "    with open(f\"./logs/{ks}-watch_label-{int(time())}.md\", \"w\") as f:\n",
    "        lines = []\n",
    "        for e in ret:\n",
    "            line = metric_2_str(e)\n",
    "            lines.append(line)\n",
    "            lines.append('\\n')\n",
    "        f.write(\"\\n\".join(lines))\n",
    "\n",
    "print(f\"找到的最棒的参数是：\\n{dict_2_str(base)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'objective': 'multi:softmax',\n",
       " 'eta': 0.1,\n",
       " 'nthread': 8,\n",
       " 'num_class': 10,\n",
       " 'gpu_id': 0,\n",
       " 'tree_method': 'gpu_hist',\n",
       " 'eval_metric': ['mlogloss', 'auc', 'merror'],\n",
       " 'max_depth': 9,\n",
       " 'min_child_weight': 9,\n",
       " 'gamma': 0.2,\n",
       " 'subsample': 0.9,\n",
       " 'colsample_bytree': 0.6,\n",
       " 'reg_alpha': 0}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gridsearch_results = gridsearch_xgb(all_params, xg_train, xg_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_cv_results_sh = gridsearch_cv_xgb(data.values, watch_label_res.values, all_params, n_round=200, verbose_eval=False, n_class=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 584,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "最小误差与最大AUC对应的模型不一致 : [93 19]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-584-646c9ffaa26f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mopt_idxs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mopt_idxs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mopt_idxs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"最小误差与最大AUC对应的模型不一致 : {opt_idxs}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mopt_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopt_idxs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: 最小误差与最大AUC对应的模型不一致 : [93 19]"
     ]
    }
   ],
   "source": [
    "arr = np.array([[-e['test_error'] for e in gridsearch_results], [e['w_auc'] for e in gridsearch_results]], dtype=np.float32)\n",
    "opt_idxs = arr.argmax(axis=1)\n",
    "if opt_idxs[0] != opt_idxs[1]:\n",
    "     warnings.warn(f\"最小误差与最大AUC对应的模型不一致 : {opt_idxs}。选择误差最小的模型 : {opt_idxs[0]}\")\n",
    "\n",
    "opt_idx = opt_idxs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 583,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'test_error': 0.7289070620796754,\n",
       "  'aucs': array([0.57555597, 0.58567653, 0.50347593, 0.50017473, 0.50017414,\n",
       "         0.5000983 , 0.50292636, 0.50040085, 0.50878295, 0.60767447]),\n",
       "  'w_auc': 2.365403849959146,\n",
       "  'report': '              precision    recall  f1-score   support\\n\\n           0       0.33      0.19      0.24     43749\\n           1       0.29      0.68      0.41    111616\\n           2       0.23      0.01      0.03     62829\\n           3       0.13      0.00      0.00     43872\\n           4       0.15      0.00      0.00     34228\\n           5       0.18      0.00      0.00     28926\\n           6       0.33      0.01      0.01     25061\\n           7       0.16      0.00      0.00     23400\\n           8       0.31      0.02      0.04     27750\\n           9       0.24      0.56      0.34     77663\\n\\n    accuracy                           0.27    479094\\n   macro avg       0.23      0.15      0.11    479094\\nweighted avg       0.24      0.27      0.18    479094\\n',\n",
       "  'model': <xgboost.core.Booster at 0x7f35ba8b00d0>},\n",
       " {'test_error': 0.73068750600091,\n",
       "  'aucs': array([0.57882633, 0.58668642, 0.50378179, 0.50067885, 0.50046282,\n",
       "         0.50010955, 0.50312644, 0.50043677, 0.50916784, 0.60848382]),\n",
       "  'w_auc': 2.367019878746503,\n",
       "  'report': '              precision    recall  f1-score   support\\n\\n           0       0.32      0.20      0.25     43749\\n           1       0.29      0.66      0.40    111616\\n           2       0.18      0.03      0.04     62829\\n           3       0.12      0.01      0.01     43872\\n           4       0.12      0.00      0.00     34228\\n           5       0.08      0.00      0.00     28926\\n           6       0.25      0.01      0.01     25061\\n           7       0.11      0.00      0.00     23400\\n           8       0.27      0.02      0.04     27750\\n           9       0.24      0.57      0.34     77663\\n\\n    accuracy                           0.27    479094\\n   macro avg       0.20      0.15      0.11    479094\\nweighted avg       0.22      0.27      0.18    479094\\n',\n",
       "  'model': <xgboost.core.Booster at 0x7f3584f94e50>})"
      ]
     },
     "execution_count": 583,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gridsearch_results[93]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 588,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.33      0.19      0.24     43749\n",
      "           1       0.29      0.68      0.41    111616\n",
      "           2       0.23      0.01      0.03     62829\n",
      "           3       0.13      0.00      0.00     43872\n",
      "           4       0.15      0.00      0.00     34228\n",
      "           5       0.18      0.00      0.00     28926\n",
      "           6       0.33      0.01      0.01     25061\n",
      "           7       0.16      0.00      0.00     23400\n",
      "           8       0.31      0.02      0.04     27750\n",
      "           9       0.24      0.56      0.34     77663\n",
      "\n",
      "    accuracy                           0.27    479094\n",
      "   macro avg       0.23      0.15      0.11    479094\n",
      "weighted avg       0.24      0.27      0.18    479094\n",
      "\n"
     ]
    }
   ],
   "source": [
    "opt_idx = opt_idxs[0]\n",
    "opt_param = all_params[opt_idx]\n",
    "print(gridsearch_results[opt_idx]['report'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 557,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(479094,)"
      ]
     },
     "execution_count": 557,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gridsearch_cv_xgb(data.values, watch_label_res, all_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {}\n",
    "# use softmax multi-class classification\n",
    "param['objective'] = 'multi:softmax'\n",
    "# scale weight of positive examples\n",
    "param['eta'] = 0.1\n",
    "param['max_depth'] = 8\n",
    "param['nthread'] = 8\n",
    "param['num_class'] = 10\n",
    "# param['gpu_id'] = 0\n",
    "# param['tree_method'] = 'gpu_hist'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "cv_res= xgb.cv(param, cv_data, num_boost_round=200,early_stopping_rounds=30,nfold=3, metrics='auc',show_stdv=True)\n",
    "print(cv_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB \n",
    "from mlxtend.classifier import StackingClassifier\n",
    "from sklearn.ensemble import (RandomForestClassifier, AdaBoostClassifier, \n",
    "                              GradientBoostingClassifier, ExtraTreesClassifier)\n",
    "from xgboost import XGBClassifier, XGBRFClassifier\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 第一层模型参数\n",
    "first_layer_params = {\n",
    "    XGBRFClassifier : {\n",
    "        'n_jobs': 4,\n",
    "        'n_estimators': 200,\n",
    "         #'max_features': 0.2,\n",
    "        'colsample_bytree': 0.6,\n",
    "        'max_depth': 8,\n",
    "        'learning_rate': .1,\n",
    "        'verbosity': 0,\n",
    "        'gpu_id': 1,\n",
    "    },\n",
    "    ExtraTreesClassifier : {\n",
    "        'n_jobs': -1,\n",
    "        'n_estimators':200,\n",
    "        #'max_features': 0.5,\n",
    "        'max_depth': 8,\n",
    "        'min_samples_leaf': 2,\n",
    "        'verbose': 0\n",
    "    },\n",
    "    AdaBoostClassifier : {\n",
    "        'n_estimators': 200,\n",
    "        'learning_rate' : 0.75\n",
    "    },\n",
    "#     GradientBoostingClassifier : { # 太慢了\n",
    "#         'n_estimators': 200,\n",
    "#          #'max_features': 0.2,\n",
    "#         'max_depth': 5,\n",
    "#         'min_samples_leaf': 2,\n",
    "#         'verbose': 0\n",
    "#     },\n",
    "#     SVC : { # 太慢了\n",
    "#         'kernel' : 'rbf',\n",
    "#         'C' : 0.025,\n",
    "#         'probability': True\n",
    "#     } \n",
    "}\n",
    "\n",
    "\n",
    "second_layer_params = {\n",
    "    XGBClassifier : {\n",
    "        'objective': 'multi:softmax',\n",
    "        'eta': 0.1,\n",
    "        'nthread': 8,\n",
    "        'num_class': 10,\n",
    "        'gpu_id': 0,\n",
    "        'tree_method': 'gpu_hist',\n",
    "        'eval_metric': ['mlogloss', 'auc', 'merror'],\n",
    "        'max_depth': 9,\n",
    "        'min_child_weight': 9,\n",
    "        'gamma': 0.2,\n",
    "        'subsample': 0.9,\n",
    "        'colsample_bytree': 0.6,\n",
    "        'reg_alpha': 0,\n",
    "        'n_estimators': 200,\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gzy/.local/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "clf = XGBRFClassifier(**first_layer_params[XGBRFClassifier])\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.10100084, 0.10091538, 0.10042646, 0.09997695, 0.09962477,\n",
       "        0.09947018, 0.09939709, 0.09926751, 0.09943457, 0.10048625]])"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# is_share 预测"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 处理数据不均衡问题"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 7338705), (1, 14319)]\n",
      "[[0.         0.99805264]\n",
      " [1.         0.00194736]]\n"
     ]
    }
   ],
   "source": [
    "items = list(Counter(is_share).items())\n",
    "items.sort(key=lambda x: x[0])\n",
    "print(items)\n",
    "\n",
    "dist = np.array(items, dtype=np.float)\n",
    "dist[:, 1] = dist[:, 1] / is_share.shape[0]\n",
    "print(dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "under_ss = np.array(items)\n",
    "under_ss_thresh = under_ss[1, 1]\n",
    "under_ss[:, 1] = np.clip(under_ss[:, 1], a_min=None, a_max=under_ss_thresh)\n",
    "\n",
    "over_ss = under_ss.copy()\n",
    "over_ss_thresh = under_ss[1, 1]\n",
    "over_ss[:, 1] = np.clip(over_ss[:, 1], a_min=over_ss_thresh, a_max=None)\n",
    "\n",
    "under_ss = dict(under_ss)\n",
    "over_ss = dict(over_ss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({0: 14319, 1: 14319}, {0: 14319, 1: 14319})"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "under_ss, over_ss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7338705,)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idxs = is_share == 0\n",
    "idxs = idxs.replace(False, np.nan).dropna().index  # 保留watch_label=0的行索引\n",
    "idxs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((7324386,), (14319,))"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "left_idxs = np.random.choice(idxs, under_ss_thresh, replace=False)  # 选择一部分保留\n",
    "del_idxs = idxs.difference(left_idxs)\n",
    "del_idxs.shape, left_idxs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((28638, 128), (28638,))"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['watch_label'] = watch_label\n",
    "resampled_data = np.delete(dataset.values, del_idxs, axis=0)\n",
    "resampled_sh = np.delete(is_share.values, del_idxs, axis=0)\n",
    "resampled_data.shape, resampled_sh.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((28638, 128), (28638,))"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 将采样后的数据重装会DataFrame\n",
    "data_sh = pd.DataFrame(resampled_data, columns=dataset.columns)\n",
    "del dataset['watch_label']\n",
    "is_share_res = pd.Series(resampled_sh)\n",
    "data_sh.shape, is_share_res.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((22910,), (5728,))"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_rate = .2\n",
    "train_idx, test_idx = train_test_split(data_sh.index, test_size=test_rate, random_state=1)\n",
    "train_idx.shape, test_idx.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_sh = data_sh.iloc[train_idx]\n",
    "X_test_sh  = data_sh.iloc[test_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_sh = is_share_res.iloc[train_idx]\n",
    "y_test_sh  = is_share_res.iloc[test_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data preparing finished ...\t\t(0.024s)\n"
     ]
    }
   ],
   "source": [
    "t0 = time()\n",
    "xg_train_sh = xgb.DMatrix(X_train_sh.values, label=y_train_sh.values, enable_categorical=True)\n",
    "xg_test_sh = xgb.DMatrix(X_test_sh.values, label=y_test_sh.values, enable_categorical=True)\n",
    "print(f\"Data preparing finished ...\\t\\t({time()-t0:.3f}s)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup parameters for xgboost\n",
    "param_sh = {\n",
    "    'objective': 'binary:hinge',\n",
    "    'eta': 0.1,\n",
    "    'nthread': 8,\n",
    "    'gpu_id': 0,\n",
    "    'tree_method': 'gpu_hist',\n",
    "    'eval_metric': ['logloss', 'auc', 'error'],\n",
    "    'max_depth': 5,\n",
    "    'min_child_weight': 1,\n",
    "    'gamma': 0.1,\n",
    "    'subsample': 0.6,\n",
    "    'colsample_bytree': 0.6,\n",
    "    'reg_alpha': 0\n",
    "}\n",
    "\n",
    "# use softmax multi-class classification\n",
    "# param_sh['objective'] = 'binary:hinge'\n",
    "# scale weight of positive examples\n",
    "# param_sh['eta'] = 0.1\n",
    "# param_sh['max_depth'] = 6\n",
    "# param_sh['nthread'] = 4\n",
    "# param_sh['gpu_id'] = 0\n",
    "# param_sh['tree_method'] = 'gpu_hist'\n",
    "# param_sh['min_child_weight'] = 7\n",
    "\n",
    "\n",
    "watchlist = [(xg_train_sh, 'train'), (xg_test_sh, 'test')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-logloss:18.45124\ttrain-auc:0.50000\ttrain-error:0.50083\ttest-logloss:18.29848\ttest-auc:0.50000\ttest-error:0.49668\n",
      "[1]\ttrain-logloss:18.45124\ttrain-auc:0.50000\ttrain-error:0.50083\ttest-logloss:18.29848\ttest-auc:0.50000\ttest-error:0.49668\n",
      "[2]\ttrain-logloss:18.45124\ttrain-auc:0.50000\ttrain-error:0.50083\ttest-logloss:18.29848\ttest-auc:0.50000\ttest-error:0.49668\n",
      "[3]\ttrain-logloss:18.45124\ttrain-auc:0.50000\ttrain-error:0.50083\ttest-logloss:18.29848\ttest-auc:0.50000\ttest-error:0.49668\n",
      "[4]\ttrain-logloss:18.45124\ttrain-auc:0.50000\ttrain-error:0.50083\ttest-logloss:18.29848\ttest-auc:0.50000\ttest-error:0.49668\n",
      "[5]\ttrain-logloss:18.45124\ttrain-auc:0.50000\ttrain-error:0.50083\ttest-logloss:18.29848\ttest-auc:0.50000\ttest-error:0.49668\n",
      "[6]\ttrain-logloss:18.21967\ttrain-auc:0.50627\ttrain-error:0.49454\ttest-logloss:18.09266\ttest-auc:0.50563\ttest-error:0.49110\n",
      "[7]\ttrain-logloss:17.52980\ttrain-auc:0.52496\ttrain-error:0.47582\ttest-logloss:17.55882\ttest-auc:0.52026\ttest-error:0.47661\n",
      "[8]\ttrain-logloss:16.68394\ttrain-auc:0.54786\ttrain-error:0.45286\ttest-logloss:16.66480\ttest-auc:0.54477\ttest-error:0.45234\n",
      "[9]\ttrain-logloss:16.08091\ttrain-auc:0.56419\ttrain-error:0.43649\ttest-logloss:16.14382\ttest-auc:0.55909\ttest-error:0.43820\n",
      "[10]\ttrain-logloss:15.72230\ttrain-auc:0.57389\ttrain-error:0.42676\ttest-logloss:15.73219\ttest-auc:0.57038\ttest-error:0.42702\n",
      "[11]\ttrain-logloss:15.54381\ttrain-auc:0.57871\ttrain-error:0.42191\ttest-logloss:15.54566\ttest-auc:0.57551\ttest-error:0.42196\n",
      "[12]\ttrain-logloss:15.34119\ttrain-auc:0.58420\ttrain-error:0.41641\ttest-logloss:15.34628\ttest-auc:0.58099\ttest-error:0.41655\n",
      "[13]\ttrain-logloss:15.19324\ttrain-auc:0.58820\ttrain-error:0.41240\ttest-logloss:15.25623\ttest-auc:0.58350\ttest-error:0.41411\n",
      "[14]\ttrain-logloss:15.05334\ttrain-auc:0.59198\ttrain-error:0.40860\ttest-logloss:15.16619\ttest-auc:0.58599\ttest-error:0.41166\n",
      "[15]\ttrain-logloss:14.97293\ttrain-auc:0.59415\ttrain-error:0.40642\ttest-logloss:15.04398\ttest-auc:0.58937\ttest-error:0.40835\n",
      "[16]\ttrain-logloss:14.90861\ttrain-auc:0.59589\ttrain-error:0.40467\ttest-logloss:15.00539\ttest-auc:0.59045\ttest-error:0.40730\n",
      "[17]\ttrain-logloss:14.88449\ttrain-auc:0.59653\ttrain-error:0.40402\ttest-logloss:14.96037\ttest-auc:0.59169\ttest-error:0.40608\n",
      "[18]\ttrain-logloss:14.78800\ttrain-auc:0.59914\ttrain-error:0.40140\ttest-logloss:14.90249\ttest-auc:0.59331\ttest-error:0.40450\n",
      "[19]\ttrain-logloss:14.70921\ttrain-auc:0.60127\ttrain-error:0.39926\ttest-logloss:14.84460\ttest-auc:0.59492\ttest-error:0.40293\n",
      "[20]\ttrain-logloss:14.61433\ttrain-auc:0.60384\ttrain-error:0.39668\ttest-logloss:14.78028\ttest-auc:0.59669\ttest-error:0.40119\n",
      "[21]\ttrain-logloss:14.52589\ttrain-auc:0.60623\ttrain-error:0.39428\ttest-logloss:14.76099\ttest-auc:0.59726\ttest-error:0.40066\n",
      "[22]\ttrain-logloss:14.37312\ttrain-auc:0.61036\ttrain-error:0.39014\ttest-logloss:14.64521\ttest-auc:0.60046\ttest-error:0.39752\n",
      "[23]\ttrain-logloss:14.29914\ttrain-auc:0.61236\ttrain-error:0.38813\ttest-logloss:14.59376\ttest-auc:0.60191\ttest-error:0.39612\n",
      "[24]\ttrain-logloss:14.22356\ttrain-auc:0.61440\ttrain-error:0.38608\ttest-logloss:14.52944\ttest-auc:0.60369\ttest-error:0.39438\n",
      "[25]\ttrain-logloss:14.22195\ttrain-auc:0.61444\ttrain-error:0.38603\ttest-logloss:14.49085\ttest-auc:0.60477\ttest-error:0.39333\n",
      "[26]\ttrain-logloss:14.05311\ttrain-auc:0.61901\ttrain-error:0.38145\ttest-logloss:14.42653\ttest-auc:0.60655\ttest-error:0.39159\n",
      "[27]\ttrain-logloss:13.95180\ttrain-auc:0.62175\ttrain-error:0.37870\ttest-logloss:14.43940\ttest-auc:0.60623\ttest-error:0.39193\n",
      "[28]\ttrain-logloss:13.86335\ttrain-auc:0.62414\ttrain-error:0.37630\ttest-logloss:14.39437\ttest-auc:0.60749\ttest-error:0.39071\n",
      "[29]\ttrain-logloss:13.80546\ttrain-auc:0.62570\ttrain-error:0.37473\ttest-logloss:14.32362\ttest-auc:0.60945\ttest-error:0.38879\n",
      "[30]\ttrain-logloss:13.72988\ttrain-auc:0.62775\ttrain-error:0.37268\ttest-logloss:14.22071\ttest-auc:0.61228\ttest-error:0.38600\n",
      "[31]\ttrain-logloss:13.70415\ttrain-auc:0.62844\ttrain-error:0.37198\ttest-logloss:14.18856\ttest-auc:0.61320\ttest-error:0.38513\n",
      "[32]\ttrain-logloss:13.63661\ttrain-auc:0.63026\ttrain-error:0.37014\ttest-logloss:14.14353\ttest-auc:0.61447\ttest-error:0.38390\n",
      "[33]\ttrain-logloss:13.57229\ttrain-auc:0.63200\ttrain-error:0.36840\ttest-logloss:14.05349\ttest-auc:0.61695\ttest-error:0.38146\n",
      "[34]\ttrain-logloss:13.45811\ttrain-auc:0.63509\ttrain-error:0.36530\ttest-logloss:13.97631\ttest-auc:0.61907\ttest-error:0.37937\n",
      "[35]\ttrain-logloss:13.43560\ttrain-auc:0.63570\ttrain-error:0.36469\ttest-logloss:13.96344\ttest-auc:0.61945\ttest-error:0.37901\n",
      "[36]\ttrain-logloss:13.40183\ttrain-auc:0.63661\ttrain-error:0.36377\ttest-logloss:13.96987\ttest-auc:0.61929\ttest-error:0.37919\n",
      "[37]\ttrain-logloss:13.35037\ttrain-auc:0.63800\ttrain-error:0.36238\ttest-logloss:13.87983\ttest-auc:0.62176\ttest-error:0.37675\n",
      "[38]\ttrain-logloss:13.26353\ttrain-auc:0.64035\ttrain-error:0.36002\ttest-logloss:13.81551\ttest-auc:0.62354\ttest-error:0.37500\n",
      "[39]\ttrain-logloss:13.21690\ttrain-auc:0.64161\ttrain-error:0.35875\ttest-logloss:13.84124\ttest-auc:0.62285\ttest-error:0.37570\n",
      "[40]\ttrain-logloss:13.09469\ttrain-auc:0.64492\ttrain-error:0.35543\ttest-logloss:13.81551\ttest-auc:0.62359\ttest-error:0.37500\n",
      "[41]\ttrain-logloss:13.06091\ttrain-auc:0.64583\ttrain-error:0.35452\ttest-logloss:13.81551\ttest-auc:0.62361\ttest-error:0.37500\n",
      "[42]\ttrain-logloss:13.02554\ttrain-auc:0.64679\ttrain-error:0.35356\ttest-logloss:13.79622\ttest-auc:0.62416\ttest-error:0.37448\n",
      "[43]\ttrain-logloss:12.99338\ttrain-auc:0.64765\ttrain-error:0.35268\ttest-logloss:13.73190\ttest-auc:0.62592\ttest-error:0.37273\n",
      "[44]\ttrain-logloss:12.92262\ttrain-auc:0.64956\ttrain-error:0.35076\ttest-logloss:13.75119\ttest-auc:0.62543\ttest-error:0.37325\n",
      "[45]\ttrain-logloss:12.87438\ttrain-auc:0.65087\ttrain-error:0.34945\ttest-logloss:13.71260\ttest-auc:0.62649\ttest-error:0.37221\n",
      "[46]\ttrain-logloss:12.79719\ttrain-auc:0.65295\ttrain-error:0.34736\ttest-logloss:13.62899\ttest-auc:0.62879\ttest-error:0.36994\n",
      "[47]\ttrain-logloss:12.76663\ttrain-auc:0.65378\ttrain-error:0.34653\ttest-logloss:13.53894\ttest-auc:0.63126\ttest-error:0.36749\n",
      "[48]\ttrain-logloss:12.72804\ttrain-auc:0.65482\ttrain-error:0.34548\ttest-logloss:13.57110\ttest-auc:0.63040\ttest-error:0.36837\n",
      "[49]\ttrain-logloss:12.69588\ttrain-auc:0.65569\ttrain-error:0.34461\ttest-logloss:13.60969\ttest-auc:0.62938\ttest-error:0.36941\n",
      "[50]\ttrain-logloss:12.66532\ttrain-auc:0.65651\ttrain-error:0.34378\ttest-logloss:13.57110\ttest-auc:0.63045\ttest-error:0.36837\n",
      "[51]\ttrain-logloss:12.60743\ttrain-auc:0.65808\ttrain-error:0.34221\ttest-logloss:13.53251\ttest-auc:0.63152\ttest-error:0.36732\n",
      "[52]\ttrain-logloss:12.56884\ttrain-auc:0.65912\ttrain-error:0.34116\ttest-logloss:13.51322\ttest-auc:0.63206\ttest-error:0.36679\n",
      "[53]\ttrain-logloss:12.51577\ttrain-auc:0.66056\ttrain-error:0.33972\ttest-logloss:13.50678\ttest-auc:0.63226\ttest-error:0.36662\n",
      "[54]\ttrain-logloss:12.49969\ttrain-auc:0.66099\ttrain-error:0.33928\ttest-logloss:13.44890\ttest-auc:0.63385\ttest-error:0.36505\n",
      "[55]\ttrain-logloss:12.46110\ttrain-auc:0.66203\ttrain-error:0.33824\ttest-logloss:13.37815\ttest-auc:0.63579\ttest-error:0.36313\n",
      "[56]\ttrain-logloss:12.42893\ttrain-auc:0.66290\ttrain-error:0.33736\ttest-logloss:13.35885\ttest-auc:0.63633\ttest-error:0.36261\n",
      "[57]\ttrain-logloss:12.40803\ttrain-auc:0.66347\ttrain-error:0.33680\ttest-logloss:13.32669\ttest-auc:0.63721\ttest-error:0.36173\n",
      "[58]\ttrain-logloss:12.38712\ttrain-auc:0.66403\ttrain-error:0.33623\ttest-logloss:13.28810\ttest-auc:0.63827\ttest-error:0.36068\n",
      "[59]\ttrain-logloss:12.36300\ttrain-auc:0.66468\ttrain-error:0.33557\ttest-logloss:13.27524\ttest-auc:0.63863\ttest-error:0.36034\n",
      "[60]\ttrain-logloss:12.32602\ttrain-auc:0.66568\ttrain-error:0.33457\ttest-logloss:13.30097\ttest-auc:0.63795\ttest-error:0.36103\n",
      "[61]\ttrain-logloss:12.30994\ttrain-auc:0.66612\ttrain-error:0.33413\ttest-logloss:13.32026\ttest-auc:0.63744\ttest-error:0.36156\n",
      "[62]\ttrain-logloss:12.29064\ttrain-auc:0.66664\ttrain-error:0.33361\ttest-logloss:13.35242\ttest-auc:0.63657\ttest-error:0.36243\n",
      "[63]\ttrain-logloss:12.27295\ttrain-auc:0.66712\ttrain-error:0.33313\ttest-logloss:13.30740\ttest-auc:0.63780\ttest-error:0.36121\n",
      "[64]\ttrain-logloss:12.24722\ttrain-auc:0.66781\ttrain-error:0.33243\ttest-logloss:13.30097\ttest-auc:0.63800\ttest-error:0.36103\n",
      "[65]\ttrain-logloss:12.23275\ttrain-auc:0.66820\ttrain-error:0.33204\ttest-logloss:13.34599\ttest-auc:0.63679\ttest-error:0.36226\n",
      "[66]\ttrain-logloss:12.22149\ttrain-auc:0.66850\ttrain-error:0.33173\ttest-logloss:13.30097\ttest-auc:0.63802\ttest-error:0.36103\n",
      "[67]\ttrain-logloss:12.21345\ttrain-auc:0.66872\ttrain-error:0.33152\ttest-logloss:13.28810\ttest-auc:0.63838\ttest-error:0.36068\n",
      "[68]\ttrain-logloss:12.20702\ttrain-auc:0.66889\ttrain-error:0.33134\ttest-logloss:13.28167\ttest-auc:0.63856\ttest-error:0.36051\n",
      "[69]\ttrain-logloss:12.16521\ttrain-auc:0.67002\ttrain-error:0.33021\ttest-logloss:13.24951\ttest-auc:0.63945\ttest-error:0.35964\n",
      "[70]\ttrain-logloss:12.14109\ttrain-auc:0.67068\ttrain-error:0.32955\ttest-logloss:13.21735\ttest-auc:0.64033\ttest-error:0.35876\n",
      "[71]\ttrain-logloss:12.14591\ttrain-auc:0.67054\ttrain-error:0.32968\ttest-logloss:13.19806\ttest-auc:0.64087\ttest-error:0.35824\n",
      "[72]\ttrain-logloss:12.12661\ttrain-auc:0.67107\ttrain-error:0.32916\ttest-logloss:13.17233\ttest-auc:0.64157\ttest-error:0.35754\n",
      "[73]\ttrain-logloss:12.10249\ttrain-auc:0.67172\ttrain-error:0.32850\ttest-logloss:13.17876\ttest-auc:0.64141\ttest-error:0.35772\n",
      "[74]\ttrain-logloss:12.10088\ttrain-auc:0.67176\ttrain-error:0.32846\ttest-logloss:13.15947\ttest-auc:0.64194\ttest-error:0.35719\n",
      "[75]\ttrain-logloss:12.09767\ttrain-auc:0.67185\ttrain-error:0.32837\ttest-logloss:13.15303\ttest-auc:0.64212\ttest-error:0.35702\n",
      "[76]\ttrain-logloss:12.09928\ttrain-auc:0.67180\ttrain-error:0.32842\ttest-logloss:13.15947\ttest-auc:0.64196\ttest-error:0.35719\n",
      "[77]\ttrain-logloss:12.11214\ttrain-auc:0.67145\ttrain-error:0.32876\ttest-logloss:13.15947\ttest-auc:0.64196\ttest-error:0.35719\n",
      "[78]\ttrain-logloss:12.08319\ttrain-auc:0.67223\ttrain-error:0.32798\ttest-logloss:13.14017\ttest-auc:0.64250\ttest-error:0.35667\n",
      "[79]\ttrain-logloss:12.07837\ttrain-auc:0.67236\ttrain-error:0.32785\ttest-logloss:13.13374\ttest-auc:0.64267\ttest-error:0.35649\n",
      "[80]\ttrain-logloss:12.06390\ttrain-auc:0.67275\ttrain-error:0.32745\ttest-logloss:13.17233\ttest-auc:0.64164\ttest-error:0.35754\n",
      "[81]\ttrain-logloss:12.03495\ttrain-auc:0.67354\ttrain-error:0.32667\ttest-logloss:13.15947\ttest-auc:0.64199\ttest-error:0.35719\n",
      "[82]\ttrain-logloss:12.02530\ttrain-auc:0.67380\ttrain-error:0.32641\ttest-logloss:13.14660\ttest-auc:0.64236\ttest-error:0.35684\n",
      "[83]\ttrain-logloss:12.01244\ttrain-auc:0.67414\ttrain-error:0.32606\ttest-logloss:13.12087\ttest-auc:0.64306\ttest-error:0.35614\n",
      "[84]\ttrain-logloss:12.00440\ttrain-auc:0.67436\ttrain-error:0.32584\ttest-logloss:13.09515\ttest-auc:0.64377\ttest-error:0.35545\n",
      "[85]\ttrain-logloss:11.97063\ttrain-auc:0.67528\ttrain-error:0.32492\ttest-logloss:13.10158\ttest-auc:0.64360\ttest-error:0.35562\n",
      "[86]\ttrain-logloss:11.96741\ttrain-auc:0.67536\ttrain-error:0.32484\ttest-logloss:13.12731\ttest-auc:0.64291\ttest-error:0.35632\n",
      "[87]\ttrain-logloss:11.95455\ttrain-auc:0.67571\ttrain-error:0.32449\ttest-logloss:13.10158\ttest-auc:0.64361\ttest-error:0.35562\n",
      "[88]\ttrain-logloss:11.94651\ttrain-auc:0.67593\ttrain-error:0.32427\ttest-logloss:13.12087\ttest-auc:0.64309\ttest-error:0.35614\n",
      "[89]\ttrain-logloss:11.93203\ttrain-auc:0.67632\ttrain-error:0.32388\ttest-logloss:13.14017\ttest-auc:0.64257\ttest-error:0.35667\n",
      "[90]\ttrain-logloss:11.91113\ttrain-auc:0.67688\ttrain-error:0.32331\ttest-logloss:13.09515\ttest-auc:0.64381\ttest-error:0.35545\n",
      "[91]\ttrain-logloss:11.90470\ttrain-auc:0.67706\ttrain-error:0.32313\ttest-logloss:13.10801\ttest-auc:0.64346\ttest-error:0.35580\n",
      "[92]\ttrain-logloss:11.87253\ttrain-auc:0.67793\ttrain-error:0.32226\ttest-logloss:13.10158\ttest-auc:0.64365\ttest-error:0.35562\n",
      "[93]\ttrain-logloss:11.85646\ttrain-auc:0.67836\ttrain-error:0.32183\ttest-logloss:13.07585\ttest-auc:0.64436\ttest-error:0.35492\n",
      "[94]\ttrain-logloss:11.85002\ttrain-auc:0.67854\ttrain-error:0.32165\ttest-logloss:13.08229\ttest-auc:0.64418\ttest-error:0.35510\n",
      "[95]\ttrain-logloss:11.82751\ttrain-auc:0.67915\ttrain-error:0.32104\ttest-logloss:13.06299\ttest-auc:0.64472\ttest-error:0.35457\n",
      "[96]\ttrain-logloss:11.82751\ttrain-auc:0.67915\ttrain-error:0.32104\ttest-logloss:13.05656\ttest-auc:0.64489\ttest-error:0.35440\n",
      "[97]\ttrain-logloss:11.80500\ttrain-auc:0.67975\ttrain-error:0.32043\ttest-logloss:13.06942\ttest-auc:0.64455\ttest-error:0.35475\n",
      "[98]\ttrain-logloss:11.80821\ttrain-auc:0.67967\ttrain-error:0.32051\ttest-logloss:13.03083\ttest-auc:0.64560\ttest-error:0.35370\n",
      "[99]\ttrain-logloss:11.78891\ttrain-auc:0.68019\ttrain-error:0.31999\ttest-logloss:12.99224\ttest-auc:0.64666\ttest-error:0.35265\n",
      "[100]\ttrain-logloss:11.79374\ttrain-auc:0.68005\ttrain-error:0.32012\ttest-logloss:12.96651\ttest-auc:0.64736\ttest-error:0.35196\n",
      "[101]\ttrain-logloss:11.77766\ttrain-auc:0.68049\ttrain-error:0.31969\ttest-logloss:12.96651\ttest-auc:0.64737\ttest-error:0.35196\n",
      "[102]\ttrain-logloss:11.75836\ttrain-auc:0.68101\ttrain-error:0.31916\ttest-logloss:12.98581\ttest-auc:0.64685\ttest-error:0.35248\n",
      "[103]\ttrain-logloss:11.74871\ttrain-auc:0.68127\ttrain-error:0.31890\ttest-logloss:12.99867\ttest-auc:0.64651\ttest-error:0.35283\n",
      "[104]\ttrain-logloss:11.73746\ttrain-auc:0.68158\ttrain-error:0.31859\ttest-logloss:12.99224\ttest-auc:0.64668\ttest-error:0.35265\n",
      "[105]\ttrain-logloss:11.73102\ttrain-auc:0.68175\ttrain-error:0.31842\ttest-logloss:12.97938\ttest-auc:0.64704\ttest-error:0.35230\n",
      "[106]\ttrain-logloss:11.72138\ttrain-auc:0.68201\ttrain-error:0.31816\ttest-logloss:12.96008\ttest-auc:0.64756\ttest-error:0.35178\n",
      "[107]\ttrain-logloss:11.69725\ttrain-auc:0.68267\ttrain-error:0.31750\ttest-logloss:12.95365\ttest-auc:0.64774\ttest-error:0.35161\n",
      "[108]\ttrain-logloss:11.68760\ttrain-auc:0.68293\ttrain-error:0.31724\ttest-logloss:12.95365\ttest-auc:0.64775\ttest-error:0.35161\n",
      "[109]\ttrain-logloss:11.67474\ttrain-auc:0.68327\ttrain-error:0.31689\ttest-logloss:12.97294\ttest-auc:0.64722\ttest-error:0.35213\n",
      "[110]\ttrain-logloss:11.66831\ttrain-auc:0.68345\ttrain-error:0.31672\ttest-logloss:12.94722\ttest-auc:0.64794\ttest-error:0.35143\n",
      "[111]\ttrain-logloss:11.64097\ttrain-auc:0.68419\ttrain-error:0.31598\ttest-logloss:12.95365\ttest-auc:0.64776\ttest-error:0.35161\n",
      "[112]\ttrain-logloss:11.63615\ttrain-auc:0.68432\ttrain-error:0.31584\ttest-logloss:12.92792\ttest-auc:0.64846\ttest-error:0.35091\n",
      "[113]\ttrain-logloss:11.62007\ttrain-auc:0.68476\ttrain-error:0.31541\ttest-logloss:12.91506\ttest-auc:0.64881\ttest-error:0.35056\n",
      "[114]\ttrain-logloss:11.62007\ttrain-auc:0.68476\ttrain-error:0.31541\ttest-logloss:12.89576\ttest-auc:0.64934\ttest-error:0.35003\n",
      "[115]\ttrain-logloss:11.59755\ttrain-auc:0.68537\ttrain-error:0.31480\ttest-logloss:12.88933\ttest-auc:0.64952\ttest-error:0.34986\n",
      "[116]\ttrain-logloss:11.58629\ttrain-auc:0.68567\ttrain-error:0.31449\ttest-logloss:12.89576\ttest-auc:0.64935\ttest-error:0.35003\n",
      "[117]\ttrain-logloss:11.56860\ttrain-auc:0.68615\ttrain-error:0.31401\ttest-logloss:12.91506\ttest-auc:0.64883\ttest-error:0.35056\n",
      "[118]\ttrain-logloss:11.56860\ttrain-auc:0.68615\ttrain-error:0.31401\ttest-logloss:12.89576\ttest-auc:0.64936\ttest-error:0.35003\n",
      "[119]\ttrain-logloss:11.54770\ttrain-auc:0.68672\ttrain-error:0.31344\ttest-logloss:12.88290\ttest-auc:0.64971\ttest-error:0.34969\n",
      "[120]\ttrain-logloss:11.55252\ttrain-auc:0.68659\ttrain-error:0.31357\ttest-logloss:12.89576\ttest-auc:0.64937\ttest-error:0.35003\n",
      "[121]\ttrain-logloss:11.54609\ttrain-auc:0.68676\ttrain-error:0.31340\ttest-logloss:12.88933\ttest-auc:0.64954\ttest-error:0.34986\n",
      "[122]\ttrain-logloss:11.54288\ttrain-auc:0.68685\ttrain-error:0.31331\ttest-logloss:12.91506\ttest-auc:0.64885\ttest-error:0.35056\n",
      "[123]\ttrain-logloss:11.54127\ttrain-auc:0.68689\ttrain-error:0.31327\ttest-logloss:12.92149\ttest-auc:0.64867\ttest-error:0.35073\n",
      "[124]\ttrain-logloss:11.52519\ttrain-auc:0.68733\ttrain-error:0.31283\ttest-logloss:12.92792\ttest-auc:0.64850\ttest-error:0.35091\n",
      "[125]\ttrain-logloss:11.52519\ttrain-auc:0.68733\ttrain-error:0.31283\ttest-logloss:12.93435\ttest-auc:0.64832\ttest-error:0.35108\n",
      "[126]\ttrain-logloss:11.50750\ttrain-auc:0.68780\ttrain-error:0.31235\ttest-logloss:12.90220\ttest-auc:0.64920\ttest-error:0.35021\n",
      "[127]\ttrain-logloss:11.50750\ttrain-auc:0.68780\ttrain-error:0.31235\ttest-logloss:12.88933\ttest-auc:0.64955\ttest-error:0.34986\n",
      "[128]\ttrain-logloss:11.49946\ttrain-auc:0.68802\ttrain-error:0.31213\ttest-logloss:12.92149\ttest-auc:0.64868\ttest-error:0.35073\n",
      "[129]\ttrain-logloss:11.48498\ttrain-auc:0.68841\ttrain-error:0.31174\ttest-logloss:12.92149\ttest-auc:0.64868\ttest-error:0.35073\n",
      "[130]\ttrain-logloss:11.46730\ttrain-auc:0.68889\ttrain-error:0.31126\ttest-logloss:12.85074\ttest-auc:0.65060\ttest-error:0.34881\n",
      "[131]\ttrain-logloss:11.47051\ttrain-auc:0.68881\ttrain-error:0.31135\ttest-logloss:12.87003\ttest-auc:0.65009\ttest-error:0.34934\n",
      "[132]\ttrain-logloss:11.44800\ttrain-auc:0.68942\ttrain-error:0.31074\ttest-logloss:12.86360\ttest-auc:0.65026\ttest-error:0.34916\n",
      "[133]\ttrain-logloss:11.44157\ttrain-auc:0.68959\ttrain-error:0.31056\ttest-logloss:12.84431\ttest-auc:0.65078\ttest-error:0.34864\n",
      "[134]\ttrain-logloss:11.43192\ttrain-auc:0.68985\ttrain-error:0.31030\ttest-logloss:12.81215\ttest-auc:0.65166\ttest-error:0.34776\n",
      "[135]\ttrain-logloss:11.42388\ttrain-auc:0.69007\ttrain-error:0.31008\ttest-logloss:12.82501\ttest-auc:0.65131\ttest-error:0.34812\n",
      "[136]\ttrain-logloss:11.39976\ttrain-auc:0.69073\ttrain-error:0.30943\ttest-logloss:12.81215\ttest-auc:0.65167\ttest-error:0.34776\n",
      "[137]\ttrain-logloss:11.38689\ttrain-auc:0.69107\ttrain-error:0.30908\ttest-logloss:12.82501\ttest-auc:0.65132\ttest-error:0.34812\n",
      "[138]\ttrain-logloss:11.37081\ttrain-auc:0.69151\ttrain-error:0.30864\ttest-logloss:12.85717\ttest-auc:0.65045\ttest-error:0.34899\n",
      "[139]\ttrain-logloss:11.35955\ttrain-auc:0.69181\ttrain-error:0.30834\ttest-logloss:12.83788\ttest-auc:0.65097\ttest-error:0.34846\n",
      "[140]\ttrain-logloss:11.34347\ttrain-auc:0.69225\ttrain-error:0.30790\ttest-logloss:12.80572\ttest-auc:0.65186\ttest-error:0.34759\n",
      "[141]\ttrain-logloss:11.32739\ttrain-auc:0.69269\ttrain-error:0.30746\ttest-logloss:12.79929\ttest-auc:0.65203\ttest-error:0.34742\n",
      "[142]\ttrain-logloss:11.31292\ttrain-auc:0.69308\ttrain-error:0.30707\ttest-logloss:12.81858\ttest-auc:0.65151\ttest-error:0.34794\n",
      "[143]\ttrain-logloss:11.31292\ttrain-auc:0.69308\ttrain-error:0.30707\ttest-logloss:12.80572\ttest-auc:0.65186\ttest-error:0.34759\n",
      "[144]\ttrain-logloss:11.30809\ttrain-auc:0.69321\ttrain-error:0.30694\ttest-logloss:12.79285\ttest-auc:0.65222\ttest-error:0.34724\n",
      "[145]\ttrain-logloss:11.30327\ttrain-auc:0.69334\ttrain-error:0.30681\ttest-logloss:12.77356\ttest-auc:0.65274\ttest-error:0.34672\n",
      "[146]\ttrain-logloss:11.31613\ttrain-auc:0.69299\ttrain-error:0.30716\ttest-logloss:12.78642\ttest-auc:0.65239\ttest-error:0.34707\n",
      "[147]\ttrain-logloss:11.32578\ttrain-auc:0.69273\ttrain-error:0.30742\ttest-logloss:12.78642\ttest-auc:0.65239\ttest-error:0.34707\n",
      "[148]\ttrain-logloss:11.30649\ttrain-auc:0.69325\ttrain-error:0.30690\ttest-logloss:12.80572\ttest-auc:0.65187\ttest-error:0.34759\n",
      "[149]\ttrain-logloss:11.29041\ttrain-auc:0.69369\ttrain-error:0.30646\ttest-logloss:12.76070\ttest-auc:0.65309\ttest-error:0.34637\n",
      "[150]\ttrain-logloss:11.26950\ttrain-auc:0.69426\ttrain-error:0.30589\ttest-logloss:12.73497\ttest-auc:0.65379\ttest-error:0.34567\n",
      "[151]\ttrain-logloss:11.23412\ttrain-auc:0.69521\ttrain-error:0.30493\ttest-logloss:12.74140\ttest-auc:0.65361\ttest-error:0.34585\n",
      "[152]\ttrain-logloss:11.22608\ttrain-auc:0.69543\ttrain-error:0.30471\ttest-logloss:12.72854\ttest-auc:0.65396\ttest-error:0.34550\n",
      "[153]\ttrain-logloss:11.21000\ttrain-auc:0.69587\ttrain-error:0.30428\ttest-logloss:12.71567\ttest-auc:0.65431\ttest-error:0.34515\n",
      "[154]\ttrain-logloss:11.21643\ttrain-auc:0.69569\ttrain-error:0.30445\ttest-logloss:12.71567\ttest-auc:0.65432\ttest-error:0.34515\n",
      "[155]\ttrain-logloss:11.21483\ttrain-auc:0.69574\ttrain-error:0.30441\ttest-logloss:12.74140\ttest-auc:0.65361\ttest-error:0.34585\n",
      "[156]\ttrain-logloss:11.18266\ttrain-auc:0.69661\ttrain-error:0.30354\ttest-logloss:12.75426\ttest-auc:0.65327\ttest-error:0.34619\n",
      "[157]\ttrain-logloss:11.16176\ttrain-auc:0.69718\ttrain-error:0.30297\ttest-logloss:12.72210\ttest-auc:0.65414\ttest-error:0.34532\n",
      "[158]\ttrain-logloss:11.13603\ttrain-auc:0.69787\ttrain-error:0.30227\ttest-logloss:12.71567\ttest-auc:0.65432\ttest-error:0.34515\n",
      "[159]\ttrain-logloss:11.13281\ttrain-auc:0.69796\ttrain-error:0.30218\ttest-logloss:12.72210\ttest-auc:0.65414\ttest-error:0.34532\n",
      "[160]\ttrain-logloss:11.12638\ttrain-auc:0.69813\ttrain-error:0.30201\ttest-logloss:12.70281\ttest-auc:0.65467\ttest-error:0.34480\n",
      "[161]\ttrain-logloss:11.10387\ttrain-auc:0.69874\ttrain-error:0.30140\ttest-logloss:12.72210\ttest-auc:0.65415\ttest-error:0.34532\n",
      "[162]\ttrain-logloss:11.09744\ttrain-auc:0.69892\ttrain-error:0.30122\ttest-logloss:12.72210\ttest-auc:0.65415\ttest-error:0.34532\n",
      "[163]\ttrain-logloss:11.09583\ttrain-auc:0.69896\ttrain-error:0.30118\ttest-logloss:12.72210\ttest-auc:0.65415\ttest-error:0.34532\n",
      "[164]\ttrain-logloss:11.08779\ttrain-auc:0.69918\ttrain-error:0.30096\ttest-logloss:12.70924\ttest-auc:0.65451\ttest-error:0.34497\n",
      "[165]\ttrain-logloss:11.08939\ttrain-auc:0.69913\ttrain-error:0.30100\ttest-logloss:12.68994\ttest-auc:0.65503\ttest-error:0.34445\n",
      "[166]\ttrain-logloss:11.08618\ttrain-auc:0.69922\ttrain-error:0.30092\ttest-logloss:12.67708\ttest-auc:0.65538\ttest-error:0.34410\n",
      "[167]\ttrain-logloss:11.06527\ttrain-auc:0.69979\ttrain-error:0.30035\ttest-logloss:12.66422\ttest-auc:0.65573\ttest-error:0.34375\n",
      "[168]\ttrain-logloss:11.06527\ttrain-auc:0.69979\ttrain-error:0.30035\ttest-logloss:12.66422\ttest-auc:0.65574\ttest-error:0.34375\n",
      "[169]\ttrain-logloss:11.04919\ttrain-auc:0.70022\ttrain-error:0.29991\ttest-logloss:12.67708\ttest-auc:0.65540\ttest-error:0.34410\n",
      "[170]\ttrain-logloss:11.02829\ttrain-auc:0.70079\ttrain-error:0.29935\ttest-logloss:12.64492\ttest-auc:0.65627\ttest-error:0.34323\n",
      "[171]\ttrain-logloss:11.01864\ttrain-auc:0.70105\ttrain-error:0.29908\ttest-logloss:12.64492\ttest-auc:0.65627\ttest-error:0.34323\n",
      "[172]\ttrain-logloss:11.02025\ttrain-auc:0.70101\ttrain-error:0.29913\ttest-logloss:12.62563\ttest-auc:0.65680\ttest-error:0.34270\n",
      "[173]\ttrain-logloss:10.99934\ttrain-auc:0.70157\ttrain-error:0.29856\ttest-logloss:12.67708\ttest-auc:0.65540\ttest-error:0.34410\n",
      "[174]\ttrain-logloss:10.98004\ttrain-auc:0.70210\ttrain-error:0.29804\ttest-logloss:12.68351\ttest-auc:0.65523\ttest-error:0.34427\n",
      "[175]\ttrain-logloss:10.97683\ttrain-auc:0.70218\ttrain-error:0.29795\ttest-logloss:12.65779\ttest-auc:0.65593\ttest-error:0.34358\n",
      "[176]\ttrain-logloss:10.95753\ttrain-auc:0.70271\ttrain-error:0.29742\ttest-logloss:12.64492\ttest-auc:0.65629\ttest-error:0.34323\n",
      "[177]\ttrain-logloss:10.94467\ttrain-auc:0.70306\ttrain-error:0.29708\ttest-logloss:12.67065\ttest-auc:0.65560\ttest-error:0.34392\n",
      "[178]\ttrain-logloss:10.94467\ttrain-auc:0.70305\ttrain-error:0.29708\ttest-logloss:12.65136\ttest-auc:0.65612\ttest-error:0.34340\n",
      "[179]\ttrain-logloss:10.91090\ttrain-auc:0.70397\ttrain-error:0.29616\ttest-logloss:12.64492\ttest-auc:0.65630\ttest-error:0.34323\n",
      "[180]\ttrain-logloss:10.91090\ttrain-auc:0.70397\ttrain-error:0.29616\ttest-logloss:12.69638\ttest-auc:0.65491\ttest-error:0.34462\n",
      "[181]\ttrain-logloss:10.89321\ttrain-auc:0.70445\ttrain-error:0.29568\ttest-logloss:12.65136\ttest-auc:0.65614\ttest-error:0.34340\n",
      "[182]\ttrain-logloss:10.86587\ttrain-auc:0.70519\ttrain-error:0.29494\ttest-logloss:12.61919\ttest-auc:0.65701\ttest-error:0.34253\n",
      "[183]\ttrain-logloss:10.85944\ttrain-auc:0.70537\ttrain-error:0.29476\ttest-logloss:12.61276\ttest-auc:0.65719\ttest-error:0.34235\n",
      "[184]\ttrain-logloss:10.85140\ttrain-auc:0.70559\ttrain-error:0.29454\ttest-logloss:12.64492\ttest-auc:0.65631\ttest-error:0.34323\n",
      "[185]\ttrain-logloss:10.83371\ttrain-auc:0.70607\ttrain-error:0.29406\ttest-logloss:12.63206\ttest-auc:0.65667\ttest-error:0.34288\n",
      "[186]\ttrain-logloss:10.82245\ttrain-auc:0.70637\ttrain-error:0.29376\ttest-logloss:12.60633\ttest-auc:0.65737\ttest-error:0.34218\n",
      "[187]\ttrain-logloss:10.81119\ttrain-auc:0.70668\ttrain-error:0.29345\ttest-logloss:12.61276\ttest-auc:0.65719\ttest-error:0.34235\n",
      "[188]\ttrain-logloss:10.81119\ttrain-auc:0.70668\ttrain-error:0.29345\ttest-logloss:12.61919\ttest-auc:0.65702\ttest-error:0.34253\n",
      "[189]\ttrain-logloss:10.81119\ttrain-auc:0.70668\ttrain-error:0.29345\ttest-logloss:12.58704\ttest-auc:0.65790\ttest-error:0.34165\n",
      "[190]\ttrain-logloss:10.79994\ttrain-auc:0.70698\ttrain-error:0.29315\ttest-logloss:12.55488\ttest-auc:0.65877\ttest-error:0.34078\n",
      "[191]\ttrain-logloss:10.79029\ttrain-auc:0.70724\ttrain-error:0.29289\ttest-logloss:12.52915\ttest-auc:0.65947\ttest-error:0.34008\n",
      "[192]\ttrain-logloss:10.76617\ttrain-auc:0.70790\ttrain-error:0.29223\ttest-logloss:12.53558\ttest-auc:0.65929\ttest-error:0.34026\n",
      "[193]\ttrain-logloss:10.77582\ttrain-auc:0.70764\ttrain-error:0.29249\ttest-logloss:12.56131\ttest-auc:0.65860\ttest-error:0.34096\n",
      "[194]\ttrain-logloss:10.75652\ttrain-auc:0.70816\ttrain-error:0.29197\ttest-logloss:12.57417\ttest-auc:0.65825\ttest-error:0.34131\n",
      "[195]\ttrain-logloss:10.76617\ttrain-auc:0.70790\ttrain-error:0.29223\ttest-logloss:12.56774\ttest-auc:0.65843\ttest-error:0.34113\n",
      "[196]\ttrain-logloss:10.72436\ttrain-auc:0.70903\ttrain-error:0.29110\ttest-logloss:12.56774\ttest-auc:0.65843\ttest-error:0.34113\n",
      "[197]\ttrain-logloss:10.71953\ttrain-auc:0.70916\ttrain-error:0.29096\ttest-logloss:12.58061\ttest-auc:0.65809\ttest-error:0.34148\n",
      "[198]\ttrain-logloss:10.71632\ttrain-auc:0.70925\ttrain-error:0.29088\ttest-logloss:12.56774\ttest-auc:0.65844\ttest-error:0.34113\n",
      "[199]\ttrain-logloss:10.69702\ttrain-auc:0.70977\ttrain-error:0.29035\ttest-logloss:12.52272\ttest-auc:0.65965\ttest-error:0.33991\n",
      "[200]\ttrain-logloss:10.69220\ttrain-auc:0.70990\ttrain-error:0.29022\ttest-logloss:12.54201\ttest-auc:0.65913\ttest-error:0.34043\n",
      "[201]\ttrain-logloss:10.66968\ttrain-auc:0.71051\ttrain-error:0.28961\ttest-logloss:12.53558\ttest-auc:0.65931\ttest-error:0.34026\n",
      "[202]\ttrain-logloss:10.66647\ttrain-auc:0.71060\ttrain-error:0.28952\ttest-logloss:12.52915\ttest-auc:0.65949\ttest-error:0.34008\n",
      "[203]\ttrain-logloss:10.63430\ttrain-auc:0.71147\ttrain-error:0.28865\ttest-logloss:12.50985\ttest-auc:0.66002\ttest-error:0.33956\n",
      "[204]\ttrain-logloss:10.62466\ttrain-auc:0.71173\ttrain-error:0.28839\ttest-logloss:12.47126\ttest-auc:0.66107\ttest-error:0.33851\n",
      "[205]\ttrain-logloss:10.59089\ttrain-auc:0.71265\ttrain-error:0.28747\ttest-logloss:12.51629\ttest-auc:0.65984\ttest-error:0.33974\n",
      "[206]\ttrain-logloss:10.58928\ttrain-auc:0.71269\ttrain-error:0.28743\ttest-logloss:12.48413\ttest-auc:0.66070\ttest-error:0.33886\n",
      "[207]\ttrain-logloss:10.58446\ttrain-auc:0.71283\ttrain-error:0.28730\ttest-logloss:12.49056\ttest-auc:0.66054\ttest-error:0.33904\n",
      "[208]\ttrain-logloss:10.57320\ttrain-auc:0.71313\ttrain-error:0.28699\ttest-logloss:12.48413\ttest-auc:0.66072\ttest-error:0.33886\n",
      "[209]\ttrain-logloss:10.55712\ttrain-auc:0.71357\ttrain-error:0.28656\ttest-logloss:12.49699\ttest-auc:0.66038\ttest-error:0.33921\n",
      "[210]\ttrain-logloss:10.53782\ttrain-auc:0.71409\ttrain-error:0.28603\ttest-logloss:12.47770\ttest-auc:0.66090\ttest-error:0.33869\n",
      "[211]\ttrain-logloss:10.51852\ttrain-auc:0.71461\ttrain-error:0.28551\ttest-logloss:12.51629\ttest-auc:0.65987\ttest-error:0.33974\n",
      "[212]\ttrain-logloss:10.51531\ttrain-auc:0.71470\ttrain-error:0.28542\ttest-logloss:12.50985\ttest-auc:0.66004\ttest-error:0.33956\n",
      "[213]\ttrain-logloss:10.51209\ttrain-auc:0.71479\ttrain-error:0.28533\ttest-logloss:12.47770\ttest-auc:0.66091\ttest-error:0.33869\n",
      "[214]\ttrain-logloss:10.49601\ttrain-auc:0.71522\ttrain-error:0.28490\ttest-logloss:12.49699\ttest-auc:0.66039\ttest-error:0.33921\n",
      "[215]\ttrain-logloss:10.49601\ttrain-auc:0.71522\ttrain-error:0.28490\ttest-logloss:12.53558\ttest-auc:0.65935\ttest-error:0.34026\n",
      "[216]\ttrain-logloss:10.48315\ttrain-auc:0.71557\ttrain-error:0.28455\ttest-logloss:12.50985\ttest-auc:0.66004\ttest-error:0.33956\n",
      "[217]\ttrain-logloss:10.48797\ttrain-auc:0.71544\ttrain-error:0.28468\ttest-logloss:12.47770\ttest-auc:0.66091\ttest-error:0.33869\n",
      "[218]\ttrain-logloss:10.47028\ttrain-auc:0.71592\ttrain-error:0.28420\ttest-logloss:12.47770\ttest-auc:0.66092\ttest-error:0.33869\n",
      "[219]\ttrain-logloss:10.46867\ttrain-auc:0.71596\ttrain-error:0.28415\ttest-logloss:12.48413\ttest-auc:0.66074\ttest-error:0.33886\n",
      "[220]\ttrain-logloss:10.46063\ttrain-auc:0.71618\ttrain-error:0.28394\ttest-logloss:12.49699\ttest-auc:0.66040\ttest-error:0.33921\n",
      "[221]\ttrain-logloss:10.45581\ttrain-auc:0.71631\ttrain-error:0.28381\ttest-logloss:12.51629\ttest-auc:0.65987\ttest-error:0.33974\n",
      "[222]\ttrain-logloss:10.43329\ttrain-auc:0.71692\ttrain-error:0.28319\ttest-logloss:12.50985\ttest-auc:0.66005\ttest-error:0.33956\n",
      "[223]\ttrain-logloss:10.42686\ttrain-auc:0.71710\ttrain-error:0.28302\ttest-logloss:12.51629\ttest-auc:0.65987\ttest-error:0.33974\n",
      "[224]\ttrain-logloss:10.40113\ttrain-auc:0.71779\ttrain-error:0.28232\ttest-logloss:12.50342\ttest-auc:0.66022\ttest-error:0.33938\n",
      "[225]\ttrain-logloss:10.39952\ttrain-auc:0.71784\ttrain-error:0.28228\ttest-logloss:12.48413\ttest-auc:0.66075\ttest-error:0.33886\n",
      "[226]\ttrain-logloss:10.39309\ttrain-auc:0.71801\ttrain-error:0.28210\ttest-logloss:12.50342\ttest-auc:0.66022\ttest-error:0.33938\n",
      "[227]\ttrain-logloss:10.38023\ttrain-auc:0.71836\ttrain-error:0.28175\ttest-logloss:12.48413\ttest-auc:0.66075\ttest-error:0.33886\n",
      "[228]\ttrain-logloss:10.37058\ttrain-auc:0.71862\ttrain-error:0.28149\ttest-logloss:12.47126\ttest-auc:0.66109\ttest-error:0.33851\n",
      "[229]\ttrain-logloss:10.35932\ttrain-auc:0.71893\ttrain-error:0.28119\ttest-logloss:12.43910\ttest-auc:0.66197\ttest-error:0.33764\n",
      "[230]\ttrain-logloss:10.35611\ttrain-auc:0.71902\ttrain-error:0.28110\ttest-logloss:12.47126\ttest-auc:0.66111\ttest-error:0.33851\n",
      "[231]\ttrain-logloss:10.34646\ttrain-auc:0.71928\ttrain-error:0.28084\ttest-logloss:12.48413\ttest-auc:0.66076\ttest-error:0.33886\n",
      "[232]\ttrain-logloss:10.34163\ttrain-auc:0.71941\ttrain-error:0.28071\ttest-logloss:12.47770\ttest-auc:0.66093\ttest-error:0.33869\n",
      "[233]\ttrain-logloss:10.32394\ttrain-auc:0.71989\ttrain-error:0.28023\ttest-logloss:12.49056\ttest-auc:0.66059\ttest-error:0.33904\n",
      "[234]\ttrain-logloss:10.31269\ttrain-auc:0.72020\ttrain-error:0.27992\ttest-logloss:12.47770\ttest-auc:0.66094\ttest-error:0.33869\n",
      "[235]\ttrain-logloss:10.27731\ttrain-auc:0.72116\ttrain-error:0.27896\ttest-logloss:12.45840\ttest-auc:0.66146\ttest-error:0.33816\n",
      "[236]\ttrain-logloss:10.29178\ttrain-auc:0.72076\ttrain-error:0.27935\ttest-logloss:12.43267\ttest-auc:0.66216\ttest-error:0.33747\n",
      "[237]\ttrain-logloss:10.28535\ttrain-auc:0.72094\ttrain-error:0.27918\ttest-logloss:12.43267\ttest-auc:0.66216\ttest-error:0.33747\n",
      "[238]\ttrain-logloss:10.28213\ttrain-auc:0.72102\ttrain-error:0.27909\ttest-logloss:12.40052\ttest-auc:0.66304\ttest-error:0.33659\n",
      "[239]\ttrain-logloss:10.27409\ttrain-auc:0.72124\ttrain-error:0.27887\ttest-logloss:12.38765\ttest-auc:0.66339\ttest-error:0.33624\n",
      "[240]\ttrain-logloss:10.25962\ttrain-auc:0.72164\ttrain-error:0.27848\ttest-logloss:12.38765\ttest-auc:0.66339\ttest-error:0.33624\n",
      "[241]\ttrain-logloss:10.25640\ttrain-auc:0.72172\ttrain-error:0.27839\ttest-logloss:12.39408\ttest-auc:0.66322\ttest-error:0.33642\n",
      "[242]\ttrain-logloss:10.25640\ttrain-auc:0.72172\ttrain-error:0.27839\ttest-logloss:12.36835\ttest-auc:0.66391\ttest-error:0.33572\n",
      "[243]\ttrain-logloss:10.24032\ttrain-auc:0.72216\ttrain-error:0.27796\ttest-logloss:12.34906\ttest-auc:0.66444\ttest-error:0.33520\n",
      "[244]\ttrain-logloss:10.22585\ttrain-auc:0.72255\ttrain-error:0.27756\ttest-logloss:12.34263\ttest-auc:0.66461\ttest-error:0.33502\n",
      "[245]\ttrain-logloss:10.22102\ttrain-auc:0.72268\ttrain-error:0.27743\ttest-logloss:12.34263\ttest-auc:0.66461\ttest-error:0.33502\n",
      "[246]\ttrain-logloss:10.20012\ttrain-auc:0.72325\ttrain-error:0.27687\ttest-logloss:12.36192\ttest-auc:0.66409\ttest-error:0.33554\n",
      "[247]\ttrain-logloss:10.18886\ttrain-auc:0.72355\ttrain-error:0.27656\ttest-logloss:12.37479\ttest-auc:0.66374\ttest-error:0.33589\n",
      "[248]\ttrain-logloss:10.18404\ttrain-auc:0.72368\ttrain-error:0.27643\ttest-logloss:12.36835\ttest-auc:0.66392\ttest-error:0.33572\n",
      "[249]\ttrain-logloss:10.18565\ttrain-auc:0.72364\ttrain-error:0.27647\ttest-logloss:12.33620\ttest-auc:0.66479\ttest-error:0.33485\n",
      "[250]\ttrain-logloss:10.18082\ttrain-auc:0.72377\ttrain-error:0.27634\ttest-logloss:12.30404\ttest-auc:0.66567\ttest-error:0.33397\n",
      "[251]\ttrain-logloss:10.17278\ttrain-auc:0.72399\ttrain-error:0.27612\ttest-logloss:12.32976\ttest-auc:0.66498\ttest-error:0.33467\n",
      "[252]\ttrain-logloss:10.15831\ttrain-auc:0.72438\ttrain-error:0.27573\ttest-logloss:12.32976\ttest-auc:0.66497\ttest-error:0.33467\n",
      "[253]\ttrain-logloss:10.15670\ttrain-auc:0.72442\ttrain-error:0.27569\ttest-logloss:12.34906\ttest-auc:0.66445\ttest-error:0.33520\n",
      "[254]\ttrain-logloss:10.15670\ttrain-auc:0.72442\ttrain-error:0.27569\ttest-logloss:12.33620\ttest-auc:0.66481\ttest-error:0.33485\n",
      "[255]\ttrain-logloss:10.15509\ttrain-auc:0.72447\ttrain-error:0.27564\ttest-logloss:12.34906\ttest-auc:0.66446\ttest-error:0.33520\n",
      "[256]\ttrain-logloss:10.15188\ttrain-auc:0.72455\ttrain-error:0.27556\ttest-logloss:12.35549\ttest-auc:0.66429\ttest-error:0.33537\n",
      "[257]\ttrain-logloss:10.13580\ttrain-auc:0.72499\ttrain-error:0.27512\ttest-logloss:12.32976\ttest-auc:0.66499\ttest-error:0.33467\n",
      "[258]\ttrain-logloss:10.11811\ttrain-auc:0.72547\ttrain-error:0.27464\ttest-logloss:12.38765\ttest-auc:0.66343\ttest-error:0.33624\n",
      "[259]\ttrain-logloss:10.09238\ttrain-auc:0.72617\ttrain-error:0.27394\ttest-logloss:12.39408\ttest-auc:0.66325\ttest-error:0.33642\n",
      "[260]\ttrain-logloss:10.08755\ttrain-auc:0.72630\ttrain-error:0.27381\ttest-logloss:12.33620\ttest-auc:0.66482\ttest-error:0.33485\n",
      "[261]\ttrain-logloss:10.09399\ttrain-auc:0.72613\ttrain-error:0.27398\ttest-logloss:12.34906\ttest-auc:0.66447\ttest-error:0.33520\n",
      "[262]\ttrain-logloss:10.07630\ttrain-auc:0.72661\ttrain-error:0.27350\ttest-logloss:12.34906\ttest-auc:0.66447\ttest-error:0.33520\n",
      "[263]\ttrain-logloss:10.05861\ttrain-auc:0.72709\ttrain-error:0.27303\ttest-logloss:12.36192\ttest-auc:0.66412\ttest-error:0.33554\n",
      "[264]\ttrain-logloss:10.05378\ttrain-auc:0.72722\ttrain-error:0.27289\ttest-logloss:12.34906\ttest-auc:0.66447\ttest-error:0.33520\n",
      "[265]\ttrain-logloss:10.03770\ttrain-auc:0.72765\ttrain-error:0.27246\ttest-logloss:12.34263\ttest-auc:0.66464\ttest-error:0.33502\n",
      "[266]\ttrain-logloss:10.02966\ttrain-auc:0.72787\ttrain-error:0.27224\ttest-logloss:12.34263\ttest-auc:0.66464\ttest-error:0.33502\n",
      "[267]\ttrain-logloss:10.00233\ttrain-auc:0.72861\ttrain-error:0.27150\ttest-logloss:12.34263\ttest-auc:0.66464\ttest-error:0.33502\n",
      "[268]\ttrain-logloss:9.99589\ttrain-auc:0.72879\ttrain-error:0.27132\ttest-logloss:12.35549\ttest-auc:0.66429\ttest-error:0.33537\n",
      "[269]\ttrain-logloss:9.98785\ttrain-auc:0.72901\ttrain-error:0.27110\ttest-logloss:12.35549\ttest-auc:0.66429\ttest-error:0.33537\n",
      "[270]\ttrain-logloss:9.99589\ttrain-auc:0.72879\ttrain-error:0.27132\ttest-logloss:12.35549\ttest-auc:0.66429\ttest-error:0.33537\n",
      "[271]\ttrain-logloss:9.99107\ttrain-auc:0.72892\ttrain-error:0.27119\ttest-logloss:12.36192\ttest-auc:0.66413\ttest-error:0.33554\n",
      "[272]\ttrain-logloss:9.97177\ttrain-auc:0.72944\ttrain-error:0.27067\ttest-logloss:12.34906\ttest-auc:0.66448\ttest-error:0.33520\n",
      "[273]\ttrain-logloss:9.96534\ttrain-auc:0.72962\ttrain-error:0.27049\ttest-logloss:12.32976\ttest-auc:0.66500\ttest-error:0.33467\n",
      "[274]\ttrain-logloss:9.95247\ttrain-auc:0.72997\ttrain-error:0.27014\ttest-logloss:12.31690\ttest-auc:0.66535\ttest-error:0.33432\n",
      "[275]\ttrain-logloss:9.95891\ttrain-auc:0.72979\ttrain-error:0.27032\ttest-logloss:12.31047\ttest-auc:0.66552\ttest-error:0.33415\n",
      "[276]\ttrain-logloss:9.95408\ttrain-auc:0.72992\ttrain-error:0.27019\ttest-logloss:12.31047\ttest-auc:0.66552\ttest-error:0.33415\n",
      "[277]\ttrain-logloss:9.92192\ttrain-auc:0.73079\ttrain-error:0.26932\ttest-logloss:12.32333\ttest-auc:0.66517\ttest-error:0.33450\n",
      "[278]\ttrain-logloss:9.91549\ttrain-auc:0.73097\ttrain-error:0.26914\ttest-logloss:12.34263\ttest-auc:0.66465\ttest-error:0.33502\n",
      "[279]\ttrain-logloss:9.91066\ttrain-auc:0.73110\ttrain-error:0.26901\ttest-logloss:12.34263\ttest-auc:0.66465\ttest-error:0.33502\n",
      "[280]\ttrain-logloss:9.90101\ttrain-auc:0.73136\ttrain-error:0.26875\ttest-logloss:12.32976\ttest-auc:0.66500\ttest-error:0.33467\n",
      "[281]\ttrain-logloss:9.89619\ttrain-auc:0.73149\ttrain-error:0.26862\ttest-logloss:12.31690\ttest-auc:0.66535\ttest-error:0.33432\n",
      "[282]\ttrain-logloss:9.90262\ttrain-auc:0.73132\ttrain-error:0.26879\ttest-logloss:12.31690\ttest-auc:0.66536\ttest-error:0.33432\n",
      "[283]\ttrain-logloss:9.89941\ttrain-auc:0.73141\ttrain-error:0.26870\ttest-logloss:12.32333\ttest-auc:0.66519\ttest-error:0.33450\n",
      "[284]\ttrain-logloss:9.88011\ttrain-auc:0.73193\ttrain-error:0.26818\ttest-logloss:12.32976\ttest-auc:0.66500\ttest-error:0.33467\n",
      "[285]\ttrain-logloss:9.86885\ttrain-auc:0.73223\ttrain-error:0.26787\ttest-logloss:12.28474\ttest-auc:0.66622\ttest-error:0.33345\n",
      "[286]\ttrain-logloss:9.87207\ttrain-auc:0.73215\ttrain-error:0.26796\ttest-logloss:12.26545\ttest-auc:0.66675\ttest-error:0.33293\n",
      "[287]\ttrain-logloss:9.86885\ttrain-auc:0.73223\ttrain-error:0.26787\ttest-logloss:12.23972\ttest-auc:0.66745\ttest-error:0.33223\n",
      "[288]\ttrain-logloss:9.86564\ttrain-auc:0.73232\ttrain-error:0.26779\ttest-logloss:12.25901\ttest-auc:0.66693\ttest-error:0.33275\n",
      "[289]\ttrain-logloss:9.84795\ttrain-auc:0.73280\ttrain-error:0.26731\ttest-logloss:12.23972\ttest-auc:0.66745\ttest-error:0.33223\n",
      "[290]\ttrain-logloss:9.83830\ttrain-auc:0.73306\ttrain-error:0.26704\ttest-logloss:12.23329\ttest-auc:0.66763\ttest-error:0.33205\n",
      "[291]\ttrain-logloss:9.83187\ttrain-auc:0.73324\ttrain-error:0.26687\ttest-logloss:12.24615\ttest-auc:0.66728\ttest-error:0.33240\n",
      "[292]\ttrain-logloss:9.82865\ttrain-auc:0.73333\ttrain-error:0.26678\ttest-logloss:12.25901\ttest-auc:0.66694\ttest-error:0.33275\n",
      "[293]\ttrain-logloss:9.80775\ttrain-auc:0.73389\ttrain-error:0.26622\ttest-logloss:12.24615\ttest-auc:0.66729\ttest-error:0.33240\n",
      "[294]\ttrain-logloss:9.80131\ttrain-auc:0.73406\ttrain-error:0.26604\ttest-logloss:12.23972\ttest-auc:0.66746\ttest-error:0.33223\n",
      "[295]\ttrain-logloss:9.80292\ttrain-auc:0.73402\ttrain-error:0.26609\ttest-logloss:12.23972\ttest-auc:0.66746\ttest-error:0.33223\n",
      "[296]\ttrain-logloss:9.76915\ttrain-auc:0.73494\ttrain-error:0.26517\ttest-logloss:12.25258\ttest-auc:0.66712\ttest-error:0.33258\n",
      "[297]\ttrain-logloss:9.77719\ttrain-auc:0.73472\ttrain-error:0.26539\ttest-logloss:12.26545\ttest-auc:0.66677\ttest-error:0.33293\n",
      "[298]\ttrain-logloss:9.78041\ttrain-auc:0.73463\ttrain-error:0.26547\ttest-logloss:12.29117\ttest-auc:0.66607\ttest-error:0.33362\n",
      "[299]\ttrain-logloss:9.77076\ttrain-auc:0.73490\ttrain-error:0.26521\ttest-logloss:12.26545\ttest-auc:0.66676\ttest-error:0.33293\n",
      "[300]\ttrain-logloss:9.77558\ttrain-auc:0.73477\ttrain-error:0.26534\ttest-logloss:12.28474\ttest-auc:0.66624\ttest-error:0.33345\n",
      "[301]\ttrain-logloss:9.76754\ttrain-auc:0.73498\ttrain-error:0.26512\ttest-logloss:12.27831\ttest-auc:0.66642\ttest-error:0.33327\n",
      "[302]\ttrain-logloss:9.74503\ttrain-auc:0.73559\ttrain-error:0.26451\ttest-logloss:12.25258\ttest-auc:0.66712\ttest-error:0.33258\n",
      "[303]\ttrain-logloss:9.74182\ttrain-auc:0.73568\ttrain-error:0.26443\ttest-logloss:12.22042\ttest-auc:0.66800\ttest-error:0.33170\n",
      "[304]\ttrain-logloss:9.74342\ttrain-auc:0.73564\ttrain-error:0.26447\ttest-logloss:12.22042\ttest-auc:0.66799\ttest-error:0.33170\n",
      "[305]\ttrain-logloss:9.71448\ttrain-auc:0.73642\ttrain-error:0.26368\ttest-logloss:12.21399\ttest-auc:0.66817\ttest-error:0.33153\n",
      "[306]\ttrain-logloss:9.70000\ttrain-auc:0.73681\ttrain-error:0.26329\ttest-logloss:12.22042\ttest-auc:0.66799\ttest-error:0.33170\n",
      "[307]\ttrain-logloss:9.70000\ttrain-auc:0.73681\ttrain-error:0.26329\ttest-logloss:12.18826\ttest-auc:0.66887\ttest-error:0.33083\n",
      "[308]\ttrain-logloss:9.68714\ttrain-auc:0.73716\ttrain-error:0.26294\ttest-logloss:12.20756\ttest-auc:0.66835\ttest-error:0.33136\n",
      "[309]\ttrain-logloss:9.67427\ttrain-auc:0.73751\ttrain-error:0.26259\ttest-logloss:12.20113\ttest-auc:0.66852\ttest-error:0.33118\n",
      "[310]\ttrain-logloss:9.66945\ttrain-auc:0.73764\ttrain-error:0.26246\ttest-logloss:12.18183\ttest-auc:0.66905\ttest-error:0.33066\n",
      "[311]\ttrain-logloss:9.67106\ttrain-auc:0.73760\ttrain-error:0.26250\ttest-logloss:12.17540\ttest-auc:0.66922\ttest-error:0.33048\n",
      "[312]\ttrain-logloss:9.66302\ttrain-auc:0.73782\ttrain-error:0.26229\ttest-logloss:12.20113\ttest-auc:0.66852\ttest-error:0.33118\n",
      "[313]\ttrain-logloss:9.66623\ttrain-auc:0.73773\ttrain-error:0.26238\ttest-logloss:12.18183\ttest-auc:0.66904\ttest-error:0.33066\n",
      "[314]\ttrain-logloss:9.65337\ttrain-auc:0.73808\ttrain-error:0.26203\ttest-logloss:12.18826\ttest-auc:0.66887\ttest-error:0.33083\n",
      "[315]\ttrain-logloss:9.64211\ttrain-auc:0.73839\ttrain-error:0.26172\ttest-logloss:12.18183\ttest-auc:0.66904\ttest-error:0.33066\n",
      "[316]\ttrain-logloss:9.63086\ttrain-auc:0.73869\ttrain-error:0.26141\ttest-logloss:12.22042\ttest-auc:0.66799\ttest-error:0.33170\n",
      "[317]\ttrain-logloss:9.63568\ttrain-auc:0.73856\ttrain-error:0.26155\ttest-logloss:12.20113\ttest-auc:0.66852\ttest-error:0.33118\n",
      "[318]\ttrain-logloss:9.63246\ttrain-auc:0.73865\ttrain-error:0.26146\ttest-logloss:12.20756\ttest-auc:0.66835\ttest-error:0.33136\n",
      "[319]\ttrain-logloss:9.61478\ttrain-auc:0.73913\ttrain-error:0.26098\ttest-logloss:12.23972\ttest-auc:0.66748\ttest-error:0.33223\n",
      "[320]\ttrain-logloss:9.60352\ttrain-auc:0.73943\ttrain-error:0.26067\ttest-logloss:12.22686\ttest-auc:0.66783\ttest-error:0.33188\n",
      "[321]\ttrain-logloss:9.60030\ttrain-auc:0.73952\ttrain-error:0.26059\ttest-logloss:12.24615\ttest-auc:0.66730\ttest-error:0.33240\n",
      "[322]\ttrain-logloss:9.57940\ttrain-auc:0.74009\ttrain-error:0.26002\ttest-logloss:12.21399\ttest-auc:0.66817\ttest-error:0.33153\n",
      "[323]\ttrain-logloss:9.57296\ttrain-auc:0.74026\ttrain-error:0.25984\ttest-logloss:12.22686\ttest-auc:0.66783\ttest-error:0.33188\n",
      "[324]\ttrain-logloss:9.55849\ttrain-auc:0.74065\ttrain-error:0.25945\ttest-logloss:12.23329\ttest-auc:0.66765\ttest-error:0.33205\n",
      "[325]\ttrain-logloss:9.55528\ttrain-auc:0.74074\ttrain-error:0.25936\ttest-logloss:12.21399\ttest-auc:0.66818\ttest-error:0.33153\n",
      "[326]\ttrain-logloss:9.56975\ttrain-auc:0.74035\ttrain-error:0.25976\ttest-logloss:12.21399\ttest-auc:0.66818\ttest-error:0.33153\n",
      "[327]\ttrain-logloss:9.53919\ttrain-auc:0.74118\ttrain-error:0.25893\ttest-logloss:12.23329\ttest-auc:0.66765\ttest-error:0.33205\n",
      "[328]\ttrain-logloss:9.53276\ttrain-auc:0.74135\ttrain-error:0.25875\ttest-logloss:12.22042\ttest-auc:0.66800\ttest-error:0.33170\n",
      "[329]\ttrain-logloss:9.51668\ttrain-auc:0.74179\ttrain-error:0.25832\ttest-logloss:12.22042\ttest-auc:0.66801\ttest-error:0.33170\n",
      "[330]\ttrain-logloss:9.51507\ttrain-auc:0.74183\ttrain-error:0.25827\ttest-logloss:12.20756\ttest-auc:0.66836\ttest-error:0.33136\n",
      "[331]\ttrain-logloss:9.51347\ttrain-auc:0.74187\ttrain-error:0.25823\ttest-logloss:12.23329\ttest-auc:0.66766\ttest-error:0.33205\n",
      "[332]\ttrain-logloss:9.48452\ttrain-auc:0.74266\ttrain-error:0.25744\ttest-logloss:12.26545\ttest-auc:0.66678\ttest-error:0.33293\n",
      "[333]\ttrain-logloss:9.47969\ttrain-auc:0.74279\ttrain-error:0.25731\ttest-logloss:12.28474\ttest-auc:0.66626\ttest-error:0.33345\n",
      "[334]\ttrain-logloss:9.48774\ttrain-auc:0.74257\ttrain-error:0.25753\ttest-logloss:12.30404\ttest-auc:0.66574\ttest-error:0.33397\n",
      "[335]\ttrain-logloss:9.48452\ttrain-auc:0.74266\ttrain-error:0.25744\ttest-logloss:12.20756\ttest-auc:0.66836\ttest-error:0.33136\n",
      "[336]\ttrain-logloss:9.48452\ttrain-auc:0.74266\ttrain-error:0.25744\ttest-logloss:12.19470\ttest-auc:0.66871\ttest-error:0.33101\n",
      "[337]\ttrain-logloss:9.47166\ttrain-auc:0.74301\ttrain-error:0.25709\ttest-logloss:12.19470\ttest-auc:0.66872\ttest-error:0.33101\n",
      "[338]\ttrain-logloss:9.46683\ttrain-auc:0.74314\ttrain-error:0.25696\ttest-logloss:12.21399\ttest-auc:0.66820\ttest-error:0.33153\n",
      "[339]\ttrain-logloss:9.46683\ttrain-auc:0.74314\ttrain-error:0.25696\ttest-logloss:12.20756\ttest-auc:0.66837\ttest-error:0.33136\n",
      "[340]\ttrain-logloss:9.48774\ttrain-auc:0.74257\ttrain-error:0.25753\ttest-logloss:12.20756\ttest-auc:0.66836\ttest-error:0.33136\n",
      "[341]\ttrain-logloss:9.47005\ttrain-auc:0.74305\ttrain-error:0.25705\ttest-logloss:12.21399\ttest-auc:0.66819\ttest-error:0.33153\n",
      "[342]\ttrain-logloss:9.45397\ttrain-auc:0.74349\ttrain-error:0.25661\ttest-logloss:12.20113\ttest-auc:0.66853\ttest-error:0.33118\n",
      "[343]\ttrain-logloss:9.45075\ttrain-auc:0.74358\ttrain-error:0.25653\ttest-logloss:12.20113\ttest-auc:0.66853\ttest-error:0.33118\n",
      "[344]\ttrain-logloss:9.43788\ttrain-auc:0.74393\ttrain-error:0.25618\ttest-logloss:12.22686\ttest-auc:0.66783\ttest-error:0.33188\n",
      "[345]\ttrain-logloss:9.43145\ttrain-auc:0.74410\ttrain-error:0.25600\ttest-logloss:12.25258\ttest-auc:0.66713\ttest-error:0.33258\n",
      "[346]\ttrain-logloss:9.43145\ttrain-auc:0.74410\ttrain-error:0.25600\ttest-logloss:12.25901\ttest-auc:0.66695\ttest-error:0.33275\n",
      "[347]\ttrain-logloss:9.41859\ttrain-auc:0.74445\ttrain-error:0.25565\ttest-logloss:12.27188\ttest-auc:0.66660\ttest-error:0.33310\n",
      "[348]\ttrain-logloss:9.41859\ttrain-auc:0.74445\ttrain-error:0.25565\ttest-logloss:12.26545\ttest-auc:0.66678\ttest-error:0.33293\n",
      "[349]\ttrain-logloss:9.42180\ttrain-auc:0.74436\ttrain-error:0.25574\ttest-logloss:12.24615\ttest-auc:0.66731\ttest-error:0.33240\n",
      "[350]\ttrain-logloss:9.41537\ttrain-auc:0.74454\ttrain-error:0.25556\ttest-logloss:12.25258\ttest-auc:0.66713\ttest-error:0.33258\n",
      "[351]\ttrain-logloss:9.40894\ttrain-auc:0.74471\ttrain-error:0.25539\ttest-logloss:12.27188\ttest-auc:0.66661\ttest-error:0.33310\n",
      "[352]\ttrain-logloss:9.39125\ttrain-auc:0.74519\ttrain-error:0.25491\ttest-logloss:12.27831\ttest-auc:0.66643\ttest-error:0.33327\n",
      "[353]\ttrain-logloss:9.39286\ttrain-auc:0.74515\ttrain-error:0.25495\ttest-logloss:12.25901\ttest-auc:0.66695\ttest-error:0.33275\n",
      "[354]\ttrain-logloss:9.40251\ttrain-auc:0.74489\ttrain-error:0.25522\ttest-logloss:12.26545\ttest-auc:0.66678\ttest-error:0.33293\n",
      "[355]\ttrain-logloss:9.39447\ttrain-auc:0.74511\ttrain-error:0.25500\ttest-logloss:12.29761\ttest-auc:0.66591\ttest-error:0.33380\n",
      "[356]\ttrain-logloss:9.37678\ttrain-auc:0.74558\ttrain-error:0.25452\ttest-logloss:12.30404\ttest-auc:0.66573\ttest-error:0.33397\n",
      "[357]\ttrain-logloss:9.38482\ttrain-auc:0.74537\ttrain-error:0.25474\ttest-logloss:12.31047\ttest-auc:0.66556\ttest-error:0.33415\n",
      "[358]\ttrain-logloss:9.35909\ttrain-auc:0.74606\ttrain-error:0.25404\ttest-logloss:12.30404\ttest-auc:0.66573\ttest-error:0.33397\n",
      "[359]\ttrain-logloss:9.34462\ttrain-auc:0.74646\ttrain-error:0.25365\ttest-logloss:12.29761\ttest-auc:0.66591\ttest-error:0.33380\n",
      "[360]\ttrain-logloss:9.33658\ttrain-auc:0.74667\ttrain-error:0.25343\ttest-logloss:12.31047\ttest-auc:0.66556\ttest-error:0.33415\n",
      "[361]\ttrain-logloss:9.30441\ttrain-auc:0.74755\ttrain-error:0.25255\ttest-logloss:12.31047\ttest-auc:0.66556\ttest-error:0.33415\n",
      "[362]\ttrain-logloss:9.30441\ttrain-auc:0.74755\ttrain-error:0.25255\ttest-logloss:12.31047\ttest-auc:0.66556\ttest-error:0.33415\n",
      "[363]\ttrain-logloss:9.29959\ttrain-auc:0.74768\ttrain-error:0.25242\ttest-logloss:12.29117\ttest-auc:0.66608\ttest-error:0.33362\n",
      "[364]\ttrain-logloss:9.29316\ttrain-auc:0.74785\ttrain-error:0.25225\ttest-logloss:12.28474\ttest-auc:0.66626\ttest-error:0.33345\n",
      "[365]\ttrain-logloss:9.28512\ttrain-auc:0.74807\ttrain-error:0.25203\ttest-logloss:12.27831\ttest-auc:0.66645\ttest-error:0.33327\n",
      "[366]\ttrain-logloss:9.29155\ttrain-auc:0.74789\ttrain-error:0.25220\ttest-logloss:12.25901\ttest-auc:0.66698\ttest-error:0.33275\n",
      "[367]\ttrain-logloss:9.26582\ttrain-auc:0.74859\ttrain-error:0.25151\ttest-logloss:12.24615\ttest-auc:0.66733\ttest-error:0.33240\n",
      "[368]\ttrain-logloss:9.28029\ttrain-auc:0.74820\ttrain-error:0.25190\ttest-logloss:12.25901\ttest-auc:0.66698\ttest-error:0.33275\n",
      "[369]\ttrain-logloss:9.25135\ttrain-auc:0.74899\ttrain-error:0.25111\ttest-logloss:12.22042\ttest-auc:0.66803\ttest-error:0.33170\n",
      "[370]\ttrain-logloss:9.23366\ttrain-auc:0.74947\ttrain-error:0.25063\ttest-logloss:12.22042\ttest-auc:0.66802\ttest-error:0.33170\n",
      "[371]\ttrain-logloss:9.23848\ttrain-auc:0.74933\ttrain-error:0.25076\ttest-logloss:12.22686\ttest-auc:0.66784\ttest-error:0.33188\n",
      "[372]\ttrain-logloss:9.23366\ttrain-auc:0.74947\ttrain-error:0.25063\ttest-logloss:12.22686\ttest-auc:0.66784\ttest-error:0.33188\n",
      "[373]\ttrain-logloss:9.22883\ttrain-auc:0.74960\ttrain-error:0.25050\ttest-logloss:12.23329\ttest-auc:0.66767\ttest-error:0.33205\n",
      "[374]\ttrain-logloss:9.21436\ttrain-auc:0.74999\ttrain-error:0.25011\ttest-logloss:12.21399\ttest-auc:0.66819\ttest-error:0.33153\n",
      "[375]\ttrain-logloss:9.22562\ttrain-auc:0.74969\ttrain-error:0.25041\ttest-logloss:12.20113\ttest-auc:0.66854\ttest-error:0.33118\n",
      "[376]\ttrain-logloss:9.21114\ttrain-auc:0.75008\ttrain-error:0.25002\ttest-logloss:12.20756\ttest-auc:0.66837\ttest-error:0.33136\n",
      "[377]\ttrain-logloss:9.21114\ttrain-auc:0.75008\ttrain-error:0.25002\ttest-logloss:12.22042\ttest-auc:0.66802\ttest-error:0.33170\n",
      "[378]\ttrain-logloss:9.21114\ttrain-auc:0.75008\ttrain-error:0.25002\ttest-logloss:12.22686\ttest-auc:0.66785\ttest-error:0.33188\n",
      "[379]\ttrain-logloss:9.21758\ttrain-auc:0.74990\ttrain-error:0.25020\ttest-logloss:12.22042\ttest-auc:0.66802\ttest-error:0.33170\n",
      "[380]\ttrain-logloss:9.19989\ttrain-auc:0.75038\ttrain-error:0.24972\ttest-logloss:12.20113\ttest-auc:0.66855\ttest-error:0.33118\n",
      "[381]\ttrain-logloss:9.19828\ttrain-auc:0.75043\ttrain-error:0.24967\ttest-logloss:12.21399\ttest-auc:0.66820\ttest-error:0.33153\n",
      "[382]\ttrain-logloss:9.19024\ttrain-auc:0.75065\ttrain-error:0.24945\ttest-logloss:12.22042\ttest-auc:0.66803\ttest-error:0.33170\n",
      "[383]\ttrain-logloss:9.17255\ttrain-auc:0.75112\ttrain-error:0.24897\ttest-logloss:12.20756\ttest-auc:0.66837\ttest-error:0.33136\n",
      "[384]\ttrain-logloss:9.16612\ttrain-auc:0.75130\ttrain-error:0.24880\ttest-logloss:12.24615\ttest-auc:0.66732\ttest-error:0.33240\n",
      "[385]\ttrain-logloss:9.16612\ttrain-auc:0.75130\ttrain-error:0.24880\ttest-logloss:12.22686\ttest-auc:0.66785\ttest-error:0.33188\n",
      "[386]\ttrain-logloss:9.15004\ttrain-auc:0.75174\ttrain-error:0.24836\ttest-logloss:12.23972\ttest-auc:0.66750\ttest-error:0.33223\n",
      "[387]\ttrain-logloss:9.13878\ttrain-auc:0.75204\ttrain-error:0.24806\ttest-logloss:12.24615\ttest-auc:0.66733\ttest-error:0.33240\n",
      "[388]\ttrain-logloss:9.14039\ttrain-auc:0.75200\ttrain-error:0.24810\ttest-logloss:12.23972\ttest-auc:0.66750\ttest-error:0.33223\n",
      "[389]\ttrain-logloss:9.12913\ttrain-auc:0.75230\ttrain-error:0.24780\ttest-logloss:12.22686\ttest-auc:0.66785\ttest-error:0.33188\n",
      "[390]\ttrain-logloss:9.12270\ttrain-auc:0.75248\ttrain-error:0.24762\ttest-logloss:12.22042\ttest-auc:0.66802\ttest-error:0.33170\n",
      "[391]\ttrain-logloss:9.12592\ttrain-auc:0.75239\ttrain-error:0.24771\ttest-logloss:12.21399\ttest-auc:0.66820\ttest-error:0.33153\n",
      "[392]\ttrain-logloss:9.11627\ttrain-auc:0.75265\ttrain-error:0.24745\ttest-logloss:12.22042\ttest-auc:0.66802\ttest-error:0.33170\n",
      "[393]\ttrain-logloss:9.10340\ttrain-auc:0.75300\ttrain-error:0.24710\ttest-logloss:12.21399\ttest-auc:0.66819\ttest-error:0.33153\n",
      "[394]\ttrain-logloss:9.10340\ttrain-auc:0.75300\ttrain-error:0.24710\ttest-logloss:12.20113\ttest-auc:0.66855\ttest-error:0.33118\n",
      "[395]\ttrain-logloss:9.09697\ttrain-auc:0.75318\ttrain-error:0.24692\ttest-logloss:12.22686\ttest-auc:0.66785\ttest-error:0.33188\n",
      "[396]\ttrain-logloss:9.09375\ttrain-auc:0.75326\ttrain-error:0.24683\ttest-logloss:12.19470\ttest-auc:0.66873\ttest-error:0.33101\n",
      "[397]\ttrain-logloss:9.08571\ttrain-auc:0.75348\ttrain-error:0.24662\ttest-logloss:12.21399\ttest-auc:0.66820\ttest-error:0.33153\n",
      "[398]\ttrain-logloss:9.06642\ttrain-auc:0.75400\ttrain-error:0.24609\ttest-logloss:12.23329\ttest-auc:0.66767\ttest-error:0.33205\n",
      "[399]\ttrain-logloss:9.06642\ttrain-auc:0.75401\ttrain-error:0.24609\ttest-logloss:12.28474\ttest-auc:0.66628\ttest-error:0.33345\n",
      "[400]\ttrain-logloss:9.06963\ttrain-auc:0.75392\ttrain-error:0.24618\ttest-logloss:12.27188\ttest-auc:0.66663\ttest-error:0.33310\n",
      "[401]\ttrain-logloss:9.05998\ttrain-auc:0.75418\ttrain-error:0.24592\ttest-logloss:12.23972\ttest-auc:0.66749\ttest-error:0.33223\n",
      "[402]\ttrain-logloss:9.04390\ttrain-auc:0.75462\ttrain-error:0.24548\ttest-logloss:12.26545\ttest-auc:0.66680\ttest-error:0.33293\n",
      "[403]\ttrain-logloss:9.04712\ttrain-auc:0.75453\ttrain-error:0.24557\ttest-logloss:12.25901\ttest-auc:0.66698\ttest-error:0.33275\n",
      "[404]\ttrain-logloss:9.05838\ttrain-auc:0.75422\ttrain-error:0.24588\ttest-logloss:12.23329\ttest-auc:0.66768\ttest-error:0.33205\n",
      "[405]\ttrain-logloss:9.04390\ttrain-auc:0.75462\ttrain-error:0.24548\ttest-logloss:12.24615\ttest-auc:0.66733\ttest-error:0.33240\n",
      "[406]\ttrain-logloss:9.04551\ttrain-auc:0.75457\ttrain-error:0.24553\ttest-logloss:12.23972\ttest-auc:0.66750\ttest-error:0.33223\n",
      "[407]\ttrain-logloss:9.03908\ttrain-auc:0.75475\ttrain-error:0.24535\ttest-logloss:12.23329\ttest-auc:0.66769\ttest-error:0.33205\n",
      "[408]\ttrain-logloss:9.02139\ttrain-auc:0.75523\ttrain-error:0.24487\ttest-logloss:12.23329\ttest-auc:0.66768\ttest-error:0.33205\n",
      "[409]\ttrain-logloss:9.01496\ttrain-auc:0.75540\ttrain-error:0.24470\ttest-logloss:12.22042\ttest-auc:0.66803\ttest-error:0.33170\n",
      "[410]\ttrain-logloss:9.02139\ttrain-auc:0.75523\ttrain-error:0.24487\ttest-logloss:12.25258\ttest-auc:0.66715\ttest-error:0.33258\n",
      "[411]\ttrain-logloss:9.01656\ttrain-auc:0.75536\ttrain-error:0.24474\ttest-logloss:12.22042\ttest-auc:0.66803\ttest-error:0.33170\n",
      "[412]\ttrain-logloss:9.00209\ttrain-auc:0.75575\ttrain-error:0.24435\ttest-logloss:12.22686\ttest-auc:0.66786\ttest-error:0.33188\n",
      "[413]\ttrain-logloss:9.00692\ttrain-auc:0.75562\ttrain-error:0.24448\ttest-logloss:12.24615\ttest-auc:0.66734\ttest-error:0.33240\n",
      "[414]\ttrain-logloss:8.99244\ttrain-auc:0.75601\ttrain-error:0.24409\ttest-logloss:12.25258\ttest-auc:0.66716\ttest-error:0.33258\n",
      "[415]\ttrain-logloss:8.99566\ttrain-auc:0.75593\ttrain-error:0.24417\ttest-logloss:12.23972\ttest-auc:0.66751\ttest-error:0.33223\n",
      "[416]\ttrain-logloss:8.98440\ttrain-auc:0.75623\ttrain-error:0.24387\ttest-logloss:12.26545\ttest-auc:0.66680\ttest-error:0.33293\n",
      "[417]\ttrain-logloss:8.96028\ttrain-auc:0.75689\ttrain-error:0.24321\ttest-logloss:12.25901\ttest-auc:0.66698\ttest-error:0.33275\n",
      "[418]\ttrain-logloss:8.97154\ttrain-auc:0.75658\ttrain-error:0.24352\ttest-logloss:12.29761\ttest-auc:0.66593\ttest-error:0.33380\n",
      "[419]\ttrain-logloss:8.96511\ttrain-auc:0.75675\ttrain-error:0.24334\ttest-logloss:12.29117\ttest-auc:0.66610\ttest-error:0.33362\n",
      "[420]\ttrain-logloss:8.95867\ttrain-auc:0.75693\ttrain-error:0.24317\ttest-logloss:12.34263\ttest-auc:0.66471\ttest-error:0.33502\n",
      "[421]\ttrain-logloss:8.94259\ttrain-auc:0.75736\ttrain-error:0.24273\ttest-logloss:12.27831\ttest-auc:0.66647\ttest-error:0.33327\n",
      "[422]\ttrain-logloss:8.93455\ttrain-auc:0.75758\ttrain-error:0.24251\ttest-logloss:12.25258\ttest-auc:0.66717\ttest-error:0.33258\n",
      "[423]\ttrain-logloss:8.92812\ttrain-auc:0.75776\ttrain-error:0.24234\ttest-logloss:12.26545\ttest-auc:0.66681\ttest-error:0.33293\n",
      "[424]\ttrain-logloss:8.91847\ttrain-auc:0.75802\ttrain-error:0.24208\ttest-logloss:12.27188\ttest-auc:0.66664\ttest-error:0.33310\n",
      "[425]\ttrain-logloss:8.91365\ttrain-auc:0.75815\ttrain-error:0.24195\ttest-logloss:12.27831\ttest-auc:0.66646\ttest-error:0.33327\n",
      "[426]\ttrain-logloss:8.90561\ttrain-auc:0.75837\ttrain-error:0.24173\ttest-logloss:12.27831\ttest-auc:0.66646\ttest-error:0.33327\n",
      "[427]\ttrain-logloss:8.90400\ttrain-auc:0.75841\ttrain-error:0.24169\ttest-logloss:12.23972\ttest-auc:0.66751\ttest-error:0.33223\n",
      "[428]\ttrain-logloss:8.89596\ttrain-auc:0.75863\ttrain-error:0.24147\ttest-logloss:12.25901\ttest-auc:0.66699\ttest-error:0.33275\n",
      "[429]\ttrain-logloss:8.88631\ttrain-auc:0.75889\ttrain-error:0.24121\ttest-logloss:12.23972\ttest-auc:0.66751\ttest-error:0.33223\n",
      "[430]\ttrain-logloss:8.88631\ttrain-auc:0.75889\ttrain-error:0.24121\ttest-logloss:12.23972\ttest-auc:0.66751\ttest-error:0.33223\n",
      "[431]\ttrain-logloss:8.87023\ttrain-auc:0.75933\ttrain-error:0.24077\ttest-logloss:12.23329\ttest-auc:0.66768\ttest-error:0.33205\n",
      "[432]\ttrain-logloss:8.86380\ttrain-auc:0.75950\ttrain-error:0.24059\ttest-logloss:12.23329\ttest-auc:0.66768\ttest-error:0.33205\n",
      "[433]\ttrain-logloss:8.86219\ttrain-auc:0.75955\ttrain-error:0.24055\ttest-logloss:12.22042\ttest-auc:0.66803\ttest-error:0.33170\n",
      "[434]\ttrain-logloss:8.86541\ttrain-auc:0.75946\ttrain-error:0.24064\ttest-logloss:12.23329\ttest-auc:0.66768\ttest-error:0.33205\n",
      "[435]\ttrain-logloss:8.85254\ttrain-auc:0.75981\ttrain-error:0.24029\ttest-logloss:12.21399\ttest-auc:0.66821\ttest-error:0.33153\n",
      "[436]\ttrain-logloss:8.84128\ttrain-auc:0.76011\ttrain-error:0.23998\ttest-logloss:12.19470\ttest-auc:0.66873\ttest-error:0.33101\n",
      "[437]\ttrain-logloss:8.83485\ttrain-auc:0.76029\ttrain-error:0.23981\ttest-logloss:12.16254\ttest-auc:0.66961\ttest-error:0.33013\n",
      "[438]\ttrain-logloss:8.83968\ttrain-auc:0.76016\ttrain-error:0.23994\ttest-logloss:12.19470\ttest-auc:0.66873\ttest-error:0.33101\n",
      "[439]\ttrain-logloss:8.83003\ttrain-auc:0.76042\ttrain-error:0.23968\ttest-logloss:12.23329\ttest-auc:0.66768\ttest-error:0.33205\n",
      "[440]\ttrain-logloss:8.82359\ttrain-auc:0.76060\ttrain-error:0.23950\ttest-logloss:12.19470\ttest-auc:0.66873\ttest-error:0.33101\n",
      "[441]\ttrain-logloss:8.82038\ttrain-auc:0.76068\ttrain-error:0.23941\ttest-logloss:12.18826\ttest-auc:0.66891\ttest-error:0.33083\n",
      "[442]\ttrain-logloss:8.79947\ttrain-auc:0.76125\ttrain-error:0.23885\ttest-logloss:12.19470\ttest-auc:0.66873\ttest-error:0.33101\n",
      "[443]\ttrain-logloss:8.79143\ttrain-auc:0.76147\ttrain-error:0.23863\ttest-logloss:12.21399\ttest-auc:0.66821\ttest-error:0.33153\n",
      "[444]\ttrain-logloss:8.77374\ttrain-auc:0.76195\ttrain-error:0.23815\ttest-logloss:12.22686\ttest-auc:0.66787\ttest-error:0.33188\n",
      "[445]\ttrain-logloss:8.75766\ttrain-auc:0.76239\ttrain-error:0.23771\ttest-logloss:12.24615\ttest-auc:0.66734\ttest-error:0.33240\n",
      "[446]\ttrain-logloss:8.74641\ttrain-auc:0.76269\ttrain-error:0.23741\ttest-logloss:12.21399\ttest-auc:0.66821\ttest-error:0.33153\n",
      "[447]\ttrain-logloss:8.75284\ttrain-auc:0.76252\ttrain-error:0.23758\ttest-logloss:12.21399\ttest-auc:0.66821\ttest-error:0.33153\n",
      "[448]\ttrain-logloss:8.73676\ttrain-auc:0.76295\ttrain-error:0.23714\ttest-logloss:12.22686\ttest-auc:0.66786\ttest-error:0.33188\n",
      "[449]\ttrain-logloss:8.72389\ttrain-auc:0.76330\ttrain-error:0.23680\ttest-logloss:12.20756\ttest-auc:0.66838\ttest-error:0.33136\n",
      "[450]\ttrain-logloss:8.71585\ttrain-auc:0.76352\ttrain-error:0.23658\ttest-logloss:12.20113\ttest-auc:0.66856\ttest-error:0.33118\n",
      "[451]\ttrain-logloss:8.71585\ttrain-auc:0.76352\ttrain-error:0.23658\ttest-logloss:12.21399\ttest-auc:0.66821\ttest-error:0.33153\n",
      "[452]\ttrain-logloss:8.71746\ttrain-auc:0.76348\ttrain-error:0.23662\ttest-logloss:12.20113\ttest-auc:0.66856\ttest-error:0.33118\n",
      "[453]\ttrain-logloss:8.72068\ttrain-auc:0.76339\ttrain-error:0.23671\ttest-logloss:12.18826\ttest-auc:0.66890\ttest-error:0.33083\n",
      "[454]\ttrain-logloss:8.71907\ttrain-auc:0.76343\ttrain-error:0.23666\ttest-logloss:12.19470\ttest-auc:0.66873\ttest-error:0.33101\n",
      "[455]\ttrain-logloss:8.71425\ttrain-auc:0.76356\ttrain-error:0.23653\ttest-logloss:12.18826\ttest-auc:0.66890\ttest-error:0.33083\n",
      "[456]\ttrain-logloss:8.71746\ttrain-auc:0.76348\ttrain-error:0.23662\ttest-logloss:12.20113\ttest-auc:0.66856\ttest-error:0.33118\n",
      "[457]\ttrain-logloss:8.72228\ttrain-auc:0.76335\ttrain-error:0.23675\ttest-logloss:12.16897\ttest-auc:0.66943\ttest-error:0.33031\n",
      "[458]\ttrain-logloss:8.73033\ttrain-auc:0.76313\ttrain-error:0.23697\ttest-logloss:12.21399\ttest-auc:0.66821\ttest-error:0.33153\n",
      "[459]\ttrain-logloss:8.71264\ttrain-auc:0.76361\ttrain-error:0.23649\ttest-logloss:12.22042\ttest-auc:0.66804\ttest-error:0.33170\n",
      "[460]\ttrain-logloss:8.70781\ttrain-auc:0.76374\ttrain-error:0.23636\ttest-logloss:12.23972\ttest-auc:0.66752\ttest-error:0.33223\n",
      "[461]\ttrain-logloss:8.69012\ttrain-auc:0.76422\ttrain-error:0.23588\ttest-logloss:12.25258\ttest-auc:0.66716\ttest-error:0.33258\n",
      "[462]\ttrain-logloss:8.68852\ttrain-auc:0.76426\ttrain-error:0.23584\ttest-logloss:12.25901\ttest-auc:0.66699\ttest-error:0.33275\n",
      "[463]\ttrain-logloss:8.67887\ttrain-auc:0.76452\ttrain-error:0.23557\ttest-logloss:12.23972\ttest-auc:0.66752\ttest-error:0.33223\n",
      "[464]\ttrain-logloss:8.67243\ttrain-auc:0.76470\ttrain-error:0.23540\ttest-logloss:12.23329\ttest-auc:0.66769\ttest-error:0.33205\n",
      "[465]\ttrain-logloss:8.68530\ttrain-auc:0.76435\ttrain-error:0.23575\ttest-logloss:12.22042\ttest-auc:0.66804\ttest-error:0.33170\n",
      "[466]\ttrain-logloss:8.66118\ttrain-auc:0.76500\ttrain-error:0.23509\ttest-logloss:12.23972\ttest-auc:0.66752\ttest-error:0.33223\n",
      "[467]\ttrain-logloss:8.65475\ttrain-auc:0.76518\ttrain-error:0.23492\ttest-logloss:12.22042\ttest-auc:0.66804\ttest-error:0.33170\n",
      "[468]\ttrain-logloss:8.64510\ttrain-auc:0.76544\ttrain-error:0.23466\ttest-logloss:12.22042\ttest-auc:0.66804\ttest-error:0.33170\n",
      "[469]\ttrain-logloss:8.62902\ttrain-auc:0.76588\ttrain-error:0.23422\ttest-logloss:12.24615\ttest-auc:0.66734\ttest-error:0.33240\n",
      "[470]\ttrain-logloss:8.62580\ttrain-auc:0.76596\ttrain-error:0.23413\ttest-logloss:12.23329\ttest-auc:0.66769\ttest-error:0.33205\n",
      "[471]\ttrain-logloss:8.61615\ttrain-auc:0.76623\ttrain-error:0.23387\ttest-logloss:12.19470\ttest-auc:0.66875\ttest-error:0.33101\n",
      "[472]\ttrain-logloss:8.61937\ttrain-auc:0.76614\ttrain-error:0.23396\ttest-logloss:12.20113\ttest-auc:0.66858\ttest-error:0.33118\n",
      "[473]\ttrain-logloss:8.60329\ttrain-auc:0.76658\ttrain-error:0.23352\ttest-logloss:12.20113\ttest-auc:0.66858\ttest-error:0.33118\n",
      "[474]\ttrain-logloss:8.59364\ttrain-auc:0.76684\ttrain-error:0.23326\ttest-logloss:12.21399\ttest-auc:0.66822\ttest-error:0.33153\n",
      "[475]\ttrain-logloss:8.60972\ttrain-auc:0.76640\ttrain-error:0.23370\ttest-logloss:12.21399\ttest-auc:0.66822\ttest-error:0.33153\n",
      "[476]\ttrain-logloss:8.60007\ttrain-auc:0.76666\ttrain-error:0.23344\ttest-logloss:12.22042\ttest-auc:0.66805\ttest-error:0.33170\n",
      "[477]\ttrain-logloss:8.60329\ttrain-auc:0.76657\ttrain-error:0.23352\ttest-logloss:12.20113\ttest-auc:0.66858\ttest-error:0.33118\n",
      "[478]\ttrain-logloss:8.59525\ttrain-auc:0.76679\ttrain-error:0.23330\ttest-logloss:12.21399\ttest-auc:0.66822\ttest-error:0.33153\n",
      "[479]\ttrain-logloss:8.58077\ttrain-auc:0.76719\ttrain-error:0.23291\ttest-logloss:12.22042\ttest-auc:0.66804\ttest-error:0.33170\n",
      "[480]\ttrain-logloss:8.57916\ttrain-auc:0.76723\ttrain-error:0.23287\ttest-logloss:12.22686\ttest-auc:0.66787\ttest-error:0.33188\n",
      "[481]\ttrain-logloss:8.58399\ttrain-auc:0.76710\ttrain-error:0.23300\ttest-logloss:12.22686\ttest-auc:0.66787\ttest-error:0.33188\n",
      "[482]\ttrain-logloss:8.57756\ttrain-auc:0.76727\ttrain-error:0.23282\ttest-logloss:12.18183\ttest-auc:0.66909\ttest-error:0.33066\n",
      "[483]\ttrain-logloss:8.57273\ttrain-auc:0.76740\ttrain-error:0.23269\ttest-logloss:12.18183\ttest-auc:0.66909\ttest-error:0.33066\n",
      "[484]\ttrain-logloss:8.56791\ttrain-auc:0.76753\ttrain-error:0.23256\ttest-logloss:12.17540\ttest-auc:0.66927\ttest-error:0.33048\n",
      "[485]\ttrain-logloss:8.56952\ttrain-auc:0.76749\ttrain-error:0.23261\ttest-logloss:12.19470\ttest-auc:0.66875\ttest-error:0.33101\n",
      "[486]\ttrain-logloss:8.56308\ttrain-auc:0.76767\ttrain-error:0.23243\ttest-logloss:12.16897\ttest-auc:0.66944\ttest-error:0.33031\n",
      "[487]\ttrain-logloss:8.54861\ttrain-auc:0.76806\ttrain-error:0.23204\ttest-logloss:12.20113\ttest-auc:0.66857\ttest-error:0.33118\n",
      "[488]\ttrain-logloss:8.54379\ttrain-auc:0.76819\ttrain-error:0.23191\ttest-logloss:12.18183\ttest-auc:0.66909\ttest-error:0.33066\n",
      "[489]\ttrain-logloss:8.54379\ttrain-auc:0.76819\ttrain-error:0.23191\ttest-logloss:12.16254\ttest-auc:0.66962\ttest-error:0.33013\n",
      "[490]\ttrain-logloss:8.53253\ttrain-auc:0.76849\ttrain-error:0.23160\ttest-logloss:12.18183\ttest-auc:0.66910\ttest-error:0.33066\n",
      "[491]\ttrain-logloss:8.51163\ttrain-auc:0.76906\ttrain-error:0.23103\ttest-logloss:12.22042\ttest-auc:0.66805\ttest-error:0.33170\n",
      "[492]\ttrain-logloss:8.50519\ttrain-auc:0.76924\ttrain-error:0.23086\ttest-logloss:12.16897\ttest-auc:0.66944\ttest-error:0.33031\n",
      "[493]\ttrain-logloss:8.50680\ttrain-auc:0.76919\ttrain-error:0.23090\ttest-logloss:12.16897\ttest-auc:0.66944\ttest-error:0.33031\n",
      "[494]\ttrain-logloss:8.49394\ttrain-auc:0.76954\ttrain-error:0.23055\ttest-logloss:12.17540\ttest-auc:0.66927\ttest-error:0.33048\n",
      "[495]\ttrain-logloss:8.47464\ttrain-auc:0.77007\ttrain-error:0.23003\ttest-logloss:12.20756\ttest-auc:0.66840\ttest-error:0.33136\n",
      "[496]\ttrain-logloss:8.47785\ttrain-auc:0.76998\ttrain-error:0.23012\ttest-logloss:12.22042\ttest-auc:0.66805\ttest-error:0.33170\n",
      "[497]\ttrain-logloss:8.47625\ttrain-auc:0.77002\ttrain-error:0.23007\ttest-logloss:12.20113\ttest-auc:0.66858\ttest-error:0.33118\n",
      "[498]\ttrain-logloss:8.46499\ttrain-auc:0.77033\ttrain-error:0.22977\ttest-logloss:12.20756\ttest-auc:0.66841\ttest-error:0.33136\n",
      "[499]\ttrain-logloss:8.46660\ttrain-auc:0.77028\ttrain-error:0.22981\ttest-logloss:12.22686\ttest-auc:0.66789\ttest-error:0.33188\n",
      "[500]\ttrain-logloss:8.45213\ttrain-auc:0.77067\ttrain-error:0.22942\ttest-logloss:12.24615\ttest-auc:0.66736\ttest-error:0.33240\n",
      "[501]\ttrain-logloss:8.43765\ttrain-auc:0.77107\ttrain-error:0.22903\ttest-logloss:12.25901\ttest-auc:0.66701\ttest-error:0.33275\n",
      "[502]\ttrain-logloss:8.42640\ttrain-auc:0.77138\ttrain-error:0.22872\ttest-logloss:12.25258\ttest-auc:0.66719\ttest-error:0.33258\n",
      "[503]\ttrain-logloss:8.42318\ttrain-auc:0.77146\ttrain-error:0.22863\ttest-logloss:12.28474\ttest-auc:0.66631\ttest-error:0.33345\n",
      "[504]\ttrain-logloss:8.41514\ttrain-auc:0.77168\ttrain-error:0.22842\ttest-logloss:12.27831\ttest-auc:0.66648\ttest-error:0.33327\n",
      "[505]\ttrain-logloss:8.41353\ttrain-auc:0.77172\ttrain-error:0.22837\ttest-logloss:12.29117\ttest-auc:0.66613\ttest-error:0.33362\n",
      "[506]\ttrain-logloss:8.40388\ttrain-auc:0.77199\ttrain-error:0.22811\ttest-logloss:12.30404\ttest-auc:0.66578\ttest-error:0.33397\n",
      "[507]\ttrain-logloss:8.37655\ttrain-auc:0.77273\ttrain-error:0.22737\ttest-logloss:12.27188\ttest-auc:0.66665\ttest-error:0.33310\n",
      "[508]\ttrain-logloss:8.37976\ttrain-auc:0.77264\ttrain-error:0.22745\ttest-logloss:12.24615\ttest-auc:0.66736\ttest-error:0.33240\n",
      "[509]\ttrain-logloss:8.37172\ttrain-auc:0.77286\ttrain-error:0.22724\ttest-logloss:12.27831\ttest-auc:0.66649\ttest-error:0.33327\n",
      "[510]\ttrain-logloss:8.36368\ttrain-auc:0.77308\ttrain-error:0.22702\ttest-logloss:12.30404\ttest-auc:0.66579\ttest-error:0.33397\n",
      "[511]\ttrain-logloss:8.36207\ttrain-auc:0.77312\ttrain-error:0.22698\ttest-logloss:12.31690\ttest-auc:0.66544\ttest-error:0.33432\n",
      "[512]\ttrain-logloss:8.36046\ttrain-auc:0.77316\ttrain-error:0.22693\ttest-logloss:12.31047\ttest-auc:0.66561\ttest-error:0.33415\n",
      "[513]\ttrain-logloss:8.35886\ttrain-auc:0.77321\ttrain-error:0.22689\ttest-logloss:12.30404\ttest-auc:0.66579\ttest-error:0.33397\n",
      "[514]\ttrain-logloss:8.35564\ttrain-auc:0.77329\ttrain-error:0.22680\ttest-logloss:12.31047\ttest-auc:0.66561\ttest-error:0.33415\n",
      "[515]\ttrain-logloss:8.34117\ttrain-auc:0.77369\ttrain-error:0.22641\ttest-logloss:12.30404\ttest-auc:0.66579\ttest-error:0.33397\n",
      "[516]\ttrain-logloss:8.32187\ttrain-auc:0.77421\ttrain-error:0.22588\ttest-logloss:12.31047\ttest-auc:0.66562\ttest-error:0.33415\n",
      "[517]\ttrain-logloss:8.31865\ttrain-auc:0.77430\ttrain-error:0.22580\ttest-logloss:12.31047\ttest-auc:0.66562\ttest-error:0.33415\n",
      "[518]\ttrain-logloss:8.32187\ttrain-auc:0.77421\ttrain-error:0.22588\ttest-logloss:12.32333\ttest-auc:0.66527\ttest-error:0.33450\n",
      "[519]\ttrain-logloss:8.31865\ttrain-auc:0.77430\ttrain-error:0.22580\ttest-logloss:12.30404\ttest-auc:0.66580\ttest-error:0.33397\n",
      "[520]\ttrain-logloss:8.32669\ttrain-auc:0.77408\ttrain-error:0.22601\ttest-logloss:12.29117\ttest-auc:0.66616\ttest-error:0.33362\n",
      "[521]\ttrain-logloss:8.31061\ttrain-auc:0.77451\ttrain-error:0.22558\ttest-logloss:12.24615\ttest-auc:0.66739\ttest-error:0.33240\n",
      "[522]\ttrain-logloss:8.29132\ttrain-auc:0.77504\ttrain-error:0.22506\ttest-logloss:12.25258\ttest-auc:0.66720\ttest-error:0.33258\n",
      "[523]\ttrain-logloss:8.29936\ttrain-auc:0.77482\ttrain-error:0.22527\ttest-logloss:12.30404\ttest-auc:0.66580\ttest-error:0.33397\n",
      "[524]\ttrain-logloss:8.29453\ttrain-auc:0.77495\ttrain-error:0.22514\ttest-logloss:12.29761\ttest-auc:0.66597\ttest-error:0.33380\n",
      "[525]\ttrain-logloss:8.29132\ttrain-auc:0.77504\ttrain-error:0.22506\ttest-logloss:12.32976\ttest-auc:0.66510\ttest-error:0.33467\n",
      "[526]\ttrain-logloss:8.27524\ttrain-auc:0.77548\ttrain-error:0.22462\ttest-logloss:12.30404\ttest-auc:0.66580\ttest-error:0.33397\n",
      "[527]\ttrain-logloss:8.28328\ttrain-auc:0.77526\ttrain-error:0.22484\ttest-logloss:12.31047\ttest-auc:0.66562\ttest-error:0.33415\n",
      "[528]\ttrain-logloss:8.28488\ttrain-auc:0.77521\ttrain-error:0.22488\ttest-logloss:12.31690\ttest-auc:0.66544\ttest-error:0.33432\n",
      "[529]\ttrain-logloss:8.27202\ttrain-auc:0.77556\ttrain-error:0.22453\ttest-logloss:12.30404\ttest-auc:0.66579\ttest-error:0.33397\n",
      "[530]\ttrain-logloss:8.27202\ttrain-auc:0.77556\ttrain-error:0.22453\ttest-logloss:12.33620\ttest-auc:0.66491\ttest-error:0.33485\n",
      "[531]\ttrain-logloss:8.24790\ttrain-auc:0.77622\ttrain-error:0.22388\ttest-logloss:12.32976\ttest-auc:0.66509\ttest-error:0.33467\n",
      "[532]\ttrain-logloss:8.24629\ttrain-auc:0.77626\ttrain-error:0.22383\ttest-logloss:12.35549\ttest-auc:0.66439\ttest-error:0.33537\n",
      "[533]\ttrain-logloss:8.25755\ttrain-auc:0.77596\ttrain-error:0.22414\ttest-logloss:12.35549\ttest-auc:0.66439\ttest-error:0.33537\n",
      "[534]\ttrain-logloss:8.26076\ttrain-auc:0.77587\ttrain-error:0.22423\ttest-logloss:12.36192\ttest-auc:0.66422\ttest-error:0.33554\n",
      "[535]\ttrain-logloss:8.24790\ttrain-auc:0.77622\ttrain-error:0.22388\ttest-logloss:12.34263\ttest-auc:0.66475\ttest-error:0.33502\n",
      "[536]\ttrain-logloss:8.24147\ttrain-auc:0.77639\ttrain-error:0.22370\ttest-logloss:12.31690\ttest-auc:0.66545\ttest-error:0.33432\n",
      "[537]\ttrain-logloss:8.24790\ttrain-auc:0.77622\ttrain-error:0.22388\ttest-logloss:12.27188\ttest-auc:0.66668\ttest-error:0.33310\n",
      "[538]\ttrain-logloss:8.24147\ttrain-auc:0.77639\ttrain-error:0.22370\ttest-logloss:12.25901\ttest-auc:0.66702\ttest-error:0.33275\n",
      "[539]\ttrain-logloss:8.24951\ttrain-auc:0.77617\ttrain-error:0.22392\ttest-logloss:12.27188\ttest-auc:0.66668\ttest-error:0.33310\n",
      "[540]\ttrain-logloss:8.24951\ttrain-auc:0.77617\ttrain-error:0.22392\ttest-logloss:12.28474\ttest-auc:0.66632\ttest-error:0.33345\n",
      "[541]\ttrain-logloss:8.24147\ttrain-auc:0.77639\ttrain-error:0.22370\ttest-logloss:12.26545\ttest-auc:0.66685\ttest-error:0.33293\n",
      "[542]\ttrain-logloss:8.23503\ttrain-auc:0.77657\ttrain-error:0.22353\ttest-logloss:12.27188\ttest-auc:0.66668\ttest-error:0.33310\n",
      "[543]\ttrain-logloss:8.22056\ttrain-auc:0.77696\ttrain-error:0.22313\ttest-logloss:12.29761\ttest-auc:0.66598\ttest-error:0.33380\n",
      "[544]\ttrain-logloss:8.23182\ttrain-auc:0.77665\ttrain-error:0.22344\ttest-logloss:12.30404\ttest-auc:0.66580\ttest-error:0.33397\n",
      "[545]\ttrain-logloss:8.21734\ttrain-auc:0.77705\ttrain-error:0.22305\ttest-logloss:12.30404\ttest-auc:0.66580\ttest-error:0.33397\n",
      "[546]\ttrain-logloss:8.22860\ttrain-auc:0.77674\ttrain-error:0.22335\ttest-logloss:12.32333\ttest-auc:0.66528\ttest-error:0.33450\n",
      "[547]\ttrain-logloss:8.22538\ttrain-auc:0.77683\ttrain-error:0.22326\ttest-logloss:12.32333\ttest-auc:0.66528\ttest-error:0.33450\n",
      "[548]\ttrain-logloss:8.22699\ttrain-auc:0.77678\ttrain-error:0.22331\ttest-logloss:12.32976\ttest-auc:0.66511\ttest-error:0.33467\n",
      "[549]\ttrain-logloss:8.23021\ttrain-auc:0.77670\ttrain-error:0.22340\ttest-logloss:12.29761\ttest-auc:0.66598\ttest-error:0.33380\n",
      "[550]\ttrain-logloss:8.20126\ttrain-auc:0.77748\ttrain-error:0.22261\ttest-logloss:12.25901\ttest-auc:0.66703\ttest-error:0.33275\n",
      "[551]\ttrain-logloss:8.18679\ttrain-auc:0.77788\ttrain-error:0.22222\ttest-logloss:12.23329\ttest-auc:0.66773\ttest-error:0.33205\n",
      "[552]\ttrain-logloss:8.18679\ttrain-auc:0.77788\ttrain-error:0.22222\ttest-logloss:12.25901\ttest-auc:0.66703\ttest-error:0.33275\n",
      "[553]\ttrain-logloss:8.17714\ttrain-auc:0.77814\ttrain-error:0.22196\ttest-logloss:12.27188\ttest-auc:0.66668\ttest-error:0.33310\n",
      "[554]\ttrain-logloss:8.17714\ttrain-auc:0.77814\ttrain-error:0.22196\ttest-logloss:12.25901\ttest-auc:0.66703\ttest-error:0.33275\n",
      "[555]\ttrain-logloss:8.17553\ttrain-auc:0.77818\ttrain-error:0.22191\ttest-logloss:12.21399\ttest-auc:0.66825\ttest-error:0.33153\n",
      "[556]\ttrain-logloss:8.16106\ttrain-auc:0.77857\ttrain-error:0.22152\ttest-logloss:12.20756\ttest-auc:0.66842\ttest-error:0.33136\n",
      "[557]\ttrain-logloss:8.16749\ttrain-auc:0.77840\ttrain-error:0.22169\ttest-logloss:12.19470\ttest-auc:0.66877\ttest-error:0.33101\n",
      "[558]\ttrain-logloss:8.13051\ttrain-auc:0.77940\ttrain-error:0.22069\ttest-logloss:12.23972\ttest-auc:0.66755\ttest-error:0.33223\n",
      "[559]\ttrain-logloss:8.12247\ttrain-auc:0.77962\ttrain-error:0.22047\ttest-logloss:12.27831\ttest-auc:0.66650\ttest-error:0.33327\n",
      "[560]\ttrain-logloss:8.13051\ttrain-auc:0.77940\ttrain-error:0.22069\ttest-logloss:12.26545\ttest-auc:0.66685\ttest-error:0.33293\n",
      "[561]\ttrain-logloss:8.12407\ttrain-auc:0.77958\ttrain-error:0.22051\ttest-logloss:12.23329\ttest-auc:0.66772\ttest-error:0.33205\n",
      "[562]\ttrain-logloss:8.12086\ttrain-auc:0.77967\ttrain-error:0.22043\ttest-logloss:12.22686\ttest-auc:0.66789\ttest-error:0.33188\n",
      "[563]\ttrain-logloss:8.12086\ttrain-auc:0.77967\ttrain-error:0.22043\ttest-logloss:12.25901\ttest-auc:0.66702\ttest-error:0.33275\n",
      "[564]\ttrain-logloss:8.12407\ttrain-auc:0.77958\ttrain-error:0.22051\ttest-logloss:12.22686\ttest-auc:0.66790\ttest-error:0.33188\n",
      "[565]\ttrain-logloss:8.12086\ttrain-auc:0.77966\ttrain-error:0.22043\ttest-logloss:12.23329\ttest-auc:0.66772\ttest-error:0.33205\n",
      "[566]\ttrain-logloss:8.11121\ttrain-auc:0.77993\ttrain-error:0.22017\ttest-logloss:12.20113\ttest-auc:0.66859\ttest-error:0.33118\n",
      "[567]\ttrain-logloss:8.09835\ttrain-auc:0.78028\ttrain-error:0.21982\ttest-logloss:12.18183\ttest-auc:0.66912\ttest-error:0.33066\n",
      "[568]\ttrain-logloss:8.09835\ttrain-auc:0.78028\ttrain-error:0.21982\ttest-logloss:12.20113\ttest-auc:0.66860\ttest-error:0.33118\n",
      "[569]\ttrain-logloss:8.10156\ttrain-auc:0.78019\ttrain-error:0.21990\ttest-logloss:12.18183\ttest-auc:0.66912\ttest-error:0.33066\n",
      "[570]\ttrain-logloss:8.08387\ttrain-auc:0.78067\ttrain-error:0.21942\ttest-logloss:12.16254\ttest-auc:0.66964\ttest-error:0.33013\n",
      "[571]\ttrain-logloss:8.09031\ttrain-auc:0.78049\ttrain-error:0.21960\ttest-logloss:12.18183\ttest-auc:0.66911\ttest-error:0.33066\n",
      "[572]\ttrain-logloss:8.09031\ttrain-auc:0.78049\ttrain-error:0.21960\ttest-logloss:12.18826\ttest-auc:0.66894\ttest-error:0.33083\n",
      "[573]\ttrain-logloss:8.07583\ttrain-auc:0.78089\ttrain-error:0.21921\ttest-logloss:12.20113\ttest-auc:0.66859\ttest-error:0.33118\n",
      "[574]\ttrain-logloss:8.06618\ttrain-auc:0.78115\ttrain-error:0.21894\ttest-logloss:12.21399\ttest-auc:0.66824\ttest-error:0.33153\n",
      "[575]\ttrain-logloss:8.06940\ttrain-auc:0.78106\ttrain-error:0.21903\ttest-logloss:12.20756\ttest-auc:0.66842\ttest-error:0.33136\n",
      "[576]\ttrain-logloss:8.05814\ttrain-auc:0.78137\ttrain-error:0.21873\ttest-logloss:12.21399\ttest-auc:0.66825\ttest-error:0.33153\n",
      "[577]\ttrain-logloss:8.05010\ttrain-auc:0.78159\ttrain-error:0.21851\ttest-logloss:12.20113\ttest-auc:0.66859\ttest-error:0.33118\n",
      "[578]\ttrain-logloss:8.05654\ttrain-auc:0.78141\ttrain-error:0.21868\ttest-logloss:12.19470\ttest-auc:0.66876\ttest-error:0.33101\n",
      "[579]\ttrain-logloss:8.04528\ttrain-auc:0.78172\ttrain-error:0.21838\ttest-logloss:12.21399\ttest-auc:0.66823\ttest-error:0.33153\n",
      "[580]\ttrain-logloss:8.03885\ttrain-auc:0.78189\ttrain-error:0.21820\ttest-logloss:12.19470\ttest-auc:0.66876\ttest-error:0.33101\n",
      "[581]\ttrain-logloss:8.02598\ttrain-auc:0.78224\ttrain-error:0.21785\ttest-logloss:12.20113\ttest-auc:0.66858\ttest-error:0.33118\n",
      "[582]\ttrain-logloss:8.01794\ttrain-auc:0.78246\ttrain-error:0.21763\ttest-logloss:12.19470\ttest-auc:0.66875\ttest-error:0.33101\n",
      "[583]\ttrain-logloss:8.00990\ttrain-auc:0.78268\ttrain-error:0.21742\ttest-logloss:12.20756\ttest-auc:0.66840\ttest-error:0.33136\n",
      "[584]\ttrain-logloss:8.01633\ttrain-auc:0.78250\ttrain-error:0.21759\ttest-logloss:12.25258\ttest-auc:0.66719\ttest-error:0.33258\n",
      "[585]\ttrain-logloss:8.01955\ttrain-auc:0.78241\ttrain-error:0.21768\ttest-logloss:12.24615\ttest-auc:0.66737\ttest-error:0.33240\n",
      "[586]\ttrain-logloss:8.02116\ttrain-auc:0.78237\ttrain-error:0.21772\ttest-logloss:12.23329\ttest-auc:0.66772\ttest-error:0.33205\n",
      "[587]\ttrain-logloss:8.00668\ttrain-auc:0.78276\ttrain-error:0.21733\ttest-logloss:12.24615\ttest-auc:0.66737\ttest-error:0.33240\n",
      "[588]\ttrain-logloss:7.99543\ttrain-auc:0.78307\ttrain-error:0.21702\ttest-logloss:12.22686\ttest-auc:0.66789\ttest-error:0.33188\n",
      "[589]\ttrain-logloss:7.98739\ttrain-auc:0.78329\ttrain-error:0.21680\ttest-logloss:12.23972\ttest-auc:0.66754\ttest-error:0.33223\n",
      "[590]\ttrain-logloss:7.98900\ttrain-auc:0.78324\ttrain-error:0.21685\ttest-logloss:12.22686\ttest-auc:0.66789\ttest-error:0.33188\n",
      "[591]\ttrain-logloss:7.98095\ttrain-auc:0.78346\ttrain-error:0.21663\ttest-logloss:12.23329\ttest-auc:0.66772\ttest-error:0.33205\n",
      "[592]\ttrain-logloss:7.97935\ttrain-auc:0.78351\ttrain-error:0.21659\ttest-logloss:12.24615\ttest-auc:0.66738\ttest-error:0.33240\n",
      "[593]\ttrain-logloss:7.95201\ttrain-auc:0.78425\ttrain-error:0.21585\ttest-logloss:12.22686\ttest-auc:0.66790\ttest-error:0.33188\n",
      "[594]\ttrain-logloss:7.95362\ttrain-auc:0.78421\ttrain-error:0.21589\ttest-logloss:12.22042\ttest-auc:0.66807\ttest-error:0.33170\n",
      "[595]\ttrain-logloss:7.94236\ttrain-auc:0.78451\ttrain-error:0.21558\ttest-logloss:12.20756\ttest-auc:0.66842\ttest-error:0.33136\n",
      "[596]\ttrain-logloss:7.94558\ttrain-auc:0.78442\ttrain-error:0.21567\ttest-logloss:12.22042\ttest-auc:0.66806\ttest-error:0.33170\n",
      "[597]\ttrain-logloss:7.95040\ttrain-auc:0.78429\ttrain-error:0.21580\ttest-logloss:12.21399\ttest-auc:0.66823\ttest-error:0.33153\n",
      "[598]\ttrain-logloss:7.94558\ttrain-auc:0.78442\ttrain-error:0.21567\ttest-logloss:12.23329\ttest-auc:0.66771\ttest-error:0.33205\n",
      "[599]\ttrain-logloss:7.94075\ttrain-auc:0.78456\ttrain-error:0.21554\ttest-logloss:12.26545\ttest-auc:0.66684\ttest-error:0.33293\n",
      "600-rounds Training finished ...\t\t(2.615s)\n"
     ]
    }
   ],
   "source": [
    "num_round = 600\n",
    "t0 = time()\n",
    "sh_bst_sm = xgb.train(param_sh, xg_train_sh, num_round, watchlist)\n",
    "print(f\"{num_round}-rounds Training finished ...\\t\\t({time()-t0:.3f}s)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test error using softmax = 0.3329259776536313\n"
     ]
    }
   ],
   "source": [
    "# get prediction\n",
    "pred_sh = sh_bst_sm.predict(xg_test_sh)\n",
    "error_rate = np.sum(pred_sh != y_test_sh) / y_test_sh.shape[0]\n",
    "print('Test error using softmax = {}'.format(error_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Counter({0.0: 2654, 1.0: 3074}), Counter({0: 2845, 1: 2883}))"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(pred_sh), Counter(y_test_sh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.63      0.65      2845\n",
      "           1       0.66      0.70      0.68      2883\n",
      "\n",
      "    accuracy                           0.67      5728\n",
      "   macro avg       0.67      0.67      0.67      5728\n",
      "weighted avg       0.67      0.67      0.67      5728\n",
      "\n"
     ]
    }
   ],
   "source": [
    "report_sh = metrics.classification_report(list(y_test_sh), list(pred_sh))\n",
    "print(report_sh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.66683815, 0.66683815])"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aucs_sh = auc(y_test_sh.astype(np.uint8), pred_sh.astype(np.uint8), [0, 1])\n",
    "aucs_sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 调参"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "base_param_sh = {  # 基本参数，不需要调参\n",
    "    'objective': 'binary:hinge',\n",
    "    'eta': 0.1,\n",
    "    'nthread': 8,\n",
    "#     'num_class': 10,\n",
    "    'gpu_id': 0,\n",
    "    'tree_method': 'gpu_hist',\n",
    "    'eval_metric': ['logloss', 'auc', 'error']\n",
    "} \n",
    "# 需要调参的参数\n",
    "ps1 = {  \n",
    "    'max_depth': list(range(5, 10, 1)),\n",
    "    'min_child_weight': list(range(1, 10, 2)),\n",
    "}\n",
    "\n",
    "ps2 = {\n",
    "    'gamma': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6]\n",
    "}\n",
    "\n",
    "ps3 = {\n",
    "    'subsample':  [i/10.0 for i in range(6,11,1)], \n",
    "    'colsample_bytree':  [i/10.0 for i in range(6,11,1)] \n",
    "}\n",
    "\n",
    "ps4 = {'reg_alpha': [0, 0.1, 0.2, 0.5, 1, 1.5, 2, 4]}\n",
    "\n",
    "\n",
    "# com_ps_sh = list(ParameterGrid(ps_sh))\n",
    "\n",
    "\n",
    "# all_params_sh = [base_param_sh.copy() for _ in range(len(com_ps_sh))] \n",
    "# for i in range(len(com_ps_sh)):\n",
    "#     all_params_sh[i].update(com_ps_sh[i])\n",
    "\n",
    "# # print(com_ps_sh)\n",
    "# print(all_params_sh.__len__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 550,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data preparing finished ...\t\t(24.924s)\n"
     ]
    }
   ],
   "source": [
    "t0 = time()\n",
    "xg_train_sh = xgb.DMatrix(X_train_sh.values, label=y_train_sh.values, enable_categorical=True)\n",
    "xg_test_sh = xgb.DMatrix(X_test_sh.values, label=y_test_sh.values, enable_categorical=True)\n",
    "print(f\"Data preparing finished ...\\t\\t({time()-t0:.3f}s)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始搜索：max_depth=[5, 6, 7, 8, 9], min_child_weight=[1, 3, 5, 7, 9]\n",
      "待搜索的参数组合数量：25\n",
      "1 / 25: 150-rounds Training finished param={'objective': 'binary:hinge', 'eta': 0.1, 'nthread': 8, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['logloss', 'auc', 'error'], 'max_depth': 5, 'min_child_weight': 1} ...\t\t(3.035s)\n",
      "2 / 25: 150-rounds Training finished param={'objective': 'binary:hinge', 'eta': 0.1, 'nthread': 8, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['logloss', 'auc', 'error'], 'max_depth': 6, 'min_child_weight': 1} ...\t\t(4.271s)\n",
      "3 / 25: 150-rounds Training finished param={'objective': 'binary:hinge', 'eta': 0.1, 'nthread': 8, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['logloss', 'auc', 'error'], 'max_depth': 7, 'min_child_weight': 1} ...\t\t(6.333s)\n",
      "4 / 25: 150-rounds Training finished param={'objective': 'binary:hinge', 'eta': 0.1, 'nthread': 8, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['logloss', 'auc', 'error'], 'max_depth': 8, 'min_child_weight': 1} ...\t\t(9.294s)\n",
      "5 / 25: 150-rounds Training finished param={'objective': 'binary:hinge', 'eta': 0.1, 'nthread': 8, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['logloss', 'auc', 'error'], 'max_depth': 9, 'min_child_weight': 1} ...\t\t(12.788s)\n",
      "6 / 25: 150-rounds Training finished param={'objective': 'binary:hinge', 'eta': 0.1, 'nthread': 8, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['logloss', 'auc', 'error'], 'max_depth': 5, 'min_child_weight': 3} ...\t\t(2.993s)\n",
      "7 / 25: 150-rounds Training finished param={'objective': 'binary:hinge', 'eta': 0.1, 'nthread': 8, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['logloss', 'auc', 'error'], 'max_depth': 6, 'min_child_weight': 3} ...\t\t(4.183s)\n",
      "8 / 25: 150-rounds Training finished param={'objective': 'binary:hinge', 'eta': 0.1, 'nthread': 8, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['logloss', 'auc', 'error'], 'max_depth': 7, 'min_child_weight': 3} ...\t\t(6.127s)\n",
      "9 / 25: 150-rounds Training finished param={'objective': 'binary:hinge', 'eta': 0.1, 'nthread': 8, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['logloss', 'auc', 'error'], 'max_depth': 8, 'min_child_weight': 3} ...\t\t(9.131s)\n",
      "10 / 25: 150-rounds Training finished param={'objective': 'binary:hinge', 'eta': 0.1, 'nthread': 8, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['logloss', 'auc', 'error'], 'max_depth': 9, 'min_child_weight': 3} ...\t\t(12.637s)\n",
      "11 / 25: 150-rounds Training finished param={'objective': 'binary:hinge', 'eta': 0.1, 'nthread': 8, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['logloss', 'auc', 'error'], 'max_depth': 5, 'min_child_weight': 5} ...\t\t(2.996s)\n",
      "12 / 25: 150-rounds Training finished param={'objective': 'binary:hinge', 'eta': 0.1, 'nthread': 8, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['logloss', 'auc', 'error'], 'max_depth': 6, 'min_child_weight': 5} ...\t\t(4.244s)\n",
      "13 / 25: 150-rounds Training finished param={'objective': 'binary:hinge', 'eta': 0.1, 'nthread': 8, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['logloss', 'auc', 'error'], 'max_depth': 7, 'min_child_weight': 5} ...\t\t(6.028s)\n",
      "14 / 25: 150-rounds Training finished param={'objective': 'binary:hinge', 'eta': 0.1, 'nthread': 8, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['logloss', 'auc', 'error'], 'max_depth': 8, 'min_child_weight': 5} ...\t\t(8.756s)\n",
      "15 / 25: 150-rounds Training finished param={'objective': 'binary:hinge', 'eta': 0.1, 'nthread': 8, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['logloss', 'auc', 'error'], 'max_depth': 9, 'min_child_weight': 5} ...\t\t(12.128s)\n",
      "16 / 25: 150-rounds Training finished param={'objective': 'binary:hinge', 'eta': 0.1, 'nthread': 8, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['logloss', 'auc', 'error'], 'max_depth': 5, 'min_child_weight': 7} ...\t\t(2.959s)\n",
      "17 / 25: 150-rounds Training finished param={'objective': 'binary:hinge', 'eta': 0.1, 'nthread': 8, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['logloss', 'auc', 'error'], 'max_depth': 6, 'min_child_weight': 7} ...\t\t(4.105s)\n",
      "18 / 25: 150-rounds Training finished param={'objective': 'binary:hinge', 'eta': 0.1, 'nthread': 8, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['logloss', 'auc', 'error'], 'max_depth': 7, 'min_child_weight': 7} ...\t\t(5.897s)\n",
      "19 / 25: 150-rounds Training finished param={'objective': 'binary:hinge', 'eta': 0.1, 'nthread': 8, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['logloss', 'auc', 'error'], 'max_depth': 8, 'min_child_weight': 7} ...\t\t(8.433s)\n",
      "20 / 25: 150-rounds Training finished param={'objective': 'binary:hinge', 'eta': 0.1, 'nthread': 8, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['logloss', 'auc', 'error'], 'max_depth': 9, 'min_child_weight': 7} ...\t\t(11.471s)\n",
      "21 / 25: 150-rounds Training finished param={'objective': 'binary:hinge', 'eta': 0.1, 'nthread': 8, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['logloss', 'auc', 'error'], 'max_depth': 5, 'min_child_weight': 9} ...\t\t(2.935s)\n",
      "22 / 25: 150-rounds Training finished param={'objective': 'binary:hinge', 'eta': 0.1, 'nthread': 8, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['logloss', 'auc', 'error'], 'max_depth': 6, 'min_child_weight': 9} ...\t\t(4.050s)\n",
      "23 / 25: 150-rounds Training finished param={'objective': 'binary:hinge', 'eta': 0.1, 'nthread': 8, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['logloss', 'auc', 'error'], 'max_depth': 7, 'min_child_weight': 9} ...\t\t(5.802s)\n",
      "24 / 25: 150-rounds Training finished param={'objective': 'binary:hinge', 'eta': 0.1, 'nthread': 8, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['logloss', 'auc', 'error'], 'max_depth': 8, 'min_child_weight': 9} ...\t\t(8.152s)\n",
      "25 / 25: 150-rounds Training finished param={'objective': 'binary:hinge', 'eta': 0.1, 'nthread': 8, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['logloss', 'auc', 'error'], 'max_depth': 9, 'min_child_weight': 9} ...\t\t(11.286s)\n",
      "eta=0.1, eval_metric=['logloss', 'auc', 'error'], gpu_id=0, max_depth=5, min_child_weight=1, nthread=8, objective=binary:hinge, tree_method=gpu_hist\n",
      "eta=0.1, eval_metric=['logloss', 'auc', 'error'], gpu_id=0, max_depth=7, min_child_weight=9, nthread=8, objective=binary:hinge, tree_method=gpu_hist\n",
      "开始搜索：gamma=[0.1, 0.2, 0.3, 0.4, 0.5, 0.6]\n",
      "待搜索的参数组合数量：6\n",
      "1 / 6: 150-rounds Training finished param={'objective': 'binary:hinge', 'eta': 0.1, 'nthread': 8, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['logloss', 'auc', 'error'], 'max_depth': 5, 'min_child_weight': 1, 'gamma': 0.1} ...\t\t(3.033s)\n",
      "2 / 6: 150-rounds Training finished param={'objective': 'binary:hinge', 'eta': 0.1, 'nthread': 8, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['logloss', 'auc', 'error'], 'max_depth': 5, 'min_child_weight': 1, 'gamma': 0.2} ...\t\t(3.015s)\n",
      "3 / 6: 150-rounds Training finished param={'objective': 'binary:hinge', 'eta': 0.1, 'nthread': 8, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['logloss', 'auc', 'error'], 'max_depth': 5, 'min_child_weight': 1, 'gamma': 0.3} ...\t\t(3.015s)\n",
      "4 / 6: 150-rounds Training finished param={'objective': 'binary:hinge', 'eta': 0.1, 'nthread': 8, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['logloss', 'auc', 'error'], 'max_depth': 5, 'min_child_weight': 1, 'gamma': 0.4} ...\t\t(3.057s)\n",
      "5 / 6: 150-rounds Training finished param={'objective': 'binary:hinge', 'eta': 0.1, 'nthread': 8, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['logloss', 'auc', 'error'], 'max_depth': 5, 'min_child_weight': 1, 'gamma': 0.5} ...\t\t(3.028s)\n",
      "6 / 6: 150-rounds Training finished param={'objective': 'binary:hinge', 'eta': 0.1, 'nthread': 8, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['logloss', 'auc', 'error'], 'max_depth': 5, 'min_child_weight': 1, 'gamma': 0.6} ...\t\t(3.047s)\n",
      "eta=0.1, eval_metric=['logloss', 'auc', 'error'], gamma=0.1, gpu_id=0, max_depth=5, min_child_weight=1, nthread=8, objective=binary:hinge, tree_method=gpu_hist\n",
      "eta=0.1, eval_metric=['logloss', 'auc', 'error'], gamma=0.3, gpu_id=0, max_depth=5, min_child_weight=1, nthread=8, objective=binary:hinge, tree_method=gpu_hist\n",
      "开始搜索：colsample_bytree=[0.6, 0.7, 0.8, 0.9, 1.0], subsample=[0.6, 0.7, 0.8, 0.9, 1.0]\n",
      "待搜索的参数组合数量：25\n",
      "1 / 25: 150-rounds Training finished param={'objective': 'binary:hinge', 'eta': 0.1, 'nthread': 8, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['logloss', 'auc', 'error'], 'max_depth': 5, 'min_child_weight': 1, 'gamma': 0.1, 'subsample': 0.6, 'colsample_bytree': 0.6} ...\t\t(2.884s)\n",
      "2 / 25: 150-rounds Training finished param={'objective': 'binary:hinge', 'eta': 0.1, 'nthread': 8, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['logloss', 'auc', 'error'], 'max_depth': 5, 'min_child_weight': 1, 'gamma': 0.1, 'subsample': 0.7, 'colsample_bytree': 0.6} ...\t\t(2.922s)\n",
      "3 / 25: 150-rounds Training finished param={'objective': 'binary:hinge', 'eta': 0.1, 'nthread': 8, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['logloss', 'auc', 'error'], 'max_depth': 5, 'min_child_weight': 1, 'gamma': 0.1, 'subsample': 0.8, 'colsample_bytree': 0.6} ...\t\t(2.947s)\n",
      "4 / 25: 150-rounds Training finished param={'objective': 'binary:hinge', 'eta': 0.1, 'nthread': 8, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['logloss', 'auc', 'error'], 'max_depth': 5, 'min_child_weight': 1, 'gamma': 0.1, 'subsample': 0.9, 'colsample_bytree': 0.6} ...\t\t(2.897s)\n",
      "5 / 25: 150-rounds Training finished param={'objective': 'binary:hinge', 'eta': 0.1, 'nthread': 8, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['logloss', 'auc', 'error'], 'max_depth': 5, 'min_child_weight': 1, 'gamma': 0.1, 'subsample': 1.0, 'colsample_bytree': 0.6} ...\t\t(2.930s)\n",
      "6 / 25: 150-rounds Training finished param={'objective': 'binary:hinge', 'eta': 0.1, 'nthread': 8, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['logloss', 'auc', 'error'], 'max_depth': 5, 'min_child_weight': 1, 'gamma': 0.1, 'subsample': 0.6, 'colsample_bytree': 0.7} ...\t\t(2.918s)\n",
      "7 / 25: 150-rounds Training finished param={'objective': 'binary:hinge', 'eta': 0.1, 'nthread': 8, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['logloss', 'auc', 'error'], 'max_depth': 5, 'min_child_weight': 1, 'gamma': 0.1, 'subsample': 0.7, 'colsample_bytree': 0.7} ...\t\t(2.911s)\n",
      "8 / 25: 150-rounds Training finished param={'objective': 'binary:hinge', 'eta': 0.1, 'nthread': 8, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['logloss', 'auc', 'error'], 'max_depth': 5, 'min_child_weight': 1, 'gamma': 0.1, 'subsample': 0.8, 'colsample_bytree': 0.7} ...\t\t(2.913s)\n",
      "9 / 25: 150-rounds Training finished param={'objective': 'binary:hinge', 'eta': 0.1, 'nthread': 8, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['logloss', 'auc', 'error'], 'max_depth': 5, 'min_child_weight': 1, 'gamma': 0.1, 'subsample': 0.9, 'colsample_bytree': 0.7} ...\t\t(2.894s)\n",
      "10 / 25: 150-rounds Training finished param={'objective': 'binary:hinge', 'eta': 0.1, 'nthread': 8, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['logloss', 'auc', 'error'], 'max_depth': 5, 'min_child_weight': 1, 'gamma': 0.1, 'subsample': 1.0, 'colsample_bytree': 0.7} ...\t\t(2.918s)\n",
      "11 / 25: 150-rounds Training finished param={'objective': 'binary:hinge', 'eta': 0.1, 'nthread': 8, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['logloss', 'auc', 'error'], 'max_depth': 5, 'min_child_weight': 1, 'gamma': 0.1, 'subsample': 0.6, 'colsample_bytree': 0.8} ...\t\t(2.878s)\n",
      "12 / 25: 150-rounds Training finished param={'objective': 'binary:hinge', 'eta': 0.1, 'nthread': 8, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['logloss', 'auc', 'error'], 'max_depth': 5, 'min_child_weight': 1, 'gamma': 0.1, 'subsample': 0.7, 'colsample_bytree': 0.8} ...\t\t(2.942s)\n",
      "13 / 25: 150-rounds Training finished param={'objective': 'binary:hinge', 'eta': 0.1, 'nthread': 8, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['logloss', 'auc', 'error'], 'max_depth': 5, 'min_child_weight': 1, 'gamma': 0.1, 'subsample': 0.8, 'colsample_bytree': 0.8} ...\t\t(2.914s)\n",
      "14 / 25: 150-rounds Training finished param={'objective': 'binary:hinge', 'eta': 0.1, 'nthread': 8, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['logloss', 'auc', 'error'], 'max_depth': 5, 'min_child_weight': 1, 'gamma': 0.1, 'subsample': 0.9, 'colsample_bytree': 0.8} ...\t\t(2.922s)\n",
      "15 / 25: 150-rounds Training finished param={'objective': 'binary:hinge', 'eta': 0.1, 'nthread': 8, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['logloss', 'auc', 'error'], 'max_depth': 5, 'min_child_weight': 1, 'gamma': 0.1, 'subsample': 1.0, 'colsample_bytree': 0.8} ...\t\t(2.936s)\n",
      "16 / 25: 150-rounds Training finished param={'objective': 'binary:hinge', 'eta': 0.1, 'nthread': 8, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['logloss', 'auc', 'error'], 'max_depth': 5, 'min_child_weight': 1, 'gamma': 0.1, 'subsample': 0.6, 'colsample_bytree': 0.9} ...\t\t(2.921s)\n",
      "17 / 25: 150-rounds Training finished param={'objective': 'binary:hinge', 'eta': 0.1, 'nthread': 8, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['logloss', 'auc', 'error'], 'max_depth': 5, 'min_child_weight': 1, 'gamma': 0.1, 'subsample': 0.7, 'colsample_bytree': 0.9} ...\t\t(2.938s)\n",
      "18 / 25: 150-rounds Training finished param={'objective': 'binary:hinge', 'eta': 0.1, 'nthread': 8, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['logloss', 'auc', 'error'], 'max_depth': 5, 'min_child_weight': 1, 'gamma': 0.1, 'subsample': 0.8, 'colsample_bytree': 0.9} ...\t\t(2.965s)\n",
      "19 / 25: 150-rounds Training finished param={'objective': 'binary:hinge', 'eta': 0.1, 'nthread': 8, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['logloss', 'auc', 'error'], 'max_depth': 5, 'min_child_weight': 1, 'gamma': 0.1, 'subsample': 0.9, 'colsample_bytree': 0.9} ...\t\t(3.075s)\n",
      "20 / 25: 150-rounds Training finished param={'objective': 'binary:hinge', 'eta': 0.1, 'nthread': 8, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['logloss', 'auc', 'error'], 'max_depth': 5, 'min_child_weight': 1, 'gamma': 0.1, 'subsample': 1.0, 'colsample_bytree': 0.9} ...\t\t(2.903s)\n",
      "21 / 25: 150-rounds Training finished param={'objective': 'binary:hinge', 'eta': 0.1, 'nthread': 8, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['logloss', 'auc', 'error'], 'max_depth': 5, 'min_child_weight': 1, 'gamma': 0.1, 'subsample': 0.6, 'colsample_bytree': 1.0} ...\t\t(2.902s)\n",
      "22 / 25: 150-rounds Training finished param={'objective': 'binary:hinge', 'eta': 0.1, 'nthread': 8, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['logloss', 'auc', 'error'], 'max_depth': 5, 'min_child_weight': 1, 'gamma': 0.1, 'subsample': 0.7, 'colsample_bytree': 1.0} ...\t\t(2.910s)\n",
      "23 / 25: 150-rounds Training finished param={'objective': 'binary:hinge', 'eta': 0.1, 'nthread': 8, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['logloss', 'auc', 'error'], 'max_depth': 5, 'min_child_weight': 1, 'gamma': 0.1, 'subsample': 0.8, 'colsample_bytree': 1.0} ...\t\t(2.951s)\n",
      "24 / 25: 150-rounds Training finished param={'objective': 'binary:hinge', 'eta': 0.1, 'nthread': 8, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['logloss', 'auc', 'error'], 'max_depth': 5, 'min_child_weight': 1, 'gamma': 0.1, 'subsample': 0.9, 'colsample_bytree': 1.0} ...\t\t(2.914s)\n",
      "25 / 25: 150-rounds Training finished param={'objective': 'binary:hinge', 'eta': 0.1, 'nthread': 8, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['logloss', 'auc', 'error'], 'max_depth': 5, 'min_child_weight': 1, 'gamma': 0.1, 'subsample': 1.0, 'colsample_bytree': 1.0} ...\t\t(2.935s)\n",
      "colsample_bytree=0.6, eta=0.1, eval_metric=['logloss', 'auc', 'error'], gamma=0.1, gpu_id=0, max_depth=5, min_child_weight=1, nthread=8, objective=binary:hinge, subsample=0.6, tree_method=gpu_hist\n",
      "colsample_bytree=0.6, eta=0.1, eval_metric=['logloss', 'auc', 'error'], gamma=0.1, gpu_id=0, max_depth=5, min_child_weight=1, nthread=8, objective=binary:hinge, subsample=0.8, tree_method=gpu_hist\n",
      "开始搜索：reg_alpha=[0, 0.1, 0.2, 0.5, 1, 1.5, 2, 4]\n",
      "待搜索的参数组合数量：8\n",
      "1 / 8: 150-rounds Training finished param={'objective': 'binary:hinge', 'eta': 0.1, 'nthread': 8, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['logloss', 'auc', 'error'], 'max_depth': 5, 'min_child_weight': 1, 'gamma': 0.1, 'subsample': 0.6, 'colsample_bytree': 0.6, 'reg_alpha': 0} ...\t\t(2.778s)\n",
      "2 / 8: 150-rounds Training finished param={'objective': 'binary:hinge', 'eta': 0.1, 'nthread': 8, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['logloss', 'auc', 'error'], 'max_depth': 5, 'min_child_weight': 1, 'gamma': 0.1, 'subsample': 0.6, 'colsample_bytree': 0.6, 'reg_alpha': 0.1} ...\t\t(2.773s)\n",
      "3 / 8: 150-rounds Training finished param={'objective': 'binary:hinge', 'eta': 0.1, 'nthread': 8, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['logloss', 'auc', 'error'], 'max_depth': 5, 'min_child_weight': 1, 'gamma': 0.1, 'subsample': 0.6, 'colsample_bytree': 0.6, 'reg_alpha': 0.2} ...\t\t(2.782s)\n",
      "4 / 8: 150-rounds Training finished param={'objective': 'binary:hinge', 'eta': 0.1, 'nthread': 8, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['logloss', 'auc', 'error'], 'max_depth': 5, 'min_child_weight': 1, 'gamma': 0.1, 'subsample': 0.6, 'colsample_bytree': 0.6, 'reg_alpha': 0.5} ...\t\t(2.783s)\n",
      "5 / 8: 150-rounds Training finished param={'objective': 'binary:hinge', 'eta': 0.1, 'nthread': 8, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['logloss', 'auc', 'error'], 'max_depth': 5, 'min_child_weight': 1, 'gamma': 0.1, 'subsample': 0.6, 'colsample_bytree': 0.6, 'reg_alpha': 1} ...\t\t(2.803s)\n",
      "6 / 8: 150-rounds Training finished param={'objective': 'binary:hinge', 'eta': 0.1, 'nthread': 8, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['logloss', 'auc', 'error'], 'max_depth': 5, 'min_child_weight': 1, 'gamma': 0.1, 'subsample': 0.6, 'colsample_bytree': 0.6, 'reg_alpha': 1.5} ...\t\t(2.803s)\n",
      "7 / 8: 150-rounds Training finished param={'objective': 'binary:hinge', 'eta': 0.1, 'nthread': 8, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['logloss', 'auc', 'error'], 'max_depth': 5, 'min_child_weight': 1, 'gamma': 0.1, 'subsample': 0.6, 'colsample_bytree': 0.6, 'reg_alpha': 2} ...\t\t(2.817s)\n",
      "8 / 8: 150-rounds Training finished param={'objective': 'binary:hinge', 'eta': 0.1, 'nthread': 8, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['logloss', 'auc', 'error'], 'max_depth': 5, 'min_child_weight': 1, 'gamma': 0.1, 'subsample': 0.6, 'colsample_bytree': 0.6, 'reg_alpha': 4} ...\t\t(2.815s)\n",
      "colsample_bytree=0.6, eta=0.1, eval_metric=['logloss', 'auc', 'error'], gamma=0.1, gpu_id=0, max_depth=5, min_child_weight=1, nthread=8, objective=binary:hinge, reg_alpha=0, subsample=0.6, tree_method=gpu_hist\n",
      "colsample_bytree=0.6, eta=0.1, eval_metric=['logloss', 'auc', 'error'], gamma=0.1, gpu_id=0, max_depth=5, min_child_weight=1, nthread=8, objective=binary:hinge, reg_alpha=0.5, subsample=0.6, tree_method=gpu_hist\n",
      "找到的最棒的参数是：\n",
      "colsample_bytree=0.6, eta=0.1, eval_metric=['logloss', 'auc', 'error'], gamma=0.1, gpu_id=0, max_depth=5, min_child_weight=1, nthread=8, objective=binary:hinge, reg_alpha=0, subsample=0.6, tree_method=gpu_hist\n"
     ]
    }
   ],
   "source": [
    "base_sh = base_param_sh.copy()\n",
    "grids_sh = [ps1, ps2, ps3, ps4]\n",
    "\n",
    "rets_sh = []\n",
    "for grid in grids_sh:\n",
    "    params = compose_param_grid(grid, base_sh)\n",
    "    print(f\"开始搜索：{dict_2_str(grid)}\\n待搜索的参数组合数量：{len(params)}\")\n",
    "    ret = gridsearch_cv_xgb(data_sh.values, is_share_res.values, params, n_round=150, verbose_eval=False, n_class=2)\n",
    "    arr = np.array([[-e['eval-error'] for e in ret], \n",
    "                    [-e['eval-logloss'] for e in ret],\n",
    "                    [e['eval-auc'] for e in ret], \n",
    "                    [e['w_auc'] for e in ret]], dtype=np.float32)\n",
    "    opt_idxs = list(set(arr.argmax(axis=1)))\n",
    "    for i in opt_idxs:\n",
    "        print(dict_2_str(ret[i]['param']))\n",
    "    opt_idx = opt_idxs[0]\n",
    "    opt_param = ret[opt_idx]['param']\n",
    "    base_sh.update(opt_param)\n",
    "    rets_sh.append(ret)\n",
    "    \n",
    "    ks = list(grid.keys())\n",
    "    ks = '-'.join(ks)\n",
    "    with open(f\"./logs/{ks}-is_share-{int(time())}.pkl\", \"wb\") as f:\n",
    "        pickle.dump(ret, f)\n",
    "    with open(f\"./logs/{ks}-is_share-{int(time())}.md\", \"w\") as f:\n",
    "        lines = []\n",
    "        for e in ret:\n",
    "            line = metric_2_str(e)\n",
    "            lines.append(line)\n",
    "            lines.append('\\n')\n",
    "        f.write(\"\\n\".join(lines))\n",
    "\n",
    "print(f\"找到的最棒的参数是：\\n{dict_2_str(base_sh)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'objective': 'binary:hinge',\n",
       " 'eta': 0.1,\n",
       " 'nthread': 8,\n",
       " 'gpu_id': 0,\n",
       " 'tree_method': 'gpu_hist',\n",
       " 'eval_metric': ['logloss', 'auc', 'error'],\n",
       " 'max_depth': 5,\n",
       " 'min_child_weight': 1,\n",
       " 'gamma': 0.1,\n",
       " 'subsample': 0.6,\n",
       " 'colsample_bytree': 0.6,\n",
       " 'reg_alpha': 0}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gridsearch_results_sh = gridsearch_xgb(all_params_sh, xg_train_sh, xg_test_sh, num_round=150, n_class=2, verbose_eval=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_cv_results_sh = gridsearch_cv_xgb(data_sh.values, is_share_res.values, all_params_sh, n_round=150, verbose_eval=False, n_class=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 753,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('greadsearch-cv-is_share.md', 'w') as f:\n",
    "    for ret in performance:\n",
    "        f.write(f\"# {', '.join([f'{k}={v}' for k, v in ret[0].items()])}\\n\")\n",
    "        for k, v in ret[1].items():\n",
    "            is_break = '\\n' if '\\n' in str(df) else ''\n",
    "            f.write(f\"- {k} :{is_break} {v}\\n\\n\")\n",
    "        f.write(f\"{'-'*50}\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 765,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48"
      ]
     },
     "execution_count": 765,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_test_error = np.array([e[1]['mean_test_error'] for e in performance])\n",
    "mean_test_error.argmin()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 617,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'objective': 'binary:hinge',\n",
       " 'eta': 0.1,\n",
       " 'nthread': 8,\n",
       " 'gpu_id': 0,\n",
       " 'tree_method': 'gpu_hist',\n",
       " 'gamma': 0.3,\n",
       " 'max_depth': 11,\n",
       " 'min_child_weight': 9}"
      ]
     },
     "execution_count": 617,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr = np.array([[-e['test_error'] for e in gridsearch_results_sh], [e['aucs'][1] for e in gridsearch_results_sh]], dtype=np.float32)\n",
    "opt_idxs_sh = arr.argmax(axis=1)\n",
    "if opt_idxs_sh[0] != opt_idxs_sh[1]:\n",
    "    warnings.warn(f\"最小误差与最大AUC对应的模型不一致 : {opt_idxs_sh}。选择误差最小的模型 : {opt_idxs_sh[0]}\")\n",
    "\n",
    "opt_idx_sh = opt_idxs_sh[0]\n",
    "all_params_sh[opt_idx_sh]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 603,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "146"
      ]
     },
     "execution_count": 603,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opt_idx_sh = opt_idxs_sh[0]\n",
    "opt_idx_sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 619,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.54      0.44      0.49      3053\n",
      "         1.0       0.50      0.60      0.54      2835\n",
      "\n",
      "    accuracy                           0.52      5888\n",
      "   macro avg       0.52      0.52      0.51      5888\n",
      "weighted avg       0.52      0.52      0.51      5888\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(gridsearch_results_sh[opt_idx_sh]['report'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 620,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>test_error</th>\n",
       "      <th>aucs</th>\n",
       "      <th>w_auc</th>\n",
       "      <th>report</th>\n",
       "      <th>model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.503057</td>\n",
       "      <td>[0.50379590202715, 0.50379590202715]</td>\n",
       "      <td>None</td>\n",
       "      <td>precision    recall  f1-score   ...</td>\n",
       "      <td>&lt;xgboost.core.Booster object at 0x7f35ba907130&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.496943</td>\n",
       "      <td>[0.508734635779073, 0.508734635779073]</td>\n",
       "      <td>None</td>\n",
       "      <td>precision    recall  f1-score   ...</td>\n",
       "      <td>&lt;xgboost.core.Booster object at 0x7f32e4009580&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.501189</td>\n",
       "      <td>[0.5048543919272165, 0.5048543919272165]</td>\n",
       "      <td>None</td>\n",
       "      <td>precision    recall  f1-score   ...</td>\n",
       "      <td>&lt;xgboost.core.Booster object at 0x7f3583f4f4f0&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.502717</td>\n",
       "      <td>[0.502360357955947, 0.502360357955947]</td>\n",
       "      <td>None</td>\n",
       "      <td>precision    recall  f1-score   ...</td>\n",
       "      <td>&lt;xgboost.core.Booster object at 0x7f3574279550&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.499321</td>\n",
       "      <td>[0.5061017844072763, 0.5061017844072763]</td>\n",
       "      <td>None</td>\n",
       "      <td>precision    recall  f1-score   ...</td>\n",
       "      <td>&lt;xgboost.core.Booster object at 0x7f32e4082730&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>0.498132</td>\n",
       "      <td>[0.5057747576472328, 0.5057747576472328]</td>\n",
       "      <td>None</td>\n",
       "      <td>precision    recall  f1-score   ...</td>\n",
       "      <td>&lt;xgboost.core.Booster object at 0x7f32468a4640&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>0.497962</td>\n",
       "      <td>[0.5046665869463117, 0.5046665869463118]</td>\n",
       "      <td>None</td>\n",
       "      <td>precision    recall  f1-score   ...</td>\n",
       "      <td>&lt;xgboost.core.Booster object at 0x7f32468a4850&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>0.499490</td>\n",
       "      <td>[0.5026636996830249, 0.5026636996830249]</td>\n",
       "      <td>None</td>\n",
       "      <td>precision    recall  f1-score   ...</td>\n",
       "      <td>&lt;xgboost.core.Booster object at 0x7f32468a46a0&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>0.491678</td>\n",
       "      <td>[0.5107513874519006, 0.5107513874519005]</td>\n",
       "      <td>None</td>\n",
       "      <td>precision    recall  f1-score   ...</td>\n",
       "      <td>&lt;xgboost.core.Booster object at 0x7f32468a4670&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>0.499151</td>\n",
       "      <td>[0.5034068320344114, 0.5034068320344114]</td>\n",
       "      <td>None</td>\n",
       "      <td>precision    recall  f1-score   ...</td>\n",
       "      <td>&lt;xgboost.core.Booster object at 0x7f32468a4820&gt;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>120 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     test_error                                      aucs w_auc  \\\n",
       "0      0.503057      [0.50379590202715, 0.50379590202715]  None   \n",
       "1      0.496943    [0.508734635779073, 0.508734635779073]  None   \n",
       "2      0.501189  [0.5048543919272165, 0.5048543919272165]  None   \n",
       "3      0.502717    [0.502360357955947, 0.502360357955947]  None   \n",
       "4      0.499321  [0.5061017844072763, 0.5061017844072763]  None   \n",
       "..          ...                                       ...   ...   \n",
       "115    0.498132  [0.5057747576472328, 0.5057747576472328]  None   \n",
       "116    0.497962  [0.5046665869463117, 0.5046665869463118]  None   \n",
       "117    0.499490  [0.5026636996830249, 0.5026636996830249]  None   \n",
       "118    0.491678  [0.5107513874519006, 0.5107513874519005]  None   \n",
       "119    0.499151  [0.5034068320344114, 0.5034068320344114]  None   \n",
       "\n",
       "                                                report  \\\n",
       "0                  precision    recall  f1-score   ...   \n",
       "1                  precision    recall  f1-score   ...   \n",
       "2                  precision    recall  f1-score   ...   \n",
       "3                  precision    recall  f1-score   ...   \n",
       "4                  precision    recall  f1-score   ...   \n",
       "..                                                 ...   \n",
       "115                precision    recall  f1-score   ...   \n",
       "116                precision    recall  f1-score   ...   \n",
       "117                precision    recall  f1-score   ...   \n",
       "118                precision    recall  f1-score   ...   \n",
       "119                precision    recall  f1-score   ...   \n",
       "\n",
       "                                               model  \n",
       "0    <xgboost.core.Booster object at 0x7f35ba907130>  \n",
       "1    <xgboost.core.Booster object at 0x7f32e4009580>  \n",
       "2    <xgboost.core.Booster object at 0x7f3583f4f4f0>  \n",
       "3    <xgboost.core.Booster object at 0x7f3574279550>  \n",
       "4    <xgboost.core.Booster object at 0x7f32e4082730>  \n",
       "..                                               ...  \n",
       "115  <xgboost.core.Booster object at 0x7f32468a4640>  \n",
       "116  <xgboost.core.Booster object at 0x7f32468a4850>  \n",
       "117  <xgboost.core.Booster object at 0x7f32468a46a0>  \n",
       "118  <xgboost.core.Booster object at 0x7f32468a4670>  \n",
       "119  <xgboost.core.Booster object at 0x7f32468a4820>  \n",
       "\n",
       "[120 rows x 5 columns]"
      ]
     },
     "execution_count": 620,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(gridsearch_results_sh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB \n",
    "from mlxtend.classifier import StackingClassifier\n",
    "from sklearn.ensemble import (RandomForestClassifier, AdaBoostClassifier, \n",
    "                              GradientBoostingClassifier, ExtraTreesClassifier)\n",
    "from xgboost import XGBClassifier, XGBRFClassifier\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 第一层模型参数\n",
    "first_layer_params = {\n",
    "    LogisticRegression : {\n",
    "        'C' : 10,\n",
    "        'random_state': 0\n",
    "    },\n",
    "    RandomForestClassifier : {\n",
    "        'n_jobs': -1,\n",
    "        'n_estimators': 200,\n",
    "         'warm_start': True, \n",
    "         #'max_features': 0.2,\n",
    "        'max_depth': 8,\n",
    "        'min_samples_leaf': 2,\n",
    "        'max_features' : 'sqrt',\n",
    "        'verbose': 0\n",
    "    },\n",
    "    ExtraTreesClassifier : {\n",
    "        'n_jobs': -1,\n",
    "        'n_estimators':200,\n",
    "        #'max_features': 0.5,\n",
    "        'max_depth': 8,\n",
    "        'min_samples_leaf': 2,\n",
    "        'verbose': 0\n",
    "    },\n",
    "    AdaBoostClassifier : {\n",
    "        'n_estimators': 200,\n",
    "        'learning_rate' : 0.75\n",
    "    },\n",
    "#     GradientBoostingClassifier : { # 太慢了\n",
    "#         'n_estimators': 200,\n",
    "#          #'max_features': 0.2,\n",
    "#         'max_depth': 5,\n",
    "#         'min_samples_leaf': 2,\n",
    "#         'verbose': 0\n",
    "#     },\n",
    "#     SVC : { # 太慢了\n",
    "#         'kernel' : 'rbf',\n",
    "#         'C' : 0.025,\n",
    "#         'probability': True\n",
    "#     } \n",
    "}\n",
    "\n",
    "\n",
    "second_layer_params = {\n",
    "    XGBClassifier : {\n",
    "        'objective': 'multi:softmax',\n",
    "        'eta': 0.1,\n",
    "        'nthread': 8,\n",
    "        'num_class': 10,\n",
    "        'gpu_id': 0,\n",
    "        'tree_method': 'gpu_hist',\n",
    "        'eval_metric': ['mlogloss', 'auc', 'merror'],\n",
    "        'max_depth': 9,\n",
    "        'min_child_weight': 9,\n",
    "        'gamma': 0.2,\n",
    "        'subsample': 0.9,\n",
    "        'colsample_bytree': 0.6,\n",
    "        'reg_alpha': 0,\n",
    "        'n_estimators': 200,\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "clfs = []\n",
    "for c, p in first_layer_params.items():\n",
    "    clfs.append(c(**p))\n",
    "\n",
    "meta = XGBClassifier(**second_layer_params[XGBClassifier])\n",
    "sclf = StackingClassifier(classifiers=clfs, \n",
    "                          meta_classifier=meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gzy/.local/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "StackingClassifier(classifiers=[RandomForestClassifier(max_depth=8,\n",
       "                                                       max_features='sqrt',\n",
       "                                                       min_samples_leaf=2,\n",
       "                                                       n_estimators=200,\n",
       "                                                       n_jobs=-1,\n",
       "                                                       warm_start=True),\n",
       "                                ExtraTreesClassifier(max_depth=8,\n",
       "                                                     min_samples_leaf=2,\n",
       "                                                     n_estimators=200,\n",
       "                                                     n_jobs=-1),\n",
       "                                AdaBoostClassifier(learning_rate=0.75,\n",
       "                                                   n_estimators=200),\n",
       "                                GradientBoostingClassifier(max_depth=5,\n",
       "                                                           min_samples_leaf=2,\n",
       "                                                           n_esti...\n",
       "                                                 interaction_constraints=None,\n",
       "                                                 learning_rate=None,\n",
       "                                                 max_delta_step=None,\n",
       "                                                 max_depth=9,\n",
       "                                                 min_child_weight=9,\n",
       "                                                 missing=nan,\n",
       "                                                 monotone_constraints=None,\n",
       "                                                 n_estimators=200, n_jobs=None,\n",
       "                                                 nthread=8, num_class=10,\n",
       "                                                 num_parallel_tree=None,\n",
       "                                                 objective='multi:softmax',\n",
       "                                                 random_state=None, reg_alpha=0,\n",
       "                                                 reg_lambda=None,\n",
       "                                                 scale_pos_weight=None,\n",
       "                                                 subsample=0.9,\n",
       "                                                 tree_method='gpu_hist',\n",
       "                                                 validate_parameters=None, ...))"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sclf.fit(X_train_sh, y_train_sh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6674231843575419"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sclf.score(X_test_sh, y_test_sh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 13.1 s, sys: 112 ms, total: 13.2 s\n",
      "Wall time: 987 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(max_depth=8, max_features='sqrt', min_samples_leaf=2,\n",
       "                       n_estimators=200, n_jobs=-1, warm_start=True)"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "clf = RandomForestClassifier(**first_layer_params[RandomForestClassifier])\n",
    "clf.fit(X_train_sh, y_train_sh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.61      0.63      2845\n",
      "           1       0.64      0.69      0.66      2883\n",
      "\n",
      "    accuracy                           0.65      5728\n",
      "   macro avg       0.65      0.65      0.65      5728\n",
      "weighted avg       0.65      0.65      0.65      5728\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tmp = clf.predict(X_test_sh)\n",
    "print(metrics.classification_report(list(y_test_sh), list(tmp)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>v_avg_watch_label_1</th>\n",
       "      <th>v_sum_watch_times_1</th>\n",
       "      <th>v_sum_watch_overs_1</th>\n",
       "      <th>v_sum_comment_times_1</th>\n",
       "      <th>v_sum_collect_times_1</th>\n",
       "      <th>v_sum_share_times_1</th>\n",
       "      <th>v_sum_quit_times_1</th>\n",
       "      <th>v_sum_skip_times_1</th>\n",
       "      <th>v_sum_watch_days_1</th>\n",
       "      <th>v_avg_watch_label_3</th>\n",
       "      <th>...</th>\n",
       "      <th>class_6</th>\n",
       "      <th>class_7</th>\n",
       "      <th>class_8</th>\n",
       "      <th>class_9</th>\n",
       "      <th>da_0</th>\n",
       "      <th>da_1</th>\n",
       "      <th>da_2</th>\n",
       "      <th>da_3</th>\n",
       "      <th>da_4</th>\n",
       "      <th>watch_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>17211</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.958333</td>\n",
       "      <td>...</td>\n",
       "      <td>0.041922</td>\n",
       "      <td>0.041925</td>\n",
       "      <td>0.041925</td>\n",
       "      <td>0.041923</td>\n",
       "      <td>0.084701</td>\n",
       "      <td>0.084811</td>\n",
       "      <td>0.661892</td>\n",
       "      <td>0.084126</td>\n",
       "      <td>0.084470</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12479</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.306212</td>\n",
       "      <td>0.037078</td>\n",
       "      <td>0.037070</td>\n",
       "      <td>0.037070</td>\n",
       "      <td>0.074563</td>\n",
       "      <td>0.075237</td>\n",
       "      <td>0.316654</td>\n",
       "      <td>0.074140</td>\n",
       "      <td>0.459407</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2532</th>\n",
       "      <td>0.208611</td>\n",
       "      <td>1951.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1866.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.213834</td>\n",
       "      <td>...</td>\n",
       "      <td>0.350094</td>\n",
       "      <td>0.041778</td>\n",
       "      <td>0.041778</td>\n",
       "      <td>0.041771</td>\n",
       "      <td>0.324958</td>\n",
       "      <td>0.083541</td>\n",
       "      <td>0.083541</td>\n",
       "      <td>0.083541</td>\n",
       "      <td>0.424418</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28572</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.041922</td>\n",
       "      <td>0.041925</td>\n",
       "      <td>0.041925</td>\n",
       "      <td>0.041923</td>\n",
       "      <td>0.084701</td>\n",
       "      <td>0.084811</td>\n",
       "      <td>0.661892</td>\n",
       "      <td>0.084126</td>\n",
       "      <td>0.084470</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19280</th>\n",
       "      <td>0.509397</td>\n",
       "      <td>3565.0</td>\n",
       "      <td>106.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>3082.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.548684</td>\n",
       "      <td>...</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050003</td>\n",
       "      <td>0.050009</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11302</th>\n",
       "      <td>1.333333</td>\n",
       "      <td>123.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.124863</td>\n",
       "      <td>...</td>\n",
       "      <td>0.270403</td>\n",
       "      <td>0.036901</td>\n",
       "      <td>0.036893</td>\n",
       "      <td>0.271428</td>\n",
       "      <td>0.073786</td>\n",
       "      <td>0.074915</td>\n",
       "      <td>0.078284</td>\n",
       "      <td>0.073786</td>\n",
       "      <td>0.699228</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4584</th>\n",
       "      <td>0.902047</td>\n",
       "      <td>684.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>531.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.040830</td>\n",
       "      <td>...</td>\n",
       "      <td>0.212644</td>\n",
       "      <td>0.031771</td>\n",
       "      <td>0.031764</td>\n",
       "      <td>0.409137</td>\n",
       "      <td>0.065150</td>\n",
       "      <td>0.328750</td>\n",
       "      <td>0.068145</td>\n",
       "      <td>0.064165</td>\n",
       "      <td>0.473790</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26296</th>\n",
       "      <td>0.674253</td>\n",
       "      <td>32513.0</td>\n",
       "      <td>388.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>245.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>22996.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.725334</td>\n",
       "      <td>...</td>\n",
       "      <td>0.037483</td>\n",
       "      <td>0.037487</td>\n",
       "      <td>0.395917</td>\n",
       "      <td>0.037484</td>\n",
       "      <td>0.075851</td>\n",
       "      <td>0.075966</td>\n",
       "      <td>0.388781</td>\n",
       "      <td>0.383735</td>\n",
       "      <td>0.075666</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9816</th>\n",
       "      <td>1.617686</td>\n",
       "      <td>12903.0</td>\n",
       "      <td>1328.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>174.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>8865.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.619248</td>\n",
       "      <td>...</td>\n",
       "      <td>0.248009</td>\n",
       "      <td>0.036880</td>\n",
       "      <td>0.036881</td>\n",
       "      <td>0.270608</td>\n",
       "      <td>0.238563</td>\n",
       "      <td>0.073745</td>\n",
       "      <td>0.073745</td>\n",
       "      <td>0.073745</td>\n",
       "      <td>0.540203</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10186</th>\n",
       "      <td>1.143805</td>\n",
       "      <td>1356.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1044.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.158740</td>\n",
       "      <td>...</td>\n",
       "      <td>0.036693</td>\n",
       "      <td>0.528539</td>\n",
       "      <td>0.036671</td>\n",
       "      <td>0.178063</td>\n",
       "      <td>0.074926</td>\n",
       "      <td>0.074954</td>\n",
       "      <td>0.124523</td>\n",
       "      <td>0.075064</td>\n",
       "      <td>0.650534</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5728 rows × 128 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       v_avg_watch_label_1  v_sum_watch_times_1  v_sum_watch_overs_1  \\\n",
       "17211             0.000000                  4.0                  0.0   \n",
       "12479             0.000000                  0.0                  0.0   \n",
       "2532              0.208611               1951.0                 27.0   \n",
       "28572             0.000000                  0.0                  0.0   \n",
       "19280             0.509397               3565.0                106.0   \n",
       "...                    ...                  ...                  ...   \n",
       "11302             1.333333                123.0                 10.0   \n",
       "4584              0.902047                684.0                 30.0   \n",
       "26296             0.674253              32513.0                388.0   \n",
       "9816              1.617686              12903.0               1328.0   \n",
       "10186             1.143805               1356.0                100.0   \n",
       "\n",
       "       v_sum_comment_times_1  v_sum_collect_times_1  v_sum_share_times_1  \\\n",
       "17211                    0.0                    0.0                  0.0   \n",
       "12479                    0.0                    0.0                  0.0   \n",
       "2532                     2.0                   24.0                  4.0   \n",
       "28572                    0.0                    0.0                  0.0   \n",
       "19280                    2.0                   55.0                 35.0   \n",
       "...                      ...                    ...                  ...   \n",
       "11302                    0.0                    0.0                  1.0   \n",
       "4584                     1.0                    5.0                  1.0   \n",
       "26296                   11.0                  245.0                 55.0   \n",
       "9816                    34.0                  174.0                 46.0   \n",
       "10186                    7.0                   22.0                  1.0   \n",
       "\n",
       "       v_sum_quit_times_1  v_sum_skip_times_1  v_sum_watch_days_1  \\\n",
       "17211                 4.0                 0.0                 1.0   \n",
       "12479                 0.0                 0.0                 0.0   \n",
       "2532               1866.0                 0.0                 1.0   \n",
       "28572                 0.0                 0.0                 0.0   \n",
       "19280              3082.0                 0.0                 1.0   \n",
       "...                   ...                 ...                 ...   \n",
       "11302                88.0                 0.0                 1.0   \n",
       "4584                531.0                 0.0                 1.0   \n",
       "26296             22996.0                 0.0                 1.0   \n",
       "9816               8865.0                 0.0                 1.0   \n",
       "10186              1044.0                 0.0                 1.0   \n",
       "\n",
       "       v_avg_watch_label_3  ...   class_6   class_7   class_8   class_9  \\\n",
       "17211             0.958333  ...  0.041922  0.041925  0.041925  0.041923   \n",
       "12479             0.000000  ...  0.306212  0.037078  0.037070  0.037070   \n",
       "2532              0.213834  ...  0.350094  0.041778  0.041778  0.041771   \n",
       "28572             0.000000  ...  0.041922  0.041925  0.041925  0.041923   \n",
       "19280             0.548684  ...  0.050000  0.050003  0.050009  0.050000   \n",
       "...                    ...  ...       ...       ...       ...       ...   \n",
       "11302             1.124863  ...  0.270403  0.036901  0.036893  0.271428   \n",
       "4584              1.040830  ...  0.212644  0.031771  0.031764  0.409137   \n",
       "26296             0.725334  ...  0.037483  0.037487  0.395917  0.037484   \n",
       "9816              1.619248  ...  0.248009  0.036880  0.036881  0.270608   \n",
       "10186             1.158740  ...  0.036693  0.528539  0.036671  0.178063   \n",
       "\n",
       "           da_0      da_1      da_2      da_3      da_4  watch_label  \n",
       "17211  0.084701  0.084811  0.661892  0.084126  0.084470          2.0  \n",
       "12479  0.074563  0.075237  0.316654  0.074140  0.459407          0.0  \n",
       "2532   0.324958  0.083541  0.083541  0.083541  0.424418          0.0  \n",
       "28572  0.084701  0.084811  0.661892  0.084126  0.084470          0.0  \n",
       "19280  0.600000  0.100000  0.100000  0.100000  0.100000          0.0  \n",
       "...         ...       ...       ...       ...       ...          ...  \n",
       "11302  0.073786  0.074915  0.078284  0.073786  0.699228          9.0  \n",
       "4584   0.065150  0.328750  0.068145  0.064165  0.473790          3.0  \n",
       "26296  0.075851  0.075966  0.388781  0.383735  0.075666          0.0  \n",
       "9816   0.238563  0.073745  0.073745  0.073745  0.540203          0.0  \n",
       "10186  0.074926  0.074954  0.124523  0.075064  0.650534          0.0  \n",
       "\n",
       "[5728 rows x 128 columns]"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = inference_dataset\n",
    "test_sh = inference_dataset.copy()\n",
    "test = xgb.DMatrix(test.values, enable_categorical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2822180, 127), 127)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inference_dataset.shape, test.num_col()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "bst_wl = wl_bst_sm  # gridsearch_results[opt_idx]['model']  \n",
    "bst_sh = sh_bst_sm  # gridsearch_results_sh[opt_idx_sh]['model']  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Counter({1.0: 578445,\n",
       "          0.0: 1061952,\n",
       "          9.0: 815791,\n",
       "          2.0: 295281,\n",
       "          3.0: 35014,\n",
       "          8.0: 14071,\n",
       "          6.0: 5183,\n",
       "          5.0: 7140,\n",
       "          4.0: 7010,\n",
       "          7.0: 2293}),\n",
       " Counter({1.0: 1011353, 0.0: 1810827}))"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wl = bst_wl.predict(test)\n",
    "test_sh['watch_label'] = wl\n",
    "test_sh = xgb.DMatrix(test_sh.values, enable_categorical=True)\n",
    "sh = bst_sh.predict(test_sh)\n",
    "Counter(wl), Counter(Counter(sh))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2822180, 61)"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df['watch_label'] = wl.astype(np.uint8)\n",
    "test_df['is_share'] = sh.astype(np.uint8)\n",
    "test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>video_id</th>\n",
       "      <th>watch_label</th>\n",
       "      <th>is_share</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1688013</td>\n",
       "      <td>32645</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4502598</td>\n",
       "      <td>41270</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5585629</td>\n",
       "      <td>16345</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1635520</td>\n",
       "      <td>28149</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4160191</td>\n",
       "      <td>40554</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2822175</th>\n",
       "      <td>5019057</td>\n",
       "      <td>18766</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2822176</th>\n",
       "      <td>5019057</td>\n",
       "      <td>12968</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2822177</th>\n",
       "      <td>4255762</td>\n",
       "      <td>21794</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2822178</th>\n",
       "      <td>171497</td>\n",
       "      <td>21578</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2822179</th>\n",
       "      <td>5642580</td>\n",
       "      <td>28914</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2822180 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         user_id  video_id  watch_label  is_share\n",
       "0        1688013     32645            1         1\n",
       "1        4502598     41270            0         1\n",
       "2        5585629     16345            9         0\n",
       "3        1635520     28149            2         1\n",
       "4        4160191     40554            1         0\n",
       "...          ...       ...          ...       ...\n",
       "2822175  5019057     18766            0         0\n",
       "2822176  5019057     12968            0         0\n",
       "2822177  4255762     21794            1         0\n",
       "2822178   171497     21578            2         0\n",
       "2822179  5642580     28914            9         0\n",
       "\n",
       "[2822180 rows x 4 columns]"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission = pd.DataFrame(test_df[['user_id', 'video_id', 'watch_label', 'is_share']])\n",
    "submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new submission saved to ../submission-1629603492.csv\n"
     ]
    }
   ],
   "source": [
    "fn = f'../submission-{int(time())}.csv'\n",
    "submission.to_csv(fn, index=False, sep=\",\")\n",
    "print(f\"new submission saved to {fn}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 保存模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "version = 18\n",
    "wl_model_name = f'wl_model_v{version}'\n",
    "sh_model_name = f'sh_model_v{version}'\n",
    "bst_wl.save_model(wl_model_name)\n",
    "bst_sh.save_model(sh_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_log(log_name, info, log_path=\"./\"):\n",
    "    import datetime\n",
    "    with open(os.path.join(log_path, log_name), 'w') as log:\n",
    "        log.write(f\"# {datetime.datetime.now().__str__()}\\n\")\n",
    "        if info.get('comment', False):\n",
    "            log.write(f\"\\n## Comment: \\n\")\n",
    "            log.write(f\"{info['comment']}\\n\")\n",
    "            \n",
    "        log.write(f\"\\n## model name: {info['model_name']}\\n\")\n",
    "        log.write(f\"- model save path : {info['model_save_path']}\\n\")\n",
    "        \n",
    "        log.write(f\"\\n## Data setup\\n\")\n",
    "        log.write(f\"- dataset.shape : {dataset.shape}\\n\")\n",
    "        log.write(f\"- dataset.columns : {dataset.columns}\\n\")\n",
    "        log.write(f\"- is resample : {info['is_resample']}\\n\")\n",
    "        log.write(f\"- Traing_Data.shape (watch_label)  : {X_train.shape}\\n\")\n",
    "        log.write(f\"- Testing_Data.shape (watch_label) : {X_test.shape}\\n\")\n",
    "        log.write(f\"- Traing_Data.shape (is_share)  : {X_train_sh.shape}\\n\")\n",
    "        log.write(f\"- Testing_Data.shape (is_share) : {X_test_sh.shape}\\n\")\n",
    "        if info.get('is_resample', False):\n",
    "            log.write(f\"- Resampled class distribution (watch_label): \\n{Counter(resampled_wl)}\\n\")\n",
    "            log.write(f\"- Resampled class distribution (is_share): \\n{Counter(resampled_sh)}\\n\")\n",
    "            \n",
    "        log.write(f\"\\n## Model Params\\n\")\n",
    "        log.write(f\"- model params (watch_label) : \\n{info['param_wl']}\\n\")\n",
    "        log.write(f\"- model params (is_share) : \\n{info['param_sh']}\\n\")\n",
    "        \n",
    "        log.write(f\"\\n## Model's Performance\\n\")\n",
    "        log.write(f\"- Aucs (watch_label) : {info['aucs']}\\n\")\n",
    "        log.write(f\"- Weighted Aucs (watch_label) : {info['w_auc']}\\n\")\n",
    "        log.write(f\"- Aucs (is_share) : {info['aucs_sh']}\\n\")\n",
    "        \n",
    "        log.write(f\"- Classification Report (watch_label) : \\n\\n{info['report']}\\n\")\n",
    "        log.write(f\"- Classification Report (is_share) : \\n\\n{info['report_sh']}\\n\")\n",
    "        \n",
    "        log.flush()\n",
    "        \n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_wl = param  # all_params[opt_idx]\n",
    "param_sh = param_sh  # all_params[opt_idx_sh]\n",
    "\n",
    "aucs = aucs  # gridsearch_results[opt_idx]['aucs']\n",
    "w_auc = w_auc  # gridsearch_results[opt_idx]['w_auc']\n",
    "aucs_sh = aucs_sh  # gridsearch_results_sh[opt_idx]['aucs']\n",
    "\n",
    "report = report  # gridsearch_results[opt_idx]['report']\n",
    "report_sh = report_sh  # gridsearch_results_sh[opt_idx]['report']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_name = f\"log_v{version}.md\"\n",
    "info = {'is_resample': True, 'model_name': [wl_model_name, sh_model_name], 'model_save_path': os.getcwd(),\n",
    "        'comment': f\"特征：基础特征+用户和视频的统计量特征。\\n数据集划分：watch_label的测试集为.2，is_share的测试集为.2。\\nwatch_label训练300rounds，is_share训练600rounds。\\n此次生成的提交是：{fn}。官方测评得分：xxx😐\",\n",
    "        'param_wl': param_wl, 'param_sh': param_sh, 'aucs': aucs, 'w_auc': w_auc, 'aucs_sh': aucs_sh, \n",
    "        'report': report, 'report_sh': report_sh}\n",
    "write_log(log_name, info, log_path=\"./logs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 服务器间同步文件"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 推向Digix服务器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ssh: connect to host 49.123.120.71 port 22: No route to host\n",
      "lost connection\n"
     ]
    }
   ],
   "source": [
    "!scp ./models.ipynb digix@49.123.120.71:/home/digix/digix/Models/models.ipynb "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "digix@49.123.120.71's password: "
     ]
    }
   ],
   "source": [
    "!scp ./ensemble.ipynb digix@49.123.120.71:/home/digix/digix/Models/ensemble_from_gzy.ipynb "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 599,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ssh: connect to host 49.123.120.71 port 22: No route to host\n",
      "lost connection\n"
     ]
    }
   ],
   "source": [
    "!scp ./log_*.md digix@49.123.120.71:/home/digix/digix/Models/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "explore-data.ipynb                            100%  306KB  10.6MB/s   00:00    \n"
     ]
    }
   ],
   "source": [
    "!scp ../explore-data.ipynb digix@49.123.120.71:/home/digix/digix/explore-data.ipynb "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 650,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "video_features.jay                            100% 9035KB  11.1MB/s   00:00    \n"
     ]
    }
   ],
   "source": [
    "!scp ../2021_3_data/traindata/video_features_data/video_features.jay digix@49.123.120.71:/home/digix/digix/dataset/new_video_features.jay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 从Digix服务器拉数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "digix@49.123.120.71's password: \n"
     ]
    }
   ],
   "source": [
    "!scp  digix@49.123.120.71:/home/digix/digix/Models/LightGBM.ipynb ./LightGBM.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scp: /home/digix/digix/Models/feature_engineering.ipynb: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!scp  digix@49.123.120.71:/home/digix/digix/Models/feature_engineering.ipynb ./feature_engineering.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "utils.py                                      100% 3860     2.6MB/s   00:00    \n",
      "data_analysis.ipynb                           100% 6566KB  11.2MB/s   00:00    \n",
      "__init__.py                                   100%    0     0.0KB/s   00:00    \n",
      "__init__.cpython-36.pyc                       100%  139   128.3KB/s   00:00    \n",
      "utils.cpython-36.pyc                          100% 4120     2.6MB/s   00:00    \n",
      "video_data.ipynb                              100%   55KB   1.7MB/s   00:00    \n",
      "user_data-checkpoint.ipynb                    100%  202KB  10.1MB/s   00:00    \n",
      "data_analysis-checkpoint.ipynb                100% 6554KB  11.0MB/s   00:00    \n",
      "utils-checkpoint.py                           100% 3860     2.4MB/s   00:00    \n",
      "video_data-checkpoint.ipynb                   100%   17KB   1.4MB/s   00:00    \n",
      "user_data.ipynb                               100%  202KB  10.3MB/s   00:00    \n"
     ]
    }
   ],
   "source": [
    "!scp -r digix@49.123.120.71:/home/digix/digix/Models/Feature_Engineering/  ./"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "video_status.csv                              100% 2008KB   9.1MB/s   00:00    \n",
      "user_status.csv                               100%  138MB   9.3MB/s   00:14    \n"
     ]
    }
   ],
   "source": [
    "!scp -r digix@49.123.120.71:/home/digix/digix/dataset/traindata/video_features_data/video_status.csv ../2021_3_data/traindata/video_features_data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user_status.csv                               100%  168MB  11.2MB/s   00:14    \n"
     ]
    }
   ],
   "source": [
    "!scp -r digix@49.123.120.71:/home/digix/digix/dataset/traindata/user_features_data/user_status.csv ../2021_3_data/traindata/user_features_data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
