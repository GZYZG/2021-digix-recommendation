{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type='text/css'>\n",
       ".datatable table.frame { margin-bottom: 0; }\n",
       ".datatable table.frame thead { border-bottom: none; }\n",
       ".datatable table.frame tr.coltypes td {  color: #FFFFFF;  line-height: 6px;  padding: 0 0.5em;}\n",
       ".datatable .bool    { background: #DDDD99; }\n",
       ".datatable .object  { background: #565656; }\n",
       ".datatable .int     { background: #5D9E5D; }\n",
       ".datatable .float   { background: #4040CC; }\n",
       ".datatable .str     { background: #CC4040; }\n",
       ".datatable .time    { background: #40CC40; }\n",
       ".datatable .row_index {  background: var(--jp-border-color3);  border-right: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  font-size: 9px;}\n",
       ".datatable .frame tbody td { text-align: left; }\n",
       ".datatable .frame tr.coltypes .row_index {  background: var(--jp-border-color0);}\n",
       ".datatable th:nth-child(2) { padding-left: 12px; }\n",
       ".datatable .hellipsis {  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .vellipsis {  background: var(--jp-layout-color0);  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .na {  color: var(--jp-cell-editor-border-color);  font-size: 80%;}\n",
       ".datatable .sp {  opacity: 0.25;}\n",
       ".datatable .footer { font-size: 9px; }\n",
       ".datatable .frame_dimensions {  background: var(--jp-border-color3);  border-top: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  display: inline-block;  opacity: 0.6;  padding: 1px 10px 1px 5px;}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from time import time\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from collections import Counter\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import NearMiss, TomekLinks\n",
    "import datatable as dt\n",
    "import warnings\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "from tools import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 给定预测标签，计算AUC\n",
    "使用OVR的策略计算每个类别的AUC\n",
    "过程：\n",
    "- 选择类别i作为正类，其他类别作为负类\n",
    "- 将真实标签中不等于i的标记为0，等于i的标记为1\n",
    "- 将预测标签中不等于i的标记为0，等于ide标记为1\n",
    "- 计算混淆矩阵\n",
    "- 计算(fpr, tpr)\n",
    "- 计算AUC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 加载数据\n",
    "训练数据加载过程：\n",
    "1. 分别加载处理好的用户特征和视频特征，以及整合的用户历史行为数据；\n",
    "2. 从用户历史行为数据中筛掉在视频特征中没出现过的video_id；\n",
    "3. 将行为数据中的user_id、video_id替换为对应用户/视频的特征\n",
    "4. 根据不同的任务划分为`watch_label`、`is_share`的数据集\n",
    "\n",
    "推断时，类似于上述过程拼接数据。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 读取数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = \"../2021_3_data\"\n",
    "test_data_dir  = os.path.join(base_dir, \"testdata\")\n",
    "train_data_dir = os.path.join(base_dir, \"traindata\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 基础特征与附加特征合并"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_status = dt.fread(os.path.join(train_data_dir, \"video_features_data/video_status.csv\"))\n",
    "user_status = dt.fread(os.path.join(train_data_dir, \"user_features_data/user_status.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "tab_user = dt.fread(os.path.join(train_data_dir, \"user_features_data/user_features.jay\"))\n",
    "tab_video = dt.fread(os.path.join(train_data_dir, \"video_features_data/video_features.jay\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_status.key = 'video_id'\n",
    "video_ws = tab_video[:, :, join(video_status)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_status.key = 'user_id'\n",
    "user_ws = tab_user[:, :, join(user_status)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_ws.to_jay(os.path.join(train_data_dir, \"video_features_data/video_features_with_status.jay\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_ws.to_jay(os.path.join(train_data_dir, \"user_features_data/user_features_with_status.jay\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class='datatable'>\n",
       "  <table class='frame'>\n",
       "  <thead>\n",
       "    <tr class='colnames'><td class='row_index'></td><th>user_id</th><th>age_0</th><th>age_1</th><th>age_2</th><th>age_3</th><th>age_4</th><th>age_5</th><th>age_6</th><th>age_7</th><th>gender_0</th><th class='vellipsis'>&hellip;</th><th>average_watch_label</th><th>sum_watch_times</th><th>sum_comment_times</th><th>sum_collect_times</th><th>sum_share_times</th></tr>\n",
       "    <tr class='coltypes'><td class='row_index'></td><td class='float' title='float64'>&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;</td><td class='float' title='float64'>&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;</td><td class='float' title='float64'>&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;</td><td class='float' title='float64'>&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;</td><td class='float' title='float64'>&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;</td><td class='float' title='float64'>&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;</td><td class='float' title='float64'>&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;</td><td class='float' title='float64'>&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;</td><td class='float' title='float64'>&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;</td><td class='float' title='float64'>&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;</td><td></td><td class='float' title='float64'>&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;</td><td class='float' title='float64'>&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;</td><td class='float' title='float64'>&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;</td><td class='float' title='float64'>&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;</td><td class='float' title='float64'>&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;</td></tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr><td class='row_index'>0</td><td>1.757e+06</td><td>0</td><td>0</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td class=vellipsis>&hellip;</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr>\n",
       "    <tr><td class='row_index'>1</td><td>17938</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td><td class=vellipsis>&hellip;</td><td>0.0967742</td><td>3</td><td>0</td><td>0</td><td>0</td></tr>\n",
       "    <tr><td class='row_index'>2</td><td>4.26352e+06</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td><td class=vellipsis>&hellip;</td><td>0.204545</td><td>2</td><td>0</td><td>0</td><td>0</td></tr>\n",
       "    <tr><td class='row_index'>3</td><td>1.4116e+06</td><td>0</td><td>0</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td><td class=vellipsis>&hellip;</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr>\n",
       "    <tr><td class='row_index'>4</td><td>3.99224e+06</td><td>0</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td><td class=vellipsis>&hellip;</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr>\n",
       "    <tr><td class='row_index'>5</td><td>4.0116e+06</td><td>0</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td><td class=vellipsis>&hellip;</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr>\n",
       "    <tr><td class='row_index'>6</td><td>4.78556e+06</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td><td class=vellipsis>&hellip;</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr>\n",
       "    <tr><td class='row_index'>7</td><td>5.11036e+06</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td><td class=vellipsis>&hellip;</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr>\n",
       "    <tr><td class='row_index'>8</td><td>1.3212e+06</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td><td class=vellipsis>&hellip;</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr>\n",
       "    <tr><td class='row_index'>9</td><td>3.20698e+06</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td class=vellipsis>&hellip;</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr>\n",
       "    <tr><td class='row_index'>10</td><td>5.18172e+06</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td class=vellipsis>&hellip;</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td></tr>\n",
       "    <tr><td class='row_index'>11</td><td>1.878e+06</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td><td class=vellipsis>&hellip;</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td></tr>\n",
       "    <tr><td class='row_index'>12</td><td>3.04464e+06</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td class=vellipsis>&hellip;</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td></tr>\n",
       "    <tr><td class='row_index'>13</td><td>108273</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td class=vellipsis>&hellip;</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr>\n",
       "    <tr><td class='row_index'>14</td><td>5.64838e+06</td><td>0</td><td>0</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td class=vellipsis>&hellip;</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr>\n",
       "    <tr><td class='row_index'>&#x22EE;</td><td class='hellipsis'>&#x22EE;</td><td class='hellipsis'>&#x22EE;</td><td class='hellipsis'>&#x22EE;</td><td class='hellipsis'>&#x22EE;</td><td class='hellipsis'>&#x22EE;</td><td class='hellipsis'>&#x22EE;</td><td class='hellipsis'>&#x22EE;</td><td class='hellipsis'>&#x22EE;</td><td class='hellipsis'>&#x22EE;</td><td class='hellipsis'>&#x22EE;</td><td class='hellipsis'>&#x22F1;</td><td class='hellipsis'>&#x22EE;</td><td class='hellipsis'>&#x22EE;</td><td class='hellipsis'>&#x22EE;</td><td class='hellipsis'>&#x22EE;</td><td class='hellipsis'>&#x22EE;</td></tr>\n",
       "    <tr><td class='row_index'>5,910,795</td><td>3.22343e+06</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td><td>1</td><td class=vellipsis>&hellip;</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr>\n",
       "    <tr><td class='row_index'>5,910,796</td><td>4.70783e+06</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td><td>1</td><td class=vellipsis>&hellip;</td><td>0.142857</td><td>3</td><td>0</td><td>0</td><td>0</td></tr>\n",
       "    <tr><td class='row_index'>5,910,797</td><td>5.90765e+06</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td><td class=vellipsis>&hellip;</td><td>0</td><td>2</td><td>0</td><td>0</td><td>0</td></tr>\n",
       "    <tr><td class='row_index'>5,910,798</td><td>3.63322e+06</td><td>0</td><td>0</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td><td class=vellipsis>&hellip;</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr>\n",
       "    <tr><td class='row_index'>5,910,799</td><td>782537</td><td>0</td><td>0</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td><td class=vellipsis>&hellip;</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr>\n",
       "  </tbody>\n",
       "  </table>\n",
       "  <div class='footer'>\n",
       "    <div class='frame_dimensions'>5,910,800 rows &times; 36 columns</div>\n",
       "  </div>\n",
       "</div>\n"
      ],
      "text/plain": [
       "<Frame#7f315506be10 5910800x36>"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_ws"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 通过 .npz 读取数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 单表读取后合并"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.6 s, sys: 2.4 s, total: 6 s\n",
      "Wall time: 12.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# 单独读取每个文件再进行合并\n",
    "user_df = read_npz_to_df(os.path.join(train_data_dir, \"user_features_data/user_features.npz\"), data_name='features', column_name='columns')\n",
    "video_df = read_npz_to_df(os.path.join(train_data_dir, \"video_features_data/video_features.npz\"), data_name='features')\n",
    "action_df = read_npz_to_df(os.path.join(train_data_dir, \"all_actions.npz\"), data_name='data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 797,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 因为将字符串保存到 .npz时会使dtype为object，重新读回DataFrame时各个列的数据类型均为 object，所以先转换类型\n",
    "dtypes = dict(zip(video_df.columns, [np.float32] * video_df.shape[1]))\n",
    "dtypes.update({'video_name': np.str})\n",
    "video_df = video_df.astype(dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 802,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 29s, sys: 5.88 s, total: 1min 35s\n",
      "Wall time: 40.1 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(7353024, 76)"
      ]
     },
     "execution_count": 802,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# 合并各个表\n",
    "df_train = merge_user_video_action(user_df, video_df, action_df)\n",
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 805,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez(os.path.join(train_data_dir, \"train.npz\"), data=df_train.to_pandas().values, columns=df_train.to_pandas().columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 777,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 626 ms, sys: 0 ns, total: 626 ms\n",
      "Wall time: 721 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "test_df = load_table(os.path.join(test_data_dir, \"test.csv\"), ftype=\"csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 779,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 8s, sys: 12.8 s, total: 3min 20s\n",
      "Wall time: 51.2 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2822180, 73)"
      ]
     },
     "execution_count": 779,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "df_test = merge_user_video_action(user_df, video_df, test_df)\n",
    "df_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 读取合并好的数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 806,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 2s, sys: 38.4 s, total: 3min 40s\n",
      "Wall time: 3min 41s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(7353024, 76)"
      ]
     },
     "execution_count": 806,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# 直接读取保存好的合并后的训练数据\n",
    "path = os.path.join(train_data_dir, \"train.npz\")\n",
    "df_train = read_npz_to_df(path, data_name='data')\n",
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 810,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 37s, sys: 36.9 s, total: 2min 14s\n",
      "Wall time: 5min 49s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2822180, 73)"
      ]
     },
     "execution_count": 810,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# 直接读取保存好的合并后的测试数据\n",
    "path = os.path.join(test_data_dir, \"test.npz\")\n",
    "df_test = read_npz_to_df(path, data_name='data')\n",
    "df_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 通过 .jay 文件读取"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 单表读取后合并"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with_status = False\n",
    "if with_status:\n",
    "    user_features_name = \"user_features_with_status\"\n",
    "    video_features_name = \"video_features_with_status\"\n",
    "else:\n",
    "    user_features_name = \"user_features\"\n",
    "    video_features_name = \"video_features\"\n",
    "    \n",
    "p_user = os.path.join(train_data_dir, f\"user_features_data/{user_features_name}.jay\")\n",
    "p_video = os.path.join(train_data_dir, f\"video_features_data/{video_features_name}.jay\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 26.2 s, sys: 1.35 s, total: 27.5 s\n",
      "Wall time: 1.42 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(7353024, 133)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "## 使用datatable 加载训练数据\n",
    "p_act = os.path.join(train_data_dir, \"all_actions_with_status.jay\")\n",
    "\n",
    "df_train, others = load_train_test_data(None, pre_merged=False, return_others=True,\n",
    "                           **{\"p_user\": p_user, \"p_video\": p_video, \"p_action\": p_act})\n",
    "user_df = others['user']\n",
    "video_df = others['video']\n",
    "action_df = others['action']\n",
    "df_train.shape"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "action_date = dt.fread(os.path.join(train_data_dir, f\"all_actions_with_ptd.jay\"))\n",
    "video_status = dt.fread(os.path.join(train_data_dir, f\"video_features_data/video_137days_status.csv\"))\n",
    "user_status = dt.fread(os.path.join(train_data_dir, f\"user_features_data/user_137days_status.csv\"))\n",
    "video_status.key = ('video_id', 'pt_d')\n",
    "user_status.key = ('user_id', 'pt_d')\n",
    "video_status"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "tt = df_train.to_pandas()\n",
    "np.savez(os.path.join(train_data_dir, \"train\"), data=tt.values, columns=tt.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 21.4 s, sys: 1.05 s, total: 22.4 s\n",
      "Wall time: 1.16 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2822180, 130)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# p_user = os.path.join(train_data_dir, \"user_features_data/user_features.jay\")\n",
    "# p_video = os.path.join(train_data_dir, \"video_features_data/video_features.jay\")\n",
    "p_act = os.path.join(test_data_dir, \"test_with_status.jay\")\n",
    "\n",
    "#path = os.path.join(test_data_dir, \"test.jay\")\n",
    "kwargs = {\"p_user\": p_user, \"p_video\": p_video, \"p_action\": p_act}\n",
    "\n",
    "df_test, others = load_train_test_data(None, pre_merged=False, return_others=True, **kwargs)\n",
    "test_df = others['action']\n",
    "df_test.shape"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "test_df = dt.fread(os.path.join(test_data_dir, \"test.csv\"))\n",
    "video_status = dt.fread(os.path.join(train_data_dir, f\"video_features_data/video_137days_status.csv\"))\n",
    "user_status = dt.fread(os.path.join(train_data_dir, f\"user_features_data/user_137days_status.csv\"))\n",
    "video_status.key = ('video_id', 'pt_d')\n",
    "user_status.key = ('user_id', 'pt_d')\n",
    "test_df['pt_d'] = 20210503\n",
    "test_df = test_df[:, :, dt.join(video_status)]\n",
    "test_df = test_df[:, :, dt.join(user_status)]\n",
    "test_df"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "del test_df['pt_d']\n",
    "test_df.to_jay(os.path.join(test_data_dir, \"test_with_status.jay\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_df = action_df.to_pandas()\n",
    "user_df = user_df.to_pandas()\n",
    "video_df = video_df.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = test_df.to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 读取合并好后的数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_test.to_jay(os.path.join(test_data_dir, \"test_with_status.jay\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.to_jay(os.path.join(train_data_dir, \"train_with_status.jay\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 177 ms, total: 177 ms\n",
      "Wall time: 184 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(7353024, 76)"
      ]
     },
     "execution_count": 468,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# 直接读取保存好的合并后的训练数据\n",
    "path = os.path.join(train_data_dir, \"train.jay\")\n",
    "df_train = load_train_test_data(path, pre_merged=True)\n",
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 649 µs, sys: 46 µs, total: 695 µs\n",
      "Wall time: 684 µs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2822180, 72)"
      ]
     },
     "execution_count": 454,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# 直接读取保存好的合并后的测试数据\n",
    "path = os.path.join(test_data_dir, \"test.jay\")\n",
    "df_test = load_train_test_data(path, pre_merged=True)\n",
    "df_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 处理训练数据\n",
    "可在此做一些预处理：\n",
    "- 从用户历史行为数据中筛掉在视频特征中没出现过的video_id\n",
    "- 删除多余的列\n",
    "- 调整列的顺序\n",
    "- 改变列的数据类型\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 26.8 s, sys: 3.57 s, total: 30.3 s\n",
      "Wall time: 7.55 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if isinstance(df_train, dt.Frame):\n",
    "    df_train = df_train.to_pandas()\n",
    "if isinstance(df_test, dt.Frame):\n",
    "    df_test = df_test.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7353024 entries, 0 to 7353023\n",
      "Columns: 133 entries, user_id to da_4\n",
      "dtypes: float32(40), float64(63), int32(20), int64(9), object(1)\n",
      "memory usage: 5.6+ GB\n"
     ]
    }
   ],
   "source": [
    "df_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 删除 video_name、is_watch 列\n",
    "df_train.drop(['video_name', 'is_watch'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "if 'action_df' not in dir():\n",
    "    action_df = load_table(os.path.join(train_data_dir, \"all_actions.jay\")).to_pandas()\n",
    "if 'video_df' not in dir():\n",
    "    video_df = load_table(os.path.join(train_data_dir, \"video_features_data/video_features.jay\")).to_pandas()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# 从用户历史行为数据中筛掉在视频特征中没出现过的video_id\n",
    "idx1 = pd.Index(action_df['video_id'].unique())\n",
    "idx2 = pd.Index(video_df['video_id'])\n",
    "not_exists = idx1.difference(idx2)\n",
    "not_exists"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "%%time\n",
    "# 将训练数据中未出现的视频剔除\n",
    "n = 0\n",
    "for vid in not_exists:\n",
    "    tn = (df_train['video_id'] == vid).sum()\n",
    "    df_train['video_id'].replace(vid, np.nan, inplace=True)\n",
    "    n += tn\n",
    "\n",
    "if n > 0:\n",
    "    df_train.dropna(axis=0, inplace=True)\n",
    "    df_train.reset_index(drop=True, inplace=True)\n",
    "print(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 删除 video_id、user_id列\n",
    "df_train.drop(['user_id', 'video_id'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7353024, 129)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = df_train\n",
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((7353024,), (7353024,), (7353024, 127))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 准备数据\n",
    "watch_label = dataset.pop('watch_label').astype(np.uint8)\n",
    "is_share = dataset.pop('is_share').astype(np.uint8)\n",
    "watch_label.shape, is_share.shape, dataset.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 处理测试数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'test_df' not in dir():\n",
    "    test_df = pd.read_csv(os.path.join(test_data_dir, \"test.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2822180 entries, 0 to 2822179\n",
      "Data columns (total 59 columns):\n",
      " #   Column                 Dtype  \n",
      "---  ------                 -----  \n",
      " 0   user_id                int32  \n",
      " 1   video_id               int32  \n",
      " 2   v_avg_watch_label_1    float64\n",
      " 3   v_sum_watch_times_1    float64\n",
      " 4   v_sum_watch_overs_1    float64\n",
      " 5   v_sum_comment_times_1  float64\n",
      " 6   v_sum_collect_times_1  float64\n",
      " 7   v_sum_share_times_1    float64\n",
      " 8   v_sum_quit_times_1     float64\n",
      " 9   v_sum_skip_times_1     float64\n",
      " 10  v_sum_watch_days_1     float64\n",
      " 11  v_avg_watch_label_3    float64\n",
      " 12  v_sum_watch_times_3    float64\n",
      " 13  v_sum_watch_overs_3    float64\n",
      " 14  v_sum_comment_times_3  float64\n",
      " 15  v_sum_collect_times_3  float64\n",
      " 16  v_sum_share_times_3    float64\n",
      " 17  v_sum_quit_times_3     float64\n",
      " 18  v_sum_skip_times_3     float64\n",
      " 19  v_sum_watch_days_3     float64\n",
      " 20  v_avg_watch_label_7    float64\n",
      " 21  v_sum_watch_times_7    float64\n",
      " 22  v_sum_watch_overs_7    float64\n",
      " 23  v_sum_comment_times_7  float64\n",
      " 24  v_sum_collect_times_7  float64\n",
      " 25  v_sum_share_times_7    float64\n",
      " 26  v_sum_quit_times_7     float64\n",
      " 27  v_sum_skip_times_7     float64\n",
      " 28  v_sum_watch_days_7     float64\n",
      " 29  u_avg_watch_label_1    float64\n",
      " 30  u_sum_watch_times_1    float64\n",
      " 31  u_sum_watch_overs_1    float64\n",
      " 32  u_sum_quit_times_1     float64\n",
      " 33  u_sum_skip_times_1     object \n",
      " 34  u_sum_comment_times_1  float64\n",
      " 35  u_sum_collect_times_1  float64\n",
      " 36  u_sum_share_times_1    float64\n",
      " 37  u_sum_watch_time_1     float64\n",
      " 38  u_sum_watch_days_1     object \n",
      " 39  u_avg_watch_label_3    float64\n",
      " 40  u_sum_watch_times_3    float64\n",
      " 41  u_sum_watch_overs_3    float64\n",
      " 42  u_sum_quit_times_3     float64\n",
      " 43  u_sum_skip_times_3     object \n",
      " 44  u_sum_comment_times_3  float64\n",
      " 45  u_sum_collect_times_3  float64\n",
      " 46  u_sum_share_times_3    float64\n",
      " 47  u_sum_watch_time_3     float64\n",
      " 48  u_sum_watch_days_3     float64\n",
      " 49  u_avg_watch_label_7    float64\n",
      " 50  u_sum_watch_times_7    float64\n",
      " 51  u_sum_watch_overs_7    float64\n",
      " 52  u_sum_quit_times_7     float64\n",
      " 53  u_sum_skip_times_7     object \n",
      " 54  u_sum_comment_times_7  float64\n",
      " 55  u_sum_collect_times_7  float64\n",
      " 56  u_sum_share_times_7    float64\n",
      " 57  u_sum_watch_time_7     float64\n",
      " 58  u_sum_watch_days_7     float64\n",
      "dtypes: float64(53), int32(2), object(4)\n",
      "memory usage: 1.2+ GB\n"
     ]
    }
   ],
   "source": [
    "test_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2822180 entries, 0 to 2822179\n",
      "Columns: 130 entries, user_id to da_4\n",
      "dtypes: float32(40), float64(83), int32(2), object(5)\n",
      "memory usage: 2.3+ GB\n"
     ]
    }
   ],
   "source": [
    "df_test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 删除 video_name 列\n",
    "if 'video_name' in df_test.columns:\n",
    "    df_test.drop('video_name', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# 测试数据集中存在video_id没有在视频特征中出现\n",
    "idx1 = pd.Index(test_df['video_id'].unique())\n",
    "idx2 = pd.Index(video_df['video_id'].unique())\n",
    "non_exists = idx1.difference(idx2)\n",
    "non_exists"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "%%time\n",
    "t0 = time()\n",
    "n = 0\n",
    "for vid in not_exists:\n",
    "    tn = (test_df['video_id'] == vid).sum()\n",
    "#     df_test = action_df[action_df['video_id'] != vid]\n",
    "    n += tn\n",
    "\n",
    "print(f\"在视频特征中不存在的video_id在测试数据集中出现的次数 = {n}\\t\\t(cost {time() - t0:.3f}s)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 删除 video_id、user_id 列\n",
    "df_test.drop(['user_id', 'video_id'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2822180, 127)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inference_dataset = df_test\n",
    "inference_dataset.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# watch_label 预测"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 处理数据不均衡问题"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 5176743), (1, 557421), (2, 314107), (3, 219188), (4, 172404), (5, 143001), (6, 125092), (7, 117749), (8, 138798), (9, 388521)]\n",
      "[[0.         0.70402912]\n",
      " [1.         0.0758084 ]\n",
      " [2.         0.04271807]\n",
      " [3.         0.02980923]\n",
      " [4.         0.02344668]\n",
      " [5.         0.01944792]\n",
      " [6.         0.01701232]\n",
      " [7.         0.01601368]\n",
      " [8.         0.01887632]\n",
      " [9.         0.05283826]]\n"
     ]
    }
   ],
   "source": [
    "items = list(Counter(watch_label).items())\n",
    "items.sort(key=lambda x: x[0])\n",
    "print(items)\n",
    "\n",
    "dist = np.array(items, dtype=np.float)\n",
    "dist[:, 1] = dist[:, 1] / watch_label.shape[0]\n",
    "print(dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "under_ss = np.array(items)\n",
    "under_ss_thresh = under_ss[9, 1]  # 设置每个类别样本数目的上限\n",
    "under_ss[:, 1] = np.clip(under_ss[:, 1], a_min=None, a_max=under_ss_thresh)\n",
    "\n",
    "over_ss = under_ss.copy()\n",
    "over_ss_thresh = under_ss[2, 1]  # 设置每个类别样本数据的下限\n",
    "over_ss[:, 1] = np.clip(over_ss[:, 1], a_min=over_ss_thresh, a_max=None)\n",
    "\n",
    "under_ss = dict(under_ss)\n",
    "over_ss = dict(over_ss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({0: 388521,\n",
       "  1: 388521,\n",
       "  2: 314107,\n",
       "  3: 219188,\n",
       "  4: 172404,\n",
       "  5: 143001,\n",
       "  6: 125092,\n",
       "  7: 117749,\n",
       "  8: 138798,\n",
       "  9: 388521},\n",
       " {0: 388521,\n",
       "  1: 388521,\n",
       "  2: 314107,\n",
       "  3: 314107,\n",
       "  4: 314107,\n",
       "  5: 314107,\n",
       "  6: 314107,\n",
       "  7: 314107,\n",
       "  8: 314107,\n",
       "  9: 388521})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "under_ss, over_ss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({0: 4788222})\n",
      "Counter({1: 168900})\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Int64Index([      1,       3,       8,       9,      11,      12,      13,\n",
       "                 14,      15,      16,\n",
       "            ...\n",
       "            7353006, 7353007, 7353008, 7353010, 7353014, 7353016, 7353019,\n",
       "            7353020, 7353022, 7353023],\n",
       "           dtype='int64', length=4957122)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del_idxs = pd.Index([], dtype=int)\n",
    "for l, n in items:\n",
    "    if n > under_ss_thresh:\n",
    "        t_idxs = watch_label == l\n",
    "        t_idxs = t_idxs.replace(False, np.nan).dropna().index  # 保留watch_label=l的行索引\n",
    "        t_left_idxs = np.random.choice(t_idxs, under_ss_thresh, replace=False)  # 选择一部分保留，注意replace参数，为True时会重复采样\n",
    "        t_del_idxs = t_idxs.difference(t_left_idxs)\n",
    "        print(Counter(watch_label[t_del_idxs]))\n",
    "                \n",
    "        del_idxs = del_idxs.union(t_del_idxs)\n",
    "del_idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({2: 314107,\n",
       "         0: 5176743,\n",
       "         5: 143001,\n",
       "         4: 172404,\n",
       "         1: 557421,\n",
       "         9: 388521,\n",
       "         3: 219188,\n",
       "         8: 138798,\n",
       "         7: 117749,\n",
       "         6: 125092})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(watch_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2395902, 127), (2395902,))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resampled_data = np.delete(dataset.values, del_idxs, axis=0)\n",
    "resampled_wl = np.delete(watch_label.values, del_idxs, axis=0)\n",
    "resampled_data.shape, resampled_wl.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({2: 314107,\n",
       "         5: 143001,\n",
       "         4: 172404,\n",
       "         0: 388521,\n",
       "         9: 388521,\n",
       "         3: 219188,\n",
       "         1: 388521,\n",
       "         8: 138798,\n",
       "         7: 117749,\n",
       "         6: 125092})"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(resampled_wl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 速度太慢，难以忍受！\n",
    "nm  = TomekLinks()\n",
    "smt = SMOTE(sampling_strategy=over_ss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nm' is not defined"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X_r, y_r = nm.fit_resample(dataset, watch_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'smt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'smt' is not defined"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X_r, y_r = smt.fit_resample(resampled_data, resampled_wl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2395902, 127), (2395902,))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 将采样后的数据重装回 DataFrame\n",
    "data = pd.DataFrame(resampled_data, columns=dataset.columns)\n",
    "watch_label_res = pd.Series(resampled_wl)\n",
    "data.shape, watch_label_res.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2036516,), (359386,))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_rate = 0.15\n",
    "train_idx, test_idx = train_test_split(data.index, test_size=test_rate, random_state=0)\n",
    "train_idx.shape, test_idx.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = data.iloc[train_idx]\n",
    "X_test  = data.iloc[test_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = watch_label_res.iloc[train_idx]\n",
    "y_test  = watch_label_res.iloc[test_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2036516, 127), (2036516,), (359386, 127), (359386,))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, y_train.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data preparing finished ...\t\t(1.887s)\n"
     ]
    }
   ],
   "source": [
    "t0 = time()\n",
    "xg_train = xgb.DMatrix(X_train.values, label=y_train.values, enable_categorical=True)\n",
    "xg_test = xgb.DMatrix(X_test.values, label=y_test.values, enable_categorical=True)\n",
    "print(f\"Data preparing finished ...\\t\\t({time()-t0:.3f}s)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup parameters for xgboost\n",
    "param = {\n",
    "    'objective': 'multi:softmax',\n",
    "    'eta': 0.1,\n",
    "    'nthread': 8,\n",
    "    'num_class': 10,\n",
    "    'gpu_id': 0,\n",
    "    'tree_method': 'gpu_hist',\n",
    "    'eval_metric': ['mlogloss', 'auc', 'merror'],\n",
    "    'max_depth': 9,\n",
    "    'min_child_weight': 9,\n",
    "    'gamma': 0.2,\n",
    "    'subsample': 0.9,\n",
    "    'colsample_bytree': 0.6,\n",
    "    'reg_alpha': 0\n",
    "}\n",
    "\n",
    "watchlist = [(xg_train, 'train'), (xg_test, 'test')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-mlogloss:2.27344\ttrain-auc:0.63151\ttrain-merror:0.74003\ttest-mlogloss:2.27419\ttest-auc:0.62399\ttest-merror:0.74374\n",
      "[1]\ttrain-mlogloss:2.24973\ttrain-auc:0.63802\ttrain-merror:0.73721\ttest-mlogloss:2.25121\ttest-auc:0.62846\ttest-merror:0.74120\n",
      "[2]\ttrain-mlogloss:2.22957\ttrain-auc:0.64220\ttrain-merror:0.73564\ttest-mlogloss:2.23166\ttest-auc:0.63181\ttest-merror:0.73919\n",
      "[3]\ttrain-mlogloss:2.21234\ttrain-auc:0.64447\ttrain-merror:0.73462\ttest-mlogloss:2.21506\ttest-auc:0.63333\ttest-merror:0.73892\n",
      "[4]\ttrain-mlogloss:2.19784\ttrain-auc:0.64638\ttrain-merror:0.73406\ttest-mlogloss:2.20113\ttest-auc:0.63465\ttest-merror:0.73809\n",
      "[5]\ttrain-mlogloss:2.18420\ttrain-auc:0.64777\ttrain-merror:0.73361\ttest-mlogloss:2.18809\ttest-auc:0.63550\ttest-merror:0.73797\n",
      "[6]\ttrain-mlogloss:2.17214\ttrain-auc:0.64898\ttrain-merror:0.73315\ttest-mlogloss:2.17665\ttest-auc:0.63623\ttest-merror:0.73750\n",
      "[7]\ttrain-mlogloss:2.16168\ttrain-auc:0.64985\ttrain-merror:0.73295\ttest-mlogloss:2.16676\ttest-auc:0.63660\ttest-merror:0.73753\n",
      "[8]\ttrain-mlogloss:2.15210\ttrain-auc:0.65076\ttrain-merror:0.73252\ttest-mlogloss:2.15774\ttest-auc:0.63713\ttest-merror:0.73724\n",
      "[9]\ttrain-mlogloss:2.14342\ttrain-auc:0.65163\ttrain-merror:0.73212\ttest-mlogloss:2.14961\ttest-auc:0.63762\ttest-merror:0.73712\n",
      "[10]\ttrain-mlogloss:2.13569\ttrain-auc:0.65230\ttrain-merror:0.73194\ttest-mlogloss:2.14245\ttest-auc:0.63794\ttest-merror:0.73686\n",
      "[11]\ttrain-mlogloss:2.12886\ttrain-auc:0.65300\ttrain-merror:0.73167\ttest-mlogloss:2.13616\ttest-auc:0.63829\ttest-merror:0.73675\n",
      "[12]\ttrain-mlogloss:2.12264\ttrain-auc:0.65373\ttrain-merror:0.73130\ttest-mlogloss:2.13049\ttest-auc:0.63870\ttest-merror:0.73643\n",
      "[13]\ttrain-mlogloss:2.11684\ttrain-auc:0.65443\ttrain-merror:0.73081\ttest-mlogloss:2.12527\ttest-auc:0.63902\ttest-merror:0.73614\n",
      "[14]\ttrain-mlogloss:2.11153\ttrain-auc:0.65519\ttrain-merror:0.73041\ttest-mlogloss:2.12053\ttest-auc:0.63941\ttest-merror:0.73596\n",
      "[15]\ttrain-mlogloss:2.10672\ttrain-auc:0.65578\ttrain-merror:0.73009\ttest-mlogloss:2.11624\ttest-auc:0.63973\ttest-merror:0.73580\n",
      "[16]\ttrain-mlogloss:2.10245\ttrain-auc:0.65643\ttrain-merror:0.72984\ttest-mlogloss:2.11251\ttest-auc:0.64004\ttest-merror:0.73559\n",
      "[17]\ttrain-mlogloss:2.09840\ttrain-auc:0.65696\ttrain-merror:0.72952\ttest-mlogloss:2.10893\ttest-auc:0.64036\ttest-merror:0.73520\n",
      "[18]\ttrain-mlogloss:2.09479\ttrain-auc:0.65750\ttrain-merror:0.72932\ttest-mlogloss:2.10585\ttest-auc:0.64058\ttest-merror:0.73500\n",
      "[19]\ttrain-mlogloss:2.09155\ttrain-auc:0.65805\ttrain-merror:0.72903\ttest-mlogloss:2.10313\ttest-auc:0.64082\ttest-merror:0.73490\n",
      "[20]\ttrain-mlogloss:2.08849\ttrain-auc:0.65854\ttrain-merror:0.72883\ttest-mlogloss:2.10060\ttest-auc:0.64098\ttest-merror:0.73469\n",
      "[21]\ttrain-mlogloss:2.08561\ttrain-auc:0.65913\ttrain-merror:0.72851\ttest-mlogloss:2.09827\ttest-auc:0.64125\ttest-merror:0.73454\n",
      "[22]\ttrain-mlogloss:2.08299\ttrain-auc:0.65963\ttrain-merror:0.72828\ttest-mlogloss:2.09614\ttest-auc:0.64147\ttest-merror:0.73449\n",
      "[23]\ttrain-mlogloss:2.08054\ttrain-auc:0.66010\ttrain-merror:0.72793\ttest-mlogloss:2.09418\ttest-auc:0.64171\ttest-merror:0.73440\n",
      "[24]\ttrain-mlogloss:2.07817\ttrain-auc:0.66070\ttrain-merror:0.72755\ttest-mlogloss:2.09238\ttest-auc:0.64192\ttest-merror:0.73421\n",
      "[25]\ttrain-mlogloss:2.07608\ttrain-auc:0.66120\ttrain-merror:0.72734\ttest-mlogloss:2.09080\ttest-auc:0.64212\ttest-merror:0.73401\n",
      "[26]\ttrain-mlogloss:2.07412\ttrain-auc:0.66168\ttrain-merror:0.72710\ttest-mlogloss:2.08934\ttest-auc:0.64230\ttest-merror:0.73381\n",
      "[27]\ttrain-mlogloss:2.07219\ttrain-auc:0.66221\ttrain-merror:0.72681\ttest-mlogloss:2.08792\ttest-auc:0.64251\ttest-merror:0.73359\n",
      "[28]\ttrain-mlogloss:2.07032\ttrain-auc:0.66281\ttrain-merror:0.72644\ttest-mlogloss:2.08658\ttest-auc:0.64277\ttest-merror:0.73341\n",
      "[29]\ttrain-mlogloss:2.06856\ttrain-auc:0.66342\ttrain-merror:0.72613\ttest-mlogloss:2.08537\ttest-auc:0.64296\ttest-merror:0.73319\n",
      "[30]\ttrain-mlogloss:2.06694\ttrain-auc:0.66391\ttrain-merror:0.72592\ttest-mlogloss:2.08426\ttest-auc:0.64312\ttest-merror:0.73300\n",
      "[31]\ttrain-mlogloss:2.06545\ttrain-auc:0.66438\ttrain-merror:0.72568\ttest-mlogloss:2.08328\ttest-auc:0.64327\ttest-merror:0.73293\n",
      "[32]\ttrain-mlogloss:2.06397\ttrain-auc:0.66489\ttrain-merror:0.72540\ttest-mlogloss:2.08233\ttest-auc:0.64344\ttest-merror:0.73251\n",
      "[33]\ttrain-mlogloss:2.06263\ttrain-auc:0.66538\ttrain-merror:0.72513\ttest-mlogloss:2.08148\ttest-auc:0.64361\ttest-merror:0.73250\n",
      "[34]\ttrain-mlogloss:2.06128\ttrain-auc:0.66592\ttrain-merror:0.72480\ttest-mlogloss:2.08067\ttest-auc:0.64376\ttest-merror:0.73231\n",
      "[35]\ttrain-mlogloss:2.06006\ttrain-auc:0.66641\ttrain-merror:0.72447\ttest-mlogloss:2.07992\ttest-auc:0.64392\ttest-merror:0.73222\n",
      "[36]\ttrain-mlogloss:2.05889\ttrain-auc:0.66688\ttrain-merror:0.72426\ttest-mlogloss:2.07926\ttest-auc:0.64406\ttest-merror:0.73227\n",
      "[37]\ttrain-mlogloss:2.05775\ttrain-auc:0.66736\ttrain-merror:0.72397\ttest-mlogloss:2.07862\ttest-auc:0.64423\ttest-merror:0.73207\n",
      "[38]\ttrain-mlogloss:2.05666\ttrain-auc:0.66778\ttrain-merror:0.72375\ttest-mlogloss:2.07801\ttest-auc:0.64436\ttest-merror:0.73205\n",
      "[39]\ttrain-mlogloss:2.05558\ttrain-auc:0.66824\ttrain-merror:0.72338\ttest-mlogloss:2.07742\ttest-auc:0.64450\ttest-merror:0.73187\n",
      "[40]\ttrain-mlogloss:2.05458\ttrain-auc:0.66870\ttrain-merror:0.72313\ttest-mlogloss:2.07692\ttest-auc:0.64461\ttest-merror:0.73185\n",
      "[41]\ttrain-mlogloss:2.05367\ttrain-auc:0.66907\ttrain-merror:0.72292\ttest-mlogloss:2.07645\ttest-auc:0.64472\ttest-merror:0.73170\n",
      "[42]\ttrain-mlogloss:2.05271\ttrain-auc:0.66953\ttrain-merror:0.72265\ttest-mlogloss:2.07600\ttest-auc:0.64481\ttest-merror:0.73152\n",
      "[43]\ttrain-mlogloss:2.05181\ttrain-auc:0.66996\ttrain-merror:0.72238\ttest-mlogloss:2.07559\ttest-auc:0.64491\ttest-merror:0.73142\n",
      "[44]\ttrain-mlogloss:2.05092\ttrain-auc:0.67037\ttrain-merror:0.72216\ttest-mlogloss:2.07520\ttest-auc:0.64501\ttest-merror:0.73124\n",
      "[45]\ttrain-mlogloss:2.05004\ttrain-auc:0.67082\ttrain-merror:0.72186\ttest-mlogloss:2.07482\ttest-auc:0.64512\ttest-merror:0.73115\n",
      "[46]\ttrain-mlogloss:2.04923\ttrain-auc:0.67124\ttrain-merror:0.72163\ttest-mlogloss:2.07448\ttest-auc:0.64522\ttest-merror:0.73105\n",
      "[47]\ttrain-mlogloss:2.04845\ttrain-auc:0.67163\ttrain-merror:0.72144\ttest-mlogloss:2.07413\ttest-auc:0.64534\ttest-merror:0.73098\n",
      "[48]\ttrain-mlogloss:2.04765\ttrain-auc:0.67206\ttrain-merror:0.72120\ttest-mlogloss:2.07385\ttest-auc:0.64541\ttest-merror:0.73095\n",
      "[49]\ttrain-mlogloss:2.04687\ttrain-auc:0.67247\ttrain-merror:0.72098\ttest-mlogloss:2.07352\ttest-auc:0.64552\ttest-merror:0.73087\n",
      "[50]\ttrain-mlogloss:2.04610\ttrain-auc:0.67290\ttrain-merror:0.72076\ttest-mlogloss:2.07323\ttest-auc:0.64561\ttest-merror:0.73083\n",
      "[51]\ttrain-mlogloss:2.04536\ttrain-auc:0.67330\ttrain-merror:0.72062\ttest-mlogloss:2.07300\ttest-auc:0.64566\ttest-merror:0.73080\n",
      "[52]\ttrain-mlogloss:2.04459\ttrain-auc:0.67373\ttrain-merror:0.72032\ttest-mlogloss:2.07272\ttest-auc:0.64575\ttest-merror:0.73072\n",
      "[53]\ttrain-mlogloss:2.04386\ttrain-auc:0.67413\ttrain-merror:0.72008\ttest-mlogloss:2.07245\ttest-auc:0.64584\ttest-merror:0.73069\n",
      "[54]\ttrain-mlogloss:2.04319\ttrain-auc:0.67450\ttrain-merror:0.71979\ttest-mlogloss:2.07223\ttest-auc:0.64592\ttest-merror:0.73063\n",
      "[55]\ttrain-mlogloss:2.04253\ttrain-auc:0.67488\ttrain-merror:0.71959\ttest-mlogloss:2.07200\ttest-auc:0.64600\ttest-merror:0.73038\n",
      "[56]\ttrain-mlogloss:2.04185\ttrain-auc:0.67526\ttrain-merror:0.71941\ttest-mlogloss:2.07176\ttest-auc:0.64609\ttest-merror:0.73032\n",
      "[57]\ttrain-mlogloss:2.04117\ttrain-auc:0.67570\ttrain-merror:0.71918\ttest-mlogloss:2.07159\ttest-auc:0.64615\ttest-merror:0.73043\n",
      "[58]\ttrain-mlogloss:2.04050\ttrain-auc:0.67610\ttrain-merror:0.71891\ttest-mlogloss:2.07137\ttest-auc:0.64622\ttest-merror:0.73039\n",
      "[59]\ttrain-mlogloss:2.03986\ttrain-auc:0.67648\ttrain-merror:0.71872\ttest-mlogloss:2.07121\ttest-auc:0.64628\ttest-merror:0.73045\n",
      "[60]\ttrain-mlogloss:2.03922\ttrain-auc:0.67685\ttrain-merror:0.71850\ttest-mlogloss:2.07102\ttest-auc:0.64634\ttest-merror:0.73036\n",
      "[61]\ttrain-mlogloss:2.03856\ttrain-auc:0.67728\ttrain-merror:0.71823\ttest-mlogloss:2.07084\ttest-auc:0.64643\ttest-merror:0.73020\n",
      "[62]\ttrain-mlogloss:2.03794\ttrain-auc:0.67766\ttrain-merror:0.71799\ttest-mlogloss:2.07065\ttest-auc:0.64651\ttest-merror:0.73008\n",
      "[63]\ttrain-mlogloss:2.03737\ttrain-auc:0.67797\ttrain-merror:0.71780\ttest-mlogloss:2.07049\ttest-auc:0.64659\ttest-merror:0.73002\n",
      "[64]\ttrain-mlogloss:2.03685\ttrain-auc:0.67829\ttrain-merror:0.71763\ttest-mlogloss:2.07033\ttest-auc:0.64664\ttest-merror:0.73009\n",
      "[65]\ttrain-mlogloss:2.03624\ttrain-auc:0.67865\ttrain-merror:0.71736\ttest-mlogloss:2.07017\ttest-auc:0.64671\ttest-merror:0.72993\n",
      "[66]\ttrain-mlogloss:2.03565\ttrain-auc:0.67901\ttrain-merror:0.71717\ttest-mlogloss:2.07000\ttest-auc:0.64679\ttest-merror:0.72991\n",
      "[67]\ttrain-mlogloss:2.03511\ttrain-auc:0.67936\ttrain-merror:0.71695\ttest-mlogloss:2.06986\ttest-auc:0.64685\ttest-merror:0.72992\n",
      "[68]\ttrain-mlogloss:2.03455\ttrain-auc:0.67970\ttrain-merror:0.71671\ttest-mlogloss:2.06973\ttest-auc:0.64690\ttest-merror:0.72977\n",
      "[69]\ttrain-mlogloss:2.03396\ttrain-auc:0.68007\ttrain-merror:0.71653\ttest-mlogloss:2.06962\ttest-auc:0.64695\ttest-merror:0.72980\n",
      "[70]\ttrain-mlogloss:2.03337\ttrain-auc:0.68043\ttrain-merror:0.71624\ttest-mlogloss:2.06950\ttest-auc:0.64699\ttest-merror:0.72976\n",
      "[71]\ttrain-mlogloss:2.03276\ttrain-auc:0.68082\ttrain-merror:0.71604\ttest-mlogloss:2.06935\ttest-auc:0.64707\ttest-merror:0.72976\n",
      "[72]\ttrain-mlogloss:2.03223\ttrain-auc:0.68113\ttrain-merror:0.71585\ttest-mlogloss:2.06923\ttest-auc:0.64712\ttest-merror:0.72974\n",
      "[73]\ttrain-mlogloss:2.03170\ttrain-auc:0.68149\ttrain-merror:0.71570\ttest-mlogloss:2.06912\ttest-auc:0.64718\ttest-merror:0.72977\n",
      "[74]\ttrain-mlogloss:2.03114\ttrain-auc:0.68183\ttrain-merror:0.71546\ttest-mlogloss:2.06898\ttest-auc:0.64724\ttest-merror:0.72971\n",
      "[75]\ttrain-mlogloss:2.03062\ttrain-auc:0.68214\ttrain-merror:0.71527\ttest-mlogloss:2.06888\ttest-auc:0.64728\ttest-merror:0.72966\n",
      "[76]\ttrain-mlogloss:2.03014\ttrain-auc:0.68243\ttrain-merror:0.71511\ttest-mlogloss:2.06881\ttest-auc:0.64729\ttest-merror:0.72966\n",
      "[77]\ttrain-mlogloss:2.02968\ttrain-auc:0.68271\ttrain-merror:0.71489\ttest-mlogloss:2.06874\ttest-auc:0.64732\ttest-merror:0.72954\n",
      "[78]\ttrain-mlogloss:2.02920\ttrain-auc:0.68300\ttrain-merror:0.71472\ttest-mlogloss:2.06861\ttest-auc:0.64738\ttest-merror:0.72955\n",
      "[79]\ttrain-mlogloss:2.02866\ttrain-auc:0.68336\ttrain-merror:0.71449\ttest-mlogloss:2.06853\ttest-auc:0.64741\ttest-merror:0.72939\n",
      "[80]\ttrain-mlogloss:2.02818\ttrain-auc:0.68366\ttrain-merror:0.71430\ttest-mlogloss:2.06845\ttest-auc:0.64744\ttest-merror:0.72931\n",
      "[81]\ttrain-mlogloss:2.02761\ttrain-auc:0.68405\ttrain-merror:0.71405\ttest-mlogloss:2.06837\ttest-auc:0.64748\ttest-merror:0.72938\n",
      "[82]\ttrain-mlogloss:2.02711\ttrain-auc:0.68437\ttrain-merror:0.71392\ttest-mlogloss:2.06827\ttest-auc:0.64753\ttest-merror:0.72922\n",
      "[83]\ttrain-mlogloss:2.02668\ttrain-auc:0.68463\ttrain-merror:0.71374\ttest-mlogloss:2.06819\ttest-auc:0.64756\ttest-merror:0.72922\n",
      "[84]\ttrain-mlogloss:2.02620\ttrain-auc:0.68494\ttrain-merror:0.71352\ttest-mlogloss:2.06811\ttest-auc:0.64760\ttest-merror:0.72916\n",
      "[85]\ttrain-mlogloss:2.02575\ttrain-auc:0.68522\ttrain-merror:0.71338\ttest-mlogloss:2.06804\ttest-auc:0.64763\ttest-merror:0.72913\n",
      "[86]\ttrain-mlogloss:2.02528\ttrain-auc:0.68551\ttrain-merror:0.71322\ttest-mlogloss:2.06795\ttest-auc:0.64767\ttest-merror:0.72908\n",
      "[87]\ttrain-mlogloss:2.02481\ttrain-auc:0.68580\ttrain-merror:0.71304\ttest-mlogloss:2.06786\ttest-auc:0.64772\ttest-merror:0.72904\n",
      "[88]\ttrain-mlogloss:2.02432\ttrain-auc:0.68613\ttrain-merror:0.71284\ttest-mlogloss:2.06778\ttest-auc:0.64776\ttest-merror:0.72909\n",
      "[89]\ttrain-mlogloss:2.02385\ttrain-auc:0.68642\ttrain-merror:0.71266\ttest-mlogloss:2.06770\ttest-auc:0.64779\ttest-merror:0.72903\n",
      "[90]\ttrain-mlogloss:2.02341\ttrain-auc:0.68669\ttrain-merror:0.71246\ttest-mlogloss:2.06764\ttest-auc:0.64782\ttest-merror:0.72908\n",
      "[91]\ttrain-mlogloss:2.02291\ttrain-auc:0.68700\ttrain-merror:0.71224\ttest-mlogloss:2.06757\ttest-auc:0.64784\ttest-merror:0.72903\n",
      "[92]\ttrain-mlogloss:2.02249\ttrain-auc:0.68727\ttrain-merror:0.71209\ttest-mlogloss:2.06751\ttest-auc:0.64787\ttest-merror:0.72902\n",
      "[93]\ttrain-mlogloss:2.02203\ttrain-auc:0.68758\ttrain-merror:0.71191\ttest-mlogloss:2.06745\ttest-auc:0.64789\ttest-merror:0.72891\n",
      "[94]\ttrain-mlogloss:2.02163\ttrain-auc:0.68783\ttrain-merror:0.71173\ttest-mlogloss:2.06739\ttest-auc:0.64794\ttest-merror:0.72887\n",
      "[95]\ttrain-mlogloss:2.02124\ttrain-auc:0.68807\ttrain-merror:0.71156\ttest-mlogloss:2.06732\ttest-auc:0.64797\ttest-merror:0.72884\n",
      "[96]\ttrain-mlogloss:2.02082\ttrain-auc:0.68834\ttrain-merror:0.71141\ttest-mlogloss:2.06724\ttest-auc:0.64802\ttest-merror:0.72879\n",
      "[97]\ttrain-mlogloss:2.02035\ttrain-auc:0.68865\ttrain-merror:0.71122\ttest-mlogloss:2.06720\ttest-auc:0.64803\ttest-merror:0.72875\n",
      "[98]\ttrain-mlogloss:2.01993\ttrain-auc:0.68892\ttrain-merror:0.71105\ttest-mlogloss:2.06715\ttest-auc:0.64806\ttest-merror:0.72879\n",
      "[99]\ttrain-mlogloss:2.01952\ttrain-auc:0.68918\ttrain-merror:0.71090\ttest-mlogloss:2.06710\ttest-auc:0.64808\ttest-merror:0.72875\n",
      "[100]\ttrain-mlogloss:2.01913\ttrain-auc:0.68942\ttrain-merror:0.71078\ttest-mlogloss:2.06704\ttest-auc:0.64812\ttest-merror:0.72876\n",
      "[101]\ttrain-mlogloss:2.01875\ttrain-auc:0.68967\ttrain-merror:0.71058\ttest-mlogloss:2.06699\ttest-auc:0.64814\ttest-merror:0.72868\n",
      "[102]\ttrain-mlogloss:2.01831\ttrain-auc:0.68996\ttrain-merror:0.71037\ttest-mlogloss:2.06692\ttest-auc:0.64816\ttest-merror:0.72864\n",
      "[103]\ttrain-mlogloss:2.01789\ttrain-auc:0.69023\ttrain-merror:0.71018\ttest-mlogloss:2.06686\ttest-auc:0.64819\ttest-merror:0.72863\n",
      "[104]\ttrain-mlogloss:2.01744\ttrain-auc:0.69051\ttrain-merror:0.70996\ttest-mlogloss:2.06680\ttest-auc:0.64821\ttest-merror:0.72869\n",
      "[105]\ttrain-mlogloss:2.01703\ttrain-auc:0.69077\ttrain-merror:0.70982\ttest-mlogloss:2.06674\ttest-auc:0.64824\ttest-merror:0.72856\n",
      "[106]\ttrain-mlogloss:2.01665\ttrain-auc:0.69100\ttrain-merror:0.70969\ttest-mlogloss:2.06671\ttest-auc:0.64825\ttest-merror:0.72856\n",
      "[107]\ttrain-mlogloss:2.01620\ttrain-auc:0.69129\ttrain-merror:0.70951\ttest-mlogloss:2.06664\ttest-auc:0.64828\ttest-merror:0.72855\n",
      "[108]\ttrain-mlogloss:2.01576\ttrain-auc:0.69156\ttrain-merror:0.70935\ttest-mlogloss:2.06659\ttest-auc:0.64831\ttest-merror:0.72854\n",
      "[109]\ttrain-mlogloss:2.01537\ttrain-auc:0.69180\ttrain-merror:0.70922\ttest-mlogloss:2.06655\ttest-auc:0.64832\ttest-merror:0.72860\n",
      "[110]\ttrain-mlogloss:2.01499\ttrain-auc:0.69203\ttrain-merror:0.70906\ttest-mlogloss:2.06650\ttest-auc:0.64833\ttest-merror:0.72852\n",
      "[111]\ttrain-mlogloss:2.01461\ttrain-auc:0.69228\ttrain-merror:0.70887\ttest-mlogloss:2.06648\ttest-auc:0.64834\ttest-merror:0.72842\n",
      "[112]\ttrain-mlogloss:2.01419\ttrain-auc:0.69256\ttrain-merror:0.70876\ttest-mlogloss:2.06646\ttest-auc:0.64833\ttest-merror:0.72846\n",
      "[113]\ttrain-mlogloss:2.01383\ttrain-auc:0.69279\ttrain-merror:0.70863\ttest-mlogloss:2.06643\ttest-auc:0.64835\ttest-merror:0.72837\n",
      "[114]\ttrain-mlogloss:2.01349\ttrain-auc:0.69300\ttrain-merror:0.70847\ttest-mlogloss:2.06640\ttest-auc:0.64836\ttest-merror:0.72826\n",
      "[115]\ttrain-mlogloss:2.01307\ttrain-auc:0.69327\ttrain-merror:0.70831\ttest-mlogloss:2.06635\ttest-auc:0.64838\ttest-merror:0.72823\n",
      "[116]\ttrain-mlogloss:2.01268\ttrain-auc:0.69352\ttrain-merror:0.70813\ttest-mlogloss:2.06631\ttest-auc:0.64840\ttest-merror:0.72833\n",
      "[117]\ttrain-mlogloss:2.01227\ttrain-auc:0.69378\ttrain-merror:0.70795\ttest-mlogloss:2.06626\ttest-auc:0.64843\ttest-merror:0.72827\n",
      "[118]\ttrain-mlogloss:2.01183\ttrain-auc:0.69404\ttrain-merror:0.70779\ttest-mlogloss:2.06621\ttest-auc:0.64846\ttest-merror:0.72822\n",
      "[119]\ttrain-mlogloss:2.01141\ttrain-auc:0.69431\ttrain-merror:0.70759\ttest-mlogloss:2.06615\ttest-auc:0.64849\ttest-merror:0.72828\n",
      "[120]\ttrain-mlogloss:2.01099\ttrain-auc:0.69457\ttrain-merror:0.70743\ttest-mlogloss:2.06609\ttest-auc:0.64851\ttest-merror:0.72818\n",
      "[121]\ttrain-mlogloss:2.01060\ttrain-auc:0.69482\ttrain-merror:0.70725\ttest-mlogloss:2.06605\ttest-auc:0.64854\ttest-merror:0.72825\n",
      "[122]\ttrain-mlogloss:2.01020\ttrain-auc:0.69507\ttrain-merror:0.70705\ttest-mlogloss:2.06602\ttest-auc:0.64855\ttest-merror:0.72821\n",
      "[123]\ttrain-mlogloss:2.00978\ttrain-auc:0.69532\ttrain-merror:0.70687\ttest-mlogloss:2.06596\ttest-auc:0.64857\ttest-merror:0.72818\n",
      "[124]\ttrain-mlogloss:2.00942\ttrain-auc:0.69554\ttrain-merror:0.70677\ttest-mlogloss:2.06595\ttest-auc:0.64858\ttest-merror:0.72809\n",
      "[125]\ttrain-mlogloss:2.00912\ttrain-auc:0.69572\ttrain-merror:0.70666\ttest-mlogloss:2.06594\ttest-auc:0.64857\ttest-merror:0.72817\n",
      "[126]\ttrain-mlogloss:2.00874\ttrain-auc:0.69595\ttrain-merror:0.70647\ttest-mlogloss:2.06592\ttest-auc:0.64857\ttest-merror:0.72816\n",
      "[127]\ttrain-mlogloss:2.00830\ttrain-auc:0.69623\ttrain-merror:0.70628\ttest-mlogloss:2.06588\ttest-auc:0.64858\ttest-merror:0.72816\n",
      "[128]\ttrain-mlogloss:2.00794\ttrain-auc:0.69645\ttrain-merror:0.70616\ttest-mlogloss:2.06584\ttest-auc:0.64860\ttest-merror:0.72811\n",
      "[129]\ttrain-mlogloss:2.00762\ttrain-auc:0.69664\ttrain-merror:0.70603\ttest-mlogloss:2.06580\ttest-auc:0.64863\ttest-merror:0.72812\n",
      "[130]\ttrain-mlogloss:2.00721\ttrain-auc:0.69690\ttrain-merror:0.70581\ttest-mlogloss:2.06576\ttest-auc:0.64865\ttest-merror:0.72808\n",
      "[131]\ttrain-mlogloss:2.00682\ttrain-auc:0.69714\ttrain-merror:0.70565\ttest-mlogloss:2.06574\ttest-auc:0.64865\ttest-merror:0.72806\n",
      "[132]\ttrain-mlogloss:2.00644\ttrain-auc:0.69739\ttrain-merror:0.70547\ttest-mlogloss:2.06573\ttest-auc:0.64866\ttest-merror:0.72795\n",
      "[133]\ttrain-mlogloss:2.00609\ttrain-auc:0.69760\ttrain-merror:0.70532\ttest-mlogloss:2.06569\ttest-auc:0.64868\ttest-merror:0.72793\n",
      "[134]\ttrain-mlogloss:2.00569\ttrain-auc:0.69784\ttrain-merror:0.70521\ttest-mlogloss:2.06564\ttest-auc:0.64871\ttest-merror:0.72797\n",
      "[135]\ttrain-mlogloss:2.00538\ttrain-auc:0.69802\ttrain-merror:0.70505\ttest-mlogloss:2.06563\ttest-auc:0.64872\ttest-merror:0.72794\n",
      "[136]\ttrain-mlogloss:2.00506\ttrain-auc:0.69823\ttrain-merror:0.70495\ttest-mlogloss:2.06561\ttest-auc:0.64872\ttest-merror:0.72796\n",
      "[137]\ttrain-mlogloss:2.00467\ttrain-auc:0.69847\ttrain-merror:0.70481\ttest-mlogloss:2.06557\ttest-auc:0.64874\ttest-merror:0.72791\n",
      "[138]\ttrain-mlogloss:2.00436\ttrain-auc:0.69864\ttrain-merror:0.70468\ttest-mlogloss:2.06554\ttest-auc:0.64876\ttest-merror:0.72785\n",
      "[139]\ttrain-mlogloss:2.00409\ttrain-auc:0.69879\ttrain-merror:0.70458\ttest-mlogloss:2.06552\ttest-auc:0.64877\ttest-merror:0.72782\n",
      "[140]\ttrain-mlogloss:2.00375\ttrain-auc:0.69901\ttrain-merror:0.70445\ttest-mlogloss:2.06551\ttest-auc:0.64877\ttest-merror:0.72775\n",
      "[141]\ttrain-mlogloss:2.00341\ttrain-auc:0.69923\ttrain-merror:0.70429\ttest-mlogloss:2.06550\ttest-auc:0.64877\ttest-merror:0.72779\n",
      "[142]\ttrain-mlogloss:2.00300\ttrain-auc:0.69949\ttrain-merror:0.70408\ttest-mlogloss:2.06547\ttest-auc:0.64878\ttest-merror:0.72779\n",
      "[143]\ttrain-mlogloss:2.00265\ttrain-auc:0.69971\ttrain-merror:0.70396\ttest-mlogloss:2.06545\ttest-auc:0.64880\ttest-merror:0.72781\n",
      "[144]\ttrain-mlogloss:2.00238\ttrain-auc:0.69987\ttrain-merror:0.70388\ttest-mlogloss:2.06542\ttest-auc:0.64881\ttest-merror:0.72785\n",
      "[145]\ttrain-mlogloss:2.00208\ttrain-auc:0.70005\ttrain-merror:0.70376\ttest-mlogloss:2.06539\ttest-auc:0.64882\ttest-merror:0.72779\n",
      "[146]\ttrain-mlogloss:2.00173\ttrain-auc:0.70025\ttrain-merror:0.70356\ttest-mlogloss:2.06538\ttest-auc:0.64882\ttest-merror:0.72784\n",
      "[147]\ttrain-mlogloss:2.00140\ttrain-auc:0.70046\ttrain-merror:0.70341\ttest-mlogloss:2.06534\ttest-auc:0.64884\ttest-merror:0.72774\n",
      "[148]\ttrain-mlogloss:2.00105\ttrain-auc:0.70067\ttrain-merror:0.70328\ttest-mlogloss:2.06533\ttest-auc:0.64886\ttest-merror:0.72769\n",
      "[149]\ttrain-mlogloss:2.00073\ttrain-auc:0.70088\ttrain-merror:0.70318\ttest-mlogloss:2.06531\ttest-auc:0.64887\ttest-merror:0.72768\n",
      "[150]\ttrain-mlogloss:2.00036\ttrain-auc:0.70111\ttrain-merror:0.70305\ttest-mlogloss:2.06529\ttest-auc:0.64887\ttest-merror:0.72769\n",
      "[151]\ttrain-mlogloss:1.99996\ttrain-auc:0.70137\ttrain-merror:0.70286\ttest-mlogloss:2.06526\ttest-auc:0.64889\ttest-merror:0.72767\n",
      "[152]\ttrain-mlogloss:1.99962\ttrain-auc:0.70159\ttrain-merror:0.70273\ttest-mlogloss:2.06524\ttest-auc:0.64890\ttest-merror:0.72758\n",
      "[153]\ttrain-mlogloss:1.99921\ttrain-auc:0.70185\ttrain-merror:0.70252\ttest-mlogloss:2.06521\ttest-auc:0.64891\ttest-merror:0.72754\n",
      "[154]\ttrain-mlogloss:1.99883\ttrain-auc:0.70208\ttrain-merror:0.70237\ttest-mlogloss:2.06521\ttest-auc:0.64890\ttest-merror:0.72753\n",
      "[155]\ttrain-mlogloss:1.99844\ttrain-auc:0.70232\ttrain-merror:0.70219\ttest-mlogloss:2.06518\ttest-auc:0.64892\ttest-merror:0.72752\n",
      "[156]\ttrain-mlogloss:1.99806\ttrain-auc:0.70256\ttrain-merror:0.70202\ttest-mlogloss:2.06515\ttest-auc:0.64893\ttest-merror:0.72745\n",
      "[157]\ttrain-mlogloss:1.99779\ttrain-auc:0.70272\ttrain-merror:0.70186\ttest-mlogloss:2.06511\ttest-auc:0.64895\ttest-merror:0.72744\n",
      "[158]\ttrain-mlogloss:1.99742\ttrain-auc:0.70295\ttrain-merror:0.70166\ttest-mlogloss:2.06509\ttest-auc:0.64895\ttest-merror:0.72756\n",
      "[159]\ttrain-mlogloss:1.99709\ttrain-auc:0.70314\ttrain-merror:0.70149\ttest-mlogloss:2.06509\ttest-auc:0.64895\ttest-merror:0.72761\n",
      "[160]\ttrain-mlogloss:1.99678\ttrain-auc:0.70332\ttrain-merror:0.70137\ttest-mlogloss:2.06508\ttest-auc:0.64895\ttest-merror:0.72759\n",
      "[161]\ttrain-mlogloss:1.99647\ttrain-auc:0.70352\ttrain-merror:0.70124\ttest-mlogloss:2.06507\ttest-auc:0.64895\ttest-merror:0.72769\n",
      "[162]\ttrain-mlogloss:1.99611\ttrain-auc:0.70373\ttrain-merror:0.70108\ttest-mlogloss:2.06505\ttest-auc:0.64896\ttest-merror:0.72748\n",
      "[163]\ttrain-mlogloss:1.99581\ttrain-auc:0.70391\ttrain-merror:0.70095\ttest-mlogloss:2.06503\ttest-auc:0.64897\ttest-merror:0.72734\n",
      "[164]\ttrain-mlogloss:1.99543\ttrain-auc:0.70414\ttrain-merror:0.70078\ttest-mlogloss:2.06499\ttest-auc:0.64899\ttest-merror:0.72737\n",
      "[165]\ttrain-mlogloss:1.99507\ttrain-auc:0.70437\ttrain-merror:0.70062\ttest-mlogloss:2.06498\ttest-auc:0.64899\ttest-merror:0.72735\n",
      "[166]\ttrain-mlogloss:1.99477\ttrain-auc:0.70456\ttrain-merror:0.70053\ttest-mlogloss:2.06496\ttest-auc:0.64900\ttest-merror:0.72731\n",
      "[167]\ttrain-mlogloss:1.99441\ttrain-auc:0.70478\ttrain-merror:0.70036\ttest-mlogloss:2.06493\ttest-auc:0.64902\ttest-merror:0.72733\n",
      "[168]\ttrain-mlogloss:1.99411\ttrain-auc:0.70495\ttrain-merror:0.70022\ttest-mlogloss:2.06492\ttest-auc:0.64903\ttest-merror:0.72734\n",
      "[169]\ttrain-mlogloss:1.99376\ttrain-auc:0.70517\ttrain-merror:0.70010\ttest-mlogloss:2.06491\ttest-auc:0.64903\ttest-merror:0.72731\n",
      "[170]\ttrain-mlogloss:1.99347\ttrain-auc:0.70535\ttrain-merror:0.69996\ttest-mlogloss:2.06490\ttest-auc:0.64904\ttest-merror:0.72733\n",
      "[171]\ttrain-mlogloss:1.99321\ttrain-auc:0.70551\ttrain-merror:0.69988\ttest-mlogloss:2.06490\ttest-auc:0.64904\ttest-merror:0.72733\n",
      "[172]\ttrain-mlogloss:1.99287\ttrain-auc:0.70572\ttrain-merror:0.69973\ttest-mlogloss:2.06488\ttest-auc:0.64905\ttest-merror:0.72728\n",
      "[173]\ttrain-mlogloss:1.99259\ttrain-auc:0.70589\ttrain-merror:0.69963\ttest-mlogloss:2.06487\ttest-auc:0.64905\ttest-merror:0.72725\n",
      "[174]\ttrain-mlogloss:1.99228\ttrain-auc:0.70608\ttrain-merror:0.69949\ttest-mlogloss:2.06484\ttest-auc:0.64907\ttest-merror:0.72725\n",
      "[175]\ttrain-mlogloss:1.99200\ttrain-auc:0.70624\ttrain-merror:0.69938\ttest-mlogloss:2.06483\ttest-auc:0.64908\ttest-merror:0.72725\n",
      "[176]\ttrain-mlogloss:1.99177\ttrain-auc:0.70637\ttrain-merror:0.69924\ttest-mlogloss:2.06483\ttest-auc:0.64908\ttest-merror:0.72723\n",
      "[177]\ttrain-mlogloss:1.99143\ttrain-auc:0.70656\ttrain-merror:0.69906\ttest-mlogloss:2.06479\ttest-auc:0.64910\ttest-merror:0.72727\n",
      "[178]\ttrain-mlogloss:1.99114\ttrain-auc:0.70674\ttrain-merror:0.69892\ttest-mlogloss:2.06478\ttest-auc:0.64910\ttest-merror:0.72725\n",
      "[179]\ttrain-mlogloss:1.99082\ttrain-auc:0.70694\ttrain-merror:0.69878\ttest-mlogloss:2.06480\ttest-auc:0.64909\ttest-merror:0.72721\n",
      "[180]\ttrain-mlogloss:1.99056\ttrain-auc:0.70709\ttrain-merror:0.69864\ttest-mlogloss:2.06479\ttest-auc:0.64909\ttest-merror:0.72717\n",
      "[181]\ttrain-mlogloss:1.99028\ttrain-auc:0.70725\ttrain-merror:0.69855\ttest-mlogloss:2.06476\ttest-auc:0.64911\ttest-merror:0.72711\n",
      "[182]\ttrain-mlogloss:1.99000\ttrain-auc:0.70742\ttrain-merror:0.69847\ttest-mlogloss:2.06475\ttest-auc:0.64911\ttest-merror:0.72707\n",
      "[183]\ttrain-mlogloss:1.98965\ttrain-auc:0.70763\ttrain-merror:0.69834\ttest-mlogloss:2.06473\ttest-auc:0.64913\ttest-merror:0.72704\n",
      "[184]\ttrain-mlogloss:1.98935\ttrain-auc:0.70781\ttrain-merror:0.69821\ttest-mlogloss:2.06471\ttest-auc:0.64914\ttest-merror:0.72709\n",
      "[185]\ttrain-mlogloss:1.98908\ttrain-auc:0.70796\ttrain-merror:0.69808\ttest-mlogloss:2.06468\ttest-auc:0.64915\ttest-merror:0.72706\n",
      "[186]\ttrain-mlogloss:1.98878\ttrain-auc:0.70815\ttrain-merror:0.69790\ttest-mlogloss:2.06467\ttest-auc:0.64915\ttest-merror:0.72711\n",
      "[187]\ttrain-mlogloss:1.98849\ttrain-auc:0.70834\ttrain-merror:0.69776\ttest-mlogloss:2.06467\ttest-auc:0.64914\ttest-merror:0.72714\n",
      "[188]\ttrain-mlogloss:1.98820\ttrain-auc:0.70851\ttrain-merror:0.69762\ttest-mlogloss:2.06467\ttest-auc:0.64915\ttest-merror:0.72707\n",
      "[189]\ttrain-mlogloss:1.98787\ttrain-auc:0.70872\ttrain-merror:0.69749\ttest-mlogloss:2.06466\ttest-auc:0.64916\ttest-merror:0.72711\n",
      "[190]\ttrain-mlogloss:1.98756\ttrain-auc:0.70891\ttrain-merror:0.69735\ttest-mlogloss:2.06465\ttest-auc:0.64916\ttest-merror:0.72706\n",
      "[191]\ttrain-mlogloss:1.98725\ttrain-auc:0.70910\ttrain-merror:0.69719\ttest-mlogloss:2.06465\ttest-auc:0.64916\ttest-merror:0.72706\n",
      "[192]\ttrain-mlogloss:1.98697\ttrain-auc:0.70927\ttrain-merror:0.69712\ttest-mlogloss:2.06463\ttest-auc:0.64917\ttest-merror:0.72713\n",
      "[193]\ttrain-mlogloss:1.98666\ttrain-auc:0.70945\ttrain-merror:0.69704\ttest-mlogloss:2.06464\ttest-auc:0.64917\ttest-merror:0.72712\n",
      "[194]\ttrain-mlogloss:1.98639\ttrain-auc:0.70961\ttrain-merror:0.69693\ttest-mlogloss:2.06464\ttest-auc:0.64917\ttest-merror:0.72711\n",
      "[195]\ttrain-mlogloss:1.98611\ttrain-auc:0.70978\ttrain-merror:0.69681\ttest-mlogloss:2.06462\ttest-auc:0.64918\ttest-merror:0.72714\n",
      "[196]\ttrain-mlogloss:1.98579\ttrain-auc:0.70999\ttrain-merror:0.69666\ttest-mlogloss:2.06463\ttest-auc:0.64917\ttest-merror:0.72703\n",
      "[197]\ttrain-mlogloss:1.98548\ttrain-auc:0.71017\ttrain-merror:0.69654\ttest-mlogloss:2.06463\ttest-auc:0.64917\ttest-merror:0.72703\n",
      "[198]\ttrain-mlogloss:1.98514\ttrain-auc:0.71038\ttrain-merror:0.69640\ttest-mlogloss:2.06463\ttest-auc:0.64917\ttest-merror:0.72703\n",
      "[199]\ttrain-mlogloss:1.98481\ttrain-auc:0.71058\ttrain-merror:0.69620\ttest-mlogloss:2.06460\ttest-auc:0.64918\ttest-merror:0.72709\n",
      "[200]\ttrain-mlogloss:1.98451\ttrain-auc:0.71077\ttrain-merror:0.69603\ttest-mlogloss:2.06459\ttest-auc:0.64919\ttest-merror:0.72710\n",
      "[201]\ttrain-mlogloss:1.98419\ttrain-auc:0.71096\ttrain-merror:0.69590\ttest-mlogloss:2.06457\ttest-auc:0.64920\ttest-merror:0.72711\n",
      "[202]\ttrain-mlogloss:1.98394\ttrain-auc:0.71110\ttrain-merror:0.69578\ttest-mlogloss:2.06456\ttest-auc:0.64921\ttest-merror:0.72715\n",
      "[203]\ttrain-mlogloss:1.98368\ttrain-auc:0.71124\ttrain-merror:0.69568\ttest-mlogloss:2.06454\ttest-auc:0.64923\ttest-merror:0.72713\n",
      "[204]\ttrain-mlogloss:1.98343\ttrain-auc:0.71138\ttrain-merror:0.69557\ttest-mlogloss:2.06453\ttest-auc:0.64923\ttest-merror:0.72708\n",
      "[205]\ttrain-mlogloss:1.98311\ttrain-auc:0.71158\ttrain-merror:0.69546\ttest-mlogloss:2.06453\ttest-auc:0.64923\ttest-merror:0.72708\n",
      "[206]\ttrain-mlogloss:1.98281\ttrain-auc:0.71175\ttrain-merror:0.69532\ttest-mlogloss:2.06452\ttest-auc:0.64923\ttest-merror:0.72706\n",
      "[207]\ttrain-mlogloss:1.98250\ttrain-auc:0.71193\ttrain-merror:0.69520\ttest-mlogloss:2.06451\ttest-auc:0.64924\ttest-merror:0.72703\n",
      "[208]\ttrain-mlogloss:1.98225\ttrain-auc:0.71206\ttrain-merror:0.69508\ttest-mlogloss:2.06451\ttest-auc:0.64924\ttest-merror:0.72701\n",
      "[209]\ttrain-mlogloss:1.98194\ttrain-auc:0.71225\ttrain-merror:0.69497\ttest-mlogloss:2.06449\ttest-auc:0.64925\ttest-merror:0.72704\n",
      "[210]\ttrain-mlogloss:1.98164\ttrain-auc:0.71242\ttrain-merror:0.69481\ttest-mlogloss:2.06448\ttest-auc:0.64926\ttest-merror:0.72709\n",
      "[211]\ttrain-mlogloss:1.98134\ttrain-auc:0.71259\ttrain-merror:0.69467\ttest-mlogloss:2.06449\ttest-auc:0.64926\ttest-merror:0.72706\n",
      "[212]\ttrain-mlogloss:1.98115\ttrain-auc:0.71270\ttrain-merror:0.69457\ttest-mlogloss:2.06449\ttest-auc:0.64926\ttest-merror:0.72713\n",
      "[213]\ttrain-mlogloss:1.98082\ttrain-auc:0.71289\ttrain-merror:0.69447\ttest-mlogloss:2.06448\ttest-auc:0.64926\ttest-merror:0.72713\n",
      "[214]\ttrain-mlogloss:1.98045\ttrain-auc:0.71312\ttrain-merror:0.69425\ttest-mlogloss:2.06449\ttest-auc:0.64925\ttest-merror:0.72716\n",
      "[215]\ttrain-mlogloss:1.98018\ttrain-auc:0.71328\ttrain-merror:0.69414\ttest-mlogloss:2.06449\ttest-auc:0.64925\ttest-merror:0.72716\n",
      "[216]\ttrain-mlogloss:1.97986\ttrain-auc:0.71347\ttrain-merror:0.69403\ttest-mlogloss:2.06447\ttest-auc:0.64926\ttest-merror:0.72706\n",
      "[217]\ttrain-mlogloss:1.97958\ttrain-auc:0.71363\ttrain-merror:0.69390\ttest-mlogloss:2.06447\ttest-auc:0.64926\ttest-merror:0.72701\n",
      "[218]\ttrain-mlogloss:1.97923\ttrain-auc:0.71383\ttrain-merror:0.69378\ttest-mlogloss:2.06445\ttest-auc:0.64927\ttest-merror:0.72703\n",
      "[219]\ttrain-mlogloss:1.97889\ttrain-auc:0.71401\ttrain-merror:0.69361\ttest-mlogloss:2.06445\ttest-auc:0.64927\ttest-merror:0.72694\n",
      "[220]\ttrain-mlogloss:1.97861\ttrain-auc:0.71416\ttrain-merror:0.69350\ttest-mlogloss:2.06444\ttest-auc:0.64928\ttest-merror:0.72699\n",
      "[221]\ttrain-mlogloss:1.97829\ttrain-auc:0.71434\ttrain-merror:0.69335\ttest-mlogloss:2.06443\ttest-auc:0.64928\ttest-merror:0.72706\n",
      "[222]\ttrain-mlogloss:1.97801\ttrain-auc:0.71450\ttrain-merror:0.69320\ttest-mlogloss:2.06444\ttest-auc:0.64927\ttest-merror:0.72709\n",
      "[223]\ttrain-mlogloss:1.97774\ttrain-auc:0.71466\ttrain-merror:0.69309\ttest-mlogloss:2.06444\ttest-auc:0.64927\ttest-merror:0.72705\n",
      "[224]\ttrain-mlogloss:1.97746\ttrain-auc:0.71482\ttrain-merror:0.69294\ttest-mlogloss:2.06443\ttest-auc:0.64928\ttest-merror:0.72702\n",
      "[225]\ttrain-mlogloss:1.97712\ttrain-auc:0.71502\ttrain-merror:0.69283\ttest-mlogloss:2.06443\ttest-auc:0.64928\ttest-merror:0.72693\n",
      "[226]\ttrain-mlogloss:1.97683\ttrain-auc:0.71518\ttrain-merror:0.69269\ttest-mlogloss:2.06443\ttest-auc:0.64928\ttest-merror:0.72700\n",
      "[227]\ttrain-mlogloss:1.97655\ttrain-auc:0.71535\ttrain-merror:0.69255\ttest-mlogloss:2.06442\ttest-auc:0.64928\ttest-merror:0.72702\n",
      "[228]\ttrain-mlogloss:1.97627\ttrain-auc:0.71551\ttrain-merror:0.69244\ttest-mlogloss:2.06441\ttest-auc:0.64928\ttest-merror:0.72703\n",
      "[229]\ttrain-mlogloss:1.97600\ttrain-auc:0.71567\ttrain-merror:0.69230\ttest-mlogloss:2.06443\ttest-auc:0.64928\ttest-merror:0.72706\n",
      "[230]\ttrain-mlogloss:1.97575\ttrain-auc:0.71581\ttrain-merror:0.69221\ttest-mlogloss:2.06442\ttest-auc:0.64928\ttest-merror:0.72704\n",
      "[231]\ttrain-mlogloss:1.97541\ttrain-auc:0.71601\ttrain-merror:0.69207\ttest-mlogloss:2.06439\ttest-auc:0.64930\ttest-merror:0.72706\n",
      "[232]\ttrain-mlogloss:1.97515\ttrain-auc:0.71617\ttrain-merror:0.69199\ttest-mlogloss:2.06439\ttest-auc:0.64929\ttest-merror:0.72706\n",
      "[233]\ttrain-mlogloss:1.97486\ttrain-auc:0.71632\ttrain-merror:0.69180\ttest-mlogloss:2.06439\ttest-auc:0.64930\ttest-merror:0.72701\n",
      "[234]\ttrain-mlogloss:1.97459\ttrain-auc:0.71648\ttrain-merror:0.69171\ttest-mlogloss:2.06438\ttest-auc:0.64930\ttest-merror:0.72707\n",
      "[235]\ttrain-mlogloss:1.97431\ttrain-auc:0.71664\ttrain-merror:0.69161\ttest-mlogloss:2.06437\ttest-auc:0.64931\ttest-merror:0.72703\n",
      "[236]\ttrain-mlogloss:1.97406\ttrain-auc:0.71679\ttrain-merror:0.69148\ttest-mlogloss:2.06437\ttest-auc:0.64931\ttest-merror:0.72706\n",
      "[237]\ttrain-mlogloss:1.97381\ttrain-auc:0.71694\ttrain-merror:0.69136\ttest-mlogloss:2.06437\ttest-auc:0.64931\ttest-merror:0.72700\n",
      "[238]\ttrain-mlogloss:1.97352\ttrain-auc:0.71710\ttrain-merror:0.69121\ttest-mlogloss:2.06438\ttest-auc:0.64930\ttest-merror:0.72699\n",
      "[239]\ttrain-mlogloss:1.97322\ttrain-auc:0.71726\ttrain-merror:0.69110\ttest-mlogloss:2.06438\ttest-auc:0.64929\ttest-merror:0.72703\n",
      "[240]\ttrain-mlogloss:1.97298\ttrain-auc:0.71740\ttrain-merror:0.69097\ttest-mlogloss:2.06439\ttest-auc:0.64929\ttest-merror:0.72700\n",
      "[241]\ttrain-mlogloss:1.97272\ttrain-auc:0.71756\ttrain-merror:0.69085\ttest-mlogloss:2.06439\ttest-auc:0.64929\ttest-merror:0.72698\n",
      "[242]\ttrain-mlogloss:1.97246\ttrain-auc:0.71770\ttrain-merror:0.69068\ttest-mlogloss:2.06440\ttest-auc:0.64928\ttest-merror:0.72700\n",
      "[243]\ttrain-mlogloss:1.97216\ttrain-auc:0.71787\ttrain-merror:0.69058\ttest-mlogloss:2.06440\ttest-auc:0.64928\ttest-merror:0.72698\n",
      "[244]\ttrain-mlogloss:1.97190\ttrain-auc:0.71802\ttrain-merror:0.69045\ttest-mlogloss:2.06438\ttest-auc:0.64928\ttest-merror:0.72702\n",
      "[245]\ttrain-mlogloss:1.97164\ttrain-auc:0.71817\ttrain-merror:0.69033\ttest-mlogloss:2.06438\ttest-auc:0.64928\ttest-merror:0.72705\n",
      "[246]\ttrain-mlogloss:1.97136\ttrain-auc:0.71833\ttrain-merror:0.69020\ttest-mlogloss:2.06438\ttest-auc:0.64928\ttest-merror:0.72700\n",
      "[247]\ttrain-mlogloss:1.97106\ttrain-auc:0.71850\ttrain-merror:0.69008\ttest-mlogloss:2.06439\ttest-auc:0.64928\ttest-merror:0.72698\n",
      "[248]\ttrain-mlogloss:1.97084\ttrain-auc:0.71861\ttrain-merror:0.68999\ttest-mlogloss:2.06438\ttest-auc:0.64928\ttest-merror:0.72695\n",
      "[249]\ttrain-mlogloss:1.97060\ttrain-auc:0.71874\ttrain-merror:0.68988\ttest-mlogloss:2.06438\ttest-auc:0.64928\ttest-merror:0.72699\n",
      "[250]\ttrain-mlogloss:1.97036\ttrain-auc:0.71887\ttrain-merror:0.68978\ttest-mlogloss:2.06439\ttest-auc:0.64927\ttest-merror:0.72694\n",
      "[251]\ttrain-mlogloss:1.97008\ttrain-auc:0.71904\ttrain-merror:0.68964\ttest-mlogloss:2.06440\ttest-auc:0.64925\ttest-merror:0.72693\n",
      "[252]\ttrain-mlogloss:1.96980\ttrain-auc:0.71920\ttrain-merror:0.68953\ttest-mlogloss:2.06440\ttest-auc:0.64926\ttest-merror:0.72688\n",
      "[253]\ttrain-mlogloss:1.96953\ttrain-auc:0.71934\ttrain-merror:0.68942\ttest-mlogloss:2.06440\ttest-auc:0.64926\ttest-merror:0.72682\n",
      "[254]\ttrain-mlogloss:1.96923\ttrain-auc:0.71952\ttrain-merror:0.68927\ttest-mlogloss:2.06438\ttest-auc:0.64927\ttest-merror:0.72684\n",
      "[255]\ttrain-mlogloss:1.96892\ttrain-auc:0.71969\ttrain-merror:0.68910\ttest-mlogloss:2.06439\ttest-auc:0.64926\ttest-merror:0.72678\n",
      "[256]\ttrain-mlogloss:1.96869\ttrain-auc:0.71981\ttrain-merror:0.68901\ttest-mlogloss:2.06439\ttest-auc:0.64926\ttest-merror:0.72677\n",
      "[257]\ttrain-mlogloss:1.96839\ttrain-auc:0.71998\ttrain-merror:0.68888\ttest-mlogloss:2.06440\ttest-auc:0.64926\ttest-merror:0.72675\n",
      "[258]\ttrain-mlogloss:1.96809\ttrain-auc:0.72014\ttrain-merror:0.68872\ttest-mlogloss:2.06440\ttest-auc:0.64926\ttest-merror:0.72681\n",
      "[259]\ttrain-mlogloss:1.96782\ttrain-auc:0.72028\ttrain-merror:0.68860\ttest-mlogloss:2.06439\ttest-auc:0.64926\ttest-merror:0.72681\n",
      "[260]\ttrain-mlogloss:1.96751\ttrain-auc:0.72047\ttrain-merror:0.68844\ttest-mlogloss:2.06439\ttest-auc:0.64926\ttest-merror:0.72682\n",
      "[261]\ttrain-mlogloss:1.96720\ttrain-auc:0.72064\ttrain-merror:0.68826\ttest-mlogloss:2.06438\ttest-auc:0.64927\ttest-merror:0.72680\n",
      "[262]\ttrain-mlogloss:1.96694\ttrain-auc:0.72079\ttrain-merror:0.68815\ttest-mlogloss:2.06437\ttest-auc:0.64928\ttest-merror:0.72678\n",
      "[263]\ttrain-mlogloss:1.96666\ttrain-auc:0.72095\ttrain-merror:0.68799\ttest-mlogloss:2.06437\ttest-auc:0.64928\ttest-merror:0.72675\n",
      "[264]\ttrain-mlogloss:1.96637\ttrain-auc:0.72110\ttrain-merror:0.68788\ttest-mlogloss:2.06436\ttest-auc:0.64928\ttest-merror:0.72679\n",
      "[265]\ttrain-mlogloss:1.96609\ttrain-auc:0.72123\ttrain-merror:0.68775\ttest-mlogloss:2.06436\ttest-auc:0.64928\ttest-merror:0.72682\n",
      "[266]\ttrain-mlogloss:1.96585\ttrain-auc:0.72137\ttrain-merror:0.68764\ttest-mlogloss:2.06435\ttest-auc:0.64929\ttest-merror:0.72679\n",
      "[267]\ttrain-mlogloss:1.96558\ttrain-auc:0.72150\ttrain-merror:0.68748\ttest-mlogloss:2.06436\ttest-auc:0.64929\ttest-merror:0.72689\n",
      "[268]\ttrain-mlogloss:1.96526\ttrain-auc:0.72169\ttrain-merror:0.68735\ttest-mlogloss:2.06436\ttest-auc:0.64929\ttest-merror:0.72690\n",
      "[269]\ttrain-mlogloss:1.96494\ttrain-auc:0.72186\ttrain-merror:0.68720\ttest-mlogloss:2.06436\ttest-auc:0.64929\ttest-merror:0.72684\n",
      "[270]\ttrain-mlogloss:1.96468\ttrain-auc:0.72201\ttrain-merror:0.68708\ttest-mlogloss:2.06435\ttest-auc:0.64929\ttest-merror:0.72676\n",
      "[271]\ttrain-mlogloss:1.96443\ttrain-auc:0.72215\ttrain-merror:0.68698\ttest-mlogloss:2.06436\ttest-auc:0.64929\ttest-merror:0.72673\n",
      "[272]\ttrain-mlogloss:1.96413\ttrain-auc:0.72230\ttrain-merror:0.68686\ttest-mlogloss:2.06435\ttest-auc:0.64929\ttest-merror:0.72671\n",
      "[273]\ttrain-mlogloss:1.96384\ttrain-auc:0.72246\ttrain-merror:0.68670\ttest-mlogloss:2.06436\ttest-auc:0.64929\ttest-merror:0.72673\n",
      "[274]\ttrain-mlogloss:1.96356\ttrain-auc:0.72262\ttrain-merror:0.68658\ttest-mlogloss:2.06437\ttest-auc:0.64929\ttest-merror:0.72678\n",
      "[275]\ttrain-mlogloss:1.96327\ttrain-auc:0.72278\ttrain-merror:0.68646\ttest-mlogloss:2.06437\ttest-auc:0.64929\ttest-merror:0.72678\n",
      "[276]\ttrain-mlogloss:1.96300\ttrain-auc:0.72293\ttrain-merror:0.68637\ttest-mlogloss:2.06436\ttest-auc:0.64929\ttest-merror:0.72677\n",
      "[277]\ttrain-mlogloss:1.96276\ttrain-auc:0.72306\ttrain-merror:0.68628\ttest-mlogloss:2.06436\ttest-auc:0.64928\ttest-merror:0.72673\n",
      "[278]\ttrain-mlogloss:1.96253\ttrain-auc:0.72318\ttrain-merror:0.68617\ttest-mlogloss:2.06436\ttest-auc:0.64928\ttest-merror:0.72669\n",
      "[279]\ttrain-mlogloss:1.96230\ttrain-auc:0.72331\ttrain-merror:0.68606\ttest-mlogloss:2.06438\ttest-auc:0.64927\ttest-merror:0.72662\n",
      "[280]\ttrain-mlogloss:1.96206\ttrain-auc:0.72344\ttrain-merror:0.68595\ttest-mlogloss:2.06438\ttest-auc:0.64926\ttest-merror:0.72666\n",
      "[281]\ttrain-mlogloss:1.96177\ttrain-auc:0.72360\ttrain-merror:0.68583\ttest-mlogloss:2.06438\ttest-auc:0.64927\ttest-merror:0.72664\n",
      "[282]\ttrain-mlogloss:1.96149\ttrain-auc:0.72375\ttrain-merror:0.68572\ttest-mlogloss:2.06436\ttest-auc:0.64927\ttest-merror:0.72670\n",
      "[283]\ttrain-mlogloss:1.96122\ttrain-auc:0.72391\ttrain-merror:0.68557\ttest-mlogloss:2.06437\ttest-auc:0.64926\ttest-merror:0.72672\n",
      "[284]\ttrain-mlogloss:1.96093\ttrain-auc:0.72407\ttrain-merror:0.68542\ttest-mlogloss:2.06437\ttest-auc:0.64926\ttest-merror:0.72665\n",
      "[285]\ttrain-mlogloss:1.96071\ttrain-auc:0.72418\ttrain-merror:0.68535\ttest-mlogloss:2.06437\ttest-auc:0.64926\ttest-merror:0.72667\n",
      "[286]\ttrain-mlogloss:1.96042\ttrain-auc:0.72436\ttrain-merror:0.68524\ttest-mlogloss:2.06437\ttest-auc:0.64926\ttest-merror:0.72664\n",
      "[287]\ttrain-mlogloss:1.96015\ttrain-auc:0.72451\ttrain-merror:0.68512\ttest-mlogloss:2.06438\ttest-auc:0.64925\ttest-merror:0.72662\n",
      "[288]\ttrain-mlogloss:1.95991\ttrain-auc:0.72463\ttrain-merror:0.68502\ttest-mlogloss:2.06440\ttest-auc:0.64924\ttest-merror:0.72664\n",
      "[289]\ttrain-mlogloss:1.95962\ttrain-auc:0.72480\ttrain-merror:0.68490\ttest-mlogloss:2.06442\ttest-auc:0.64922\ttest-merror:0.72666\n",
      "[290]\ttrain-mlogloss:1.95936\ttrain-auc:0.72494\ttrain-merror:0.68476\ttest-mlogloss:2.06443\ttest-auc:0.64922\ttest-merror:0.72664\n",
      "[291]\ttrain-mlogloss:1.95908\ttrain-auc:0.72508\ttrain-merror:0.68466\ttest-mlogloss:2.06443\ttest-auc:0.64922\ttest-merror:0.72665\n",
      "[292]\ttrain-mlogloss:1.95876\ttrain-auc:0.72526\ttrain-merror:0.68452\ttest-mlogloss:2.06445\ttest-auc:0.64921\ttest-merror:0.72676\n",
      "[293]\ttrain-mlogloss:1.95850\ttrain-auc:0.72541\ttrain-merror:0.68443\ttest-mlogloss:2.06446\ttest-auc:0.64920\ttest-merror:0.72679\n",
      "[294]\ttrain-mlogloss:1.95822\ttrain-auc:0.72555\ttrain-merror:0.68429\ttest-mlogloss:2.06447\ttest-auc:0.64920\ttest-merror:0.72684\n",
      "[295]\ttrain-mlogloss:1.95799\ttrain-auc:0.72566\ttrain-merror:0.68418\ttest-mlogloss:2.06448\ttest-auc:0.64919\ttest-merror:0.72684\n",
      "[296]\ttrain-mlogloss:1.95773\ttrain-auc:0.72581\ttrain-merror:0.68407\ttest-mlogloss:2.06448\ttest-auc:0.64919\ttest-merror:0.72681\n",
      "[297]\ttrain-mlogloss:1.95748\ttrain-auc:0.72594\ttrain-merror:0.68396\ttest-mlogloss:2.06448\ttest-auc:0.64919\ttest-merror:0.72677\n",
      "[298]\ttrain-mlogloss:1.95724\ttrain-auc:0.72607\ttrain-merror:0.68387\ttest-mlogloss:2.06449\ttest-auc:0.64920\ttest-merror:0.72681\n",
      "[299]\ttrain-mlogloss:1.95704\ttrain-auc:0.72618\ttrain-merror:0.68380\ttest-mlogloss:2.06451\ttest-auc:0.64919\ttest-merror:0.72678\n",
      "300-rounds Training finished ...\t\t(358.534s)\n"
     ]
    }
   ],
   "source": [
    "num_round = 300\n",
    "t0 = time()\n",
    "eval_result = {}\n",
    "def decay_eta(nth):\n",
    "    etas = [.1, .05, .03, .01]\n",
    "    return etas[(nth // 60) % len(etas)]\n",
    "\n",
    "wl_bst_sm = xgb.train(param, xg_train, num_round, watchlist, evals_result=eval_result, )\n",
    "#                       callbacks=[xgb.callback.LearningRateScheduler(decay_eta)])\n",
    "print(f\"{num_round}-rounds Training finished ...\\t\\t({time()-t0:.3f}s)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': OrderedDict([('mlogloss',\n",
       "               [2.273391,\n",
       "                2.249271,\n",
       "                2.228938,\n",
       "                2.211438,\n",
       "                2.196323,\n",
       "                2.183143,\n",
       "                2.171514,\n",
       "                2.16128,\n",
       "                2.15201,\n",
       "                2.14378,\n",
       "                2.136391,\n",
       "                2.129838,\n",
       "                2.12384,\n",
       "                2.118412,\n",
       "                2.113522,\n",
       "                2.109029,\n",
       "                2.105014,\n",
       "                2.101309,\n",
       "                2.0979,\n",
       "                2.094819,\n",
       "                2.092041,\n",
       "                2.089248,\n",
       "                2.086848,\n",
       "                2.084486,\n",
       "                2.082175,\n",
       "                2.080239,\n",
       "                2.078318,\n",
       "                2.076628,\n",
       "                2.074873,\n",
       "                2.073293,\n",
       "                2.071798,\n",
       "                2.070462,\n",
       "                2.069056,\n",
       "                2.067713,\n",
       "                2.066565,\n",
       "                2.065228,\n",
       "                2.064006,\n",
       "                2.062921,\n",
       "                2.061723,\n",
       "                2.060604,\n",
       "                2.059523,\n",
       "                2.058563,\n",
       "                2.057618,\n",
       "                2.056739,\n",
       "                2.055834,\n",
       "                2.055038,\n",
       "                2.054273,\n",
       "                2.053414,\n",
       "                2.052641,\n",
       "                2.051878,\n",
       "                2.051152,\n",
       "                2.050514,\n",
       "                2.049806,\n",
       "                2.049126,\n",
       "                2.04851,\n",
       "                2.047696,\n",
       "                2.047043,\n",
       "                2.046361,\n",
       "                2.045739,\n",
       "                2.045126,\n",
       "                2.044484,\n",
       "                2.043822,\n",
       "                2.043189,\n",
       "                2.042589,\n",
       "                2.041953,\n",
       "                2.041371,\n",
       "                2.040852,\n",
       "                2.040214,\n",
       "                2.039633,\n",
       "                2.039045,\n",
       "                2.038459,\n",
       "                2.037893,\n",
       "                2.037293,\n",
       "                2.03677,\n",
       "                2.036205,\n",
       "                2.035654,\n",
       "                2.035119,\n",
       "                2.034608,\n",
       "                2.034042,\n",
       "                2.033438,\n",
       "                2.033002,\n",
       "                2.032466,\n",
       "                2.031891,\n",
       "                2.031407,\n",
       "                2.030922,\n",
       "                2.030501,\n",
       "                2.029937,\n",
       "                2.029474,\n",
       "                2.028983,\n",
       "                2.028555,\n",
       "                2.028012,\n",
       "                2.027574,\n",
       "                2.027126,\n",
       "                2.026658,\n",
       "                2.026212,\n",
       "                2.025816,\n",
       "                2.025496,\n",
       "                2.025096,\n",
       "                2.024753,\n",
       "                2.024403,\n",
       "                2.024139,\n",
       "                2.023821,\n",
       "                2.023507,\n",
       "                2.023234,\n",
       "                2.022821,\n",
       "                2.022518,\n",
       "                2.022208,\n",
       "                2.022001,\n",
       "                2.021776,\n",
       "                2.021562,\n",
       "                2.021295,\n",
       "                2.021012,\n",
       "                2.020731,\n",
       "                2.020529,\n",
       "                2.020328,\n",
       "                2.020141,\n",
       "                2.01988,\n",
       "                2.019671,\n",
       "                2.01945,\n",
       "                2.019143,\n",
       "                2.018958,\n",
       "                2.018705,\n",
       "                2.018505,\n",
       "                2.018282,\n",
       "                2.018093,\n",
       "                2.017933,\n",
       "                2.017817,\n",
       "                2.017704,\n",
       "                2.017591,\n",
       "                2.017471,\n",
       "                2.017397,\n",
       "                2.017253,\n",
       "                2.017146,\n",
       "                2.017033,\n",
       "                2.016887,\n",
       "                2.016766,\n",
       "                2.016657,\n",
       "                2.01646,\n",
       "                2.016353,\n",
       "                2.016211,\n",
       "                2.016126,\n",
       "                2.016019,\n",
       "                2.015876,\n",
       "                2.015805,\n",
       "                2.015669,\n",
       "                2.015573,\n",
       "                2.015421,\n",
       "                2.015299,\n",
       "                2.015163,\n",
       "                2.015013,\n",
       "                2.014934,\n",
       "                2.014853,\n",
       "                2.014726,\n",
       "                2.014677,\n",
       "                2.014621,\n",
       "                2.014551,\n",
       "                2.014474,\n",
       "                2.014271,\n",
       "                2.01423,\n",
       "                2.014152,\n",
       "                2.014093,\n",
       "                2.014045,\n",
       "                2.013975,\n",
       "                2.013928,\n",
       "                2.013889,\n",
       "                2.013806,\n",
       "                2.013786,\n",
       "                2.013756,\n",
       "                2.013702,\n",
       "                2.013679,\n",
       "                2.013618,\n",
       "                2.013508,\n",
       "                2.013389,\n",
       "                2.013267,\n",
       "                2.01323,\n",
       "                2.013192,\n",
       "                2.01314,\n",
       "                2.013098,\n",
       "                2.01299,\n",
       "                2.012877,\n",
       "                2.012784,\n",
       "                2.012739,\n",
       "                2.012705,\n",
       "                2.012635,\n",
       "                2.012612,\n",
       "                2.012561,\n",
       "                2.012506,\n",
       "                2.012436,\n",
       "                2.012393,\n",
       "                2.012362,\n",
       "                2.012309,\n",
       "                2.012259,\n",
       "                2.012213,\n",
       "                2.012111,\n",
       "                2.012066,\n",
       "                2.011981,\n",
       "                2.011952,\n",
       "                2.011911,\n",
       "                2.011884,\n",
       "                2.011832]),\n",
       "              ('auc',\n",
       "               [0.609166,\n",
       "                0.613574,\n",
       "                0.615673,\n",
       "                0.617684,\n",
       "                0.618899,\n",
       "                0.620377,\n",
       "                0.621478,\n",
       "                0.62236,\n",
       "                0.623596,\n",
       "                0.624718,\n",
       "                0.625693,\n",
       "                0.626546,\n",
       "                0.627477,\n",
       "                0.628391,\n",
       "                0.629221,\n",
       "                0.630098,\n",
       "                0.630813,\n",
       "                0.631575,\n",
       "                0.632343,\n",
       "                0.633113,\n",
       "                0.633773,\n",
       "                0.634675,\n",
       "                0.635358,\n",
       "                0.636146,\n",
       "                0.637049,\n",
       "                0.637714,\n",
       "                0.638476,\n",
       "                0.63913,\n",
       "                0.639907,\n",
       "                0.64057,\n",
       "                0.641302,\n",
       "                0.641907,\n",
       "                0.642584,\n",
       "                0.643294,\n",
       "                0.64389,\n",
       "                0.644677,\n",
       "                0.645397,\n",
       "                0.646051,\n",
       "                0.646797,\n",
       "                0.64751,\n",
       "                0.648158,\n",
       "                0.648724,\n",
       "                0.649365,\n",
       "                0.649896,\n",
       "                0.650452,\n",
       "                0.650985,\n",
       "                0.651553,\n",
       "                0.652133,\n",
       "                0.652763,\n",
       "                0.653332,\n",
       "                0.653814,\n",
       "                0.654313,\n",
       "                0.654877,\n",
       "                0.655356,\n",
       "                0.655828,\n",
       "                0.656415,\n",
       "                0.656954,\n",
       "                0.65745,\n",
       "                0.657979,\n",
       "                0.658472,\n",
       "                0.658947,\n",
       "                0.659518,\n",
       "                0.660011,\n",
       "                0.66048,\n",
       "                0.661035,\n",
       "                0.661515,\n",
       "                0.661939,\n",
       "                0.66243,\n",
       "                0.662903,\n",
       "                0.663358,\n",
       "                0.663827,\n",
       "                0.664305,\n",
       "                0.664799,\n",
       "                0.665215,\n",
       "                0.665669,\n",
       "                0.66609,\n",
       "                0.666529,\n",
       "                0.666907,\n",
       "                0.66736,\n",
       "                0.667831,\n",
       "                0.668144,\n",
       "                0.668582,\n",
       "                0.669065,\n",
       "                0.66944,\n",
       "                0.669814,\n",
       "                0.67014,\n",
       "                0.670602,\n",
       "                0.670932,\n",
       "                0.671322,\n",
       "                0.671626,\n",
       "                0.672037,\n",
       "                0.672368,\n",
       "                0.672707,\n",
       "                0.673066,\n",
       "                0.673372,\n",
       "                0.673655,\n",
       "                0.673886,\n",
       "                0.674167,\n",
       "                0.674412,\n",
       "                0.67468,\n",
       "                0.674849,\n",
       "                0.675083,\n",
       "                0.675328,\n",
       "                0.675503,\n",
       "                0.675813,\n",
       "                0.676021,\n",
       "                0.676225,\n",
       "                0.67635,\n",
       "                0.676503,\n",
       "                0.676644,\n",
       "                0.676819,\n",
       "                0.677004,\n",
       "                0.677191,\n",
       "                0.677305,\n",
       "                0.677427,\n",
       "                0.677553,\n",
       "                0.677731,\n",
       "                0.677853,\n",
       "                0.678005,\n",
       "                0.67821,\n",
       "                0.678332,\n",
       "                0.678514,\n",
       "                0.678642,\n",
       "                0.678797,\n",
       "                0.678916,\n",
       "                0.67901,\n",
       "                0.679068,\n",
       "                0.679116,\n",
       "                0.679174,\n",
       "                0.679233,\n",
       "                0.679268,\n",
       "                0.679347,\n",
       "                0.679393,\n",
       "                0.679453,\n",
       "                0.679532,\n",
       "                0.679592,\n",
       "                0.679641,\n",
       "                0.679756,\n",
       "                0.679805,\n",
       "                0.679884,\n",
       "                0.67992,\n",
       "                0.679976,\n",
       "                0.680058,\n",
       "                0.680091,\n",
       "                0.680166,\n",
       "                0.680201,\n",
       "                0.680297,\n",
       "                0.680364,\n",
       "                0.680429,\n",
       "                0.680511,\n",
       "                0.680554,\n",
       "                0.680594,\n",
       "                0.68066,\n",
       "                0.680675,\n",
       "                0.680696,\n",
       "                0.680724,\n",
       "                0.680755,\n",
       "                0.680916,\n",
       "                0.68093,\n",
       "                0.680963,\n",
       "                0.680986,\n",
       "                0.681005,\n",
       "                0.681047,\n",
       "                0.681063,\n",
       "                0.681076,\n",
       "                0.681109,\n",
       "                0.681114,\n",
       "                0.681125,\n",
       "                0.681142,\n",
       "                0.681148,\n",
       "                0.681176,\n",
       "                0.681243,\n",
       "                0.681297,\n",
       "                0.68136,\n",
       "                0.681373,\n",
       "                0.681389,\n",
       "                0.681404,\n",
       "                0.681419,\n",
       "                0.681469,\n",
       "                0.681542,\n",
       "                0.681581,\n",
       "                0.681599,\n",
       "                0.681611,\n",
       "                0.681642,\n",
       "                0.681648,\n",
       "                0.681666,\n",
       "                0.681681,\n",
       "                0.681711,\n",
       "                0.681724,\n",
       "                0.681731,\n",
       "                0.681748,\n",
       "                0.681768,\n",
       "                0.681781,\n",
       "                0.681825,\n",
       "                0.681841,\n",
       "                0.681887,\n",
       "                0.681898,\n",
       "                0.681912,\n",
       "                0.681922,\n",
       "                0.681937]),\n",
       "              ('merror',\n",
       "               [0.736567,\n",
       "                0.734123,\n",
       "                0.733287,\n",
       "                0.732624,\n",
       "                0.732093,\n",
       "                0.731596,\n",
       "                0.731194,\n",
       "                0.730924,\n",
       "                0.730526,\n",
       "                0.730136,\n",
       "                0.729627,\n",
       "                0.729457,\n",
       "                0.729307,\n",
       "                0.729097,\n",
       "                0.72865,\n",
       "                0.728462,\n",
       "                0.728281,\n",
       "                0.728087,\n",
       "                0.727865,\n",
       "                0.72766,\n",
       "                0.727516,\n",
       "                0.727328,\n",
       "                0.727033,\n",
       "                0.72682,\n",
       "                0.726528,\n",
       "                0.726331,\n",
       "                0.726146,\n",
       "                0.725933,\n",
       "                0.725733,\n",
       "                0.725498,\n",
       "                0.725218,\n",
       "                0.725064,\n",
       "                0.724871,\n",
       "                0.724663,\n",
       "                0.724455,\n",
       "                0.724101,\n",
       "                0.723866,\n",
       "                0.723614,\n",
       "                0.723313,\n",
       "                0.723139,\n",
       "                0.722898,\n",
       "                0.722674,\n",
       "                0.72244,\n",
       "                0.722268,\n",
       "                0.722085,\n",
       "                0.721878,\n",
       "                0.721729,\n",
       "                0.721451,\n",
       "                0.721261,\n",
       "                0.721111,\n",
       "                0.720938,\n",
       "                0.720817,\n",
       "                0.72056,\n",
       "                0.72037,\n",
       "                0.720195,\n",
       "                0.719901,\n",
       "                0.719704,\n",
       "                0.719465,\n",
       "                0.719327,\n",
       "                0.719086,\n",
       "                0.71886,\n",
       "                0.718676,\n",
       "                0.718485,\n",
       "                0.718345,\n",
       "                0.718081,\n",
       "                0.717925,\n",
       "                0.717798,\n",
       "                0.717584,\n",
       "                0.717402,\n",
       "                0.717194,\n",
       "                0.716954,\n",
       "                0.716775,\n",
       "                0.716569,\n",
       "                0.71638,\n",
       "                0.716216,\n",
       "                0.716011,\n",
       "                0.715823,\n",
       "                0.715645,\n",
       "                0.715486,\n",
       "                0.715216,\n",
       "                0.71509,\n",
       "                0.714905,\n",
       "                0.714731,\n",
       "                0.714552,\n",
       "                0.714427,\n",
       "                0.714291,\n",
       "                0.714078,\n",
       "                0.71394,\n",
       "                0.71374,\n",
       "                0.713625,\n",
       "                0.713416,\n",
       "                0.71326,\n",
       "                0.713118,\n",
       "                0.71294,\n",
       "                0.712771,\n",
       "                0.712584,\n",
       "                0.712466,\n",
       "                0.712347,\n",
       "                0.712194,\n",
       "                0.712007,\n",
       "                0.711866,\n",
       "                0.711762,\n",
       "                0.711646,\n",
       "                0.711515,\n",
       "                0.711271,\n",
       "                0.71117,\n",
       "                0.711008,\n",
       "                0.710895,\n",
       "                0.710786,\n",
       "                0.710699,\n",
       "                0.710572,\n",
       "                0.710429,\n",
       "                0.710301,\n",
       "                0.710233,\n",
       "                0.710098,\n",
       "                0.709965,\n",
       "                0.709799,\n",
       "                0.709727,\n",
       "                0.709592,\n",
       "                0.709422,\n",
       "                0.709292,\n",
       "                0.709139,\n",
       "                0.708986,\n",
       "                0.708837,\n",
       "                0.708743,\n",
       "                0.70863,\n",
       "                0.708564,\n",
       "                0.708518,\n",
       "                0.708465,\n",
       "                0.708408,\n",
       "                0.708354,\n",
       "                0.708295,\n",
       "                0.708238,\n",
       "                0.708167,\n",
       "                0.708065,\n",
       "                0.708008,\n",
       "                0.70796,\n",
       "                0.707828,\n",
       "                0.707759,\n",
       "                0.707672,\n",
       "                0.707609,\n",
       "                0.707545,\n",
       "                0.707455,\n",
       "                0.707402,\n",
       "                0.707346,\n",
       "                0.707296,\n",
       "                0.707199,\n",
       "                0.707134,\n",
       "                0.707047,\n",
       "                0.706961,\n",
       "                0.706909,\n",
       "                0.706842,\n",
       "                0.706767,\n",
       "                0.706737,\n",
       "                0.706718,\n",
       "                0.706687,\n",
       "                0.706645,\n",
       "                0.70648,\n",
       "                0.706468,\n",
       "                0.706433,\n",
       "                0.706415,\n",
       "                0.706409,\n",
       "                0.706365,\n",
       "                0.706351,\n",
       "                0.706338,\n",
       "                0.706305,\n",
       "                0.706298,\n",
       "                0.706279,\n",
       "                0.706264,\n",
       "                0.706252,\n",
       "                0.706232,\n",
       "                0.706168,\n",
       "                0.706121,\n",
       "                0.706088,\n",
       "                0.70608,\n",
       "                0.706062,\n",
       "                0.706038,\n",
       "                0.70602,\n",
       "                0.705938,\n",
       "                0.705859,\n",
       "                0.705835,\n",
       "                0.705819,\n",
       "                0.705809,\n",
       "                0.705772,\n",
       "                0.705756,\n",
       "                0.705754,\n",
       "                0.705724,\n",
       "                0.705688,\n",
       "                0.705675,\n",
       "                0.705644,\n",
       "                0.705621,\n",
       "                0.705604,\n",
       "                0.705576,\n",
       "                0.705528,\n",
       "                0.705507,\n",
       "                0.705469,\n",
       "                0.705457,\n",
       "                0.705441,\n",
       "                0.705429,\n",
       "                0.705403])]),\n",
       " 'test': OrderedDict([('mlogloss',\n",
       "               [2.273966,\n",
       "                2.250428,\n",
       "                2.230677,\n",
       "                2.213754,\n",
       "                2.199225,\n",
       "                2.186585,\n",
       "                2.175527,\n",
       "                2.165833,\n",
       "                2.157122,\n",
       "                2.149473,\n",
       "                2.142651,\n",
       "                2.136662,\n",
       "                2.131266,\n",
       "                2.126448,\n",
       "                2.122146,\n",
       "                2.118271,\n",
       "                2.114859,\n",
       "                2.111758,\n",
       "                2.108978,\n",
       "                2.106493,\n",
       "                2.104311,\n",
       "                2.102141,\n",
       "                2.100348,\n",
       "                2.098594,\n",
       "                2.096948,\n",
       "                2.095634,\n",
       "                2.094353,\n",
       "                2.093259,\n",
       "                2.092155,\n",
       "                2.091199,\n",
       "                2.090338,\n",
       "                2.089609,\n",
       "                2.088794,\n",
       "                2.088084,\n",
       "                2.087566,\n",
       "                2.086879,\n",
       "                2.086292,\n",
       "                2.085867,\n",
       "                2.085336,\n",
       "                2.08488,\n",
       "                2.084435,\n",
       "                2.084051,\n",
       "                2.083765,\n",
       "                2.083475,\n",
       "                2.083183,\n",
       "                2.083005,\n",
       "                2.08284,\n",
       "                2.082611,\n",
       "                2.082492,\n",
       "                2.082344,\n",
       "                2.082157,\n",
       "                2.08208,\n",
       "                2.082,\n",
       "                2.081881,\n",
       "                2.081804,\n",
       "                2.081594,\n",
       "                2.081518,\n",
       "                2.081397,\n",
       "                2.081355,\n",
       "                2.081302,\n",
       "                2.081173,\n",
       "                2.081138,\n",
       "                2.081072,\n",
       "                2.08103,\n",
       "                2.080996,\n",
       "                2.080967,\n",
       "                2.080953,\n",
       "                2.080883,\n",
       "                2.08084,\n",
       "                2.080801,\n",
       "                2.080752,\n",
       "                2.080732,\n",
       "                2.080695,\n",
       "                2.080673,\n",
       "                2.080658,\n",
       "                2.080602,\n",
       "                2.080593,\n",
       "                2.080565,\n",
       "                2.080539,\n",
       "                2.080512,\n",
       "                2.080488,\n",
       "                2.080473,\n",
       "                2.080469,\n",
       "                2.08046,\n",
       "                2.080437,\n",
       "                2.080427,\n",
       "                2.080432,\n",
       "                2.080418,\n",
       "                2.080394,\n",
       "                2.080377,\n",
       "                2.080375,\n",
       "                2.080362,\n",
       "                2.080354,\n",
       "                2.080343,\n",
       "                2.080349,\n",
       "                2.08034,\n",
       "                2.080347,\n",
       "                2.080342,\n",
       "                2.080329,\n",
       "                2.080311,\n",
       "                2.080294,\n",
       "                2.080276,\n",
       "                2.080289,\n",
       "                2.080271,\n",
       "                2.08027,\n",
       "                2.080249,\n",
       "                2.080216,\n",
       "                2.080196,\n",
       "                2.080184,\n",
       "                2.080182,\n",
       "                2.080157,\n",
       "                2.080133,\n",
       "                2.08011,\n",
       "                2.080096,\n",
       "                2.08008,\n",
       "                2.080082,\n",
       "                2.080074,\n",
       "                2.080047,\n",
       "                2.080029,\n",
       "                2.080013,\n",
       "                2.079996,\n",
       "                2.079986,\n",
       "                2.079977,\n",
       "                2.079978,\n",
       "                2.079966,\n",
       "                2.079957,\n",
       "                2.07994,\n",
       "                2.079918,\n",
       "                2.079911,\n",
       "                2.07991,\n",
       "                2.079897,\n",
       "                2.079887,\n",
       "                2.07987,\n",
       "                2.079848,\n",
       "                2.079819,\n",
       "                2.079782,\n",
       "                2.079779,\n",
       "                2.079745,\n",
       "                2.07973,\n",
       "                2.079714,\n",
       "                2.07969,\n",
       "                2.079686,\n",
       "                2.079694,\n",
       "                2.079683,\n",
       "                2.079683,\n",
       "                2.079665,\n",
       "                2.079645,\n",
       "                2.079633,\n",
       "                2.079602,\n",
       "                2.079573,\n",
       "                2.079575,\n",
       "                2.079566,\n",
       "                2.079545,\n",
       "                2.079539,\n",
       "                2.079537,\n",
       "                2.079527,\n",
       "                2.07952,\n",
       "                2.079513,\n",
       "                2.079507,\n",
       "                2.079505,\n",
       "                2.0795,\n",
       "                2.079493,\n",
       "                2.079496,\n",
       "                2.079484,\n",
       "                2.079475,\n",
       "                2.07947,\n",
       "                2.079468,\n",
       "                2.079465,\n",
       "                2.079451,\n",
       "                2.07945,\n",
       "                2.079444,\n",
       "                2.079444,\n",
       "                2.079428,\n",
       "                2.079413,\n",
       "                2.079404,\n",
       "                2.079399,\n",
       "                2.079379,\n",
       "                2.079372,\n",
       "                2.079349,\n",
       "                2.079342,\n",
       "                2.079335,\n",
       "                2.079337,\n",
       "                2.079336,\n",
       "                2.079337,\n",
       "                2.079331,\n",
       "                2.079327,\n",
       "                2.079313,\n",
       "                2.079311,\n",
       "                2.079301,\n",
       "                2.079287,\n",
       "                2.079275,\n",
       "                2.079274,\n",
       "                2.079258,\n",
       "                2.079248,\n",
       "                2.079233,\n",
       "                2.07923,\n",
       "                2.07923,\n",
       "                2.079229,\n",
       "                2.079225,\n",
       "                2.079212]),\n",
       "              ('auc',\n",
       "               [0.598253,\n",
       "                0.601141,\n",
       "                0.602342,\n",
       "                0.603543,\n",
       "                0.604139,\n",
       "                0.60494,\n",
       "                0.605443,\n",
       "                0.605851,\n",
       "                0.606437,\n",
       "                0.606949,\n",
       "                0.607375,\n",
       "                0.607739,\n",
       "                0.608041,\n",
       "                0.608347,\n",
       "                0.60867,\n",
       "                0.608922,\n",
       "                0.609089,\n",
       "                0.60928,\n",
       "                0.609503,\n",
       "                0.609693,\n",
       "                0.60982,\n",
       "                0.61015,\n",
       "                0.610302,\n",
       "                0.610542,\n",
       "                0.610849,\n",
       "                0.61098,\n",
       "                0.611182,\n",
       "                0.611318,\n",
       "                0.611519,\n",
       "                0.61164,\n",
       "                0.611776,\n",
       "                0.611853,\n",
       "                0.612054,\n",
       "                0.61219,\n",
       "                0.612242,\n",
       "                0.612456,\n",
       "                0.61262,\n",
       "                0.612678,\n",
       "                0.612842,\n",
       "                0.612986,\n",
       "                0.613119,\n",
       "                0.61323,\n",
       "                0.613288,\n",
       "                0.613347,\n",
       "                0.613418,\n",
       "                0.613423,\n",
       "                0.613461,\n",
       "                0.613496,\n",
       "                0.61352,\n",
       "                0.61354,\n",
       "                0.6136,\n",
       "                0.613591,\n",
       "                0.613589,\n",
       "                0.613613,\n",
       "                0.613626,\n",
       "                0.61372,\n",
       "                0.613748,\n",
       "                0.613798,\n",
       "                0.613788,\n",
       "                0.6138,\n",
       "                0.613858,\n",
       "                0.613856,\n",
       "                0.613858,\n",
       "                0.613861,\n",
       "                0.613865,\n",
       "                0.613865,\n",
       "                0.613868,\n",
       "                0.613893,\n",
       "                0.613916,\n",
       "                0.613913,\n",
       "                0.613942,\n",
       "                0.613944,\n",
       "                0.613963,\n",
       "                0.613981,\n",
       "                0.61398,\n",
       "                0.614018,\n",
       "                0.614012,\n",
       "                0.61403,\n",
       "                0.614051,\n",
       "                0.614065,\n",
       "                0.614085,\n",
       "                0.614102,\n",
       "                0.614087,\n",
       "                0.614089,\n",
       "                0.614105,\n",
       "                0.614113,\n",
       "                0.6141,\n",
       "                0.61411,\n",
       "                0.614128,\n",
       "                0.614128,\n",
       "                0.614123,\n",
       "                0.614124,\n",
       "                0.614132,\n",
       "                0.61414,\n",
       "                0.614134,\n",
       "                0.614134,\n",
       "                0.614128,\n",
       "                0.614134,\n",
       "                0.614138,\n",
       "                0.614145,\n",
       "                0.614153,\n",
       "                0.614162,\n",
       "                0.61415,\n",
       "                0.614162,\n",
       "                0.614155,\n",
       "                0.614178,\n",
       "                0.614195,\n",
       "                0.614212,\n",
       "                0.614221,\n",
       "                0.614221,\n",
       "                0.614237,\n",
       "                0.614244,\n",
       "                0.61426,\n",
       "                0.61427,\n",
       "                0.614282,\n",
       "                0.614284,\n",
       "                0.61429,\n",
       "                0.614312,\n",
       "                0.614321,\n",
       "                0.614324,\n",
       "                0.614341,\n",
       "                0.614347,\n",
       "                0.614351,\n",
       "                0.61435,\n",
       "                0.614353,\n",
       "                0.614354,\n",
       "                0.614365,\n",
       "                0.61438,\n",
       "                0.614384,\n",
       "                0.614381,\n",
       "                0.614389,\n",
       "                0.614395,\n",
       "                0.614405,\n",
       "                0.61442,\n",
       "                0.614442,\n",
       "                0.61446,\n",
       "                0.614462,\n",
       "                0.614475,\n",
       "                0.614485,\n",
       "                0.614492,\n",
       "                0.614502,\n",
       "                0.614504,\n",
       "                0.614497,\n",
       "                0.614501,\n",
       "                0.614498,\n",
       "                0.614504,\n",
       "                0.614515,\n",
       "                0.614524,\n",
       "                0.614536,\n",
       "                0.614551,\n",
       "                0.61455,\n",
       "                0.614554,\n",
       "                0.614567,\n",
       "                0.614571,\n",
       "                0.614571,\n",
       "                0.614577,\n",
       "                0.61458,\n",
       "                0.614577,\n",
       "                0.61458,\n",
       "                0.61458,\n",
       "                0.614584,\n",
       "                0.614586,\n",
       "                0.614584,\n",
       "                0.61459,\n",
       "                0.614594,\n",
       "                0.614598,\n",
       "                0.6146,\n",
       "                0.614601,\n",
       "                0.614609,\n",
       "                0.61461,\n",
       "                0.614611,\n",
       "                0.614604,\n",
       "                0.614612,\n",
       "                0.614622,\n",
       "                0.614628,\n",
       "                0.61463,\n",
       "                0.61464,\n",
       "                0.614643,\n",
       "                0.614659,\n",
       "                0.614665,\n",
       "                0.614668,\n",
       "                0.614668,\n",
       "                0.614669,\n",
       "                0.614669,\n",
       "                0.614672,\n",
       "                0.614676,\n",
       "                0.614683,\n",
       "                0.614684,\n",
       "                0.614688,\n",
       "                0.614696,\n",
       "                0.614702,\n",
       "                0.614704,\n",
       "                0.614712,\n",
       "                0.614716,\n",
       "                0.614726,\n",
       "                0.614729,\n",
       "                0.61473,\n",
       "                0.61473,\n",
       "                0.614731,\n",
       "                0.614737]),\n",
       "              ('merror',\n",
       "               [0.73782,\n",
       "                0.736012,\n",
       "                0.734923,\n",
       "                0.734415,\n",
       "                0.734394,\n",
       "                0.733806,\n",
       "                0.733643,\n",
       "                0.733631,\n",
       "                0.73342,\n",
       "                0.732988,\n",
       "                0.732547,\n",
       "                0.732556,\n",
       "                0.732391,\n",
       "                0.732311,\n",
       "                0.732046,\n",
       "                0.73203,\n",
       "                0.731683,\n",
       "                0.731566,\n",
       "                0.731479,\n",
       "                0.731341,\n",
       "                0.73133,\n",
       "                0.731284,\n",
       "                0.731188,\n",
       "                0.731063,\n",
       "                0.730976,\n",
       "                0.730907,\n",
       "                0.730913,\n",
       "                0.730882,\n",
       "                0.730976,\n",
       "                0.730836,\n",
       "                0.730779,\n",
       "                0.730742,\n",
       "                0.730683,\n",
       "                0.730523,\n",
       "                0.730479,\n",
       "                0.730354,\n",
       "                0.730351,\n",
       "                0.730258,\n",
       "                0.730009,\n",
       "                0.729901,\n",
       "                0.729949,\n",
       "                0.729951,\n",
       "                0.729876,\n",
       "                0.729736,\n",
       "                0.729702,\n",
       "                0.72969,\n",
       "                0.729656,\n",
       "                0.729569,\n",
       "                0.729602,\n",
       "                0.729469,\n",
       "                0.7294,\n",
       "                0.729448,\n",
       "                0.729412,\n",
       "                0.729385,\n",
       "                0.729333,\n",
       "                0.729258,\n",
       "                0.729235,\n",
       "                0.729224,\n",
       "                0.729268,\n",
       "                0.729203,\n",
       "                0.729218,\n",
       "                0.72917,\n",
       "                0.729254,\n",
       "                0.729312,\n",
       "                0.729208,\n",
       "                0.729199,\n",
       "                0.729228,\n",
       "                0.729258,\n",
       "                0.72926,\n",
       "                0.72926,\n",
       "                0.729176,\n",
       "                0.729158,\n",
       "                0.729145,\n",
       "                0.729137,\n",
       "                0.729116,\n",
       "                0.729064,\n",
       "                0.729074,\n",
       "                0.728995,\n",
       "                0.728963,\n",
       "                0.728934,\n",
       "                0.728982,\n",
       "                0.728997,\n",
       "                0.729076,\n",
       "                0.729028,\n",
       "                0.729053,\n",
       "                0.729068,\n",
       "                0.729001,\n",
       "                0.729036,\n",
       "                0.728955,\n",
       "                0.72892,\n",
       "                0.728886,\n",
       "                0.72892,\n",
       "                0.728799,\n",
       "                0.728817,\n",
       "                0.728861,\n",
       "                0.728884,\n",
       "                0.728847,\n",
       "                0.728849,\n",
       "                0.728865,\n",
       "                0.728867,\n",
       "                0.728805,\n",
       "                0.728799,\n",
       "                0.728834,\n",
       "                0.72879,\n",
       "                0.728803,\n",
       "                0.728744,\n",
       "                0.728688,\n",
       "                0.728663,\n",
       "                0.728609,\n",
       "                0.72864,\n",
       "                0.728581,\n",
       "                0.728598,\n",
       "                0.728586,\n",
       "                0.72855,\n",
       "                0.728525,\n",
       "                0.728571,\n",
       "                0.728538,\n",
       "                0.728569,\n",
       "                0.728561,\n",
       "                0.728542,\n",
       "                0.728563,\n",
       "                0.728604,\n",
       "                0.728617,\n",
       "                0.728682,\n",
       "                0.728667,\n",
       "                0.728623,\n",
       "                0.728665,\n",
       "                0.728632,\n",
       "                0.728638,\n",
       "                0.728625,\n",
       "                0.728669,\n",
       "                0.728565,\n",
       "                0.728506,\n",
       "                0.728554,\n",
       "                0.728504,\n",
       "                0.728462,\n",
       "                0.728475,\n",
       "                0.72846,\n",
       "                0.72841,\n",
       "                0.728364,\n",
       "                0.728392,\n",
       "                0.728348,\n",
       "                0.728398,\n",
       "                0.728406,\n",
       "                0.7284,\n",
       "                0.728389,\n",
       "                0.728379,\n",
       "                0.728398,\n",
       "                0.728362,\n",
       "                0.728312,\n",
       "                0.728314,\n",
       "                0.728258,\n",
       "                0.72827,\n",
       "                0.72825,\n",
       "                0.728233,\n",
       "                0.728225,\n",
       "                0.728222,\n",
       "                0.728225,\n",
       "                0.728212,\n",
       "                0.728254,\n",
       "                0.728247,\n",
       "                0.728237,\n",
       "                0.72825,\n",
       "                0.728252,\n",
       "                0.728258,\n",
       "                0.728306,\n",
       "                0.728293,\n",
       "                0.728279,\n",
       "                0.728268,\n",
       "                0.728266,\n",
       "                0.728277,\n",
       "                0.728266,\n",
       "                0.728235,\n",
       "                0.72826,\n",
       "                0.728245,\n",
       "                0.728235,\n",
       "                0.72822,\n",
       "                0.728212,\n",
       "                0.728179,\n",
       "                0.728131,\n",
       "                0.728166,\n",
       "                0.728193,\n",
       "                0.728218,\n",
       "                0.728222,\n",
       "                0.728206,\n",
       "                0.728197,\n",
       "                0.728195,\n",
       "                0.728195,\n",
       "                0.728195,\n",
       "                0.728183,\n",
       "                0.728183,\n",
       "                0.728197,\n",
       "                0.728202,\n",
       "                0.728214,\n",
       "                0.728229,\n",
       "                0.728227,\n",
       "                0.728225,\n",
       "                0.728239,\n",
       "                0.728229,\n",
       "                0.728218])])}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test error using softmax = 0.7267812324353202\n"
     ]
    }
   ],
   "source": [
    "# get prediction\n",
    "pred = wl_bst_sm.predict(xg_test)\n",
    "# pred = pred.astype(np.uint8)\n",
    "error_rate = np.sum(pred != y_test) / y_test.shape[0]\n",
    "print('Test error using softmax = {}'.format(error_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[0]\\teval-mlogloss:2.064507\\teval-auc:0.649186\\teval-merror:0.726781'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_s = wl_bst_sm.eval(xg_test)\n",
    "# eval_dict = eval_str_2_dict(eval_s)\n",
    "eval_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.6676927 , 0.59549384, 0.51434806, 0.50466707, 0.5041683 ,\n",
       "        0.50455988, 0.50658242, 0.50320763, 0.51051575, 0.63840191]),\n",
       " 2.4069354911900125)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = np.arange(0, 1, 0.1)\n",
    "aucs = auc(y_test.astype(np.uint8), pred.astype(np.uint8), np.arange(param['num_class']))\n",
    "# aucs[aucs == 0.5] = 0\n",
    "w_auc = (aucs * weights).sum()\n",
    "aucs, w_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "report = metrics.classification_report(list(y_test), list(pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.36      0.51      0.42     58076\n",
      "           1       0.26      0.43      0.32     58168\n",
      "           2       0.18      0.10      0.13     46949\n",
      "           3       0.17      0.02      0.03     33008\n",
      "           4       0.19      0.01      0.02     26050\n",
      "           5       0.20      0.01      0.02     21748\n",
      "           6       0.23      0.02      0.03     18650\n",
      "           7       0.19      0.01      0.02     17723\n",
      "           8       0.24      0.03      0.05     20595\n",
      "           9       0.26      0.63      0.37     58419\n",
      "\n",
      "    accuracy                           0.27    359386\n",
      "   macro avg       0.23      0.18      0.14    359386\n",
      "weighted avg       0.24      0.27      0.21    359386\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 调参"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn import svm, datasets\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from collections.abc import Iterable\n",
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "base_param = {  # 基本参数，不需要调参\n",
    "    'objective': 'multi:softmax',\n",
    "    'eta': 0.1,\n",
    "    'nthread': 8,\n",
    "    'num_class': 10,\n",
    "    'gpu_id': 0,\n",
    "    'tree_method': 'gpu_hist',\n",
    "    'eval_metric': ['mlogloss', 'auc', 'merror']\n",
    "} \n",
    "# 需要调参的参数\n",
    "ps1 = {  \n",
    "    'max_depth': list(range(5, 14, 2)),\n",
    "    'min_child_weight': list(range(1, 10, 2)),\n",
    "}\n",
    "\n",
    "ps2 = {\n",
    "    'gamma': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6]\n",
    "}\n",
    "\n",
    "ps3 = {\n",
    "    'subsample':  [i/10.0 for i in range(6,11,1)], \n",
    "    'colsample_bytree':  [i/10.0 for i in range(6,11,1)] \n",
    "}\n",
    "\n",
    "ps4 = {'reg_alpha': [0, 0.1, 0.2, 0.5, 1, 1.5, 2, 4]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 529,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data preparing finished ...\t\t(5.092s)\n"
     ]
    }
   ],
   "source": [
    "t0 = time()\n",
    "xg_train = xgb.DMatrix(X_train.values, label=y_train.values, enable_categorical=True)\n",
    "xg_test = xgb.DMatrix(X_test.values, label=y_test.values, enable_categorical=True)\n",
    "print(f\"Data preparing finished ...\\t\\t({time()-t0:.3f}s)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def myproduct(*iterables):\n",
    "    n = len(iterables)\n",
    "#     print(iterables)\n",
    "    if n == 0:\n",
    "        return None \n",
    "    \n",
    "    ret = []\n",
    "    ret.extend([[e] for e in iterables[0].copy()])\n",
    "    if n == 1:\n",
    "        return ret\n",
    "\n",
    "    # 将需要调参的参数进行组合，即笛卡尔乘积。类似于sklearn中的 ParameterGrid\n",
    "    for k in range(1, n):\n",
    "        v = iterables[k].copy()\n",
    "        l = len(ret)\n",
    "        ret = [ret[i%l].copy() for i in range(len(v) * len(ret))]\n",
    "        for i, e in enumerate(ret):\n",
    "            e.append(v[i // l])\n",
    "    return ret\n",
    "\n",
    "def compose_param_grid(grid, base):\n",
    "    items = list(grid.items())\n",
    "    iterables = [item[1] for item in items]\n",
    "    keys = [item[0] for item in items]\n",
    "\n",
    "    ret = myproduct(*iterables)\n",
    "    com_ps = [dict(zip(keys, e)) for e in ret]\n",
    "\n",
    "\n",
    "    all_params = [base.copy() for _ in range(len(com_ps))] \n",
    "    for i in range(len(com_ps)):\n",
    "        all_params[i].update(com_ps[i])\n",
    "        \n",
    "    return all_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始搜索：colsample_bytree=[0.6, 0.7, 0.8, 0.9, 1.0], subsample=[0.6, 0.7, 0.8, 0.9, 1.0]\n",
      "待搜索的参数组合数量：25\n",
      "1 / 25: 200-rounds Training finished param={'objective': 'multi:softmax', 'eta': 0.1, 'nthread': 8, 'num_class': 10, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['mlogloss', 'auc', 'merror'], 'max_depth': 9, 'min_child_weight': 9, 'gamma': 0.2, 'subsample': 0.6, 'colsample_bytree': 0.6} ...\t\t(820.946s)\n",
      "2 / 25: 200-rounds Training finished param={'objective': 'multi:softmax', 'eta': 0.1, 'nthread': 8, 'num_class': 10, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['mlogloss', 'auc', 'merror'], 'max_depth': 9, 'min_child_weight': 9, 'gamma': 0.2, 'subsample': 0.7, 'colsample_bytree': 0.6} ...\t\t(816.416s)\n",
      "3 / 25: 200-rounds Training finished param={'objective': 'multi:softmax', 'eta': 0.1, 'nthread': 8, 'num_class': 10, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['mlogloss', 'auc', 'merror'], 'max_depth': 9, 'min_child_weight': 9, 'gamma': 0.2, 'subsample': 0.8, 'colsample_bytree': 0.6} ...\t\t(828.957s)\n",
      "4 / 25: 200-rounds Training finished param={'objective': 'multi:softmax', 'eta': 0.1, 'nthread': 8, 'num_class': 10, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['mlogloss', 'auc', 'merror'], 'max_depth': 9, 'min_child_weight': 9, 'gamma': 0.2, 'subsample': 0.9, 'colsample_bytree': 0.6} ...\t\t(838.505s)\n",
      "5 / 25: 200-rounds Training finished param={'objective': 'multi:softmax', 'eta': 0.1, 'nthread': 8, 'num_class': 10, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['mlogloss', 'auc', 'merror'], 'max_depth': 9, 'min_child_weight': 9, 'gamma': 0.2, 'subsample': 1.0, 'colsample_bytree': 0.6} ...\t\t(823.276s)\n",
      "6 / 25: 200-rounds Training finished param={'objective': 'multi:softmax', 'eta': 0.1, 'nthread': 8, 'num_class': 10, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['mlogloss', 'auc', 'merror'], 'max_depth': 9, 'min_child_weight': 9, 'gamma': 0.2, 'subsample': 0.6, 'colsample_bytree': 0.7} ...\t\t(828.692s)\n",
      "7 / 25: 200-rounds Training finished param={'objective': 'multi:softmax', 'eta': 0.1, 'nthread': 8, 'num_class': 10, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['mlogloss', 'auc', 'merror'], 'max_depth': 9, 'min_child_weight': 9, 'gamma': 0.2, 'subsample': 0.7, 'colsample_bytree': 0.7} ...\t\t(848.865s)\n",
      "8 / 25: 200-rounds Training finished param={'objective': 'multi:softmax', 'eta': 0.1, 'nthread': 8, 'num_class': 10, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['mlogloss', 'auc', 'merror'], 'max_depth': 9, 'min_child_weight': 9, 'gamma': 0.2, 'subsample': 0.8, 'colsample_bytree': 0.7} ...\t\t(854.009s)\n",
      "9 / 25: 200-rounds Training finished param={'objective': 'multi:softmax', 'eta': 0.1, 'nthread': 8, 'num_class': 10, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['mlogloss', 'auc', 'merror'], 'max_depth': 9, 'min_child_weight': 9, 'gamma': 0.2, 'subsample': 0.9, 'colsample_bytree': 0.7} ...\t\t(863.061s)\n",
      "10 / 25: 200-rounds Training finished param={'objective': 'multi:softmax', 'eta': 0.1, 'nthread': 8, 'num_class': 10, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['mlogloss', 'auc', 'merror'], 'max_depth': 9, 'min_child_weight': 9, 'gamma': 0.2, 'subsample': 1.0, 'colsample_bytree': 0.7} ...\t\t(854.728s)\n",
      "11 / 25: 200-rounds Training finished param={'objective': 'multi:softmax', 'eta': 0.1, 'nthread': 8, 'num_class': 10, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['mlogloss', 'auc', 'merror'], 'max_depth': 9, 'min_child_weight': 9, 'gamma': 0.2, 'subsample': 0.6, 'colsample_bytree': 0.8} ...\t\t(833.519s)\n",
      "12 / 25: 200-rounds Training finished param={'objective': 'multi:softmax', 'eta': 0.1, 'nthread': 8, 'num_class': 10, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['mlogloss', 'auc', 'merror'], 'max_depth': 9, 'min_child_weight': 9, 'gamma': 0.2, 'subsample': 0.7, 'colsample_bytree': 0.8} ...\t\t(836.241s)\n",
      "13 / 25: 200-rounds Training finished param={'objective': 'multi:softmax', 'eta': 0.1, 'nthread': 8, 'num_class': 10, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['mlogloss', 'auc', 'merror'], 'max_depth': 9, 'min_child_weight': 9, 'gamma': 0.2, 'subsample': 0.8, 'colsample_bytree': 0.8} ...\t\t(852.569s)\n",
      "14 / 25: 200-rounds Training finished param={'objective': 'multi:softmax', 'eta': 0.1, 'nthread': 8, 'num_class': 10, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['mlogloss', 'auc', 'merror'], 'max_depth': 9, 'min_child_weight': 9, 'gamma': 0.2, 'subsample': 0.9, 'colsample_bytree': 0.8} ...\t\t(867.649s)\n",
      "15 / 25: 200-rounds Training finished param={'objective': 'multi:softmax', 'eta': 0.1, 'nthread': 8, 'num_class': 10, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['mlogloss', 'auc', 'merror'], 'max_depth': 9, 'min_child_weight': 9, 'gamma': 0.2, 'subsample': 1.0, 'colsample_bytree': 0.8} ...\t\t(837.901s)\n",
      "16 / 25: 200-rounds Training finished param={'objective': 'multi:softmax', 'eta': 0.1, 'nthread': 8, 'num_class': 10, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['mlogloss', 'auc', 'merror'], 'max_depth': 9, 'min_child_weight': 9, 'gamma': 0.2, 'subsample': 0.6, 'colsample_bytree': 0.9} ...\t\t(827.339s)\n",
      "17 / 25: 200-rounds Training finished param={'objective': 'multi:softmax', 'eta': 0.1, 'nthread': 8, 'num_class': 10, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['mlogloss', 'auc', 'merror'], 'max_depth': 9, 'min_child_weight': 9, 'gamma': 0.2, 'subsample': 0.7, 'colsample_bytree': 0.9} ...\t\t(829.429s)\n",
      "18 / 25: 200-rounds Training finished param={'objective': 'multi:softmax', 'eta': 0.1, 'nthread': 8, 'num_class': 10, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['mlogloss', 'auc', 'merror'], 'max_depth': 9, 'min_child_weight': 9, 'gamma': 0.2, 'subsample': 0.8, 'colsample_bytree': 0.9} ...\t\t(854.584s)\n",
      "19 / 25: 200-rounds Training finished param={'objective': 'multi:softmax', 'eta': 0.1, 'nthread': 8, 'num_class': 10, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['mlogloss', 'auc', 'merror'], 'max_depth': 9, 'min_child_weight': 9, 'gamma': 0.2, 'subsample': 0.9, 'colsample_bytree': 0.9} ...\t\t(865.975s)\n",
      "20 / 25: 200-rounds Training finished param={'objective': 'multi:softmax', 'eta': 0.1, 'nthread': 8, 'num_class': 10, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['mlogloss', 'auc', 'merror'], 'max_depth': 9, 'min_child_weight': 9, 'gamma': 0.2, 'subsample': 1.0, 'colsample_bytree': 0.9} ...\t\t(830.648s)\n",
      "21 / 25: 200-rounds Training finished param={'objective': 'multi:softmax', 'eta': 0.1, 'nthread': 8, 'num_class': 10, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['mlogloss', 'auc', 'merror'], 'max_depth': 9, 'min_child_weight': 9, 'gamma': 0.2, 'subsample': 0.6, 'colsample_bytree': 1.0} ...\t\t(846.784s)\n",
      "22 / 25: 200-rounds Training finished param={'objective': 'multi:softmax', 'eta': 0.1, 'nthread': 8, 'num_class': 10, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['mlogloss', 'auc', 'merror'], 'max_depth': 9, 'min_child_weight': 9, 'gamma': 0.2, 'subsample': 0.7, 'colsample_bytree': 1.0} ...\t\t(852.846s)\n",
      "23 / 25: 200-rounds Training finished param={'objective': 'multi:softmax', 'eta': 0.1, 'nthread': 8, 'num_class': 10, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['mlogloss', 'auc', 'merror'], 'max_depth': 9, 'min_child_weight': 9, 'gamma': 0.2, 'subsample': 0.8, 'colsample_bytree': 1.0} ...\t\t(850.433s)\n",
      "24 / 25: 200-rounds Training finished param={'objective': 'multi:softmax', 'eta': 0.1, 'nthread': 8, 'num_class': 10, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['mlogloss', 'auc', 'merror'], 'max_depth': 9, 'min_child_weight': 9, 'gamma': 0.2, 'subsample': 0.9, 'colsample_bytree': 1.0} ...\t\t(865.453s)\n",
      "25 / 25: 200-rounds Training finished param={'objective': 'multi:softmax', 'eta': 0.1, 'nthread': 8, 'num_class': 10, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['mlogloss', 'auc', 'merror'], 'max_depth': 9, 'min_child_weight': 9, 'gamma': 0.2, 'subsample': 1.0, 'colsample_bytree': 1.0} ...\t\t(833.121s)\n",
      "colsample_bytree=0.6, eta=0.1, eval_metric=['mlogloss', 'auc', 'merror'], gamma=0.2, gpu_id=0, max_depth=9, min_child_weight=9, nthread=8, num_class=10, objective=multi:softmax, subsample=0.9, tree_method=gpu_hist\n",
      "colsample_bytree=1.0, eta=0.1, eval_metric=['mlogloss', 'auc', 'merror'], gamma=0.2, gpu_id=0, max_depth=9, min_child_weight=9, nthread=8, num_class=10, objective=multi:softmax, subsample=0.6, tree_method=gpu_hist\n",
      "colsample_bytree=1.0, eta=0.1, eval_metric=['mlogloss', 'auc', 'merror'], gamma=0.2, gpu_id=0, max_depth=9, min_child_weight=9, nthread=8, num_class=10, objective=multi:softmax, subsample=0.9, tree_method=gpu_hist\n",
      "开始搜索：reg_alpha=[0, 0.1, 0.2, 0.5, 1, 1.5, 2, 4]\n",
      "待搜索的参数组合数量：8\n",
      "1 / 8: 200-rounds Training finished param={'objective': 'multi:softmax', 'eta': 0.1, 'nthread': 8, 'num_class': 10, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['mlogloss', 'auc', 'merror'], 'max_depth': 9, 'min_child_weight': 9, 'gamma': 0.2, 'subsample': 0.9, 'colsample_bytree': 0.6, 'reg_alpha': 0} ...\t\t(864.738s)\n",
      "2 / 8: 200-rounds Training finished param={'objective': 'multi:softmax', 'eta': 0.1, 'nthread': 8, 'num_class': 10, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['mlogloss', 'auc', 'merror'], 'max_depth': 9, 'min_child_weight': 9, 'gamma': 0.2, 'subsample': 0.9, 'colsample_bytree': 0.6, 'reg_alpha': 0.1} ...\t\t(871.016s)\n",
      "3 / 8: 200-rounds Training finished param={'objective': 'multi:softmax', 'eta': 0.1, 'nthread': 8, 'num_class': 10, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['mlogloss', 'auc', 'merror'], 'max_depth': 9, 'min_child_weight': 9, 'gamma': 0.2, 'subsample': 0.9, 'colsample_bytree': 0.6, 'reg_alpha': 0.2} ...\t\t(854.900s)\n",
      "4 / 8: 200-rounds Training finished param={'objective': 'multi:softmax', 'eta': 0.1, 'nthread': 8, 'num_class': 10, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['mlogloss', 'auc', 'merror'], 'max_depth': 9, 'min_child_weight': 9, 'gamma': 0.2, 'subsample': 0.9, 'colsample_bytree': 0.6, 'reg_alpha': 0.5} ...\t\t(846.531s)\n",
      "5 / 8: 200-rounds Training finished param={'objective': 'multi:softmax', 'eta': 0.1, 'nthread': 8, 'num_class': 10, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['mlogloss', 'auc', 'merror'], 'max_depth': 9, 'min_child_weight': 9, 'gamma': 0.2, 'subsample': 0.9, 'colsample_bytree': 0.6, 'reg_alpha': 1} ...\t\t(857.497s)\n",
      "6 / 8: 200-rounds Training finished param={'objective': 'multi:softmax', 'eta': 0.1, 'nthread': 8, 'num_class': 10, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['mlogloss', 'auc', 'merror'], 'max_depth': 9, 'min_child_weight': 9, 'gamma': 0.2, 'subsample': 0.9, 'colsample_bytree': 0.6, 'reg_alpha': 1.5} ...\t\t(874.460s)\n",
      "7 / 8: 200-rounds Training finished param={'objective': 'multi:softmax', 'eta': 0.1, 'nthread': 8, 'num_class': 10, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['mlogloss', 'auc', 'merror'], 'max_depth': 9, 'min_child_weight': 9, 'gamma': 0.2, 'subsample': 0.9, 'colsample_bytree': 0.6, 'reg_alpha': 2} ...\t\t(875.624s)\n",
      "8 / 8: 200-rounds Training finished param={'objective': 'multi:softmax', 'eta': 0.1, 'nthread': 8, 'num_class': 10, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['mlogloss', 'auc', 'merror'], 'max_depth': 9, 'min_child_weight': 9, 'gamma': 0.2, 'subsample': 0.9, 'colsample_bytree': 0.6, 'reg_alpha': 4} ...\t\t(902.339s)\n",
      "colsample_bytree=0.6, eta=0.1, eval_metric=['mlogloss', 'auc', 'merror'], gamma=0.2, gpu_id=0, max_depth=9, min_child_weight=9, nthread=8, num_class=10, objective=multi:softmax, reg_alpha=0, subsample=0.9, tree_method=gpu_hist\n",
      "colsample_bytree=0.6, eta=0.1, eval_metric=['mlogloss', 'auc', 'merror'], gamma=0.2, gpu_id=0, max_depth=9, min_child_weight=9, nthread=8, num_class=10, objective=multi:softmax, reg_alpha=1, subsample=0.9, tree_method=gpu_hist\n",
      "colsample_bytree=0.6, eta=0.1, eval_metric=['mlogloss', 'auc', 'merror'], gamma=0.2, gpu_id=0, max_depth=9, min_child_weight=9, nthread=8, num_class=10, objective=multi:softmax, reg_alpha=4, subsample=0.9, tree_method=gpu_hist\n",
      "找到的最棒的参数是：\n",
      "colsample_bytree=0.6, eta=0.1, eval_metric=['mlogloss', 'auc', 'merror'], gamma=0.2, gpu_id=0, max_depth=9, min_child_weight=9, nthread=8, num_class=10, objective=multi:softmax, reg_alpha=0, subsample=0.9, tree_method=gpu_hist\n"
     ]
    }
   ],
   "source": [
    "base = base_param.copy()\n",
    "base.update({'max_depth': 9, 'min_child_weight': 9})\n",
    "base.update({'gamma': .2})\n",
    "grids = [ps3, ps4]\n",
    "\n",
    "rets = []\n",
    "for grid in grids:\n",
    "    params = compose_param_grid(grid, base)\n",
    "    print(f\"开始搜索：{dict_2_str(grid)}\\n待搜索的参数组合数量：{len(params)}\")\n",
    "    ret = gridsearch_cv_xgb(data.values, watch_label_res.values, params, n_round=200, verbose_eval=False, n_class=10)\n",
    "    arr = np.array([[-e['eval-merror'] for e in ret], \n",
    "                    [-e['eval-mlogloss'] for e in ret],\n",
    "                    [e['eval-auc'] for e in ret], \n",
    "                    [e['w_auc'] for e in ret]], dtype=np.float32)\n",
    "    opt_idxs = list(set(arr.argmax(axis=1)))\n",
    "    for i in opt_idxs:\n",
    "        print(dict_2_str(ret[i]['param']))\n",
    "    opt_idx = opt_idxs[0]\n",
    "    opt_param = ret[opt_idx]['param']\n",
    "    base.update(opt_param)\n",
    "    rets.append(ret)\n",
    "    \n",
    "    ks = list(grid.keys())\n",
    "    ks = '-'.join(ks)\n",
    "    with open(f\"./logs/{ks}-watch_label-{int(time())}.pkl\", \"wb\") as f:\n",
    "        pickle.dump(ret, f)\n",
    "    with open(f\"./logs/{ks}-watch_label-{int(time())}.md\", \"w\") as f:\n",
    "        lines = []\n",
    "        for e in ret:\n",
    "            line = metric_2_str(e)\n",
    "            lines.append(line)\n",
    "            lines.append('\\n')\n",
    "        f.write(\"\\n\".join(lines))\n",
    "\n",
    "print(f\"找到的最棒的参数是：\\n{dict_2_str(base)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'objective': 'multi:softmax',\n",
       " 'eta': 0.1,\n",
       " 'nthread': 8,\n",
       " 'num_class': 10,\n",
       " 'gpu_id': 0,\n",
       " 'tree_method': 'gpu_hist',\n",
       " 'eval_metric': ['mlogloss', 'auc', 'merror'],\n",
       " 'max_depth': 9,\n",
       " 'min_child_weight': 9,\n",
       " 'gamma': 0.2,\n",
       " 'subsample': 0.9,\n",
       " 'colsample_bytree': 0.6,\n",
       " 'reg_alpha': 0}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gridsearch_results = gridsearch_xgb(all_params, xg_train, xg_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_cv_results_sh = gridsearch_cv_xgb(data.values, watch_label_res.values, all_params, n_round=200, verbose_eval=False, n_class=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 584,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "最小误差与最大AUC对应的模型不一致 : [93 19]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-584-646c9ffaa26f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mopt_idxs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mopt_idxs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mopt_idxs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"最小误差与最大AUC对应的模型不一致 : {opt_idxs}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mopt_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopt_idxs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: 最小误差与最大AUC对应的模型不一致 : [93 19]"
     ]
    }
   ],
   "source": [
    "arr = np.array([[-e['test_error'] for e in gridsearch_results], [e['w_auc'] for e in gridsearch_results]], dtype=np.float32)\n",
    "opt_idxs = arr.argmax(axis=1)\n",
    "if opt_idxs[0] != opt_idxs[1]:\n",
    "     warnings.warn(f\"最小误差与最大AUC对应的模型不一致 : {opt_idxs}。选择误差最小的模型 : {opt_idxs[0]}\")\n",
    "\n",
    "opt_idx = opt_idxs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 583,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'test_error': 0.7289070620796754,\n",
       "  'aucs': array([0.57555597, 0.58567653, 0.50347593, 0.50017473, 0.50017414,\n",
       "         0.5000983 , 0.50292636, 0.50040085, 0.50878295, 0.60767447]),\n",
       "  'w_auc': 2.365403849959146,\n",
       "  'report': '              precision    recall  f1-score   support\\n\\n           0       0.33      0.19      0.24     43749\\n           1       0.29      0.68      0.41    111616\\n           2       0.23      0.01      0.03     62829\\n           3       0.13      0.00      0.00     43872\\n           4       0.15      0.00      0.00     34228\\n           5       0.18      0.00      0.00     28926\\n           6       0.33      0.01      0.01     25061\\n           7       0.16      0.00      0.00     23400\\n           8       0.31      0.02      0.04     27750\\n           9       0.24      0.56      0.34     77663\\n\\n    accuracy                           0.27    479094\\n   macro avg       0.23      0.15      0.11    479094\\nweighted avg       0.24      0.27      0.18    479094\\n',\n",
       "  'model': <xgboost.core.Booster at 0x7f35ba8b00d0>},\n",
       " {'test_error': 0.73068750600091,\n",
       "  'aucs': array([0.57882633, 0.58668642, 0.50378179, 0.50067885, 0.50046282,\n",
       "         0.50010955, 0.50312644, 0.50043677, 0.50916784, 0.60848382]),\n",
       "  'w_auc': 2.367019878746503,\n",
       "  'report': '              precision    recall  f1-score   support\\n\\n           0       0.32      0.20      0.25     43749\\n           1       0.29      0.66      0.40    111616\\n           2       0.18      0.03      0.04     62829\\n           3       0.12      0.01      0.01     43872\\n           4       0.12      0.00      0.00     34228\\n           5       0.08      0.00      0.00     28926\\n           6       0.25      0.01      0.01     25061\\n           7       0.11      0.00      0.00     23400\\n           8       0.27      0.02      0.04     27750\\n           9       0.24      0.57      0.34     77663\\n\\n    accuracy                           0.27    479094\\n   macro avg       0.20      0.15      0.11    479094\\nweighted avg       0.22      0.27      0.18    479094\\n',\n",
       "  'model': <xgboost.core.Booster at 0x7f3584f94e50>})"
      ]
     },
     "execution_count": 583,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gridsearch_results[93]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 588,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.33      0.19      0.24     43749\n",
      "           1       0.29      0.68      0.41    111616\n",
      "           2       0.23      0.01      0.03     62829\n",
      "           3       0.13      0.00      0.00     43872\n",
      "           4       0.15      0.00      0.00     34228\n",
      "           5       0.18      0.00      0.00     28926\n",
      "           6       0.33      0.01      0.01     25061\n",
      "           7       0.16      0.00      0.00     23400\n",
      "           8       0.31      0.02      0.04     27750\n",
      "           9       0.24      0.56      0.34     77663\n",
      "\n",
      "    accuracy                           0.27    479094\n",
      "   macro avg       0.23      0.15      0.11    479094\n",
      "weighted avg       0.24      0.27      0.18    479094\n",
      "\n"
     ]
    }
   ],
   "source": [
    "opt_idx = opt_idxs[0]\n",
    "opt_param = all_params[opt_idx]\n",
    "print(gridsearch_results[opt_idx]['report'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 557,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(479094,)"
      ]
     },
     "execution_count": 557,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gridsearch_cv_xgb(data.values, watch_label_res, all_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {}\n",
    "# use softmax multi-class classification\n",
    "param['objective'] = 'multi:softmax'\n",
    "# scale weight of positive examples\n",
    "param['eta'] = 0.1\n",
    "param['max_depth'] = 8\n",
    "param['nthread'] = 8\n",
    "param['num_class'] = 10\n",
    "# param['gpu_id'] = 0\n",
    "# param['tree_method'] = 'gpu_hist'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "cv_res= xgb.cv(param, cv_data, num_boost_round=200,early_stopping_rounds=30,nfold=3, metrics='auc',show_stdv=True)\n",
    "print(cv_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB \n",
    "from mlxtend.classifier import StackingClassifier\n",
    "from sklearn.ensemble import (RandomForestClassifier, AdaBoostClassifier, \n",
    "                              GradientBoostingClassifier, ExtraTreesClassifier)\n",
    "from xgboost import XGBClassifier, XGBRFClassifier\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 第一层模型参数\n",
    "first_layer_params = {\n",
    "#     LogisticRegression : {\n",
    "#         'C' : 10,\n",
    "#         'random_state': 0\n",
    "#     },\n",
    "    XGBRFClassifier : {\n",
    "        'n_jobs': 4,\n",
    "        'n_estimators': 200,\n",
    "         #'max_features': 0.2,\n",
    "        'colsample_bytree': 0.6,\n",
    "        'max_depth': 8,\n",
    "        'learning_rate': .1,\n",
    "        'verbosity': 0,\n",
    "        'gpu_id': 1,\n",
    "    },\n",
    "    ExtraTreesClassifier : {\n",
    "        'n_jobs': -1,\n",
    "        'n_estimators':200,\n",
    "        #'max_features': 0.5,\n",
    "        'max_depth': 8,\n",
    "        'min_samples_leaf': 2,\n",
    "        'verbose': 0\n",
    "    },\n",
    "    AdaBoostClassifier : {\n",
    "        'n_estimators': 200,\n",
    "        'learning_rate' : 0.75\n",
    "    },\n",
    "#     GradientBoostingClassifier : { # 太慢了\n",
    "#         'n_estimators': 200,\n",
    "#          #'max_features': 0.2,\n",
    "#         'max_depth': 5,\n",
    "#         'min_samples_leaf': 2,\n",
    "#         'verbose': 0\n",
    "#     },\n",
    "#     SVC : { # 太慢了\n",
    "#         'kernel' : 'rbf',\n",
    "#         'C' : 0.025,\n",
    "#         'probability': True\n",
    "#     } \n",
    "}\n",
    "\n",
    "\n",
    "second_layer_params = {\n",
    "    XGBClassifier : {\n",
    "        'objective': 'multi:softmax',\n",
    "        'eta': 0.1,\n",
    "        'nthread': 8,\n",
    "        'num_class': 10,\n",
    "        'gpu_id': 0,\n",
    "        'tree_method': 'gpu_hist',\n",
    "        'eval_metric': ['mlogloss', 'auc', 'merror'],\n",
    "        'max_depth': 9,\n",
    "        'min_child_weight': 9,\n",
    "        'gamma': 0.2,\n",
    "        'subsample': 0.9,\n",
    "        'colsample_bytree': 0.6,\n",
    "        'reg_alpha': 0,\n",
    "        'n_estimators': 200,\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "clfs = []\n",
    "for c, p in first_layer_params.items():\n",
    "    clfs.append(c(**p))\n",
    "\n",
    "meta = XGBClassifier(**second_layer_params[XGBClassifier])\n",
    "sclf = StackingClassifier(classifiers=clfs, \n",
    "                          meta_classifier=meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gzy/.local/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "sclf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = sclf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Counter({9: 156864,\n",
       "          0: 97583,\n",
       "          3: 1479,\n",
       "          1: 86151,\n",
       "          2: 14444,\n",
       "          6: 829,\n",
       "          7: 368,\n",
       "          5: 400,\n",
       "          4: 530,\n",
       "          8: 738}),\n",
       " Counter({7: 17483,\n",
       "          6: 18603,\n",
       "          9: 58127,\n",
       "          3: 32872,\n",
       "          2: 47359,\n",
       "          4: 26076,\n",
       "          0: 58260,\n",
       "          1: 58161,\n",
       "          8: 20959,\n",
       "          5: 21486}))"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(tmp), Counter(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gzy/.local/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "clf = XGBRFClassifier(**first_layer_params[XGBRFClassifier])\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.10100084, 0.10091538, 0.10042646, 0.09997695, 0.09962477,\n",
       "        0.09947018, 0.09939709, 0.09926751, 0.09943457, 0.10048625]])"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# is_share 预测"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 处理数据不均衡问题"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 7338705), (1, 14319)]\n",
      "[[0.         0.99805264]\n",
      " [1.         0.00194736]]\n"
     ]
    }
   ],
   "source": [
    "items = list(Counter(is_share).items())\n",
    "items.sort(key=lambda x: x[0])\n",
    "print(items)\n",
    "\n",
    "dist = np.array(items, dtype=np.float)\n",
    "dist[:, 1] = dist[:, 1] / is_share.shape[0]\n",
    "print(dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "under_ss = np.array(items)\n",
    "under_ss_thresh = under_ss[1, 1]\n",
    "under_ss[:, 1] = np.clip(under_ss[:, 1], a_min=None, a_max=under_ss_thresh)\n",
    "\n",
    "over_ss = under_ss.copy()\n",
    "over_ss_thresh = under_ss[1, 1]\n",
    "over_ss[:, 1] = np.clip(over_ss[:, 1], a_min=over_ss_thresh, a_max=None)\n",
    "\n",
    "under_ss = dict(under_ss)\n",
    "over_ss = dict(over_ss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({0: 14319, 1: 14319}, {0: 14319, 1: 14319})"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "under_ss, over_ss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7338705,)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idxs = is_share == 0\n",
    "idxs = idxs.replace(False, np.nan).dropna().index  # 保留watch_label=0的行索引\n",
    "idxs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((7324386,), (14319,))"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "left_idxs = np.random.choice(idxs, under_ss_thresh, replace=False)  # 选择一部分保留\n",
    "del_idxs = idxs.difference(left_idxs)\n",
    "del_idxs.shape, left_idxs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((28638, 128), (28638,))"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['watch_label'] = watch_label\n",
    "resampled_data = np.delete(dataset.values, del_idxs, axis=0)\n",
    "resampled_sh = np.delete(is_share.values, del_idxs, axis=0)\n",
    "resampled_data.shape, resampled_sh.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((28638, 128), (28638,))"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 将采样后的数据重装会DataFrame\n",
    "data_sh = pd.DataFrame(resampled_data, columns=dataset.columns)\n",
    "del dataset['watch_label']\n",
    "is_share_res = pd.Series(resampled_sh)\n",
    "data_sh.shape, is_share_res.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((22910,), (5728,))"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_rate = .2\n",
    "train_idx, test_idx = train_test_split(data_sh.index, test_size=test_rate, random_state=1)\n",
    "train_idx.shape, test_idx.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_sh = data_sh.iloc[train_idx]\n",
    "X_test_sh  = data_sh.iloc[test_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_sh = is_share_res.iloc[train_idx]\n",
    "y_test_sh  = is_share_res.iloc[test_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data preparing finished ...\t\t(0.020s)\n"
     ]
    }
   ],
   "source": [
    "t0 = time()\n",
    "xg_train_sh = xgb.DMatrix(X_train_sh.values, label=y_train_sh.values, enable_categorical=True)\n",
    "xg_test_sh = xgb.DMatrix(X_test_sh.values, label=y_test_sh.values, enable_categorical=True)\n",
    "print(f\"Data preparing finished ...\\t\\t({time()-t0:.3f}s)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup parameters for xgboost\n",
    "param_sh = {\n",
    "    'objective': 'binary:hinge',\n",
    "    'eta': 0.1,\n",
    "    'nthread': 8,\n",
    "    'gpu_id': 0,\n",
    "    'tree_method': 'gpu_hist',\n",
    "    'eval_metric': ['logloss', 'auc', 'error'],\n",
    "    'max_depth': 5,\n",
    "    'min_child_weight': 1,\n",
    "    'gamma': 0.1,\n",
    "    'subsample': 0.6,\n",
    "    'colsample_bytree': 0.6,\n",
    "    'reg_alpha': 0\n",
    "}\n",
    "\n",
    "# use softmax multi-class classification\n",
    "# param_sh['objective'] = 'binary:hinge'\n",
    "# scale weight of positive examples\n",
    "# param_sh['eta'] = 0.1\n",
    "# param_sh['max_depth'] = 6\n",
    "# param_sh['nthread'] = 4\n",
    "# param_sh['gpu_id'] = 0\n",
    "# param_sh['tree_method'] = 'gpu_hist'\n",
    "# param_sh['min_child_weight'] = 7\n",
    "\n",
    "\n",
    "watchlist = [(xg_train_sh, 'train'), (xg_test_sh, 'test')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-logloss:18.39978\ttrain-auc:0.50000\ttrain-error:0.49943\ttest-logloss:18.50429\ttest-auc:0.50000\ttest-error:0.50227\n",
      "[1]\ttrain-logloss:18.39978\ttrain-auc:0.50000\ttrain-error:0.49943\ttest-logloss:18.50429\ttest-auc:0.50000\ttest-error:0.50227\n",
      "[2]\ttrain-logloss:18.39978\ttrain-auc:0.50000\ttrain-error:0.49943\ttest-logloss:18.50429\ttest-auc:0.50000\ttest-error:0.50227\n",
      "[3]\ttrain-logloss:18.39978\ttrain-auc:0.50000\ttrain-error:0.49943\ttest-logloss:18.50429\ttest-auc:0.50000\ttest-error:0.50227\n",
      "[4]\ttrain-logloss:18.39978\ttrain-auc:0.50000\ttrain-error:0.49943\ttest-logloss:18.50429\ttest-auc:0.50000\ttest-error:0.50227\n",
      "[5]\ttrain-logloss:18.39978\ttrain-auc:0.50000\ttrain-error:0.49943\ttest-logloss:18.50429\ttest-auc:0.50000\ttest-error:0.50227\n",
      "[6]\ttrain-logloss:18.39978\ttrain-auc:0.50000\ttrain-error:0.49943\ttest-logloss:18.50429\ttest-auc:0.50000\ttest-error:0.50227\n",
      "[7]\ttrain-logloss:17.54910\ttrain-auc:0.52312\ttrain-error:0.47634\ttest-logloss:17.73891\ttest-auc:0.52066\ttest-error:0.48149\n",
      "[8]\ttrain-logloss:16.74183\ttrain-auc:0.54507\ttrain-error:0.45443\ttest-logloss:16.86418\ttest-auc:0.54426\ttest-error:0.45775\n",
      "[9]\ttrain-logloss:16.00372\ttrain-auc:0.56514\ttrain-error:0.43439\ttest-logloss:16.11810\ttest-auc:0.56435\ttest-error:0.43750\n",
      "[10]\ttrain-logloss:15.52933\ttrain-auc:0.57805\ttrain-error:0.42152\ttest-logloss:15.55853\ttest-auc:0.57943\ttest-error:0.42231\n",
      "[11]\ttrain-logloss:15.33475\ttrain-auc:0.58334\ttrain-error:0.41624\ttest-logloss:15.23694\ttest-auc:0.58809\ttest-error:0.41358\n",
      "[12]\ttrain-logloss:15.15304\ttrain-auc:0.58829\ttrain-error:0.41130\ttest-logloss:15.19192\ttest-auc:0.58925\ttest-error:0.41236\n",
      "[13]\ttrain-logloss:15.06299\ttrain-auc:0.59074\ttrain-error:0.40886\ttest-logloss:15.06971\ttest-auc:0.59254\ttest-error:0.40904\n",
      "[14]\ttrain-logloss:14.98097\ttrain-auc:0.59298\ttrain-error:0.40664\ttest-logloss:15.01826\ttest-auc:0.59390\ttest-error:0.40765\n",
      "[15]\ttrain-logloss:14.91343\ttrain-auc:0.59481\ttrain-error:0.40480\ttest-logloss:14.92178\ttest-auc:0.59649\ttest-error:0.40503\n",
      "[16]\ttrain-logloss:14.89092\ttrain-auc:0.59543\ttrain-error:0.40419\ttest-logloss:14.87033\ttest-auc:0.59788\ttest-error:0.40363\n",
      "[17]\ttrain-logloss:14.89253\ttrain-auc:0.59539\ttrain-error:0.40423\ttest-logloss:14.86389\ttest-auc:0.59804\ttest-error:0.40346\n",
      "[18]\ttrain-logloss:14.85393\ttrain-auc:0.59644\ttrain-error:0.40319\ttest-logloss:14.81887\ttest-auc:0.59925\ttest-error:0.40224\n",
      "[19]\ttrain-logloss:14.83946\ttrain-auc:0.59684\ttrain-error:0.40279\ttest-logloss:14.78028\ttest-auc:0.60029\ttest-error:0.40119\n",
      "[20]\ttrain-logloss:14.81373\ttrain-auc:0.59754\ttrain-error:0.40209\ttest-logloss:14.74169\ttest-auc:0.60133\ttest-error:0.40014\n",
      "[21]\ttrain-logloss:14.76549\ttrain-auc:0.59885\ttrain-error:0.40079\ttest-logloss:14.67737\ttest-auc:0.60306\ttest-error:0.39839\n",
      "[22]\ttrain-logloss:14.68830\ttrain-auc:0.60095\ttrain-error:0.39869\ttest-logloss:14.66451\ttest-auc:0.60339\ttest-error:0.39804\n",
      "[23]\ttrain-logloss:14.64810\ttrain-auc:0.60204\ttrain-error:0.39760\ttest-logloss:14.58733\ttest-auc:0.60547\ttest-error:0.39595\n",
      "[24]\ttrain-logloss:14.56930\ttrain-auc:0.60419\ttrain-error:0.39546\ttest-logloss:14.54874\ttest-auc:0.60650\ttest-error:0.39490\n",
      "[25]\ttrain-logloss:14.54196\ttrain-auc:0.60493\ttrain-error:0.39472\ttest-logloss:14.49728\ttest-auc:0.60789\ttest-error:0.39351\n",
      "[26]\ttrain-logloss:14.45513\ttrain-auc:0.60729\ttrain-error:0.39236\ttest-logloss:14.38151\ttest-auc:0.61101\ttest-error:0.39036\n",
      "[27]\ttrain-logloss:14.37794\ttrain-auc:0.60940\ttrain-error:0.39027\ttest-logloss:14.33005\ttest-auc:0.61238\ttest-error:0.38897\n",
      "[28]\ttrain-logloss:14.32166\ttrain-auc:0.61093\ttrain-error:0.38874\ttest-logloss:14.24644\ttest-auc:0.61463\ttest-error:0.38670\n",
      "[29]\ttrain-logloss:14.23482\ttrain-auc:0.61329\ttrain-error:0.38638\ttest-logloss:14.13067\ttest-auc:0.61775\ttest-error:0.38355\n",
      "[30]\ttrain-logloss:14.18336\ttrain-auc:0.61469\ttrain-error:0.38499\ttest-logloss:14.10494\ttest-auc:0.61843\ttest-error:0.38286\n",
      "[31]\ttrain-logloss:14.09652\ttrain-auc:0.61705\ttrain-error:0.38263\ttest-logloss:14.05349\ttest-auc:0.61981\ttest-error:0.38146\n",
      "[32]\ttrain-logloss:14.01130\ttrain-auc:0.61937\ttrain-error:0.38031\ttest-logloss:14.03419\ttest-auc:0.62031\ttest-error:0.38094\n",
      "[33]\ttrain-logloss:13.90999\ttrain-auc:0.62213\ttrain-error:0.37756\ttest-logloss:13.96987\ttest-auc:0.62203\ttest-error:0.37919\n",
      "[34]\ttrain-logloss:13.86496\ttrain-auc:0.62336\ttrain-error:0.37634\ttest-logloss:13.89912\ttest-auc:0.62393\ttest-error:0.37727\n",
      "[35]\ttrain-logloss:13.79099\ttrain-auc:0.62537\ttrain-error:0.37433\ttest-logloss:13.91199\ttest-auc:0.62357\ttest-error:0.37762\n",
      "[36]\ttrain-logloss:13.73792\ttrain-auc:0.62681\ttrain-error:0.37289\ttest-logloss:13.86053\ttest-auc:0.62495\ttest-error:0.37622\n",
      "[37]\ttrain-logloss:13.69772\ttrain-auc:0.62791\ttrain-error:0.37180\ttest-logloss:13.78978\ttest-auc:0.62685\ttest-error:0.37430\n",
      "[38]\ttrain-logloss:13.63661\ttrain-auc:0.62957\ttrain-error:0.37014\ttest-logloss:13.75119\ttest-auc:0.62788\ttest-error:0.37325\n",
      "[39]\ttrain-logloss:13.59319\ttrain-auc:0.63075\ttrain-error:0.36897\ttest-logloss:13.75119\ttest-auc:0.62787\ttest-error:0.37325\n",
      "[40]\ttrain-logloss:13.50796\ttrain-auc:0.63307\ttrain-error:0.36665\ttest-logloss:13.74476\ttest-auc:0.62803\ttest-error:0.37308\n",
      "[41]\ttrain-logloss:13.47741\ttrain-auc:0.63390\ttrain-error:0.36582\ttest-logloss:13.70617\ttest-auc:0.62906\ttest-error:0.37203\n",
      "[42]\ttrain-logloss:13.29087\ttrain-auc:0.63898\ttrain-error:0.36076\ttest-logloss:13.50678\ttest-auc:0.63442\ttest-error:0.36662\n",
      "[43]\ttrain-logloss:13.19117\ttrain-auc:0.64169\ttrain-error:0.35805\ttest-logloss:13.39744\ttest-auc:0.63735\ttest-error:0.36365\n",
      "[44]\ttrain-logloss:13.09951\ttrain-auc:0.64419\ttrain-error:0.35557\ttest-logloss:13.37172\ttest-auc:0.63802\ttest-error:0.36295\n",
      "[45]\ttrain-logloss:13.06252\ttrain-auc:0.64519\ttrain-error:0.35456\ttest-logloss:13.29453\ttest-auc:0.64010\ttest-error:0.36086\n",
      "[46]\ttrain-logloss:12.99177\ttrain-auc:0.64712\ttrain-error:0.35264\ttest-logloss:13.20449\ttest-auc:0.64252\ttest-error:0.35841\n",
      "[47]\ttrain-logloss:12.94352\ttrain-auc:0.64843\ttrain-error:0.35133\ttest-logloss:13.23665\ttest-auc:0.64164\ttest-error:0.35929\n",
      "[48]\ttrain-logloss:12.90493\ttrain-auc:0.64948\ttrain-error:0.35028\ttest-logloss:13.18519\ttest-auc:0.64301\ttest-error:0.35789\n",
      "[49]\ttrain-logloss:12.85990\ttrain-auc:0.65071\ttrain-error:0.34906\ttest-logloss:13.12731\ttest-auc:0.64457\ttest-error:0.35632\n",
      "[50]\ttrain-logloss:12.81327\ttrain-auc:0.65198\ttrain-error:0.34780\ttest-logloss:13.08872\ttest-auc:0.64559\ttest-error:0.35527\n",
      "[51]\ttrain-logloss:12.79075\ttrain-auc:0.65259\ttrain-error:0.34719\ttest-logloss:13.04369\ttest-auc:0.64681\ttest-error:0.35405\n",
      "[52]\ttrain-logloss:12.73769\ttrain-auc:0.65404\ttrain-error:0.34574\ttest-logloss:13.05013\ttest-auc:0.64663\ttest-error:0.35423\n",
      "[53]\ttrain-logloss:12.68784\ttrain-auc:0.65539\ttrain-error:0.34439\ttest-logloss:12.96008\ttest-auc:0.64905\ttest-error:0.35178\n",
      "[54]\ttrain-logloss:12.63156\ttrain-auc:0.65692\ttrain-error:0.34286\ttest-logloss:12.88290\ttest-auc:0.65114\ttest-error:0.34969\n",
      "[55]\ttrain-logloss:12.55919\ttrain-auc:0.65889\ttrain-error:0.34090\ttest-logloss:12.83145\ttest-auc:0.65252\ttest-error:0.34829\n",
      "[56]\ttrain-logloss:12.53507\ttrain-auc:0.65955\ttrain-error:0.34024\ttest-logloss:12.76713\ttest-auc:0.65425\ttest-error:0.34654\n",
      "[57]\ttrain-logloss:12.50773\ttrain-auc:0.66029\ttrain-error:0.33950\ttest-logloss:12.81858\ttest-auc:0.65284\ttest-error:0.34794\n",
      "[58]\ttrain-logloss:12.49004\ttrain-auc:0.66078\ttrain-error:0.33902\ttest-logloss:12.81215\ttest-auc:0.65301\ttest-error:0.34776\n",
      "[59]\ttrain-logloss:12.45466\ttrain-auc:0.66174\ttrain-error:0.33806\ttest-logloss:12.76713\ttest-auc:0.65422\ttest-error:0.34654\n",
      "[60]\ttrain-logloss:12.42089\ttrain-auc:0.66266\ttrain-error:0.33714\ttest-logloss:12.70281\ttest-auc:0.65595\ttest-error:0.34480\n",
      "[61]\ttrain-logloss:12.42411\ttrain-auc:0.66258\ttrain-error:0.33723\ttest-logloss:12.71567\ttest-auc:0.65559\ttest-error:0.34515\n",
      "[62]\ttrain-logloss:12.39516\ttrain-auc:0.66336\ttrain-error:0.33645\ttest-logloss:12.70924\ttest-auc:0.65576\ttest-error:0.34497\n",
      "[63]\ttrain-logloss:12.39195\ttrain-auc:0.66345\ttrain-error:0.33636\ttest-logloss:12.65136\ttest-auc:0.65732\ttest-error:0.34340\n",
      "[64]\ttrain-logloss:12.33406\ttrain-auc:0.66503\ttrain-error:0.33479\ttest-logloss:12.63849\ttest-auc:0.65766\ttest-error:0.34305\n",
      "[65]\ttrain-logloss:12.31315\ttrain-auc:0.66560\ttrain-error:0.33422\ttest-logloss:12.63849\ttest-auc:0.65764\ttest-error:0.34305\n",
      "[66]\ttrain-logloss:12.29546\ttrain-auc:0.66608\ttrain-error:0.33374\ttest-logloss:12.63206\ttest-auc:0.65781\ttest-error:0.34288\n",
      "[67]\ttrain-logloss:12.27295\ttrain-auc:0.66670\ttrain-error:0.33313\ttest-logloss:12.58704\ttest-auc:0.65902\ttest-error:0.34165\n",
      "[68]\ttrain-logloss:12.26973\ttrain-auc:0.66678\ttrain-error:0.33304\ttest-logloss:12.59347\ttest-auc:0.65883\ttest-error:0.34183\n",
      "[69]\ttrain-logloss:12.22631\ttrain-auc:0.66797\ttrain-error:0.33186\ttest-logloss:12.61276\ttest-auc:0.65830\ttest-error:0.34235\n",
      "[70]\ttrain-logloss:12.20541\ttrain-auc:0.66853\ttrain-error:0.33130\ttest-logloss:12.59990\ttest-auc:0.65864\ttest-error:0.34200\n",
      "[71]\ttrain-logloss:12.17325\ttrain-auc:0.66941\ttrain-error:0.33042\ttest-logloss:12.59990\ttest-auc:0.65863\ttest-error:0.34200\n",
      "[72]\ttrain-logloss:12.14269\ttrain-auc:0.67024\ttrain-error:0.32959\ttest-logloss:12.56131\ttest-auc:0.65966\ttest-error:0.34096\n",
      "[73]\ttrain-logloss:12.12179\ttrain-auc:0.67081\ttrain-error:0.32903\ttest-logloss:12.52915\ttest-auc:0.66053\ttest-error:0.34008\n",
      "[74]\ttrain-logloss:12.11696\ttrain-auc:0.67094\ttrain-error:0.32890\ttest-logloss:12.49056\ttest-auc:0.66157\ttest-error:0.33904\n",
      "[75]\ttrain-logloss:12.11696\ttrain-auc:0.67095\ttrain-error:0.32890\ttest-logloss:12.51629\ttest-auc:0.66086\ttest-error:0.33974\n",
      "[76]\ttrain-logloss:12.09767\ttrain-auc:0.67147\ttrain-error:0.32837\ttest-logloss:12.47770\ttest-auc:0.66190\ttest-error:0.33869\n",
      "[77]\ttrain-logloss:12.10088\ttrain-auc:0.67139\ttrain-error:0.32846\ttest-logloss:12.48413\ttest-auc:0.66171\ttest-error:0.33886\n",
      "[78]\ttrain-logloss:12.08802\ttrain-auc:0.67174\ttrain-error:0.32811\ttest-logloss:12.49699\ttest-auc:0.66136\ttest-error:0.33921\n",
      "[79]\ttrain-logloss:12.07837\ttrain-auc:0.67200\ttrain-error:0.32785\ttest-logloss:12.50985\ttest-auc:0.66100\ttest-error:0.33956\n",
      "[80]\ttrain-logloss:12.06068\ttrain-auc:0.67248\ttrain-error:0.32737\ttest-logloss:12.49056\ttest-auc:0.66152\ttest-error:0.33904\n",
      "[81]\ttrain-logloss:12.03656\ttrain-auc:0.67314\ttrain-error:0.32671\ttest-logloss:12.50985\ttest-auc:0.66099\ttest-error:0.33956\n",
      "[82]\ttrain-logloss:12.03978\ttrain-auc:0.67305\ttrain-error:0.32680\ttest-logloss:12.49699\ttest-auc:0.66133\ttest-error:0.33921\n",
      "[83]\ttrain-logloss:12.02691\ttrain-auc:0.67340\ttrain-error:0.32645\ttest-logloss:12.48413\ttest-auc:0.66167\ttest-error:0.33886\n",
      "[84]\ttrain-logloss:12.00279\ttrain-auc:0.67406\ttrain-error:0.32580\ttest-logloss:12.44554\ttest-auc:0.66271\ttest-error:0.33781\n",
      "[85]\ttrain-logloss:11.99958\ttrain-auc:0.67415\ttrain-error:0.32571\ttest-logloss:12.43910\ttest-auc:0.66288\ttest-error:0.33764\n",
      "[86]\ttrain-logloss:11.97867\ttrain-auc:0.67472\ttrain-error:0.32514\ttest-logloss:12.42624\ttest-auc:0.66323\ttest-error:0.33729\n",
      "[87]\ttrain-logloss:11.96580\ttrain-auc:0.67507\ttrain-error:0.32479\ttest-logloss:12.40052\ttest-auc:0.66392\ttest-error:0.33659\n",
      "[88]\ttrain-logloss:11.95776\ttrain-auc:0.67529\ttrain-error:0.32457\ttest-logloss:12.40695\ttest-auc:0.66374\ttest-error:0.33677\n",
      "[89]\ttrain-logloss:11.93043\ttrain-auc:0.67603\ttrain-error:0.32383\ttest-logloss:12.38122\ttest-auc:0.66443\ttest-error:0.33607\n",
      "[90]\ttrain-logloss:11.90791\ttrain-auc:0.67664\ttrain-error:0.32322\ttest-logloss:12.38765\ttest-auc:0.66425\ttest-error:0.33624\n",
      "[91]\ttrain-logloss:11.89022\ttrain-auc:0.67712\ttrain-error:0.32274\ttest-logloss:12.39408\ttest-auc:0.66407\ttest-error:0.33642\n",
      "[92]\ttrain-logloss:11.89666\ttrain-auc:0.67695\ttrain-error:0.32292\ttest-logloss:12.36835\ttest-auc:0.66476\ttest-error:0.33572\n",
      "[93]\ttrain-logloss:11.87575\ttrain-auc:0.67752\ttrain-error:0.32235\ttest-logloss:12.38122\ttest-auc:0.66440\ttest-error:0.33607\n",
      "[94]\ttrain-logloss:11.87253\ttrain-auc:0.67761\ttrain-error:0.32226\ttest-logloss:12.40052\ttest-auc:0.66387\ttest-error:0.33659\n",
      "[95]\ttrain-logloss:11.88540\ttrain-auc:0.67726\ttrain-error:0.32261\ttest-logloss:12.38765\ttest-auc:0.66421\ttest-error:0.33624\n",
      "[96]\ttrain-logloss:11.87897\ttrain-auc:0.67743\ttrain-error:0.32244\ttest-logloss:12.40052\ttest-auc:0.66386\ttest-error:0.33659\n",
      "[97]\ttrain-logloss:11.87414\ttrain-auc:0.67757\ttrain-error:0.32231\ttest-logloss:12.40695\ttest-auc:0.66368\ttest-error:0.33677\n",
      "[98]\ttrain-logloss:11.86449\ttrain-auc:0.67783\ttrain-error:0.32204\ttest-logloss:12.40052\ttest-auc:0.66386\ttest-error:0.33659\n",
      "[99]\ttrain-logloss:11.83394\ttrain-auc:0.67866\ttrain-error:0.32121\ttest-logloss:12.40695\ttest-auc:0.66368\ttest-error:0.33677\n",
      "[100]\ttrain-logloss:11.82751\ttrain-auc:0.67883\ttrain-error:0.32104\ttest-logloss:12.44554\ttest-auc:0.66263\ttest-error:0.33781\n",
      "[101]\ttrain-logloss:11.81304\ttrain-auc:0.67923\ttrain-error:0.32065\ttest-logloss:12.43267\ttest-auc:0.66298\ttest-error:0.33747\n",
      "[102]\ttrain-logloss:11.80982\ttrain-auc:0.67932\ttrain-error:0.32056\ttest-logloss:12.40052\ttest-auc:0.66385\ttest-error:0.33659\n",
      "[103]\ttrain-logloss:11.79052\ttrain-auc:0.67984\ttrain-error:0.32004\ttest-logloss:12.37479\ttest-auc:0.66454\ttest-error:0.33589\n",
      "[104]\ttrain-logloss:11.76962\ttrain-auc:0.68041\ttrain-error:0.31947\ttest-logloss:12.38122\ttest-auc:0.66436\ttest-error:0.33607\n",
      "[105]\ttrain-logloss:11.76640\ttrain-auc:0.68050\ttrain-error:0.31938\ttest-logloss:12.33620\ttest-auc:0.66559\ttest-error:0.33485\n",
      "[106]\ttrain-logloss:11.75032\ttrain-auc:0.68093\ttrain-error:0.31894\ttest-logloss:12.36835\ttest-auc:0.66471\ttest-error:0.33572\n",
      "[107]\ttrain-logloss:11.75353\ttrain-auc:0.68085\ttrain-error:0.31903\ttest-logloss:12.38765\ttest-auc:0.66418\ttest-error:0.33624\n",
      "[108]\ttrain-logloss:11.72138\ttrain-auc:0.68172\ttrain-error:0.31816\ttest-logloss:12.37479\ttest-auc:0.66452\ttest-error:0.33589\n",
      "[109]\ttrain-logloss:11.72459\ttrain-auc:0.68164\ttrain-error:0.31825\ttest-logloss:12.34906\ttest-auc:0.66522\ttest-error:0.33520\n",
      "[110]\ttrain-logloss:11.71816\ttrain-auc:0.68181\ttrain-error:0.31807\ttest-logloss:12.35549\ttest-auc:0.66505\ttest-error:0.33537\n",
      "[111]\ttrain-logloss:11.70690\ttrain-auc:0.68212\ttrain-error:0.31777\ttest-logloss:12.35549\ttest-auc:0.66504\ttest-error:0.33537\n",
      "[112]\ttrain-logloss:11.70208\ttrain-auc:0.68225\ttrain-error:0.31763\ttest-logloss:12.37479\ttest-auc:0.66451\ttest-error:0.33589\n",
      "[113]\ttrain-logloss:11.68439\ttrain-auc:0.68273\ttrain-error:0.31715\ttest-logloss:12.35549\ttest-auc:0.66504\ttest-error:0.33537\n",
      "[114]\ttrain-logloss:11.66670\ttrain-auc:0.68321\ttrain-error:0.31667\ttest-logloss:12.35549\ttest-auc:0.66504\ttest-error:0.33537\n",
      "[115]\ttrain-logloss:11.66831\ttrain-auc:0.68317\ttrain-error:0.31672\ttest-logloss:12.34906\ttest-auc:0.66521\ttest-error:0.33520\n",
      "[116]\ttrain-logloss:11.67474\ttrain-auc:0.68300\ttrain-error:0.31689\ttest-logloss:12.36192\ttest-auc:0.66486\ttest-error:0.33554\n",
      "[117]\ttrain-logloss:11.67313\ttrain-auc:0.68304\ttrain-error:0.31685\ttest-logloss:12.32333\ttest-auc:0.66591\ttest-error:0.33450\n",
      "[118]\ttrain-logloss:11.66027\ttrain-auc:0.68339\ttrain-error:0.31650\ttest-logloss:12.32333\ttest-auc:0.66591\ttest-error:0.33450\n",
      "[119]\ttrain-logloss:11.63615\ttrain-auc:0.68405\ttrain-error:0.31584\ttest-logloss:12.34263\ttest-auc:0.66536\ttest-error:0.33502\n",
      "[120]\ttrain-logloss:11.62489\ttrain-auc:0.68435\ttrain-error:0.31554\ttest-logloss:12.36835\ttest-auc:0.66466\ttest-error:0.33572\n",
      "[121]\ttrain-logloss:11.62167\ttrain-auc:0.68444\ttrain-error:0.31545\ttest-logloss:12.38122\ttest-auc:0.66431\ttest-error:0.33607\n",
      "[122]\ttrain-logloss:11.62489\ttrain-auc:0.68435\ttrain-error:0.31554\ttest-logloss:12.35549\ttest-auc:0.66501\ttest-error:0.33537\n",
      "[123]\ttrain-logloss:11.60559\ttrain-auc:0.68488\ttrain-error:0.31501\ttest-logloss:12.37479\ttest-auc:0.66448\ttest-error:0.33589\n",
      "[124]\ttrain-logloss:11.61524\ttrain-auc:0.68462\ttrain-error:0.31528\ttest-logloss:12.32333\ttest-auc:0.66588\ttest-error:0.33450\n",
      "[125]\ttrain-logloss:11.61041\ttrain-auc:0.68475\ttrain-error:0.31515\ttest-logloss:12.34263\ttest-auc:0.66535\ttest-error:0.33502\n",
      "[126]\ttrain-logloss:11.57504\ttrain-auc:0.68571\ttrain-error:0.31419\ttest-logloss:12.35549\ttest-auc:0.66500\ttest-error:0.33537\n",
      "[127]\ttrain-logloss:11.56700\ttrain-auc:0.68593\ttrain-error:0.31397\ttest-logloss:12.32333\ttest-auc:0.66587\ttest-error:0.33450\n",
      "[128]\ttrain-logloss:11.56217\ttrain-auc:0.68606\ttrain-error:0.31384\ttest-logloss:12.31690\ttest-auc:0.66604\ttest-error:0.33432\n",
      "[129]\ttrain-logloss:11.54288\ttrain-auc:0.68658\ttrain-error:0.31331\ttest-logloss:12.33620\ttest-auc:0.66552\ttest-error:0.33485\n",
      "[130]\ttrain-logloss:11.53805\ttrain-auc:0.68671\ttrain-error:0.31318\ttest-logloss:12.31047\ttest-auc:0.66622\ttest-error:0.33415\n",
      "[131]\ttrain-logloss:11.51715\ttrain-auc:0.68728\ttrain-error:0.31261\ttest-logloss:12.29761\ttest-auc:0.66656\ttest-error:0.33380\n",
      "[132]\ttrain-logloss:11.50750\ttrain-auc:0.68754\ttrain-error:0.31235\ttest-logloss:12.29761\ttest-auc:0.66656\ttest-error:0.33380\n",
      "[133]\ttrain-logloss:11.49785\ttrain-auc:0.68781\ttrain-error:0.31209\ttest-logloss:12.29761\ttest-auc:0.66656\ttest-error:0.33380\n",
      "[134]\ttrain-logloss:11.48820\ttrain-auc:0.68807\ttrain-error:0.31183\ttest-logloss:12.28474\ttest-auc:0.66690\ttest-error:0.33345\n",
      "[135]\ttrain-logloss:11.47855\ttrain-auc:0.68833\ttrain-error:0.31157\ttest-logloss:12.29761\ttest-auc:0.66655\ttest-error:0.33380\n",
      "[136]\ttrain-logloss:11.48981\ttrain-auc:0.68803\ttrain-error:0.31187\ttest-logloss:12.31047\ttest-auc:0.66620\ttest-error:0.33415\n",
      "[137]\ttrain-logloss:11.48338\ttrain-auc:0.68820\ttrain-error:0.31170\ttest-logloss:12.27831\ttest-auc:0.66707\ttest-error:0.33327\n",
      "[138]\ttrain-logloss:11.47855\ttrain-auc:0.68833\ttrain-error:0.31157\ttest-logloss:12.27188\ttest-auc:0.66724\ttest-error:0.33310\n",
      "[139]\ttrain-logloss:11.44961\ttrain-auc:0.68912\ttrain-error:0.31078\ttest-logloss:12.27831\ttest-auc:0.66706\ttest-error:0.33327\n",
      "[140]\ttrain-logloss:11.43031\ttrain-auc:0.68965\ttrain-error:0.31026\ttest-logloss:12.27188\ttest-auc:0.66724\ttest-error:0.33310\n",
      "[141]\ttrain-logloss:11.41101\ttrain-auc:0.69017\ttrain-error:0.30973\ttest-logloss:12.23972\ttest-auc:0.66810\ttest-error:0.33223\n",
      "[142]\ttrain-logloss:11.39815\ttrain-auc:0.69052\ttrain-error:0.30939\ttest-logloss:12.25901\ttest-auc:0.66758\ttest-error:0.33275\n",
      "[143]\ttrain-logloss:11.38528\ttrain-auc:0.69087\ttrain-error:0.30904\ttest-logloss:12.25258\ttest-auc:0.66775\ttest-error:0.33258\n",
      "[144]\ttrain-logloss:11.40940\ttrain-auc:0.69022\ttrain-error:0.30969\ttest-logloss:12.20756\ttest-auc:0.66897\ttest-error:0.33136\n",
      "[145]\ttrain-logloss:11.38689\ttrain-auc:0.69083\ttrain-error:0.30908\ttest-logloss:12.16897\ttest-auc:0.67001\ttest-error:0.33031\n",
      "[146]\ttrain-logloss:11.37724\ttrain-auc:0.69109\ttrain-error:0.30882\ttest-logloss:12.17540\ttest-auc:0.66984\ttest-error:0.33048\n",
      "[147]\ttrain-logloss:11.37403\ttrain-auc:0.69118\ttrain-error:0.30873\ttest-logloss:12.18826\ttest-auc:0.66949\ttest-error:0.33083\n",
      "[148]\ttrain-logloss:11.35473\ttrain-auc:0.69170\ttrain-error:0.30821\ttest-logloss:12.19470\ttest-auc:0.66931\ttest-error:0.33101\n",
      "[149]\ttrain-logloss:11.32900\ttrain-auc:0.69240\ttrain-error:0.30751\ttest-logloss:12.16897\ttest-auc:0.67001\ttest-error:0.33031\n",
      "[150]\ttrain-logloss:11.27915\ttrain-auc:0.69376\ttrain-error:0.30616\ttest-logloss:12.16254\ttest-auc:0.67018\ttest-error:0.33013\n",
      "[151]\ttrain-logloss:11.29523\ttrain-auc:0.69332\ttrain-error:0.30659\ttest-logloss:12.16897\ttest-auc:0.67000\ttest-error:0.33031\n",
      "[152]\ttrain-logloss:11.28880\ttrain-auc:0.69350\ttrain-error:0.30642\ttest-logloss:12.16897\ttest-auc:0.67000\ttest-error:0.33031\n",
      "[153]\ttrain-logloss:11.28719\ttrain-auc:0.69354\ttrain-error:0.30637\ttest-logloss:12.15611\ttest-auc:0.67035\ttest-error:0.32996\n",
      "[154]\ttrain-logloss:11.26789\ttrain-auc:0.69406\ttrain-error:0.30585\ttest-logloss:12.18826\ttest-auc:0.66947\ttest-error:0.33083\n",
      "[155]\ttrain-logloss:11.26468\ttrain-auc:0.69415\ttrain-error:0.30576\ttest-logloss:12.17540\ttest-auc:0.66982\ttest-error:0.33048\n",
      "[156]\ttrain-logloss:11.25664\ttrain-auc:0.69437\ttrain-error:0.30554\ttest-logloss:12.14967\ttest-auc:0.67051\ttest-error:0.32978\n",
      "[157]\ttrain-logloss:11.24538\ttrain-auc:0.69468\ttrain-error:0.30524\ttest-logloss:12.13038\ttest-auc:0.67104\ttest-error:0.32926\n",
      "[158]\ttrain-logloss:11.24860\ttrain-auc:0.69459\ttrain-error:0.30533\ttest-logloss:12.16897\ttest-auc:0.66999\ttest-error:0.33031\n",
      "[159]\ttrain-logloss:11.24538\ttrain-auc:0.69468\ttrain-error:0.30524\ttest-logloss:12.14967\ttest-auc:0.67051\ttest-error:0.32978\n",
      "[160]\ttrain-logloss:11.23895\ttrain-auc:0.69485\ttrain-error:0.30506\ttest-logloss:12.16897\ttest-auc:0.66998\ttest-error:0.33031\n",
      "[161]\ttrain-logloss:11.21804\ttrain-auc:0.69542\ttrain-error:0.30450\ttest-logloss:12.13681\ttest-auc:0.67085\ttest-error:0.32943\n",
      "[162]\ttrain-logloss:11.21965\ttrain-auc:0.69537\ttrain-error:0.30454\ttest-logloss:12.14967\ttest-auc:0.67050\ttest-error:0.32978\n",
      "[163]\ttrain-logloss:11.21322\ttrain-auc:0.69555\ttrain-error:0.30436\ttest-logloss:12.17540\ttest-auc:0.66980\ttest-error:0.33048\n",
      "[164]\ttrain-logloss:11.19714\ttrain-auc:0.69599\ttrain-error:0.30393\ttest-logloss:12.16897\ttest-auc:0.66997\ttest-error:0.33031\n",
      "[165]\ttrain-logloss:11.16498\ttrain-auc:0.69686\ttrain-error:0.30306\ttest-logloss:12.16254\ttest-auc:0.67015\ttest-error:0.33013\n",
      "[166]\ttrain-logloss:11.14407\ttrain-auc:0.69743\ttrain-error:0.30249\ttest-logloss:12.18183\ttest-auc:0.66963\ttest-error:0.33066\n",
      "[167]\ttrain-logloss:11.15372\ttrain-auc:0.69717\ttrain-error:0.30275\ttest-logloss:12.17540\ttest-auc:0.66980\ttest-error:0.33048\n",
      "[168]\ttrain-logloss:11.15050\ttrain-auc:0.69725\ttrain-error:0.30266\ttest-logloss:12.20113\ttest-auc:0.66910\ttest-error:0.33118\n",
      "[169]\ttrain-logloss:11.13764\ttrain-auc:0.69761\ttrain-error:0.30231\ttest-logloss:12.18183\ttest-auc:0.66962\ttest-error:0.33066\n",
      "[170]\ttrain-logloss:11.11673\ttrain-auc:0.69817\ttrain-error:0.30175\ttest-logloss:12.16897\ttest-auc:0.66997\ttest-error:0.33031\n",
      "[171]\ttrain-logloss:11.09744\ttrain-auc:0.69870\ttrain-error:0.30122\ttest-logloss:12.14967\ttest-auc:0.67049\ttest-error:0.32978\n",
      "[172]\ttrain-logloss:11.09261\ttrain-auc:0.69883\ttrain-error:0.30109\ttest-logloss:12.13681\ttest-auc:0.67084\ttest-error:0.32943\n",
      "[173]\ttrain-logloss:11.08779\ttrain-auc:0.69896\ttrain-error:0.30096\ttest-logloss:12.16897\ttest-auc:0.66996\ttest-error:0.33031\n",
      "[174]\ttrain-logloss:11.09744\ttrain-auc:0.69870\ttrain-error:0.30122\ttest-logloss:12.12395\ttest-auc:0.67119\ttest-error:0.32909\n",
      "[175]\ttrain-logloss:11.08618\ttrain-auc:0.69900\ttrain-error:0.30092\ttest-logloss:12.16897\ttest-auc:0.66996\ttest-error:0.33031\n",
      "[176]\ttrain-logloss:11.07492\ttrain-auc:0.69931\ttrain-error:0.30061\ttest-logloss:12.14967\ttest-auc:0.67048\ttest-error:0.32978\n",
      "[177]\ttrain-logloss:11.07171\ttrain-auc:0.69940\ttrain-error:0.30052\ttest-logloss:12.14967\ttest-auc:0.67048\ttest-error:0.32978\n",
      "[178]\ttrain-logloss:11.07010\ttrain-auc:0.69944\ttrain-error:0.30048\ttest-logloss:12.13681\ttest-auc:0.67083\ttest-error:0.32943\n",
      "[179]\ttrain-logloss:11.05723\ttrain-auc:0.69979\ttrain-error:0.30013\ttest-logloss:12.16254\ttest-auc:0.67013\ttest-error:0.33013\n",
      "[180]\ttrain-logloss:11.06527\ttrain-auc:0.69957\ttrain-error:0.30035\ttest-logloss:12.16254\ttest-auc:0.67013\ttest-error:0.33013\n",
      "[181]\ttrain-logloss:11.05080\ttrain-auc:0.69997\ttrain-error:0.29996\ttest-logloss:12.16897\ttest-auc:0.66995\ttest-error:0.33031\n",
      "[182]\ttrain-logloss:11.04598\ttrain-auc:0.70010\ttrain-error:0.29983\ttest-logloss:12.14967\ttest-auc:0.67048\ttest-error:0.32978\n",
      "[183]\ttrain-logloss:11.02990\ttrain-auc:0.70053\ttrain-error:0.29939\ttest-logloss:12.14967\ttest-auc:0.67047\ttest-error:0.32978\n",
      "[184]\ttrain-logloss:11.02185\ttrain-auc:0.70075\ttrain-error:0.29917\ttest-logloss:12.16897\ttest-auc:0.66994\ttest-error:0.33031\n",
      "[185]\ttrain-logloss:11.01864\ttrain-auc:0.70084\ttrain-error:0.29908\ttest-logloss:12.14967\ttest-auc:0.67047\ttest-error:0.32978\n",
      "[186]\ttrain-logloss:11.01703\ttrain-auc:0.70088\ttrain-error:0.29904\ttest-logloss:12.13681\ttest-auc:0.67081\ttest-error:0.32943\n",
      "[187]\ttrain-logloss:10.98969\ttrain-auc:0.70163\ttrain-error:0.29830\ttest-logloss:12.12395\ttest-auc:0.67116\ttest-error:0.32909\n",
      "[188]\ttrain-logloss:10.97683\ttrain-auc:0.70198\ttrain-error:0.29795\ttest-logloss:12.11108\ttest-auc:0.67151\ttest-error:0.32874\n",
      "[189]\ttrain-logloss:10.96075\ttrain-auc:0.70241\ttrain-error:0.29751\ttest-logloss:12.10465\ttest-auc:0.67168\ttest-error:0.32856\n",
      "[190]\ttrain-logloss:10.96396\ttrain-auc:0.70232\ttrain-error:0.29760\ttest-logloss:12.11108\ttest-auc:0.67151\ttest-error:0.32874\n",
      "[191]\ttrain-logloss:10.96075\ttrain-auc:0.70241\ttrain-error:0.29751\ttest-logloss:12.12395\ttest-auc:0.67116\ttest-error:0.32909\n",
      "[192]\ttrain-logloss:10.95753\ttrain-auc:0.70250\ttrain-error:0.29742\ttest-logloss:12.14967\ttest-auc:0.67046\ttest-error:0.32978\n",
      "[193]\ttrain-logloss:10.94306\ttrain-auc:0.70289\ttrain-error:0.29703\ttest-logloss:12.12395\ttest-auc:0.67115\ttest-error:0.32909\n",
      "[194]\ttrain-logloss:10.92215\ttrain-auc:0.70346\ttrain-error:0.29646\ttest-logloss:12.14967\ttest-auc:0.67045\ttest-error:0.32978\n",
      "[195]\ttrain-logloss:10.91894\ttrain-auc:0.70355\ttrain-error:0.29638\ttest-logloss:12.13038\ttest-auc:0.67098\ttest-error:0.32926\n",
      "[196]\ttrain-logloss:10.91250\ttrain-auc:0.70372\ttrain-error:0.29620\ttest-logloss:12.13038\ttest-auc:0.67098\ttest-error:0.32926\n",
      "[197]\ttrain-logloss:10.89482\ttrain-auc:0.70420\ttrain-error:0.29572\ttest-logloss:12.14967\ttest-auc:0.67045\ttest-error:0.32978\n",
      "[198]\ttrain-logloss:10.88678\ttrain-auc:0.70442\ttrain-error:0.29550\ttest-logloss:12.14967\ttest-auc:0.67045\ttest-error:0.32978\n",
      "[199]\ttrain-logloss:10.89642\ttrain-auc:0.70416\ttrain-error:0.29577\ttest-logloss:12.13038\ttest-auc:0.67098\ttest-error:0.32926\n",
      "[200]\ttrain-logloss:10.89482\ttrain-auc:0.70420\ttrain-error:0.29572\ttest-logloss:12.12395\ttest-auc:0.67115\ttest-error:0.32909\n",
      "[201]\ttrain-logloss:10.88356\ttrain-auc:0.70451\ttrain-error:0.29542\ttest-logloss:12.11751\ttest-auc:0.67132\ttest-error:0.32891\n",
      "[202]\ttrain-logloss:10.84336\ttrain-auc:0.70560\ttrain-error:0.29433\ttest-logloss:12.05320\ttest-auc:0.67306\ttest-error:0.32716\n",
      "[203]\ttrain-logloss:10.82728\ttrain-auc:0.70604\ttrain-error:0.29389\ttest-logloss:12.04676\ttest-auc:0.67323\ttest-error:0.32699\n",
      "[204]\ttrain-logloss:10.82084\ttrain-auc:0.70621\ttrain-error:0.29372\ttest-logloss:12.05320\ttest-auc:0.67306\ttest-error:0.32716\n",
      "[205]\ttrain-logloss:10.81763\ttrain-auc:0.70630\ttrain-error:0.29363\ttest-logloss:12.05963\ttest-auc:0.67288\ttest-error:0.32734\n",
      "[206]\ttrain-logloss:10.81119\ttrain-auc:0.70647\ttrain-error:0.29345\ttest-logloss:12.06606\ttest-auc:0.67271\ttest-error:0.32751\n",
      "[207]\ttrain-logloss:10.80476\ttrain-auc:0.70665\ttrain-error:0.29328\ttest-logloss:12.05320\ttest-auc:0.67306\ttest-error:0.32716\n",
      "[208]\ttrain-logloss:10.79994\ttrain-auc:0.70678\ttrain-error:0.29315\ttest-logloss:12.04033\ttest-auc:0.67341\ttest-error:0.32682\n",
      "[209]\ttrain-logloss:10.81119\ttrain-auc:0.70648\ttrain-error:0.29345\ttest-logloss:12.04033\ttest-auc:0.67341\ttest-error:0.32682\n",
      "[210]\ttrain-logloss:10.80476\ttrain-auc:0.70665\ttrain-error:0.29328\ttest-logloss:12.05963\ttest-auc:0.67289\ttest-error:0.32734\n",
      "[211]\ttrain-logloss:10.78386\ttrain-auc:0.70722\ttrain-error:0.29271\ttest-logloss:12.07892\ttest-auc:0.67236\ttest-error:0.32786\n",
      "[212]\ttrain-logloss:10.77260\ttrain-auc:0.70752\ttrain-error:0.29241\ttest-logloss:12.09179\ttest-auc:0.67201\ttest-error:0.32821\n",
      "[213]\ttrain-logloss:10.75330\ttrain-auc:0.70805\ttrain-error:0.29188\ttest-logloss:12.07249\ttest-auc:0.67254\ttest-error:0.32769\n",
      "[214]\ttrain-logloss:10.74526\ttrain-auc:0.70827\ttrain-error:0.29166\ttest-logloss:12.09179\ttest-auc:0.67201\ttest-error:0.32821\n",
      "[215]\ttrain-logloss:10.72757\ttrain-auc:0.70875\ttrain-error:0.29118\ttest-logloss:12.08536\ttest-auc:0.67218\ttest-error:0.32804\n",
      "[216]\ttrain-logloss:10.71792\ttrain-auc:0.70901\ttrain-error:0.29092\ttest-logloss:12.05963\ttest-auc:0.67288\ttest-error:0.32734\n",
      "[217]\ttrain-logloss:10.72436\ttrain-auc:0.70883\ttrain-error:0.29110\ttest-logloss:12.05320\ttest-auc:0.67306\ttest-error:0.32716\n",
      "[218]\ttrain-logloss:10.72436\ttrain-auc:0.70883\ttrain-error:0.29110\ttest-logloss:12.05963\ttest-auc:0.67288\ttest-error:0.32734\n",
      "[219]\ttrain-logloss:10.70024\ttrain-auc:0.70949\ttrain-error:0.29044\ttest-logloss:12.02747\ttest-auc:0.67376\ttest-error:0.32647\n",
      "[220]\ttrain-logloss:10.68737\ttrain-auc:0.70984\ttrain-error:0.29009\ttest-logloss:12.02747\ttest-auc:0.67376\ttest-error:0.32647\n",
      "[221]\ttrain-logloss:10.67933\ttrain-auc:0.71005\ttrain-error:0.28987\ttest-logloss:12.02747\ttest-auc:0.67376\ttest-error:0.32647\n",
      "[222]\ttrain-logloss:10.67772\ttrain-auc:0.71010\ttrain-error:0.28983\ttest-logloss:12.02747\ttest-auc:0.67376\ttest-error:0.32647\n",
      "[223]\ttrain-logloss:10.68094\ttrain-auc:0.71001\ttrain-error:0.28992\ttest-logloss:12.02747\ttest-auc:0.67376\ttest-error:0.32647\n",
      "[224]\ttrain-logloss:10.66486\ttrain-auc:0.71045\ttrain-error:0.28948\ttest-logloss:12.05963\ttest-auc:0.67289\ttest-error:0.32734\n",
      "[225]\ttrain-logloss:10.64235\ttrain-auc:0.71106\ttrain-error:0.28887\ttest-logloss:12.07249\ttest-auc:0.67254\ttest-error:0.32769\n",
      "[226]\ttrain-logloss:10.63109\ttrain-auc:0.71137\ttrain-error:0.28856\ttest-logloss:12.08536\ttest-auc:0.67219\ttest-error:0.32804\n",
      "[227]\ttrain-logloss:10.60697\ttrain-auc:0.71202\ttrain-error:0.28791\ttest-logloss:12.11108\ttest-auc:0.67148\ttest-error:0.32874\n",
      "[228]\ttrain-logloss:10.60214\ttrain-auc:0.71215\ttrain-error:0.28778\ttest-logloss:12.09822\ttest-auc:0.67183\ttest-error:0.32839\n",
      "[229]\ttrain-logloss:10.60214\ttrain-auc:0.71215\ttrain-error:0.28778\ttest-logloss:12.07249\ttest-auc:0.67254\ttest-error:0.32769\n",
      "[230]\ttrain-logloss:10.57963\ttrain-auc:0.71276\ttrain-error:0.28717\ttest-logloss:12.09822\ttest-auc:0.67184\ttest-error:0.32839\n",
      "[231]\ttrain-logloss:10.56194\ttrain-auc:0.71324\ttrain-error:0.28669\ttest-logloss:12.07892\ttest-auc:0.67236\ttest-error:0.32786\n",
      "[232]\ttrain-logloss:10.55229\ttrain-auc:0.71350\ttrain-error:0.28642\ttest-logloss:12.10465\ttest-auc:0.67167\ttest-error:0.32856\n",
      "[233]\ttrain-logloss:10.53782\ttrain-auc:0.71390\ttrain-error:0.28603\ttest-logloss:12.07892\ttest-auc:0.67237\ttest-error:0.32786\n",
      "[234]\ttrain-logloss:10.51852\ttrain-auc:0.71442\ttrain-error:0.28551\ttest-logloss:12.08536\ttest-auc:0.67219\ttest-error:0.32804\n",
      "[235]\ttrain-logloss:10.51209\ttrain-auc:0.71460\ttrain-error:0.28533\ttest-logloss:12.09179\ttest-auc:0.67202\ttest-error:0.32821\n",
      "[236]\ttrain-logloss:10.49601\ttrain-auc:0.71503\ttrain-error:0.28490\ttest-logloss:12.14324\ttest-auc:0.67062\ttest-error:0.32961\n",
      "[237]\ttrain-logloss:10.49601\ttrain-auc:0.71503\ttrain-error:0.28490\ttest-logloss:12.12395\ttest-auc:0.67113\ttest-error:0.32909\n",
      "[238]\ttrain-logloss:10.49440\ttrain-auc:0.71508\ttrain-error:0.28485\ttest-logloss:12.12395\ttest-auc:0.67113\ttest-error:0.32909\n",
      "[239]\ttrain-logloss:10.50083\ttrain-auc:0.71490\ttrain-error:0.28503\ttest-logloss:12.13681\ttest-auc:0.67078\ttest-error:0.32943\n",
      "[240]\ttrain-logloss:10.48797\ttrain-auc:0.71525\ttrain-error:0.28468\ttest-logloss:12.12395\ttest-auc:0.67113\ttest-error:0.32909\n",
      "[241]\ttrain-logloss:10.47993\ttrain-auc:0.71547\ttrain-error:0.28446\ttest-logloss:12.13038\ttest-auc:0.67095\ttest-error:0.32926\n",
      "[242]\ttrain-logloss:10.46867\ttrain-auc:0.71578\ttrain-error:0.28415\ttest-logloss:12.13038\ttest-auc:0.67096\ttest-error:0.32926\n",
      "[243]\ttrain-logloss:10.45420\ttrain-auc:0.71617\ttrain-error:0.28376\ttest-logloss:12.14967\ttest-auc:0.67043\ttest-error:0.32978\n",
      "[244]\ttrain-logloss:10.42043\ttrain-auc:0.71708\ttrain-error:0.28285\ttest-logloss:12.14324\ttest-auc:0.67061\ttest-error:0.32961\n",
      "[245]\ttrain-logloss:10.39631\ttrain-auc:0.71774\ttrain-error:0.28219\ttest-logloss:12.18826\ttest-auc:0.66938\ttest-error:0.33083\n",
      "[246]\ttrain-logloss:10.38505\ttrain-auc:0.71805\ttrain-error:0.28189\ttest-logloss:12.18826\ttest-auc:0.66939\ttest-error:0.33083\n",
      "[247]\ttrain-logloss:10.34807\ttrain-auc:0.71905\ttrain-error:0.28088\ttest-logloss:12.19470\ttest-auc:0.66921\ttest-error:0.33101\n",
      "[248]\ttrain-logloss:10.33520\ttrain-auc:0.71940\ttrain-error:0.28053\ttest-logloss:12.18183\ttest-auc:0.66955\ttest-error:0.33066\n",
      "[249]\ttrain-logloss:10.34002\ttrain-auc:0.71927\ttrain-error:0.28066\ttest-logloss:12.20756\ttest-auc:0.66885\ttest-error:0.33136\n",
      "[250]\ttrain-logloss:10.32394\ttrain-auc:0.71971\ttrain-error:0.28023\ttest-logloss:12.20756\ttest-auc:0.66885\ttest-error:0.33136\n",
      "[251]\ttrain-logloss:10.32716\ttrain-auc:0.71962\ttrain-error:0.28031\ttest-logloss:12.21399\ttest-auc:0.66868\ttest-error:0.33153\n",
      "[252]\ttrain-logloss:10.32555\ttrain-auc:0.71966\ttrain-error:0.28027\ttest-logloss:12.20113\ttest-auc:0.66903\ttest-error:0.33118\n",
      "[253]\ttrain-logloss:10.31912\ttrain-auc:0.71984\ttrain-error:0.28010\ttest-logloss:12.22686\ttest-auc:0.66833\ttest-error:0.33188\n",
      "[254]\ttrain-logloss:10.30786\ttrain-auc:0.72014\ttrain-error:0.27979\ttest-logloss:12.22042\ttest-auc:0.66851\ttest-error:0.33170\n",
      "[255]\ttrain-logloss:10.28535\ttrain-auc:0.72075\ttrain-error:0.27918\ttest-logloss:12.20113\ttest-auc:0.66903\ttest-error:0.33118\n",
      "[256]\ttrain-logloss:10.28374\ttrain-auc:0.72080\ttrain-error:0.27914\ttest-logloss:12.20113\ttest-auc:0.66903\ttest-error:0.33118\n",
      "[257]\ttrain-logloss:10.27892\ttrain-auc:0.72093\ttrain-error:0.27901\ttest-logloss:12.17540\ttest-auc:0.66972\ttest-error:0.33048\n",
      "[258]\ttrain-logloss:10.26605\ttrain-auc:0.72128\ttrain-error:0.27866\ttest-logloss:12.16897\ttest-auc:0.66990\ttest-error:0.33031\n",
      "[259]\ttrain-logloss:10.25962\ttrain-auc:0.72145\ttrain-error:0.27848\ttest-logloss:12.18183\ttest-auc:0.66954\ttest-error:0.33066\n",
      "[260]\ttrain-logloss:10.25962\ttrain-auc:0.72145\ttrain-error:0.27848\ttest-logloss:12.16897\ttest-auc:0.66989\ttest-error:0.33031\n",
      "[261]\ttrain-logloss:10.24997\ttrain-auc:0.72171\ttrain-error:0.27822\ttest-logloss:12.14967\ttest-auc:0.67042\ttest-error:0.32978\n",
      "[262]\ttrain-logloss:10.23871\ttrain-auc:0.72202\ttrain-error:0.27791\ttest-logloss:12.18183\ttest-auc:0.66955\ttest-error:0.33066\n",
      "[263]\ttrain-logloss:10.24997\ttrain-auc:0.72171\ttrain-error:0.27822\ttest-logloss:12.18826\ttest-auc:0.66937\ttest-error:0.33083\n",
      "[264]\ttrain-logloss:10.23871\ttrain-auc:0.72202\ttrain-error:0.27791\ttest-logloss:12.14324\ttest-auc:0.67060\ttest-error:0.32961\n",
      "[265]\ttrain-logloss:10.23389\ttrain-auc:0.72215\ttrain-error:0.27778\ttest-logloss:12.15611\ttest-auc:0.67025\ttest-error:0.32996\n",
      "[266]\ttrain-logloss:10.21781\ttrain-auc:0.72259\ttrain-error:0.27735\ttest-logloss:12.16254\ttest-auc:0.67007\ttest-error:0.33013\n",
      "[267]\ttrain-logloss:10.19690\ttrain-auc:0.72315\ttrain-error:0.27678\ttest-logloss:12.17540\ttest-auc:0.66973\ttest-error:0.33048\n",
      "[268]\ttrain-logloss:10.20012\ttrain-auc:0.72307\ttrain-error:0.27687\ttest-logloss:12.19470\ttest-auc:0.66920\ttest-error:0.33101\n",
      "[269]\ttrain-logloss:10.19369\ttrain-auc:0.72324\ttrain-error:0.27669\ttest-logloss:12.21399\ttest-auc:0.66868\ttest-error:0.33153\n",
      "[270]\ttrain-logloss:10.18243\ttrain-auc:0.72355\ttrain-error:0.27639\ttest-logloss:12.20756\ttest-auc:0.66885\ttest-error:0.33136\n",
      "[271]\ttrain-logloss:10.16314\ttrain-auc:0.72407\ttrain-error:0.27586\ttest-logloss:12.20113\ttest-auc:0.66902\ttest-error:0.33118\n",
      "[272]\ttrain-logloss:10.15670\ttrain-auc:0.72425\ttrain-error:0.27569\ttest-logloss:12.18183\ttest-auc:0.66954\ttest-error:0.33066\n",
      "[273]\ttrain-logloss:10.14705\ttrain-auc:0.72451\ttrain-error:0.27543\ttest-logloss:12.18826\ttest-auc:0.66937\ttest-error:0.33083\n",
      "[274]\ttrain-logloss:10.13258\ttrain-auc:0.72490\ttrain-error:0.27503\ttest-logloss:12.20113\ttest-auc:0.66902\ttest-error:0.33118\n",
      "[275]\ttrain-logloss:10.12936\ttrain-auc:0.72499\ttrain-error:0.27494\ttest-logloss:12.22686\ttest-auc:0.66833\ttest-error:0.33188\n",
      "[276]\ttrain-logloss:10.10685\ttrain-auc:0.72560\ttrain-error:0.27433\ttest-logloss:12.22042\ttest-auc:0.66850\ttest-error:0.33170\n",
      "[277]\ttrain-logloss:10.09720\ttrain-auc:0.72586\ttrain-error:0.27407\ttest-logloss:12.23972\ttest-auc:0.66798\ttest-error:0.33223\n",
      "[278]\ttrain-logloss:10.09077\ttrain-auc:0.72604\ttrain-error:0.27390\ttest-logloss:12.20113\ttest-auc:0.66902\ttest-error:0.33118\n",
      "[279]\ttrain-logloss:10.08273\ttrain-auc:0.72625\ttrain-error:0.27368\ttest-logloss:12.20756\ttest-auc:0.66885\ttest-error:0.33136\n",
      "[280]\ttrain-logloss:10.07147\ttrain-auc:0.72656\ttrain-error:0.27337\ttest-logloss:12.20756\ttest-auc:0.66885\ttest-error:0.33136\n",
      "[281]\ttrain-logloss:10.06987\ttrain-auc:0.72660\ttrain-error:0.27333\ttest-logloss:12.23329\ttest-auc:0.66815\ttest-error:0.33205\n",
      "[282]\ttrain-logloss:10.04092\ttrain-auc:0.72739\ttrain-error:0.27254\ttest-logloss:12.22686\ttest-auc:0.66832\ttest-error:0.33188\n",
      "[283]\ttrain-logloss:10.04574\ttrain-auc:0.72726\ttrain-error:0.27268\ttest-logloss:12.18826\ttest-auc:0.66937\ttest-error:0.33083\n",
      "[284]\ttrain-logloss:10.02806\ttrain-auc:0.72774\ttrain-error:0.27220\ttest-logloss:12.18826\ttest-auc:0.66937\ttest-error:0.33083\n",
      "[285]\ttrain-logloss:10.02002\ttrain-auc:0.72796\ttrain-error:0.27198\ttest-logloss:12.17540\ttest-auc:0.66972\ttest-error:0.33048\n",
      "[286]\ttrain-logloss:10.00233\ttrain-auc:0.72844\ttrain-error:0.27150\ttest-logloss:12.16897\ttest-auc:0.66989\ttest-error:0.33031\n",
      "[287]\ttrain-logloss:9.99589\ttrain-auc:0.72861\ttrain-error:0.27132\ttest-logloss:12.16254\ttest-auc:0.67007\ttest-error:0.33013\n",
      "[288]\ttrain-logloss:9.97660\ttrain-auc:0.72914\ttrain-error:0.27080\ttest-logloss:12.18826\ttest-auc:0.66936\ttest-error:0.33083\n",
      "[289]\ttrain-logloss:9.97338\ttrain-auc:0.72922\ttrain-error:0.27071\ttest-logloss:12.16254\ttest-auc:0.67005\ttest-error:0.33013\n",
      "[290]\ttrain-logloss:9.95408\ttrain-auc:0.72975\ttrain-error:0.27019\ttest-logloss:12.20113\ttest-auc:0.66901\ttest-error:0.33118\n",
      "[291]\ttrain-logloss:9.92675\ttrain-auc:0.73049\ttrain-error:0.26945\ttest-logloss:12.18826\ttest-auc:0.66936\ttest-error:0.33083\n",
      "[292]\ttrain-logloss:9.94765\ttrain-auc:0.72992\ttrain-error:0.27001\ttest-logloss:12.17540\ttest-auc:0.66971\ttest-error:0.33048\n",
      "[293]\ttrain-logloss:9.94283\ttrain-auc:0.73005\ttrain-error:0.26988\ttest-logloss:12.18826\ttest-auc:0.66936\ttest-error:0.33083\n",
      "[294]\ttrain-logloss:9.93961\ttrain-auc:0.73014\ttrain-error:0.26980\ttest-logloss:12.15611\ttest-auc:0.67023\ttest-error:0.32996\n",
      "[295]\ttrain-logloss:9.94122\ttrain-auc:0.73010\ttrain-error:0.26984\ttest-logloss:12.17540\ttest-auc:0.66971\ttest-error:0.33048\n",
      "[296]\ttrain-logloss:9.91710\ttrain-auc:0.73075\ttrain-error:0.26918\ttest-logloss:12.17540\ttest-auc:0.66971\ttest-error:0.33048\n",
      "[297]\ttrain-logloss:9.90745\ttrain-auc:0.73101\ttrain-error:0.26892\ttest-logloss:12.20113\ttest-auc:0.66901\ttest-error:0.33118\n",
      "[298]\ttrain-logloss:9.90584\ttrain-auc:0.73106\ttrain-error:0.26888\ttest-logloss:12.18826\ttest-auc:0.66936\ttest-error:0.33083\n",
      "[299]\ttrain-logloss:9.91388\ttrain-auc:0.73084\ttrain-error:0.26910\ttest-logloss:12.18826\ttest-auc:0.66937\ttest-error:0.33083\n",
      "[300]\ttrain-logloss:9.89780\ttrain-auc:0.73128\ttrain-error:0.26866\ttest-logloss:12.19470\ttest-auc:0.66919\ttest-error:0.33101\n",
      "[301]\ttrain-logloss:9.90101\ttrain-auc:0.73119\ttrain-error:0.26875\ttest-logloss:12.18183\ttest-auc:0.66954\ttest-error:0.33066\n",
      "[302]\ttrain-logloss:9.88976\ttrain-auc:0.73149\ttrain-error:0.26844\ttest-logloss:12.18183\ttest-auc:0.66954\ttest-error:0.33066\n",
      "[303]\ttrain-logloss:9.88494\ttrain-auc:0.73162\ttrain-error:0.26831\ttest-logloss:12.21399\ttest-auc:0.66867\ttest-error:0.33153\n",
      "[304]\ttrain-logloss:9.87046\ttrain-auc:0.73202\ttrain-error:0.26792\ttest-logloss:12.17540\ttest-auc:0.66971\ttest-error:0.33048\n",
      "[305]\ttrain-logloss:9.86242\ttrain-auc:0.73224\ttrain-error:0.26770\ttest-logloss:12.18826\ttest-auc:0.66937\ttest-error:0.33083\n",
      "[306]\ttrain-logloss:9.84152\ttrain-auc:0.73280\ttrain-error:0.26713\ttest-logloss:12.20756\ttest-auc:0.66885\ttest-error:0.33136\n",
      "[307]\ttrain-logloss:9.82383\ttrain-auc:0.73328\ttrain-error:0.26665\ttest-logloss:12.17540\ttest-auc:0.66971\ttest-error:0.33048\n",
      "[308]\ttrain-logloss:9.82383\ttrain-auc:0.73328\ttrain-error:0.26665\ttest-logloss:12.18826\ttest-auc:0.66937\ttest-error:0.33083\n",
      "[309]\ttrain-logloss:9.82544\ttrain-auc:0.73324\ttrain-error:0.26670\ttest-logloss:12.15611\ttest-auc:0.67024\ttest-error:0.32996\n",
      "[310]\ttrain-logloss:9.82383\ttrain-auc:0.73328\ttrain-error:0.26665\ttest-logloss:12.16897\ttest-auc:0.66989\ttest-error:0.33031\n",
      "[311]\ttrain-logloss:9.80292\ttrain-auc:0.73385\ttrain-error:0.26609\ttest-logloss:12.14967\ttest-auc:0.67041\ttest-error:0.32978\n",
      "[312]\ttrain-logloss:9.79167\ttrain-auc:0.73416\ttrain-error:0.26578\ttest-logloss:12.11751\ttest-auc:0.67129\ttest-error:0.32891\n",
      "[313]\ttrain-logloss:9.76915\ttrain-auc:0.73477\ttrain-error:0.26517\ttest-logloss:12.12395\ttest-auc:0.67111\ttest-error:0.32909\n",
      "[314]\ttrain-logloss:9.75629\ttrain-auc:0.73512\ttrain-error:0.26482\ttest-logloss:12.15611\ttest-auc:0.67023\ttest-error:0.32996\n",
      "[315]\ttrain-logloss:9.75629\ttrain-auc:0.73512\ttrain-error:0.26482\ttest-logloss:12.14967\ttest-auc:0.67041\ttest-error:0.32978\n",
      "[316]\ttrain-logloss:9.75146\ttrain-auc:0.73525\ttrain-error:0.26469\ttest-logloss:12.14967\ttest-auc:0.67041\ttest-error:0.32978\n",
      "[317]\ttrain-logloss:9.75789\ttrain-auc:0.73507\ttrain-error:0.26486\ttest-logloss:12.16254\ttest-auc:0.67006\ttest-error:0.33013\n",
      "[318]\ttrain-logloss:9.76272\ttrain-auc:0.73494\ttrain-error:0.26499\ttest-logloss:12.16897\ttest-auc:0.66988\ttest-error:0.33031\n",
      "[319]\ttrain-logloss:9.73538\ttrain-auc:0.73568\ttrain-error:0.26425\ttest-logloss:12.18826\ttest-auc:0.66936\ttest-error:0.33083\n",
      "[320]\ttrain-logloss:9.73217\ttrain-auc:0.73577\ttrain-error:0.26416\ttest-logloss:12.18183\ttest-auc:0.66954\ttest-error:0.33066\n",
      "[321]\ttrain-logloss:9.72091\ttrain-auc:0.73608\ttrain-error:0.26386\ttest-logloss:12.20756\ttest-auc:0.66884\ttest-error:0.33136\n",
      "[322]\ttrain-logloss:9.73538\ttrain-auc:0.73568\ttrain-error:0.26425\ttest-logloss:12.16897\ttest-auc:0.66989\ttest-error:0.33031\n",
      "[323]\ttrain-logloss:9.73377\ttrain-auc:0.73573\ttrain-error:0.26421\ttest-logloss:12.18826\ttest-auc:0.66936\ttest-error:0.33083\n",
      "[324]\ttrain-logloss:9.71769\ttrain-auc:0.73616\ttrain-error:0.26377\ttest-logloss:12.18826\ttest-auc:0.66936\ttest-error:0.33083\n",
      "[325]\ttrain-logloss:9.71126\ttrain-auc:0.73634\ttrain-error:0.26360\ttest-logloss:12.20756\ttest-auc:0.66884\ttest-error:0.33136\n",
      "[326]\ttrain-logloss:9.71769\ttrain-auc:0.73616\ttrain-error:0.26377\ttest-logloss:12.22686\ttest-auc:0.66832\ttest-error:0.33188\n",
      "[327]\ttrain-logloss:9.70804\ttrain-auc:0.73643\ttrain-error:0.26351\ttest-logloss:12.19470\ttest-auc:0.66919\ttest-error:0.33101\n",
      "[328]\ttrain-logloss:9.70161\ttrain-auc:0.73660\ttrain-error:0.26333\ttest-logloss:12.19470\ttest-auc:0.66919\ttest-error:0.33101\n",
      "[329]\ttrain-logloss:9.68875\ttrain-auc:0.73695\ttrain-error:0.26299\ttest-logloss:12.19470\ttest-auc:0.66919\ttest-error:0.33101\n",
      "[330]\ttrain-logloss:9.66302\ttrain-auc:0.73765\ttrain-error:0.26229\ttest-logloss:12.18826\ttest-auc:0.66936\ttest-error:0.33083\n",
      "[331]\ttrain-logloss:9.66302\ttrain-auc:0.73765\ttrain-error:0.26229\ttest-logloss:12.18826\ttest-auc:0.66936\ttest-error:0.33083\n",
      "[332]\ttrain-logloss:9.65498\ttrain-auc:0.73787\ttrain-error:0.26207\ttest-logloss:12.18826\ttest-auc:0.66936\ttest-error:0.33083\n",
      "[333]\ttrain-logloss:9.64372\ttrain-auc:0.73817\ttrain-error:0.26176\ttest-logloss:12.21399\ttest-auc:0.66866\ttest-error:0.33153\n",
      "[334]\ttrain-logloss:9.63086\ttrain-auc:0.73852\ttrain-error:0.26141\ttest-logloss:12.20113\ttest-auc:0.66901\ttest-error:0.33118\n",
      "[335]\ttrain-logloss:9.62925\ttrain-auc:0.73857\ttrain-error:0.26137\ttest-logloss:12.20113\ttest-auc:0.66901\ttest-error:0.33118\n",
      "[336]\ttrain-logloss:9.63407\ttrain-auc:0.73843\ttrain-error:0.26150\ttest-logloss:12.19470\ttest-auc:0.66919\ttest-error:0.33101\n",
      "[337]\ttrain-logloss:9.63086\ttrain-auc:0.73852\ttrain-error:0.26141\ttest-logloss:12.18183\ttest-auc:0.66953\ttest-error:0.33066\n",
      "[338]\ttrain-logloss:9.63568\ttrain-auc:0.73839\ttrain-error:0.26155\ttest-logloss:12.16254\ttest-auc:0.67005\ttest-error:0.33013\n",
      "[339]\ttrain-logloss:9.61638\ttrain-auc:0.73891\ttrain-error:0.26102\ttest-logloss:12.18826\ttest-auc:0.66935\ttest-error:0.33083\n",
      "[340]\ttrain-logloss:9.60834\ttrain-auc:0.73913\ttrain-error:0.26080\ttest-logloss:12.16897\ttest-auc:0.66987\ttest-error:0.33031\n",
      "[341]\ttrain-logloss:9.59387\ttrain-auc:0.73953\ttrain-error:0.26041\ttest-logloss:12.20756\ttest-auc:0.66883\ttest-error:0.33136\n",
      "[342]\ttrain-logloss:9.57940\ttrain-auc:0.73992\ttrain-error:0.26002\ttest-logloss:12.17540\ttest-auc:0.66970\ttest-error:0.33048\n",
      "[343]\ttrain-logloss:9.56171\ttrain-auc:0.74040\ttrain-error:0.25954\ttest-logloss:12.15611\ttest-auc:0.67022\ttest-error:0.32996\n",
      "[344]\ttrain-logloss:9.57296\ttrain-auc:0.74009\ttrain-error:0.25984\ttest-logloss:12.18826\ttest-auc:0.66935\ttest-error:0.33083\n",
      "[345]\ttrain-logloss:9.57618\ttrain-auc:0.74001\ttrain-error:0.25993\ttest-logloss:12.18183\ttest-auc:0.66953\ttest-error:0.33066\n",
      "[346]\ttrain-logloss:9.57618\ttrain-auc:0.74001\ttrain-error:0.25993\ttest-logloss:12.16897\ttest-auc:0.66988\ttest-error:0.33031\n",
      "[347]\ttrain-logloss:9.56653\ttrain-auc:0.74027\ttrain-error:0.25967\ttest-logloss:12.19470\ttest-auc:0.66918\ttest-error:0.33101\n",
      "[348]\ttrain-logloss:9.55849\ttrain-auc:0.74049\ttrain-error:0.25945\ttest-logloss:12.16254\ttest-auc:0.67005\ttest-error:0.33013\n",
      "[349]\ttrain-logloss:9.55206\ttrain-auc:0.74066\ttrain-error:0.25927\ttest-logloss:12.16254\ttest-auc:0.67005\ttest-error:0.33013\n",
      "[350]\ttrain-logloss:9.53276\ttrain-auc:0.74118\ttrain-error:0.25875\ttest-logloss:12.16897\ttest-auc:0.66988\ttest-error:0.33031\n",
      "[351]\ttrain-logloss:9.51990\ttrain-auc:0.74153\ttrain-error:0.25840\ttest-logloss:12.14967\ttest-auc:0.67040\ttest-error:0.32978\n",
      "[352]\ttrain-logloss:9.51990\ttrain-auc:0.74153\ttrain-error:0.25840\ttest-logloss:12.15611\ttest-auc:0.67023\ttest-error:0.32996\n",
      "[353]\ttrain-logloss:9.50703\ttrain-auc:0.74188\ttrain-error:0.25805\ttest-logloss:12.14967\ttest-auc:0.67040\ttest-error:0.32978\n",
      "[354]\ttrain-logloss:9.50060\ttrain-auc:0.74206\ttrain-error:0.25788\ttest-logloss:12.16254\ttest-auc:0.67005\ttest-error:0.33013\n",
      "[355]\ttrain-logloss:9.49738\ttrain-auc:0.74215\ttrain-error:0.25779\ttest-logloss:12.15611\ttest-auc:0.67022\ttest-error:0.32996\n",
      "[356]\ttrain-logloss:9.47648\ttrain-auc:0.74271\ttrain-error:0.25722\ttest-logloss:12.16254\ttest-auc:0.67005\ttest-error:0.33013\n",
      "[357]\ttrain-logloss:9.48774\ttrain-auc:0.74241\ttrain-error:0.25753\ttest-logloss:12.14324\ttest-auc:0.67057\ttest-error:0.32961\n",
      "[358]\ttrain-logloss:9.47326\ttrain-auc:0.74280\ttrain-error:0.25714\ttest-logloss:12.14324\ttest-auc:0.67057\ttest-error:0.32961\n",
      "[359]\ttrain-logloss:9.47326\ttrain-auc:0.74280\ttrain-error:0.25714\ttest-logloss:12.14324\ttest-auc:0.67057\ttest-error:0.32961\n",
      "[360]\ttrain-logloss:9.47487\ttrain-auc:0.74276\ttrain-error:0.25718\ttest-logloss:12.14967\ttest-auc:0.67040\ttest-error:0.32978\n",
      "[361]\ttrain-logloss:9.46683\ttrain-auc:0.74298\ttrain-error:0.25696\ttest-logloss:12.14967\ttest-auc:0.67040\ttest-error:0.32978\n",
      "[362]\ttrain-logloss:9.46040\ttrain-auc:0.74315\ttrain-error:0.25679\ttest-logloss:12.16897\ttest-auc:0.66987\ttest-error:0.33031\n",
      "[363]\ttrain-logloss:9.44753\ttrain-auc:0.74350\ttrain-error:0.25644\ttest-logloss:12.16254\ttest-auc:0.67005\ttest-error:0.33013\n",
      "[364]\ttrain-logloss:9.43628\ttrain-auc:0.74381\ttrain-error:0.25613\ttest-logloss:12.13038\ttest-auc:0.67093\ttest-error:0.32926\n",
      "[365]\ttrain-logloss:9.43145\ttrain-auc:0.74394\ttrain-error:0.25600\ttest-logloss:12.16897\ttest-auc:0.66988\ttest-error:0.33031\n",
      "[366]\ttrain-logloss:9.43306\ttrain-auc:0.74389\ttrain-error:0.25605\ttest-logloss:12.18183\ttest-auc:0.66952\ttest-error:0.33066\n",
      "[367]\ttrain-logloss:9.41859\ttrain-auc:0.74428\ttrain-error:0.25565\ttest-logloss:12.19470\ttest-auc:0.66917\ttest-error:0.33101\n",
      "[368]\ttrain-logloss:9.41376\ttrain-auc:0.74442\ttrain-error:0.25552\ttest-logloss:12.20113\ttest-auc:0.66900\ttest-error:0.33118\n",
      "[369]\ttrain-logloss:9.40572\ttrain-auc:0.74463\ttrain-error:0.25530\ttest-logloss:12.16897\ttest-auc:0.66987\ttest-error:0.33031\n",
      "[370]\ttrain-logloss:9.40090\ttrain-auc:0.74477\ttrain-error:0.25517\ttest-logloss:12.15611\ttest-auc:0.67022\ttest-error:0.32996\n",
      "[371]\ttrain-logloss:9.38321\ttrain-auc:0.74525\ttrain-error:0.25469\ttest-logloss:12.13681\ttest-auc:0.67075\ttest-error:0.32943\n",
      "[372]\ttrain-logloss:9.37356\ttrain-auc:0.74551\ttrain-error:0.25443\ttest-logloss:12.14324\ttest-auc:0.67058\ttest-error:0.32961\n",
      "[373]\ttrain-logloss:9.36552\ttrain-auc:0.74572\ttrain-error:0.25421\ttest-logloss:12.16897\ttest-auc:0.66988\ttest-error:0.33031\n",
      "[374]\ttrain-logloss:9.36391\ttrain-auc:0.74577\ttrain-error:0.25417\ttest-logloss:12.15611\ttest-auc:0.67022\ttest-error:0.32996\n",
      "[375]\ttrain-logloss:9.33979\ttrain-auc:0.74642\ttrain-error:0.25351\ttest-logloss:12.22042\ttest-auc:0.66848\ttest-error:0.33170\n",
      "[376]\ttrain-logloss:9.34140\ttrain-auc:0.74638\ttrain-error:0.25356\ttest-logloss:12.20756\ttest-auc:0.66883\ttest-error:0.33136\n",
      "[377]\ttrain-logloss:9.33658\ttrain-auc:0.74651\ttrain-error:0.25343\ttest-logloss:12.20113\ttest-auc:0.66901\ttest-error:0.33118\n",
      "[378]\ttrain-logloss:9.33175\ttrain-auc:0.74664\ttrain-error:0.25330\ttest-logloss:12.21399\ttest-auc:0.66866\ttest-error:0.33153\n",
      "[379]\ttrain-logloss:9.33497\ttrain-auc:0.74655\ttrain-error:0.25338\ttest-logloss:12.23972\ttest-auc:0.66796\ttest-error:0.33223\n",
      "[380]\ttrain-logloss:9.30763\ttrain-auc:0.74729\ttrain-error:0.25264\ttest-logloss:12.23972\ttest-auc:0.66796\ttest-error:0.33223\n",
      "[381]\ttrain-logloss:9.28833\ttrain-auc:0.74782\ttrain-error:0.25212\ttest-logloss:12.23972\ttest-auc:0.66796\ttest-error:0.33223\n",
      "[382]\ttrain-logloss:9.28994\ttrain-auc:0.74777\ttrain-error:0.25216\ttest-logloss:12.25258\ttest-auc:0.66761\ttest-error:0.33258\n",
      "[383]\ttrain-logloss:9.29959\ttrain-auc:0.74751\ttrain-error:0.25242\ttest-logloss:12.21399\ttest-auc:0.66866\ttest-error:0.33153\n",
      "[384]\ttrain-logloss:9.29476\ttrain-auc:0.74764\ttrain-error:0.25229\ttest-logloss:12.22042\ttest-auc:0.66848\ttest-error:0.33170\n",
      "[385]\ttrain-logloss:9.27064\ttrain-auc:0.74830\ttrain-error:0.25164\ttest-logloss:12.18826\ttest-auc:0.66936\ttest-error:0.33083\n",
      "[386]\ttrain-logloss:9.26904\ttrain-auc:0.74834\ttrain-error:0.25159\ttest-logloss:12.20113\ttest-auc:0.66900\ttest-error:0.33118\n",
      "[387]\ttrain-logloss:9.26904\ttrain-auc:0.74834\ttrain-error:0.25159\ttest-logloss:12.22042\ttest-auc:0.66848\ttest-error:0.33170\n",
      "[388]\ttrain-logloss:9.24331\ttrain-auc:0.74904\ttrain-error:0.25089\ttest-logloss:12.15611\ttest-auc:0.67023\ttest-error:0.32996\n",
      "[389]\ttrain-logloss:9.23366\ttrain-auc:0.74930\ttrain-error:0.25063\ttest-logloss:12.16897\ttest-auc:0.66988\ttest-error:0.33031\n",
      "[390]\ttrain-logloss:9.24009\ttrain-auc:0.74913\ttrain-error:0.25081\ttest-logloss:12.16897\ttest-auc:0.66988\ttest-error:0.33031\n",
      "[391]\ttrain-logloss:9.22723\ttrain-auc:0.74948\ttrain-error:0.25046\ttest-logloss:12.17540\ttest-auc:0.66971\ttest-error:0.33048\n",
      "[392]\ttrain-logloss:9.21436\ttrain-auc:0.74983\ttrain-error:0.25011\ttest-logloss:12.18183\ttest-auc:0.66953\ttest-error:0.33066\n",
      "[393]\ttrain-logloss:9.21275\ttrain-auc:0.74987\ttrain-error:0.25006\ttest-logloss:12.17540\ttest-auc:0.66971\ttest-error:0.33048\n",
      "[394]\ttrain-logloss:9.21597\ttrain-auc:0.74978\ttrain-error:0.25015\ttest-logloss:12.16897\ttest-auc:0.66988\ttest-error:0.33031\n",
      "[395]\ttrain-logloss:9.20954\ttrain-auc:0.74996\ttrain-error:0.24998\ttest-logloss:12.18183\ttest-auc:0.66953\ttest-error:0.33066\n",
      "[396]\ttrain-logloss:9.20310\ttrain-auc:0.75013\ttrain-error:0.24980\ttest-logloss:12.18183\ttest-auc:0.66953\ttest-error:0.33066\n",
      "[397]\ttrain-logloss:9.19667\ttrain-auc:0.75031\ttrain-error:0.24963\ttest-logloss:12.18183\ttest-auc:0.66953\ttest-error:0.33066\n",
      "[398]\ttrain-logloss:9.18059\ttrain-auc:0.75074\ttrain-error:0.24919\ttest-logloss:12.18183\ttest-auc:0.66953\ttest-error:0.33066\n",
      "[399]\ttrain-logloss:9.18220\ttrain-auc:0.75070\ttrain-error:0.24924\ttest-logloss:12.21399\ttest-auc:0.66866\ttest-error:0.33153\n",
      "[400]\ttrain-logloss:9.18381\ttrain-auc:0.75066\ttrain-error:0.24928\ttest-logloss:12.18826\ttest-auc:0.66936\ttest-error:0.33083\n",
      "[401]\ttrain-logloss:9.17737\ttrain-auc:0.75083\ttrain-error:0.24910\ttest-logloss:12.18183\ttest-auc:0.66953\ttest-error:0.33066\n",
      "[402]\ttrain-logloss:9.15647\ttrain-auc:0.75140\ttrain-error:0.24854\ttest-logloss:12.22042\ttest-auc:0.66848\ttest-error:0.33170\n",
      "[403]\ttrain-logloss:9.15486\ttrain-auc:0.75144\ttrain-error:0.24849\ttest-logloss:12.22686\ttest-auc:0.66831\ttest-error:0.33188\n",
      "[404]\ttrain-logloss:9.15325\ttrain-auc:0.75148\ttrain-error:0.24845\ttest-logloss:12.22042\ttest-auc:0.66849\ttest-error:0.33170\n",
      "[405]\ttrain-logloss:9.14039\ttrain-auc:0.75183\ttrain-error:0.24810\ttest-logloss:12.23329\ttest-auc:0.66814\ttest-error:0.33205\n",
      "[406]\ttrain-logloss:9.13717\ttrain-auc:0.75192\ttrain-error:0.24801\ttest-logloss:12.25258\ttest-auc:0.66761\ttest-error:0.33258\n",
      "[407]\ttrain-logloss:9.12752\ttrain-auc:0.75218\ttrain-error:0.24775\ttest-logloss:12.23972\ttest-auc:0.66796\ttest-error:0.33223\n",
      "[408]\ttrain-logloss:9.10662\ttrain-auc:0.75275\ttrain-error:0.24718\ttest-logloss:12.22042\ttest-auc:0.66848\ttest-error:0.33170\n",
      "[409]\ttrain-logloss:9.07928\ttrain-auc:0.75349\ttrain-error:0.24644\ttest-logloss:12.22686\ttest-auc:0.66831\ttest-error:0.33188\n",
      "[410]\ttrain-logloss:9.07606\ttrain-auc:0.75358\ttrain-error:0.24635\ttest-logloss:12.25901\ttest-auc:0.66743\ttest-error:0.33275\n",
      "[411]\ttrain-logloss:9.06642\ttrain-auc:0.75384\ttrain-error:0.24609\ttest-logloss:12.25901\ttest-auc:0.66744\ttest-error:0.33275\n",
      "[412]\ttrain-logloss:9.07285\ttrain-auc:0.75367\ttrain-error:0.24627\ttest-logloss:12.25901\ttest-auc:0.66743\ttest-error:0.33275\n",
      "[413]\ttrain-logloss:9.05355\ttrain-auc:0.75419\ttrain-error:0.24574\ttest-logloss:12.27188\ttest-auc:0.66708\ttest-error:0.33310\n",
      "[414]\ttrain-logloss:9.04551\ttrain-auc:0.75441\ttrain-error:0.24553\ttest-logloss:12.25901\ttest-auc:0.66743\ttest-error:0.33275\n",
      "[415]\ttrain-logloss:9.04229\ttrain-auc:0.75450\ttrain-error:0.24544\ttest-logloss:12.26545\ttest-auc:0.66726\ttest-error:0.33293\n",
      "[416]\ttrain-logloss:9.05194\ttrain-auc:0.75424\ttrain-error:0.24570\ttest-logloss:12.27831\ttest-auc:0.66691\ttest-error:0.33327\n",
      "[417]\ttrain-logloss:9.04229\ttrain-auc:0.75450\ttrain-error:0.24544\ttest-logloss:12.24615\ttest-auc:0.66778\ttest-error:0.33240\n",
      "[418]\ttrain-logloss:9.02782\ttrain-auc:0.75489\ttrain-error:0.24505\ttest-logloss:12.24615\ttest-auc:0.66778\ttest-error:0.33240\n",
      "[419]\ttrain-logloss:9.01978\ttrain-auc:0.75511\ttrain-error:0.24483\ttest-logloss:12.23972\ttest-auc:0.66795\ttest-error:0.33223\n",
      "[420]\ttrain-logloss:9.00853\ttrain-auc:0.75541\ttrain-error:0.24452\ttest-logloss:12.25901\ttest-auc:0.66743\ttest-error:0.33275\n",
      "[421]\ttrain-logloss:8.98601\ttrain-auc:0.75603\ttrain-error:0.24391\ttest-logloss:12.23972\ttest-auc:0.66795\ttest-error:0.33223\n",
      "[422]\ttrain-logloss:8.97958\ttrain-auc:0.75620\ttrain-error:0.24374\ttest-logloss:12.27188\ttest-auc:0.66708\ttest-error:0.33310\n",
      "[423]\ttrain-logloss:8.97636\ttrain-auc:0.75629\ttrain-error:0.24365\ttest-logloss:12.25258\ttest-auc:0.66761\ttest-error:0.33258\n",
      "[424]\ttrain-logloss:8.97636\ttrain-auc:0.75629\ttrain-error:0.24365\ttest-logloss:12.27188\ttest-auc:0.66708\ttest-error:0.33310\n",
      "[425]\ttrain-logloss:8.96028\ttrain-auc:0.75672\ttrain-error:0.24321\ttest-logloss:12.25901\ttest-auc:0.66743\ttest-error:0.33275\n",
      "[426]\ttrain-logloss:8.94420\ttrain-auc:0.75716\ttrain-error:0.24278\ttest-logloss:12.26545\ttest-auc:0.66725\ttest-error:0.33293\n",
      "[427]\ttrain-logloss:8.93616\ttrain-auc:0.75738\ttrain-error:0.24256\ttest-logloss:12.28474\ttest-auc:0.66673\ttest-error:0.33345\n",
      "[428]\ttrain-logloss:8.91365\ttrain-auc:0.75799\ttrain-error:0.24195\ttest-logloss:12.27831\ttest-auc:0.66690\ttest-error:0.33327\n",
      "[429]\ttrain-logloss:8.90239\ttrain-auc:0.75830\ttrain-error:0.24164\ttest-logloss:12.27188\ttest-auc:0.66708\ttest-error:0.33310\n",
      "[430]\ttrain-logloss:8.90561\ttrain-auc:0.75821\ttrain-error:0.24173\ttest-logloss:12.27831\ttest-auc:0.66691\ttest-error:0.33327\n",
      "[431]\ttrain-logloss:8.89113\ttrain-auc:0.75860\ttrain-error:0.24134\ttest-logloss:12.29117\ttest-auc:0.66656\ttest-error:0.33362\n",
      "[432]\ttrain-logloss:8.89757\ttrain-auc:0.75843\ttrain-error:0.24151\ttest-logloss:12.31690\ttest-auc:0.66586\ttest-error:0.33432\n",
      "[433]\ttrain-logloss:8.88309\ttrain-auc:0.75882\ttrain-error:0.24112\ttest-logloss:12.29761\ttest-auc:0.66638\ttest-error:0.33380\n",
      "[434]\ttrain-logloss:8.88149\ttrain-auc:0.75887\ttrain-error:0.24107\ttest-logloss:12.29761\ttest-auc:0.66638\ttest-error:0.33380\n",
      "[435]\ttrain-logloss:8.86541\ttrain-auc:0.75930\ttrain-error:0.24064\ttest-logloss:12.27831\ttest-auc:0.66691\ttest-error:0.33327\n",
      "[436]\ttrain-logloss:8.86058\ttrain-auc:0.75943\ttrain-error:0.24051\ttest-logloss:12.27831\ttest-auc:0.66691\ttest-error:0.33327\n",
      "[437]\ttrain-logloss:8.86058\ttrain-auc:0.75943\ttrain-error:0.24051\ttest-logloss:12.29761\ttest-auc:0.66638\ttest-error:0.33380\n",
      "[438]\ttrain-logloss:8.85576\ttrain-auc:0.75956\ttrain-error:0.24038\ttest-logloss:12.27831\ttest-auc:0.66691\ttest-error:0.33327\n",
      "[439]\ttrain-logloss:8.84450\ttrain-auc:0.75987\ttrain-error:0.24007\ttest-logloss:12.25901\ttest-auc:0.66743\ttest-error:0.33275\n",
      "[440]\ttrain-logloss:8.84450\ttrain-auc:0.75987\ttrain-error:0.24007\ttest-logloss:12.27831\ttest-auc:0.66690\ttest-error:0.33327\n",
      "[441]\ttrain-logloss:8.83807\ttrain-auc:0.76004\ttrain-error:0.23989\ttest-logloss:12.27188\ttest-auc:0.66708\ttest-error:0.33310\n",
      "[442]\ttrain-logloss:8.83646\ttrain-auc:0.76009\ttrain-error:0.23985\ttest-logloss:12.29117\ttest-auc:0.66655\ttest-error:0.33362\n",
      "[443]\ttrain-logloss:8.82842\ttrain-auc:0.76030\ttrain-error:0.23963\ttest-logloss:12.29761\ttest-auc:0.66638\ttest-error:0.33380\n",
      "[444]\ttrain-logloss:8.83324\ttrain-auc:0.76017\ttrain-error:0.23976\ttest-logloss:12.27188\ttest-auc:0.66708\ttest-error:0.33310\n",
      "[445]\ttrain-logloss:8.83003\ttrain-auc:0.76026\ttrain-error:0.23968\ttest-logloss:12.29761\ttest-auc:0.66638\ttest-error:0.33380\n",
      "[446]\ttrain-logloss:8.82038\ttrain-auc:0.76052\ttrain-error:0.23941\ttest-logloss:12.28474\ttest-auc:0.66673\ttest-error:0.33345\n",
      "[447]\ttrain-logloss:8.81395\ttrain-auc:0.76070\ttrain-error:0.23924\ttest-logloss:12.31690\ttest-auc:0.66586\ttest-error:0.33432\n",
      "[448]\ttrain-logloss:8.80590\ttrain-auc:0.76091\ttrain-error:0.23902\ttest-logloss:12.31047\ttest-auc:0.66604\ttest-error:0.33415\n",
      "[449]\ttrain-logloss:8.81395\ttrain-auc:0.76070\ttrain-error:0.23924\ttest-logloss:12.29761\ttest-auc:0.66638\ttest-error:0.33380\n",
      "[450]\ttrain-logloss:8.81555\ttrain-auc:0.76065\ttrain-error:0.23928\ttest-logloss:12.29761\ttest-auc:0.66639\ttest-error:0.33380\n",
      "[451]\ttrain-logloss:8.80590\ttrain-auc:0.76092\ttrain-error:0.23902\ttest-logloss:12.27188\ttest-auc:0.66708\ttest-error:0.33310\n",
      "[452]\ttrain-logloss:8.78661\ttrain-auc:0.76144\ttrain-error:0.23850\ttest-logloss:12.26545\ttest-auc:0.66726\ttest-error:0.33293\n",
      "[453]\ttrain-logloss:8.77535\ttrain-auc:0.76174\ttrain-error:0.23819\ttest-logloss:12.27188\ttest-auc:0.66708\ttest-error:0.33310\n",
      "[454]\ttrain-logloss:8.75766\ttrain-auc:0.76222\ttrain-error:0.23771\ttest-logloss:12.29117\ttest-auc:0.66656\ttest-error:0.33362\n",
      "[455]\ttrain-logloss:8.73676\ttrain-auc:0.76279\ttrain-error:0.23714\ttest-logloss:12.26545\ttest-auc:0.66726\ttest-error:0.33293\n",
      "[456]\ttrain-logloss:8.73997\ttrain-auc:0.76270\ttrain-error:0.23723\ttest-logloss:12.27831\ttest-auc:0.66691\ttest-error:0.33327\n",
      "[457]\ttrain-logloss:8.73193\ttrain-auc:0.76292\ttrain-error:0.23701\ttest-logloss:12.25258\ttest-auc:0.66760\ttest-error:0.33258\n",
      "[458]\ttrain-logloss:8.72068\ttrain-auc:0.76323\ttrain-error:0.23671\ttest-logloss:12.27188\ttest-auc:0.66708\ttest-error:0.33310\n",
      "[459]\ttrain-logloss:8.71264\ttrain-auc:0.76345\ttrain-error:0.23649\ttest-logloss:12.29117\ttest-auc:0.66656\ttest-error:0.33362\n",
      "[460]\ttrain-logloss:8.71585\ttrain-auc:0.76336\ttrain-error:0.23658\ttest-logloss:12.31047\ttest-auc:0.66603\ttest-error:0.33415\n",
      "[461]\ttrain-logloss:8.71103\ttrain-auc:0.76349\ttrain-error:0.23645\ttest-logloss:12.30404\ttest-auc:0.66621\ttest-error:0.33397\n",
      "[462]\ttrain-logloss:8.71425\ttrain-auc:0.76340\ttrain-error:0.23653\ttest-logloss:12.34906\ttest-auc:0.66499\ttest-error:0.33520\n",
      "[463]\ttrain-logloss:8.70620\ttrain-auc:0.76362\ttrain-error:0.23632\ttest-logloss:12.33620\ttest-auc:0.66534\ttest-error:0.33485\n",
      "[464]\ttrain-logloss:8.71103\ttrain-auc:0.76349\ttrain-error:0.23645\ttest-logloss:12.32333\ttest-auc:0.66568\ttest-error:0.33450\n",
      "[465]\ttrain-logloss:8.70942\ttrain-auc:0.76353\ttrain-error:0.23640\ttest-logloss:12.33620\ttest-auc:0.66534\ttest-error:0.33485\n",
      "[466]\ttrain-logloss:8.67726\ttrain-auc:0.76441\ttrain-error:0.23553\ttest-logloss:12.33620\ttest-auc:0.66534\ttest-error:0.33485\n",
      "[467]\ttrain-logloss:8.66761\ttrain-auc:0.76467\ttrain-error:0.23527\ttest-logloss:12.34263\ttest-auc:0.66516\ttest-error:0.33502\n",
      "[468]\ttrain-logloss:8.67404\ttrain-auc:0.76449\ttrain-error:0.23544\ttest-logloss:12.30404\ttest-auc:0.66621\ttest-error:0.33397\n",
      "[469]\ttrain-logloss:8.66600\ttrain-auc:0.76471\ttrain-error:0.23522\ttest-logloss:12.31047\ttest-auc:0.66604\ttest-error:0.33415\n",
      "[470]\ttrain-logloss:8.65314\ttrain-auc:0.76506\ttrain-error:0.23488\ttest-logloss:12.31690\ttest-auc:0.66586\ttest-error:0.33432\n",
      "[471]\ttrain-logloss:8.64671\ttrain-auc:0.76524\ttrain-error:0.23470\ttest-logloss:12.33620\ttest-auc:0.66534\ttest-error:0.33485\n",
      "[472]\ttrain-logloss:8.64027\ttrain-auc:0.76541\ttrain-error:0.23453\ttest-logloss:12.32333\ttest-auc:0.66568\ttest-error:0.33450\n",
      "[473]\ttrain-logloss:8.63545\ttrain-auc:0.76554\ttrain-error:0.23439\ttest-logloss:12.36835\ttest-auc:0.66446\ttest-error:0.33572\n",
      "[474]\ttrain-logloss:8.64027\ttrain-auc:0.76541\ttrain-error:0.23453\ttest-logloss:12.37479\ttest-auc:0.66429\ttest-error:0.33589\n",
      "[475]\ttrain-logloss:8.64188\ttrain-auc:0.76537\ttrain-error:0.23457\ttest-logloss:12.36835\ttest-auc:0.66446\ttest-error:0.33572\n",
      "[476]\ttrain-logloss:8.63866\ttrain-auc:0.76545\ttrain-error:0.23448\ttest-logloss:12.37479\ttest-auc:0.66429\ttest-error:0.33589\n",
      "[477]\ttrain-logloss:8.62580\ttrain-auc:0.76580\ttrain-error:0.23413\ttest-logloss:12.37479\ttest-auc:0.66429\ttest-error:0.33589\n",
      "[478]\ttrain-logloss:8.63062\ttrain-auc:0.76567\ttrain-error:0.23427\ttest-logloss:12.35549\ttest-auc:0.66481\ttest-error:0.33537\n",
      "[479]\ttrain-logloss:8.62258\ttrain-auc:0.76589\ttrain-error:0.23405\ttest-logloss:12.38122\ttest-auc:0.66411\ttest-error:0.33607\n",
      "[480]\ttrain-logloss:8.61133\ttrain-auc:0.76620\ttrain-error:0.23374\ttest-logloss:12.37479\ttest-auc:0.66428\ttest-error:0.33589\n",
      "[481]\ttrain-logloss:8.61615\ttrain-auc:0.76606\ttrain-error:0.23387\ttest-logloss:12.34906\ttest-auc:0.66498\ttest-error:0.33520\n",
      "[482]\ttrain-logloss:8.61293\ttrain-auc:0.76615\ttrain-error:0.23378\ttest-logloss:12.30404\ttest-auc:0.66620\ttest-error:0.33397\n",
      "[483]\ttrain-logloss:8.61937\ttrain-auc:0.76598\ttrain-error:0.23396\ttest-logloss:12.30404\ttest-auc:0.66620\ttest-error:0.33397\n",
      "[484]\ttrain-logloss:8.60650\ttrain-auc:0.76633\ttrain-error:0.23361\ttest-logloss:12.27188\ttest-auc:0.66708\ttest-error:0.33310\n",
      "[485]\ttrain-logloss:8.60650\ttrain-auc:0.76633\ttrain-error:0.23361\ttest-logloss:12.29761\ttest-auc:0.66638\ttest-error:0.33380\n",
      "[486]\ttrain-logloss:8.61615\ttrain-auc:0.76606\ttrain-error:0.23387\ttest-logloss:12.32333\ttest-auc:0.66568\ttest-error:0.33450\n",
      "[487]\ttrain-logloss:8.59203\ttrain-auc:0.76672\ttrain-error:0.23322\ttest-logloss:12.30404\ttest-auc:0.66620\ttest-error:0.33397\n",
      "[488]\ttrain-logloss:8.58399\ttrain-auc:0.76694\ttrain-error:0.23300\ttest-logloss:12.29761\ttest-auc:0.66638\ttest-error:0.33380\n",
      "[489]\ttrain-logloss:8.59685\ttrain-auc:0.76659\ttrain-error:0.23335\ttest-logloss:12.30404\ttest-auc:0.66620\ttest-error:0.33397\n",
      "[490]\ttrain-logloss:8.59042\ttrain-auc:0.76676\ttrain-error:0.23317\ttest-logloss:12.31690\ttest-auc:0.66585\ttest-error:0.33432\n",
      "[491]\ttrain-logloss:8.58560\ttrain-auc:0.76689\ttrain-error:0.23304\ttest-logloss:12.30404\ttest-auc:0.66620\ttest-error:0.33397\n",
      "[492]\ttrain-logloss:8.57434\ttrain-auc:0.76720\ttrain-error:0.23274\ttest-logloss:12.29761\ttest-auc:0.66638\ttest-error:0.33380\n",
      "[493]\ttrain-logloss:8.55344\ttrain-auc:0.76777\ttrain-error:0.23217\ttest-logloss:12.29761\ttest-auc:0.66638\ttest-error:0.33380\n",
      "[494]\ttrain-logloss:8.54539\ttrain-auc:0.76798\ttrain-error:0.23195\ttest-logloss:12.31047\ttest-auc:0.66603\ttest-error:0.33415\n",
      "[495]\ttrain-logloss:8.53092\ttrain-auc:0.76838\ttrain-error:0.23156\ttest-logloss:12.31690\ttest-auc:0.66585\ttest-error:0.33432\n",
      "[496]\ttrain-logloss:8.53414\ttrain-auc:0.76829\ttrain-error:0.23165\ttest-logloss:12.31690\ttest-auc:0.66585\ttest-error:0.33432\n",
      "[497]\ttrain-logloss:8.52449\ttrain-auc:0.76855\ttrain-error:0.23138\ttest-logloss:12.31690\ttest-auc:0.66585\ttest-error:0.33432\n",
      "[498]\ttrain-logloss:8.52127\ttrain-auc:0.76864\ttrain-error:0.23130\ttest-logloss:12.31047\ttest-auc:0.66603\ttest-error:0.33415\n",
      "[499]\ttrain-logloss:8.51323\ttrain-auc:0.76886\ttrain-error:0.23108\ttest-logloss:12.30404\ttest-auc:0.66621\ttest-error:0.33397\n",
      "[500]\ttrain-logloss:8.50358\ttrain-auc:0.76912\ttrain-error:0.23082\ttest-logloss:12.30404\ttest-auc:0.66620\ttest-error:0.33397\n",
      "[501]\ttrain-logloss:8.49715\ttrain-auc:0.76929\ttrain-error:0.23064\ttest-logloss:12.29117\ttest-auc:0.66656\ttest-error:0.33362\n",
      "[502]\ttrain-logloss:8.47946\ttrain-auc:0.76977\ttrain-error:0.23016\ttest-logloss:12.30404\ttest-auc:0.66621\ttest-error:0.33397\n",
      "[503]\ttrain-logloss:8.47464\ttrain-auc:0.76991\ttrain-error:0.23003\ttest-logloss:12.29761\ttest-auc:0.66638\ttest-error:0.33380\n",
      "[504]\ttrain-logloss:8.47303\ttrain-auc:0.76995\ttrain-error:0.22999\ttest-logloss:12.31690\ttest-auc:0.66585\ttest-error:0.33432\n",
      "[505]\ttrain-logloss:8.46338\ttrain-auc:0.77021\ttrain-error:0.22973\ttest-logloss:12.30404\ttest-auc:0.66621\ttest-error:0.33397\n",
      "[506]\ttrain-logloss:8.46660\ttrain-auc:0.77012\ttrain-error:0.22981\ttest-logloss:12.31047\ttest-auc:0.66603\ttest-error:0.33415\n",
      "[507]\ttrain-logloss:8.45856\ttrain-auc:0.77034\ttrain-error:0.22959\ttest-logloss:12.30404\ttest-auc:0.66621\ttest-error:0.33397\n",
      "[508]\ttrain-logloss:8.46017\ttrain-auc:0.77030\ttrain-error:0.22964\ttest-logloss:12.29761\ttest-auc:0.66639\ttest-error:0.33380\n",
      "[509]\ttrain-logloss:8.45695\ttrain-auc:0.77038\ttrain-error:0.22955\ttest-logloss:12.30404\ttest-auc:0.66622\ttest-error:0.33397\n",
      "[510]\ttrain-logloss:8.44569\ttrain-auc:0.77069\ttrain-error:0.22925\ttest-logloss:12.27831\ttest-auc:0.66692\ttest-error:0.33327\n",
      "[511]\ttrain-logloss:8.43765\ttrain-auc:0.77091\ttrain-error:0.22903\ttest-logloss:12.31047\ttest-auc:0.66604\ttest-error:0.33415\n",
      "[512]\ttrain-logloss:8.43765\ttrain-auc:0.77091\ttrain-error:0.22903\ttest-logloss:12.29117\ttest-auc:0.66656\ttest-error:0.33362\n",
      "[513]\ttrain-logloss:8.42640\ttrain-auc:0.77121\ttrain-error:0.22872\ttest-logloss:12.32976\ttest-auc:0.66552\ttest-error:0.33467\n",
      "[514]\ttrain-logloss:8.43765\ttrain-auc:0.77091\ttrain-error:0.22903\ttest-logloss:12.32333\ttest-auc:0.66569\ttest-error:0.33450\n",
      "[515]\ttrain-logloss:8.42157\ttrain-auc:0.77135\ttrain-error:0.22859\ttest-logloss:12.32976\ttest-auc:0.66551\ttest-error:0.33467\n",
      "[516]\ttrain-logloss:8.42640\ttrain-auc:0.77121\ttrain-error:0.22872\ttest-logloss:12.34906\ttest-auc:0.66499\ttest-error:0.33520\n",
      "[517]\ttrain-logloss:8.40871\ttrain-auc:0.77170\ttrain-error:0.22824\ttest-logloss:12.35549\ttest-auc:0.66481\ttest-error:0.33537\n",
      "[518]\ttrain-logloss:8.40067\ttrain-auc:0.77191\ttrain-error:0.22802\ttest-logloss:12.36835\ttest-auc:0.66446\ttest-error:0.33572\n",
      "[519]\ttrain-logloss:8.39906\ttrain-auc:0.77196\ttrain-error:0.22798\ttest-logloss:12.36192\ttest-auc:0.66464\ttest-error:0.33554\n",
      "[520]\ttrain-logloss:8.38298\ttrain-auc:0.77239\ttrain-error:0.22754\ttest-logloss:12.33620\ttest-auc:0.66534\ttest-error:0.33485\n",
      "[521]\ttrain-logloss:8.37976\ttrain-auc:0.77248\ttrain-error:0.22745\ttest-logloss:12.34263\ttest-auc:0.66516\ttest-error:0.33502\n",
      "[522]\ttrain-logloss:8.37976\ttrain-auc:0.77248\ttrain-error:0.22745\ttest-logloss:12.32333\ttest-auc:0.66569\ttest-error:0.33450\n",
      "[523]\ttrain-logloss:8.38137\ttrain-auc:0.77244\ttrain-error:0.22750\ttest-logloss:12.29117\ttest-auc:0.66656\ttest-error:0.33362\n",
      "[524]\ttrain-logloss:8.37655\ttrain-auc:0.77257\ttrain-error:0.22737\ttest-logloss:12.28474\ttest-auc:0.66673\ttest-error:0.33345\n",
      "[525]\ttrain-logloss:8.35242\ttrain-auc:0.77322\ttrain-error:0.22671\ttest-logloss:12.29761\ttest-auc:0.66639\ttest-error:0.33380\n",
      "[526]\ttrain-logloss:8.36207\ttrain-auc:0.77296\ttrain-error:0.22698\ttest-logloss:12.27831\ttest-auc:0.66691\ttest-error:0.33327\n",
      "[527]\ttrain-logloss:8.33795\ttrain-auc:0.77362\ttrain-error:0.22632\ttest-logloss:12.29117\ttest-auc:0.66656\ttest-error:0.33362\n",
      "[528]\ttrain-logloss:8.32187\ttrain-auc:0.77405\ttrain-error:0.22588\ttest-logloss:12.29761\ttest-auc:0.66638\ttest-error:0.33380\n",
      "[529]\ttrain-logloss:8.30418\ttrain-auc:0.77453\ttrain-error:0.22540\ttest-logloss:12.32333\ttest-auc:0.66569\ttest-error:0.33450\n",
      "[530]\ttrain-logloss:8.30096\ttrain-auc:0.77462\ttrain-error:0.22532\ttest-logloss:12.32333\ttest-auc:0.66569\ttest-error:0.33450\n",
      "[531]\ttrain-logloss:8.28649\ttrain-auc:0.77501\ttrain-error:0.22492\ttest-logloss:12.30404\ttest-auc:0.66621\ttest-error:0.33397\n",
      "[532]\ttrain-logloss:8.29132\ttrain-auc:0.77488\ttrain-error:0.22506\ttest-logloss:12.29117\ttest-auc:0.66655\ttest-error:0.33362\n",
      "[533]\ttrain-logloss:8.27363\ttrain-auc:0.77536\ttrain-error:0.22457\ttest-logloss:12.27831\ttest-auc:0.66690\ttest-error:0.33327\n",
      "[534]\ttrain-logloss:8.25594\ttrain-auc:0.77584\ttrain-error:0.22409\ttest-logloss:12.31690\ttest-auc:0.66585\ttest-error:0.33432\n",
      "[535]\ttrain-logloss:8.24951\ttrain-auc:0.77602\ttrain-error:0.22392\ttest-logloss:12.31690\ttest-auc:0.66586\ttest-error:0.33432\n",
      "[536]\ttrain-logloss:8.23986\ttrain-auc:0.77628\ttrain-error:0.22366\ttest-logloss:12.27188\ttest-auc:0.66708\ttest-error:0.33310\n",
      "[537]\ttrain-logloss:8.23503\ttrain-auc:0.77641\ttrain-error:0.22353\ttest-logloss:12.27831\ttest-auc:0.66690\ttest-error:0.33327\n",
      "[538]\ttrain-logloss:8.23343\ttrain-auc:0.77645\ttrain-error:0.22348\ttest-logloss:12.28474\ttest-auc:0.66672\ttest-error:0.33345\n",
      "[539]\ttrain-logloss:8.23986\ttrain-auc:0.77628\ttrain-error:0.22366\ttest-logloss:12.29761\ttest-auc:0.66638\ttest-error:0.33380\n",
      "[540]\ttrain-logloss:8.23503\ttrain-auc:0.77641\ttrain-error:0.22353\ttest-logloss:12.27188\ttest-auc:0.66707\ttest-error:0.33310\n",
      "[541]\ttrain-logloss:8.21574\ttrain-auc:0.77693\ttrain-error:0.22300\ttest-logloss:12.27831\ttest-auc:0.66689\ttest-error:0.33327\n",
      "[542]\ttrain-logloss:8.20770\ttrain-auc:0.77715\ttrain-error:0.22279\ttest-logloss:12.29761\ttest-auc:0.66637\ttest-error:0.33380\n",
      "[543]\ttrain-logloss:8.19805\ttrain-auc:0.77741\ttrain-error:0.22252\ttest-logloss:12.27188\ttest-auc:0.66707\ttest-error:0.33310\n",
      "[544]\ttrain-logloss:8.19161\ttrain-auc:0.77759\ttrain-error:0.22235\ttest-logloss:12.28474\ttest-auc:0.66672\ttest-error:0.33345\n",
      "[545]\ttrain-logloss:8.19966\ttrain-auc:0.77737\ttrain-error:0.22257\ttest-logloss:12.23972\ttest-auc:0.66794\ttest-error:0.33223\n",
      "[546]\ttrain-logloss:8.18840\ttrain-auc:0.77767\ttrain-error:0.22226\ttest-logloss:12.24615\ttest-auc:0.66777\ttest-error:0.33240\n",
      "[547]\ttrain-logloss:8.18679\ttrain-auc:0.77772\ttrain-error:0.22222\ttest-logloss:12.26545\ttest-auc:0.66725\ttest-error:0.33293\n",
      "[548]\ttrain-logloss:8.20126\ttrain-auc:0.77733\ttrain-error:0.22261\ttest-logloss:12.27831\ttest-auc:0.66689\ttest-error:0.33327\n",
      "[549]\ttrain-logloss:8.18197\ttrain-auc:0.77785\ttrain-error:0.22209\ttest-logloss:12.27831\ttest-auc:0.66688\ttest-error:0.33327\n",
      "[550]\ttrain-logloss:8.18197\ttrain-auc:0.77785\ttrain-error:0.22209\ttest-logloss:12.28474\ttest-auc:0.66671\ttest-error:0.33345\n",
      "[551]\ttrain-logloss:8.18679\ttrain-auc:0.77772\ttrain-error:0.22222\ttest-logloss:12.31690\ttest-auc:0.66584\ttest-error:0.33432\n",
      "[552]\ttrain-logloss:8.18840\ttrain-auc:0.77767\ttrain-error:0.22226\ttest-logloss:12.29117\ttest-auc:0.66654\ttest-error:0.33362\n",
      "[553]\ttrain-logloss:8.17714\ttrain-auc:0.77798\ttrain-error:0.22196\ttest-logloss:12.28474\ttest-auc:0.66671\ttest-error:0.33345\n",
      "[554]\ttrain-logloss:8.17071\ttrain-auc:0.77816\ttrain-error:0.22178\ttest-logloss:12.29761\ttest-auc:0.66636\ttest-error:0.33380\n",
      "[555]\ttrain-logloss:8.15463\ttrain-auc:0.77859\ttrain-error:0.22134\ttest-logloss:12.31047\ttest-auc:0.66602\ttest-error:0.33415\n",
      "[556]\ttrain-logloss:8.14016\ttrain-auc:0.77898\ttrain-error:0.22095\ttest-logloss:12.30404\ttest-auc:0.66619\ttest-error:0.33397\n",
      "[557]\ttrain-logloss:8.14659\ttrain-auc:0.77881\ttrain-error:0.22113\ttest-logloss:12.31690\ttest-auc:0.66584\ttest-error:0.33432\n",
      "[558]\ttrain-logloss:8.15141\ttrain-auc:0.77868\ttrain-error:0.22126\ttest-logloss:12.32976\ttest-auc:0.66549\ttest-error:0.33467\n",
      "[559]\ttrain-logloss:8.14498\ttrain-auc:0.77885\ttrain-error:0.22108\ttest-logloss:12.31690\ttest-auc:0.66584\ttest-error:0.33432\n",
      "[560]\ttrain-logloss:8.13533\ttrain-auc:0.77912\ttrain-error:0.22082\ttest-logloss:12.31047\ttest-auc:0.66602\ttest-error:0.33415\n",
      "[561]\ttrain-logloss:8.13051\ttrain-auc:0.77925\ttrain-error:0.22069\ttest-logloss:12.30404\ttest-auc:0.66619\ttest-error:0.33397\n",
      "[562]\ttrain-logloss:8.11443\ttrain-auc:0.77968\ttrain-error:0.22025\ttest-logloss:12.29761\ttest-auc:0.66637\ttest-error:0.33380\n",
      "[563]\ttrain-logloss:8.09352\ttrain-auc:0.78025\ttrain-error:0.21969\ttest-logloss:12.28474\ttest-auc:0.66672\ttest-error:0.33345\n",
      "[564]\ttrain-logloss:8.08870\ttrain-auc:0.78038\ttrain-error:0.21956\ttest-logloss:12.27831\ttest-auc:0.66689\ttest-error:0.33327\n",
      "[565]\ttrain-logloss:8.09191\ttrain-auc:0.78029\ttrain-error:0.21964\ttest-logloss:12.31690\ttest-auc:0.66584\ttest-error:0.33432\n",
      "[566]\ttrain-logloss:8.09031\ttrain-auc:0.78034\ttrain-error:0.21960\ttest-logloss:12.32333\ttest-auc:0.66566\ttest-error:0.33450\n",
      "[567]\ttrain-logloss:8.08870\ttrain-auc:0.78038\ttrain-error:0.21956\ttest-logloss:12.31690\ttest-auc:0.66584\ttest-error:0.33432\n",
      "[568]\ttrain-logloss:8.09191\ttrain-auc:0.78029\ttrain-error:0.21964\ttest-logloss:12.31047\ttest-auc:0.66602\ttest-error:0.33415\n",
      "[569]\ttrain-logloss:8.08548\ttrain-auc:0.78047\ttrain-error:0.21947\ttest-logloss:12.33620\ttest-auc:0.66532\ttest-error:0.33485\n",
      "[570]\ttrain-logloss:8.06940\ttrain-auc:0.78090\ttrain-error:0.21903\ttest-logloss:12.30404\ttest-auc:0.66619\ttest-error:0.33397\n",
      "[571]\ttrain-logloss:8.07262\ttrain-auc:0.78082\ttrain-error:0.21912\ttest-logloss:12.31047\ttest-auc:0.66601\ttest-error:0.33415\n",
      "[572]\ttrain-logloss:8.07744\ttrain-auc:0.78069\ttrain-error:0.21925\ttest-logloss:12.28474\ttest-auc:0.66671\ttest-error:0.33345\n",
      "[573]\ttrain-logloss:8.09674\ttrain-auc:0.78016\ttrain-error:0.21977\ttest-logloss:12.24615\ttest-auc:0.66775\ttest-error:0.33240\n",
      "[574]\ttrain-logloss:8.08709\ttrain-auc:0.78042\ttrain-error:0.21951\ttest-logloss:12.25258\ttest-auc:0.66758\ttest-error:0.33258\n",
      "[575]\ttrain-logloss:8.07583\ttrain-auc:0.78073\ttrain-error:0.21921\ttest-logloss:12.27831\ttest-auc:0.66688\ttest-error:0.33327\n",
      "[576]\ttrain-logloss:8.07262\ttrain-auc:0.78082\ttrain-error:0.21912\ttest-logloss:12.31047\ttest-auc:0.66602\ttest-error:0.33415\n",
      "[577]\ttrain-logloss:8.05654\ttrain-auc:0.78125\ttrain-error:0.21868\ttest-logloss:12.28474\ttest-auc:0.66672\ttest-error:0.33345\n",
      "[578]\ttrain-logloss:8.05171\ttrain-auc:0.78138\ttrain-error:0.21855\ttest-logloss:12.30404\ttest-auc:0.66619\ttest-error:0.33397\n",
      "[579]\ttrain-logloss:8.05171\ttrain-auc:0.78138\ttrain-error:0.21855\ttest-logloss:12.29117\ttest-auc:0.66654\ttest-error:0.33362\n",
      "[580]\ttrain-logloss:8.05171\ttrain-auc:0.78138\ttrain-error:0.21855\ttest-logloss:12.32976\ttest-auc:0.66550\ttest-error:0.33467\n",
      "[581]\ttrain-logloss:8.03563\ttrain-auc:0.78182\ttrain-error:0.21811\ttest-logloss:12.31690\ttest-auc:0.66585\ttest-error:0.33432\n",
      "[582]\ttrain-logloss:8.03885\ttrain-auc:0.78173\ttrain-error:0.21820\ttest-logloss:12.33620\ttest-auc:0.66532\ttest-error:0.33485\n",
      "[583]\ttrain-logloss:8.03081\ttrain-auc:0.78195\ttrain-error:0.21798\ttest-logloss:12.33620\ttest-auc:0.66532\ttest-error:0.33485\n",
      "[584]\ttrain-logloss:8.02277\ttrain-auc:0.78217\ttrain-error:0.21776\ttest-logloss:12.31690\ttest-auc:0.66584\ttest-error:0.33432\n",
      "[585]\ttrain-logloss:8.00508\ttrain-auc:0.78265\ttrain-error:0.21729\ttest-logloss:12.32976\ttest-auc:0.66549\ttest-error:0.33467\n",
      "[586]\ttrain-logloss:7.99864\ttrain-auc:0.78283\ttrain-error:0.21711\ttest-logloss:12.33620\ttest-auc:0.66532\ttest-error:0.33485\n",
      "[587]\ttrain-logloss:8.00347\ttrain-auc:0.78270\ttrain-error:0.21724\ttest-logloss:12.32976\ttest-auc:0.66549\ttest-error:0.33467\n",
      "[588]\ttrain-logloss:7.99864\ttrain-auc:0.78283\ttrain-error:0.21711\ttest-logloss:12.34263\ttest-auc:0.66514\ttest-error:0.33502\n",
      "[589]\ttrain-logloss:8.00025\ttrain-auc:0.78278\ttrain-error:0.21715\ttest-logloss:12.32976\ttest-auc:0.66549\ttest-error:0.33467\n",
      "[590]\ttrain-logloss:7.99060\ttrain-auc:0.78304\ttrain-error:0.21689\ttest-logloss:12.32976\ttest-auc:0.66549\ttest-error:0.33467\n",
      "[591]\ttrain-logloss:7.98739\ttrain-auc:0.78313\ttrain-error:0.21680\ttest-logloss:12.31690\ttest-auc:0.66584\ttest-error:0.33432\n",
      "[592]\ttrain-logloss:7.97452\ttrain-auc:0.78348\ttrain-error:0.21646\ttest-logloss:12.26545\ttest-auc:0.66724\ttest-error:0.33293\n",
      "[593]\ttrain-logloss:7.96166\ttrain-auc:0.78383\ttrain-error:0.21611\ttest-logloss:12.27831\ttest-auc:0.66690\ttest-error:0.33327\n",
      "[594]\ttrain-logloss:7.96809\ttrain-auc:0.78366\ttrain-error:0.21628\ttest-logloss:12.27188\ttest-auc:0.66707\ttest-error:0.33310\n",
      "[595]\ttrain-logloss:7.94879\ttrain-auc:0.78418\ttrain-error:0.21576\ttest-logloss:12.27831\ttest-auc:0.66690\ttest-error:0.33327\n",
      "[596]\ttrain-logloss:7.93754\ttrain-auc:0.78448\ttrain-error:0.21545\ttest-logloss:12.25901\ttest-auc:0.66742\ttest-error:0.33275\n",
      "[597]\ttrain-logloss:7.94719\ttrain-auc:0.78422\ttrain-error:0.21571\ttest-logloss:12.27831\ttest-auc:0.66690\ttest-error:0.33327\n",
      "[598]\ttrain-logloss:7.92789\ttrain-auc:0.78475\ttrain-error:0.21519\ttest-logloss:12.28474\ttest-auc:0.66672\ttest-error:0.33345\n",
      "[599]\ttrain-logloss:7.92628\ttrain-auc:0.78479\ttrain-error:0.21515\ttest-logloss:12.31047\ttest-auc:0.66603\ttest-error:0.33415\n",
      "600-rounds Training finished ...\t\t(2.779s)\n"
     ]
    }
   ],
   "source": [
    "num_round = 600\n",
    "t0 = time()\n",
    "sh_bst_sm = xgb.train(param_sh, xg_train_sh, num_round, watchlist)\n",
    "print(f\"{num_round}-rounds Training finished ...\\t\\t({time()-t0:.3f}s)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test error using softmax = 0.33414804469273746\n"
     ]
    }
   ],
   "source": [
    "# get prediction\n",
    "pred_sh = sh_bst_sm.predict(xg_test_sh)\n",
    "error_rate = np.sum(pred_sh != y_test_sh) / y_test_sh.shape[0]\n",
    "print('Test error using softmax = {}'.format(error_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Counter({0.0: 2647, 1.0: 3081}), Counter({1: 2851, 0: 2877}))"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(pred_sh), Counter(y_test_sh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.63      0.65      2877\n",
      "           1       0.65      0.70      0.68      2851\n",
      "\n",
      "    accuracy                           0.67      5728\n",
      "   macro avg       0.67      0.67      0.67      5728\n",
      "weighted avg       0.67      0.67      0.67      5728\n",
      "\n"
     ]
    }
   ],
   "source": [
    "report_sh = metrics.classification_report(list(y_test_sh), list(pred_sh))\n",
    "print(report_sh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.66602734, 0.66602734])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aucs_sh = auc(y_test_sh.astype(np.uint8), pred_sh.astype(np.uint8), [0, 1])\n",
    "aucs_sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 调参"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "base_param_sh = {  # 基本参数，不需要调参\n",
    "    'objective': 'binary:hinge',\n",
    "    'eta': 0.1,\n",
    "    'nthread': 8,\n",
    "#     'num_class': 10,\n",
    "    'gpu_id': 0,\n",
    "    'tree_method': 'gpu_hist',\n",
    "    'eval_metric': ['logloss', 'auc', 'error']\n",
    "} \n",
    "# 需要调参的参数\n",
    "ps1 = {  \n",
    "    'max_depth': list(range(5, 10, 1)),\n",
    "    'min_child_weight': list(range(1, 10, 2)),\n",
    "}\n",
    "\n",
    "ps2 = {\n",
    "    'gamma': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6]\n",
    "}\n",
    "\n",
    "ps3 = {\n",
    "    'subsample':  [i/10.0 for i in range(6,11,1)], \n",
    "    'colsample_bytree':  [i/10.0 for i in range(6,11,1)] \n",
    "}\n",
    "\n",
    "ps4 = {'reg_alpha': [0, 0.1, 0.2, 0.5, 1, 1.5, 2, 4]}\n",
    "\n",
    "\n",
    "# com_ps_sh = list(ParameterGrid(ps_sh))\n",
    "\n",
    "\n",
    "# all_params_sh = [base_param_sh.copy() for _ in range(len(com_ps_sh))] \n",
    "# for i in range(len(com_ps_sh)):\n",
    "#     all_params_sh[i].update(com_ps_sh[i])\n",
    "\n",
    "# # print(com_ps_sh)\n",
    "# print(all_params_sh.__len__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 550,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data preparing finished ...\t\t(24.924s)\n"
     ]
    }
   ],
   "source": [
    "t0 = time()\n",
    "xg_train_sh = xgb.DMatrix(X_train_sh.values, label=y_train_sh.values, enable_categorical=True)\n",
    "xg_test_sh = xgb.DMatrix(X_test_sh.values, label=y_test_sh.values, enable_categorical=True)\n",
    "print(f\"Data preparing finished ...\\t\\t({time()-t0:.3f}s)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始搜索：max_depth=[5, 6, 7, 8, 9], min_child_weight=[1, 3, 5, 7, 9]\n",
      "待搜索的参数组合数量：25\n",
      "1 / 25: 150-rounds Training finished param={'objective': 'binary:hinge', 'eta': 0.1, 'nthread': 8, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['logloss', 'auc', 'error'], 'max_depth': 5, 'min_child_weight': 1} ...\t\t(3.035s)\n",
      "2 / 25: 150-rounds Training finished param={'objective': 'binary:hinge', 'eta': 0.1, 'nthread': 8, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['logloss', 'auc', 'error'], 'max_depth': 6, 'min_child_weight': 1} ...\t\t(4.271s)\n",
      "3 / 25: 150-rounds Training finished param={'objective': 'binary:hinge', 'eta': 0.1, 'nthread': 8, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['logloss', 'auc', 'error'], 'max_depth': 7, 'min_child_weight': 1} ...\t\t(6.333s)\n",
      "4 / 25: 150-rounds Training finished param={'objective': 'binary:hinge', 'eta': 0.1, 'nthread': 8, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['logloss', 'auc', 'error'], 'max_depth': 8, 'min_child_weight': 1} ...\t\t(9.294s)\n",
      "5 / 25: 150-rounds Training finished param={'objective': 'binary:hinge', 'eta': 0.1, 'nthread': 8, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['logloss', 'auc', 'error'], 'max_depth': 9, 'min_child_weight': 1} ...\t\t(12.788s)\n",
      "6 / 25: 150-rounds Training finished param={'objective': 'binary:hinge', 'eta': 0.1, 'nthread': 8, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['logloss', 'auc', 'error'], 'max_depth': 5, 'min_child_weight': 3} ...\t\t(2.993s)\n",
      "7 / 25: 150-rounds Training finished param={'objective': 'binary:hinge', 'eta': 0.1, 'nthread': 8, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['logloss', 'auc', 'error'], 'max_depth': 6, 'min_child_weight': 3} ...\t\t(4.183s)\n",
      "8 / 25: 150-rounds Training finished param={'objective': 'binary:hinge', 'eta': 0.1, 'nthread': 8, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['logloss', 'auc', 'error'], 'max_depth': 7, 'min_child_weight': 3} ...\t\t(6.127s)\n",
      "9 / 25: 150-rounds Training finished param={'objective': 'binary:hinge', 'eta': 0.1, 'nthread': 8, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['logloss', 'auc', 'error'], 'max_depth': 8, 'min_child_weight': 3} ...\t\t(9.131s)\n",
      "10 / 25: 150-rounds Training finished param={'objective': 'binary:hinge', 'eta': 0.1, 'nthread': 8, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['logloss', 'auc', 'error'], 'max_depth': 9, 'min_child_weight': 3} ...\t\t(12.637s)\n",
      "11 / 25: 150-rounds Training finished param={'objective': 'binary:hinge', 'eta': 0.1, 'nthread': 8, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['logloss', 'auc', 'error'], 'max_depth': 5, 'min_child_weight': 5} ...\t\t(2.996s)\n",
      "12 / 25: 150-rounds Training finished param={'objective': 'binary:hinge', 'eta': 0.1, 'nthread': 8, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['logloss', 'auc', 'error'], 'max_depth': 6, 'min_child_weight': 5} ...\t\t(4.244s)\n",
      "13 / 25: 150-rounds Training finished param={'objective': 'binary:hinge', 'eta': 0.1, 'nthread': 8, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['logloss', 'auc', 'error'], 'max_depth': 7, 'min_child_weight': 5} ...\t\t(6.028s)\n",
      "14 / 25: 150-rounds Training finished param={'objective': 'binary:hinge', 'eta': 0.1, 'nthread': 8, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['logloss', 'auc', 'error'], 'max_depth': 8, 'min_child_weight': 5} ...\t\t(8.756s)\n",
      "15 / 25: 150-rounds Training finished param={'objective': 'binary:hinge', 'eta': 0.1, 'nthread': 8, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['logloss', 'auc', 'error'], 'max_depth': 9, 'min_child_weight': 5} ...\t\t(12.128s)\n",
      "16 / 25: 150-rounds Training finished param={'objective': 'binary:hinge', 'eta': 0.1, 'nthread': 8, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['logloss', 'auc', 'error'], 'max_depth': 5, 'min_child_weight': 7} ...\t\t(2.959s)\n",
      "17 / 25: 150-rounds Training finished param={'objective': 'binary:hinge', 'eta': 0.1, 'nthread': 8, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['logloss', 'auc', 'error'], 'max_depth': 6, 'min_child_weight': 7} ...\t\t(4.105s)\n",
      "18 / 25: 150-rounds Training finished param={'objective': 'binary:hinge', 'eta': 0.1, 'nthread': 8, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['logloss', 'auc', 'error'], 'max_depth': 7, 'min_child_weight': 7} ...\t\t(5.897s)\n",
      "19 / 25: 150-rounds Training finished param={'objective': 'binary:hinge', 'eta': 0.1, 'nthread': 8, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['logloss', 'auc', 'error'], 'max_depth': 8, 'min_child_weight': 7} ...\t\t(8.433s)\n",
      "20 / 25: 150-rounds Training finished param={'objective': 'binary:hinge', 'eta': 0.1, 'nthread': 8, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['logloss', 'auc', 'error'], 'max_depth': 9, 'min_child_weight': 7} ...\t\t(11.471s)\n",
      "21 / 25: 150-rounds Training finished param={'objective': 'binary:hinge', 'eta': 0.1, 'nthread': 8, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['logloss', 'auc', 'error'], 'max_depth': 5, 'min_child_weight': 9} ...\t\t(2.935s)\n",
      "22 / 25: 150-rounds Training finished param={'objective': 'binary:hinge', 'eta': 0.1, 'nthread': 8, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['logloss', 'auc', 'error'], 'max_depth': 6, 'min_child_weight': 9} ...\t\t(4.050s)\n",
      "23 / 25: 150-rounds Training finished param={'objective': 'binary:hinge', 'eta': 0.1, 'nthread': 8, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['logloss', 'auc', 'error'], 'max_depth': 7, 'min_child_weight': 9} ...\t\t(5.802s)\n",
      "24 / 25: 150-rounds Training finished param={'objective': 'binary:hinge', 'eta': 0.1, 'nthread': 8, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['logloss', 'auc', 'error'], 'max_depth': 8, 'min_child_weight': 9} ...\t\t(8.152s)\n",
      "25 / 25: 150-rounds Training finished param={'objective': 'binary:hinge', 'eta': 0.1, 'nthread': 8, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['logloss', 'auc', 'error'], 'max_depth': 9, 'min_child_weight': 9} ...\t\t(11.286s)\n",
      "eta=0.1, eval_metric=['logloss', 'auc', 'error'], gpu_id=0, max_depth=5, min_child_weight=1, nthread=8, objective=binary:hinge, tree_method=gpu_hist\n",
      "eta=0.1, eval_metric=['logloss', 'auc', 'error'], gpu_id=0, max_depth=7, min_child_weight=9, nthread=8, objective=binary:hinge, tree_method=gpu_hist\n",
      "开始搜索：gamma=[0.1, 0.2, 0.3, 0.4, 0.5, 0.6]\n",
      "待搜索的参数组合数量：6\n",
      "1 / 6: 150-rounds Training finished param={'objective': 'binary:hinge', 'eta': 0.1, 'nthread': 8, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['logloss', 'auc', 'error'], 'max_depth': 5, 'min_child_weight': 1, 'gamma': 0.1} ...\t\t(3.033s)\n",
      "2 / 6: 150-rounds Training finished param={'objective': 'binary:hinge', 'eta': 0.1, 'nthread': 8, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['logloss', 'auc', 'error'], 'max_depth': 5, 'min_child_weight': 1, 'gamma': 0.2} ...\t\t(3.015s)\n",
      "3 / 6: 150-rounds Training finished param={'objective': 'binary:hinge', 'eta': 0.1, 'nthread': 8, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['logloss', 'auc', 'error'], 'max_depth': 5, 'min_child_weight': 1, 'gamma': 0.3} ...\t\t(3.015s)\n",
      "4 / 6: 150-rounds Training finished param={'objective': 'binary:hinge', 'eta': 0.1, 'nthread': 8, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['logloss', 'auc', 'error'], 'max_depth': 5, 'min_child_weight': 1, 'gamma': 0.4} ...\t\t(3.057s)\n",
      "5 / 6: 150-rounds Training finished param={'objective': 'binary:hinge', 'eta': 0.1, 'nthread': 8, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['logloss', 'auc', 'error'], 'max_depth': 5, 'min_child_weight': 1, 'gamma': 0.5} ...\t\t(3.028s)\n",
      "6 / 6: 150-rounds Training finished param={'objective': 'binary:hinge', 'eta': 0.1, 'nthread': 8, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['logloss', 'auc', 'error'], 'max_depth': 5, 'min_child_weight': 1, 'gamma': 0.6} ...\t\t(3.047s)\n",
      "eta=0.1, eval_metric=['logloss', 'auc', 'error'], gamma=0.1, gpu_id=0, max_depth=5, min_child_weight=1, nthread=8, objective=binary:hinge, tree_method=gpu_hist\n",
      "eta=0.1, eval_metric=['logloss', 'auc', 'error'], gamma=0.3, gpu_id=0, max_depth=5, min_child_weight=1, nthread=8, objective=binary:hinge, tree_method=gpu_hist\n",
      "开始搜索：colsample_bytree=[0.6, 0.7, 0.8, 0.9, 1.0], subsample=[0.6, 0.7, 0.8, 0.9, 1.0]\n",
      "待搜索的参数组合数量：25\n",
      "1 / 25: 150-rounds Training finished param={'objective': 'binary:hinge', 'eta': 0.1, 'nthread': 8, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['logloss', 'auc', 'error'], 'max_depth': 5, 'min_child_weight': 1, 'gamma': 0.1, 'subsample': 0.6, 'colsample_bytree': 0.6} ...\t\t(2.884s)\n",
      "2 / 25: 150-rounds Training finished param={'objective': 'binary:hinge', 'eta': 0.1, 'nthread': 8, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['logloss', 'auc', 'error'], 'max_depth': 5, 'min_child_weight': 1, 'gamma': 0.1, 'subsample': 0.7, 'colsample_bytree': 0.6} ...\t\t(2.922s)\n",
      "3 / 25: 150-rounds Training finished param={'objective': 'binary:hinge', 'eta': 0.1, 'nthread': 8, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['logloss', 'auc', 'error'], 'max_depth': 5, 'min_child_weight': 1, 'gamma': 0.1, 'subsample': 0.8, 'colsample_bytree': 0.6} ...\t\t(2.947s)\n",
      "4 / 25: 150-rounds Training finished param={'objective': 'binary:hinge', 'eta': 0.1, 'nthread': 8, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['logloss', 'auc', 'error'], 'max_depth': 5, 'min_child_weight': 1, 'gamma': 0.1, 'subsample': 0.9, 'colsample_bytree': 0.6} ...\t\t(2.897s)\n",
      "5 / 25: 150-rounds Training finished param={'objective': 'binary:hinge', 'eta': 0.1, 'nthread': 8, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['logloss', 'auc', 'error'], 'max_depth': 5, 'min_child_weight': 1, 'gamma': 0.1, 'subsample': 1.0, 'colsample_bytree': 0.6} ...\t\t(2.930s)\n",
      "6 / 25: 150-rounds Training finished param={'objective': 'binary:hinge', 'eta': 0.1, 'nthread': 8, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['logloss', 'auc', 'error'], 'max_depth': 5, 'min_child_weight': 1, 'gamma': 0.1, 'subsample': 0.6, 'colsample_bytree': 0.7} ...\t\t(2.918s)\n",
      "7 / 25: 150-rounds Training finished param={'objective': 'binary:hinge', 'eta': 0.1, 'nthread': 8, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['logloss', 'auc', 'error'], 'max_depth': 5, 'min_child_weight': 1, 'gamma': 0.1, 'subsample': 0.7, 'colsample_bytree': 0.7} ...\t\t(2.911s)\n",
      "8 / 25: 150-rounds Training finished param={'objective': 'binary:hinge', 'eta': 0.1, 'nthread': 8, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['logloss', 'auc', 'error'], 'max_depth': 5, 'min_child_weight': 1, 'gamma': 0.1, 'subsample': 0.8, 'colsample_bytree': 0.7} ...\t\t(2.913s)\n",
      "9 / 25: 150-rounds Training finished param={'objective': 'binary:hinge', 'eta': 0.1, 'nthread': 8, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['logloss', 'auc', 'error'], 'max_depth': 5, 'min_child_weight': 1, 'gamma': 0.1, 'subsample': 0.9, 'colsample_bytree': 0.7} ...\t\t(2.894s)\n",
      "10 / 25: 150-rounds Training finished param={'objective': 'binary:hinge', 'eta': 0.1, 'nthread': 8, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['logloss', 'auc', 'error'], 'max_depth': 5, 'min_child_weight': 1, 'gamma': 0.1, 'subsample': 1.0, 'colsample_bytree': 0.7} ...\t\t(2.918s)\n",
      "11 / 25: 150-rounds Training finished param={'objective': 'binary:hinge', 'eta': 0.1, 'nthread': 8, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['logloss', 'auc', 'error'], 'max_depth': 5, 'min_child_weight': 1, 'gamma': 0.1, 'subsample': 0.6, 'colsample_bytree': 0.8} ...\t\t(2.878s)\n",
      "12 / 25: 150-rounds Training finished param={'objective': 'binary:hinge', 'eta': 0.1, 'nthread': 8, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['logloss', 'auc', 'error'], 'max_depth': 5, 'min_child_weight': 1, 'gamma': 0.1, 'subsample': 0.7, 'colsample_bytree': 0.8} ...\t\t(2.942s)\n",
      "13 / 25: 150-rounds Training finished param={'objective': 'binary:hinge', 'eta': 0.1, 'nthread': 8, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['logloss', 'auc', 'error'], 'max_depth': 5, 'min_child_weight': 1, 'gamma': 0.1, 'subsample': 0.8, 'colsample_bytree': 0.8} ...\t\t(2.914s)\n",
      "14 / 25: 150-rounds Training finished param={'objective': 'binary:hinge', 'eta': 0.1, 'nthread': 8, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['logloss', 'auc', 'error'], 'max_depth': 5, 'min_child_weight': 1, 'gamma': 0.1, 'subsample': 0.9, 'colsample_bytree': 0.8} ...\t\t(2.922s)\n",
      "15 / 25: 150-rounds Training finished param={'objective': 'binary:hinge', 'eta': 0.1, 'nthread': 8, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['logloss', 'auc', 'error'], 'max_depth': 5, 'min_child_weight': 1, 'gamma': 0.1, 'subsample': 1.0, 'colsample_bytree': 0.8} ...\t\t(2.936s)\n",
      "16 / 25: 150-rounds Training finished param={'objective': 'binary:hinge', 'eta': 0.1, 'nthread': 8, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['logloss', 'auc', 'error'], 'max_depth': 5, 'min_child_weight': 1, 'gamma': 0.1, 'subsample': 0.6, 'colsample_bytree': 0.9} ...\t\t(2.921s)\n",
      "17 / 25: 150-rounds Training finished param={'objective': 'binary:hinge', 'eta': 0.1, 'nthread': 8, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['logloss', 'auc', 'error'], 'max_depth': 5, 'min_child_weight': 1, 'gamma': 0.1, 'subsample': 0.7, 'colsample_bytree': 0.9} ...\t\t(2.938s)\n",
      "18 / 25: 150-rounds Training finished param={'objective': 'binary:hinge', 'eta': 0.1, 'nthread': 8, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['logloss', 'auc', 'error'], 'max_depth': 5, 'min_child_weight': 1, 'gamma': 0.1, 'subsample': 0.8, 'colsample_bytree': 0.9} ...\t\t(2.965s)\n",
      "19 / 25: 150-rounds Training finished param={'objective': 'binary:hinge', 'eta': 0.1, 'nthread': 8, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['logloss', 'auc', 'error'], 'max_depth': 5, 'min_child_weight': 1, 'gamma': 0.1, 'subsample': 0.9, 'colsample_bytree': 0.9} ...\t\t(3.075s)\n",
      "20 / 25: 150-rounds Training finished param={'objective': 'binary:hinge', 'eta': 0.1, 'nthread': 8, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['logloss', 'auc', 'error'], 'max_depth': 5, 'min_child_weight': 1, 'gamma': 0.1, 'subsample': 1.0, 'colsample_bytree': 0.9} ...\t\t(2.903s)\n",
      "21 / 25: 150-rounds Training finished param={'objective': 'binary:hinge', 'eta': 0.1, 'nthread': 8, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['logloss', 'auc', 'error'], 'max_depth': 5, 'min_child_weight': 1, 'gamma': 0.1, 'subsample': 0.6, 'colsample_bytree': 1.0} ...\t\t(2.902s)\n",
      "22 / 25: 150-rounds Training finished param={'objective': 'binary:hinge', 'eta': 0.1, 'nthread': 8, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['logloss', 'auc', 'error'], 'max_depth': 5, 'min_child_weight': 1, 'gamma': 0.1, 'subsample': 0.7, 'colsample_bytree': 1.0} ...\t\t(2.910s)\n",
      "23 / 25: 150-rounds Training finished param={'objective': 'binary:hinge', 'eta': 0.1, 'nthread': 8, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['logloss', 'auc', 'error'], 'max_depth': 5, 'min_child_weight': 1, 'gamma': 0.1, 'subsample': 0.8, 'colsample_bytree': 1.0} ...\t\t(2.951s)\n",
      "24 / 25: 150-rounds Training finished param={'objective': 'binary:hinge', 'eta': 0.1, 'nthread': 8, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['logloss', 'auc', 'error'], 'max_depth': 5, 'min_child_weight': 1, 'gamma': 0.1, 'subsample': 0.9, 'colsample_bytree': 1.0} ...\t\t(2.914s)\n",
      "25 / 25: 150-rounds Training finished param={'objective': 'binary:hinge', 'eta': 0.1, 'nthread': 8, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['logloss', 'auc', 'error'], 'max_depth': 5, 'min_child_weight': 1, 'gamma': 0.1, 'subsample': 1.0, 'colsample_bytree': 1.0} ...\t\t(2.935s)\n",
      "colsample_bytree=0.6, eta=0.1, eval_metric=['logloss', 'auc', 'error'], gamma=0.1, gpu_id=0, max_depth=5, min_child_weight=1, nthread=8, objective=binary:hinge, subsample=0.6, tree_method=gpu_hist\n",
      "colsample_bytree=0.6, eta=0.1, eval_metric=['logloss', 'auc', 'error'], gamma=0.1, gpu_id=0, max_depth=5, min_child_weight=1, nthread=8, objective=binary:hinge, subsample=0.8, tree_method=gpu_hist\n",
      "开始搜索：reg_alpha=[0, 0.1, 0.2, 0.5, 1, 1.5, 2, 4]\n",
      "待搜索的参数组合数量：8\n",
      "1 / 8: 150-rounds Training finished param={'objective': 'binary:hinge', 'eta': 0.1, 'nthread': 8, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['logloss', 'auc', 'error'], 'max_depth': 5, 'min_child_weight': 1, 'gamma': 0.1, 'subsample': 0.6, 'colsample_bytree': 0.6, 'reg_alpha': 0} ...\t\t(2.778s)\n",
      "2 / 8: 150-rounds Training finished param={'objective': 'binary:hinge', 'eta': 0.1, 'nthread': 8, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['logloss', 'auc', 'error'], 'max_depth': 5, 'min_child_weight': 1, 'gamma': 0.1, 'subsample': 0.6, 'colsample_bytree': 0.6, 'reg_alpha': 0.1} ...\t\t(2.773s)\n",
      "3 / 8: 150-rounds Training finished param={'objective': 'binary:hinge', 'eta': 0.1, 'nthread': 8, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['logloss', 'auc', 'error'], 'max_depth': 5, 'min_child_weight': 1, 'gamma': 0.1, 'subsample': 0.6, 'colsample_bytree': 0.6, 'reg_alpha': 0.2} ...\t\t(2.782s)\n",
      "4 / 8: 150-rounds Training finished param={'objective': 'binary:hinge', 'eta': 0.1, 'nthread': 8, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['logloss', 'auc', 'error'], 'max_depth': 5, 'min_child_weight': 1, 'gamma': 0.1, 'subsample': 0.6, 'colsample_bytree': 0.6, 'reg_alpha': 0.5} ...\t\t(2.783s)\n",
      "5 / 8: 150-rounds Training finished param={'objective': 'binary:hinge', 'eta': 0.1, 'nthread': 8, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['logloss', 'auc', 'error'], 'max_depth': 5, 'min_child_weight': 1, 'gamma': 0.1, 'subsample': 0.6, 'colsample_bytree': 0.6, 'reg_alpha': 1} ...\t\t(2.803s)\n",
      "6 / 8: 150-rounds Training finished param={'objective': 'binary:hinge', 'eta': 0.1, 'nthread': 8, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['logloss', 'auc', 'error'], 'max_depth': 5, 'min_child_weight': 1, 'gamma': 0.1, 'subsample': 0.6, 'colsample_bytree': 0.6, 'reg_alpha': 1.5} ...\t\t(2.803s)\n",
      "7 / 8: 150-rounds Training finished param={'objective': 'binary:hinge', 'eta': 0.1, 'nthread': 8, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['logloss', 'auc', 'error'], 'max_depth': 5, 'min_child_weight': 1, 'gamma': 0.1, 'subsample': 0.6, 'colsample_bytree': 0.6, 'reg_alpha': 2} ...\t\t(2.817s)\n",
      "8 / 8: 150-rounds Training finished param={'objective': 'binary:hinge', 'eta': 0.1, 'nthread': 8, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['logloss', 'auc', 'error'], 'max_depth': 5, 'min_child_weight': 1, 'gamma': 0.1, 'subsample': 0.6, 'colsample_bytree': 0.6, 'reg_alpha': 4} ...\t\t(2.815s)\n",
      "colsample_bytree=0.6, eta=0.1, eval_metric=['logloss', 'auc', 'error'], gamma=0.1, gpu_id=0, max_depth=5, min_child_weight=1, nthread=8, objective=binary:hinge, reg_alpha=0, subsample=0.6, tree_method=gpu_hist\n",
      "colsample_bytree=0.6, eta=0.1, eval_metric=['logloss', 'auc', 'error'], gamma=0.1, gpu_id=0, max_depth=5, min_child_weight=1, nthread=8, objective=binary:hinge, reg_alpha=0.5, subsample=0.6, tree_method=gpu_hist\n",
      "找到的最棒的参数是：\n",
      "colsample_bytree=0.6, eta=0.1, eval_metric=['logloss', 'auc', 'error'], gamma=0.1, gpu_id=0, max_depth=5, min_child_weight=1, nthread=8, objective=binary:hinge, reg_alpha=0, subsample=0.6, tree_method=gpu_hist\n"
     ]
    }
   ],
   "source": [
    "base_sh = base_param_sh.copy()\n",
    "grids_sh = [ps1, ps2, ps3, ps4]\n",
    "\n",
    "rets_sh = []\n",
    "for grid in grids_sh:\n",
    "    params = compose_param_grid(grid, base_sh)\n",
    "    print(f\"开始搜索：{dict_2_str(grid)}\\n待搜索的参数组合数量：{len(params)}\")\n",
    "    ret = gridsearch_cv_xgb(data_sh.values, is_share_res.values, params, n_round=150, verbose_eval=False, n_class=2)\n",
    "    arr = np.array([[-e['eval-error'] for e in ret], \n",
    "                    [-e['eval-logloss'] for e in ret],\n",
    "                    [e['eval-auc'] for e in ret], \n",
    "                    [e['w_auc'] for e in ret]], dtype=np.float32)\n",
    "    opt_idxs = list(set(arr.argmax(axis=1)))\n",
    "    for i in opt_idxs:\n",
    "        print(dict_2_str(ret[i]['param']))\n",
    "    opt_idx = opt_idxs[0]\n",
    "    opt_param = ret[opt_idx]['param']\n",
    "    base_sh.update(opt_param)\n",
    "    rets_sh.append(ret)\n",
    "    \n",
    "    ks = list(grid.keys())\n",
    "    ks = '-'.join(ks)\n",
    "    with open(f\"./logs/{ks}-is_share-{int(time())}.pkl\", \"wb\") as f:\n",
    "        pickle.dump(ret, f)\n",
    "    with open(f\"./logs/{ks}-is_share-{int(time())}.md\", \"w\") as f:\n",
    "        lines = []\n",
    "        for e in ret:\n",
    "            line = metric_2_str(e)\n",
    "            lines.append(line)\n",
    "            lines.append('\\n')\n",
    "        f.write(\"\\n\".join(lines))\n",
    "\n",
    "print(f\"找到的最棒的参数是：\\n{dict_2_str(base_sh)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'objective': 'binary:hinge',\n",
       " 'eta': 0.1,\n",
       " 'nthread': 8,\n",
       " 'gpu_id': 0,\n",
       " 'tree_method': 'gpu_hist',\n",
       " 'eval_metric': ['logloss', 'auc', 'error'],\n",
       " 'max_depth': 5,\n",
       " 'min_child_weight': 1,\n",
       " 'gamma': 0.1,\n",
       " 'subsample': 0.6,\n",
       " 'colsample_bytree': 0.6,\n",
       " 'reg_alpha': 0}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gridsearch_results_sh = gridsearch_xgb(all_params_sh, xg_train_sh, xg_test_sh, num_round=150, n_class=2, verbose_eval=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_cv_results_sh = gridsearch_cv_xgb(data_sh.values, is_share_res.values, all_params_sh, n_round=150, verbose_eval=False, n_class=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 753,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('greadsearch-cv-is_share.md', 'w') as f:\n",
    "    for ret in performance:\n",
    "        f.write(f\"# {', '.join([f'{k}={v}' for k, v in ret[0].items()])}\\n\")\n",
    "        for k, v in ret[1].items():\n",
    "            is_break = '\\n' if '\\n' in str(df) else ''\n",
    "            f.write(f\"- {k} :{is_break} {v}\\n\\n\")\n",
    "        f.write(f\"{'-'*50}\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 765,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48"
      ]
     },
     "execution_count": 765,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_test_error = np.array([e[1]['mean_test_error'] for e in performance])\n",
    "mean_test_error.argmin()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 617,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'objective': 'binary:hinge',\n",
       " 'eta': 0.1,\n",
       " 'nthread': 8,\n",
       " 'gpu_id': 0,\n",
       " 'tree_method': 'gpu_hist',\n",
       " 'gamma': 0.3,\n",
       " 'max_depth': 11,\n",
       " 'min_child_weight': 9}"
      ]
     },
     "execution_count": 617,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr = np.array([[-e['test_error'] for e in gridsearch_results_sh], [e['aucs'][1] for e in gridsearch_results_sh]], dtype=np.float32)\n",
    "opt_idxs_sh = arr.argmax(axis=1)\n",
    "if opt_idxs_sh[0] != opt_idxs_sh[1]:\n",
    "    warnings.warn(f\"最小误差与最大AUC对应的模型不一致 : {opt_idxs_sh}。选择误差最小的模型 : {opt_idxs_sh[0]}\")\n",
    "\n",
    "opt_idx_sh = opt_idxs_sh[0]\n",
    "all_params_sh[opt_idx_sh]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 603,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "146"
      ]
     },
     "execution_count": 603,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opt_idx_sh = opt_idxs_sh[0]\n",
    "opt_idx_sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 619,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.54      0.44      0.49      3053\n",
      "         1.0       0.50      0.60      0.54      2835\n",
      "\n",
      "    accuracy                           0.52      5888\n",
      "   macro avg       0.52      0.52      0.51      5888\n",
      "weighted avg       0.52      0.52      0.51      5888\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(gridsearch_results_sh[opt_idx_sh]['report'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 620,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>test_error</th>\n",
       "      <th>aucs</th>\n",
       "      <th>w_auc</th>\n",
       "      <th>report</th>\n",
       "      <th>model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.503057</td>\n",
       "      <td>[0.50379590202715, 0.50379590202715]</td>\n",
       "      <td>None</td>\n",
       "      <td>precision    recall  f1-score   ...</td>\n",
       "      <td>&lt;xgboost.core.Booster object at 0x7f35ba907130&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.496943</td>\n",
       "      <td>[0.508734635779073, 0.508734635779073]</td>\n",
       "      <td>None</td>\n",
       "      <td>precision    recall  f1-score   ...</td>\n",
       "      <td>&lt;xgboost.core.Booster object at 0x7f32e4009580&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.501189</td>\n",
       "      <td>[0.5048543919272165, 0.5048543919272165]</td>\n",
       "      <td>None</td>\n",
       "      <td>precision    recall  f1-score   ...</td>\n",
       "      <td>&lt;xgboost.core.Booster object at 0x7f3583f4f4f0&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.502717</td>\n",
       "      <td>[0.502360357955947, 0.502360357955947]</td>\n",
       "      <td>None</td>\n",
       "      <td>precision    recall  f1-score   ...</td>\n",
       "      <td>&lt;xgboost.core.Booster object at 0x7f3574279550&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.499321</td>\n",
       "      <td>[0.5061017844072763, 0.5061017844072763]</td>\n",
       "      <td>None</td>\n",
       "      <td>precision    recall  f1-score   ...</td>\n",
       "      <td>&lt;xgboost.core.Booster object at 0x7f32e4082730&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>0.498132</td>\n",
       "      <td>[0.5057747576472328, 0.5057747576472328]</td>\n",
       "      <td>None</td>\n",
       "      <td>precision    recall  f1-score   ...</td>\n",
       "      <td>&lt;xgboost.core.Booster object at 0x7f32468a4640&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>0.497962</td>\n",
       "      <td>[0.5046665869463117, 0.5046665869463118]</td>\n",
       "      <td>None</td>\n",
       "      <td>precision    recall  f1-score   ...</td>\n",
       "      <td>&lt;xgboost.core.Booster object at 0x7f32468a4850&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>0.499490</td>\n",
       "      <td>[0.5026636996830249, 0.5026636996830249]</td>\n",
       "      <td>None</td>\n",
       "      <td>precision    recall  f1-score   ...</td>\n",
       "      <td>&lt;xgboost.core.Booster object at 0x7f32468a46a0&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>0.491678</td>\n",
       "      <td>[0.5107513874519006, 0.5107513874519005]</td>\n",
       "      <td>None</td>\n",
       "      <td>precision    recall  f1-score   ...</td>\n",
       "      <td>&lt;xgboost.core.Booster object at 0x7f32468a4670&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>0.499151</td>\n",
       "      <td>[0.5034068320344114, 0.5034068320344114]</td>\n",
       "      <td>None</td>\n",
       "      <td>precision    recall  f1-score   ...</td>\n",
       "      <td>&lt;xgboost.core.Booster object at 0x7f32468a4820&gt;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>120 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     test_error                                      aucs w_auc  \\\n",
       "0      0.503057      [0.50379590202715, 0.50379590202715]  None   \n",
       "1      0.496943    [0.508734635779073, 0.508734635779073]  None   \n",
       "2      0.501189  [0.5048543919272165, 0.5048543919272165]  None   \n",
       "3      0.502717    [0.502360357955947, 0.502360357955947]  None   \n",
       "4      0.499321  [0.5061017844072763, 0.5061017844072763]  None   \n",
       "..          ...                                       ...   ...   \n",
       "115    0.498132  [0.5057747576472328, 0.5057747576472328]  None   \n",
       "116    0.497962  [0.5046665869463117, 0.5046665869463118]  None   \n",
       "117    0.499490  [0.5026636996830249, 0.5026636996830249]  None   \n",
       "118    0.491678  [0.5107513874519006, 0.5107513874519005]  None   \n",
       "119    0.499151  [0.5034068320344114, 0.5034068320344114]  None   \n",
       "\n",
       "                                                report  \\\n",
       "0                  precision    recall  f1-score   ...   \n",
       "1                  precision    recall  f1-score   ...   \n",
       "2                  precision    recall  f1-score   ...   \n",
       "3                  precision    recall  f1-score   ...   \n",
       "4                  precision    recall  f1-score   ...   \n",
       "..                                                 ...   \n",
       "115                precision    recall  f1-score   ...   \n",
       "116                precision    recall  f1-score   ...   \n",
       "117                precision    recall  f1-score   ...   \n",
       "118                precision    recall  f1-score   ...   \n",
       "119                precision    recall  f1-score   ...   \n",
       "\n",
       "                                               model  \n",
       "0    <xgboost.core.Booster object at 0x7f35ba907130>  \n",
       "1    <xgboost.core.Booster object at 0x7f32e4009580>  \n",
       "2    <xgboost.core.Booster object at 0x7f3583f4f4f0>  \n",
       "3    <xgboost.core.Booster object at 0x7f3574279550>  \n",
       "4    <xgboost.core.Booster object at 0x7f32e4082730>  \n",
       "..                                               ...  \n",
       "115  <xgboost.core.Booster object at 0x7f32468a4640>  \n",
       "116  <xgboost.core.Booster object at 0x7f32468a4850>  \n",
       "117  <xgboost.core.Booster object at 0x7f32468a46a0>  \n",
       "118  <xgboost.core.Booster object at 0x7f32468a4670>  \n",
       "119  <xgboost.core.Booster object at 0x7f32468a4820>  \n",
       "\n",
       "[120 rows x 5 columns]"
      ]
     },
     "execution_count": 620,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(gridsearch_results_sh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB \n",
    "from mlxtend.classifier import StackingClassifier\n",
    "from sklearn.ensemble import (RandomForestClassifier, AdaBoostClassifier, \n",
    "                              GradientBoostingClassifier, ExtraTreesClassifier)\n",
    "from xgboost import XGBClassifier, XGBRFClassifier\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 第一层模型参数\n",
    "first_layer_params = {\n",
    "    LogisticRegression : {\n",
    "        'C' : 10,\n",
    "        'random_state': 0\n",
    "    },\n",
    "    RandomForestClassifier : {\n",
    "        'n_jobs': -1,\n",
    "        'n_estimators': 200,\n",
    "         'warm_start': True, \n",
    "         #'max_features': 0.2,\n",
    "        'max_depth': 8,\n",
    "        'min_samples_leaf': 2,\n",
    "        'max_features' : 'sqrt',\n",
    "        'verbose': 0\n",
    "    },\n",
    "    ExtraTreesClassifier : {\n",
    "        'n_jobs': -1,\n",
    "        'n_estimators':200,\n",
    "        #'max_features': 0.5,\n",
    "        'max_depth': 8,\n",
    "        'min_samples_leaf': 2,\n",
    "        'verbose': 0\n",
    "    },\n",
    "    AdaBoostClassifier : {\n",
    "        'n_estimators': 200,\n",
    "        'learning_rate' : 0.75\n",
    "    },\n",
    "#     GradientBoostingClassifier : { # 太慢了\n",
    "#         'n_estimators': 200,\n",
    "#          #'max_features': 0.2,\n",
    "#         'max_depth': 5,\n",
    "#         'min_samples_leaf': 2,\n",
    "#         'verbose': 0\n",
    "#     },\n",
    "#     SVC : { # 太慢了\n",
    "#         'kernel' : 'rbf',\n",
    "#         'C' : 0.025,\n",
    "#         'probability': True\n",
    "#     } \n",
    "}\n",
    "\n",
    "\n",
    "second_layer_params = {\n",
    "    XGBClassifier : {\n",
    "        'objective': 'multi:softmax',\n",
    "        'eta': 0.1,\n",
    "        'nthread': 8,\n",
    "        'num_class': 10,\n",
    "        'gpu_id': 0,\n",
    "        'tree_method': 'gpu_hist',\n",
    "        'eval_metric': ['mlogloss', 'auc', 'merror'],\n",
    "        'max_depth': 9,\n",
    "        'min_child_weight': 9,\n",
    "        'gamma': 0.2,\n",
    "        'subsample': 0.9,\n",
    "        'colsample_bytree': 0.6,\n",
    "        'reg_alpha': 0,\n",
    "        'n_estimators': 200,\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "clfs = []\n",
    "for c, p in first_layer_params.items():\n",
    "    clfs.append(c(**p))\n",
    "\n",
    "meta = XGBClassifier(**second_layer_params[XGBClassifier])\n",
    "sclf = StackingClassifier(classifiers=clfs, \n",
    "                          meta_classifier=meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gzy/.local/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "StackingClassifier(classifiers=[RandomForestClassifier(max_depth=8,\n",
       "                                                       max_features='sqrt',\n",
       "                                                       min_samples_leaf=2,\n",
       "                                                       n_estimators=200,\n",
       "                                                       n_jobs=-1,\n",
       "                                                       warm_start=True),\n",
       "                                ExtraTreesClassifier(max_depth=8,\n",
       "                                                     min_samples_leaf=2,\n",
       "                                                     n_estimators=200,\n",
       "                                                     n_jobs=-1),\n",
       "                                AdaBoostClassifier(learning_rate=0.75,\n",
       "                                                   n_estimators=200),\n",
       "                                GradientBoostingClassifier(max_depth=5,\n",
       "                                                           min_samples_leaf=2,\n",
       "                                                           n_esti...\n",
       "                                                 interaction_constraints=None,\n",
       "                                                 learning_rate=None,\n",
       "                                                 max_delta_step=None,\n",
       "                                                 max_depth=9,\n",
       "                                                 min_child_weight=9,\n",
       "                                                 missing=nan,\n",
       "                                                 monotone_constraints=None,\n",
       "                                                 n_estimators=200, n_jobs=None,\n",
       "                                                 nthread=8, num_class=10,\n",
       "                                                 num_parallel_tree=None,\n",
       "                                                 objective='multi:softmax',\n",
       "                                                 random_state=None, reg_alpha=0,\n",
       "                                                 reg_lambda=None,\n",
       "                                                 scale_pos_weight=None,\n",
       "                                                 subsample=0.9,\n",
       "                                                 tree_method='gpu_hist',\n",
       "                                                 validate_parameters=None, ...))"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sclf.fit(X_train_sh, y_train_sh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6674231843575419"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sclf.score(X_test_sh, y_test_sh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 13.1 s, sys: 112 ms, total: 13.2 s\n",
      "Wall time: 987 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(max_depth=8, max_features='sqrt', min_samples_leaf=2,\n",
       "                       n_estimators=200, n_jobs=-1, warm_start=True)"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "clf = RandomForestClassifier(**first_layer_params[RandomForestClassifier])\n",
    "clf.fit(X_train_sh, y_train_sh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.61      0.63      2845\n",
      "           1       0.64      0.69      0.66      2883\n",
      "\n",
      "    accuracy                           0.65      5728\n",
      "   macro avg       0.65      0.65      0.65      5728\n",
      "weighted avg       0.65      0.65      0.65      5728\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tmp = clf.predict(X_test_sh)\n",
    "print(metrics.classification_report(list(y_test_sh), list(tmp)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>v_avg_watch_label_1</th>\n",
       "      <th>v_sum_watch_times_1</th>\n",
       "      <th>v_sum_watch_overs_1</th>\n",
       "      <th>v_sum_comment_times_1</th>\n",
       "      <th>v_sum_collect_times_1</th>\n",
       "      <th>v_sum_share_times_1</th>\n",
       "      <th>v_sum_quit_times_1</th>\n",
       "      <th>v_sum_skip_times_1</th>\n",
       "      <th>v_sum_watch_days_1</th>\n",
       "      <th>v_avg_watch_label_3</th>\n",
       "      <th>...</th>\n",
       "      <th>class_6</th>\n",
       "      <th>class_7</th>\n",
       "      <th>class_8</th>\n",
       "      <th>class_9</th>\n",
       "      <th>da_0</th>\n",
       "      <th>da_1</th>\n",
       "      <th>da_2</th>\n",
       "      <th>da_3</th>\n",
       "      <th>da_4</th>\n",
       "      <th>watch_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>17211</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.958333</td>\n",
       "      <td>...</td>\n",
       "      <td>0.041922</td>\n",
       "      <td>0.041925</td>\n",
       "      <td>0.041925</td>\n",
       "      <td>0.041923</td>\n",
       "      <td>0.084701</td>\n",
       "      <td>0.084811</td>\n",
       "      <td>0.661892</td>\n",
       "      <td>0.084126</td>\n",
       "      <td>0.084470</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12479</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.306212</td>\n",
       "      <td>0.037078</td>\n",
       "      <td>0.037070</td>\n",
       "      <td>0.037070</td>\n",
       "      <td>0.074563</td>\n",
       "      <td>0.075237</td>\n",
       "      <td>0.316654</td>\n",
       "      <td>0.074140</td>\n",
       "      <td>0.459407</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2532</th>\n",
       "      <td>0.208611</td>\n",
       "      <td>1951.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1866.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.213834</td>\n",
       "      <td>...</td>\n",
       "      <td>0.350094</td>\n",
       "      <td>0.041778</td>\n",
       "      <td>0.041778</td>\n",
       "      <td>0.041771</td>\n",
       "      <td>0.324958</td>\n",
       "      <td>0.083541</td>\n",
       "      <td>0.083541</td>\n",
       "      <td>0.083541</td>\n",
       "      <td>0.424418</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28572</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.041922</td>\n",
       "      <td>0.041925</td>\n",
       "      <td>0.041925</td>\n",
       "      <td>0.041923</td>\n",
       "      <td>0.084701</td>\n",
       "      <td>0.084811</td>\n",
       "      <td>0.661892</td>\n",
       "      <td>0.084126</td>\n",
       "      <td>0.084470</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19280</th>\n",
       "      <td>0.509397</td>\n",
       "      <td>3565.0</td>\n",
       "      <td>106.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>3082.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.548684</td>\n",
       "      <td>...</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050003</td>\n",
       "      <td>0.050009</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11302</th>\n",
       "      <td>1.333333</td>\n",
       "      <td>123.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.124863</td>\n",
       "      <td>...</td>\n",
       "      <td>0.270403</td>\n",
       "      <td>0.036901</td>\n",
       "      <td>0.036893</td>\n",
       "      <td>0.271428</td>\n",
       "      <td>0.073786</td>\n",
       "      <td>0.074915</td>\n",
       "      <td>0.078284</td>\n",
       "      <td>0.073786</td>\n",
       "      <td>0.699228</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4584</th>\n",
       "      <td>0.902047</td>\n",
       "      <td>684.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>531.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.040830</td>\n",
       "      <td>...</td>\n",
       "      <td>0.212644</td>\n",
       "      <td>0.031771</td>\n",
       "      <td>0.031764</td>\n",
       "      <td>0.409137</td>\n",
       "      <td>0.065150</td>\n",
       "      <td>0.328750</td>\n",
       "      <td>0.068145</td>\n",
       "      <td>0.064165</td>\n",
       "      <td>0.473790</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26296</th>\n",
       "      <td>0.674253</td>\n",
       "      <td>32513.0</td>\n",
       "      <td>388.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>245.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>22996.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.725334</td>\n",
       "      <td>...</td>\n",
       "      <td>0.037483</td>\n",
       "      <td>0.037487</td>\n",
       "      <td>0.395917</td>\n",
       "      <td>0.037484</td>\n",
       "      <td>0.075851</td>\n",
       "      <td>0.075966</td>\n",
       "      <td>0.388781</td>\n",
       "      <td>0.383735</td>\n",
       "      <td>0.075666</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9816</th>\n",
       "      <td>1.617686</td>\n",
       "      <td>12903.0</td>\n",
       "      <td>1328.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>174.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>8865.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.619248</td>\n",
       "      <td>...</td>\n",
       "      <td>0.248009</td>\n",
       "      <td>0.036880</td>\n",
       "      <td>0.036881</td>\n",
       "      <td>0.270608</td>\n",
       "      <td>0.238563</td>\n",
       "      <td>0.073745</td>\n",
       "      <td>0.073745</td>\n",
       "      <td>0.073745</td>\n",
       "      <td>0.540203</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10186</th>\n",
       "      <td>1.143805</td>\n",
       "      <td>1356.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1044.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.158740</td>\n",
       "      <td>...</td>\n",
       "      <td>0.036693</td>\n",
       "      <td>0.528539</td>\n",
       "      <td>0.036671</td>\n",
       "      <td>0.178063</td>\n",
       "      <td>0.074926</td>\n",
       "      <td>0.074954</td>\n",
       "      <td>0.124523</td>\n",
       "      <td>0.075064</td>\n",
       "      <td>0.650534</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5728 rows × 128 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       v_avg_watch_label_1  v_sum_watch_times_1  v_sum_watch_overs_1  \\\n",
       "17211             0.000000                  4.0                  0.0   \n",
       "12479             0.000000                  0.0                  0.0   \n",
       "2532              0.208611               1951.0                 27.0   \n",
       "28572             0.000000                  0.0                  0.0   \n",
       "19280             0.509397               3565.0                106.0   \n",
       "...                    ...                  ...                  ...   \n",
       "11302             1.333333                123.0                 10.0   \n",
       "4584              0.902047                684.0                 30.0   \n",
       "26296             0.674253              32513.0                388.0   \n",
       "9816              1.617686              12903.0               1328.0   \n",
       "10186             1.143805               1356.0                100.0   \n",
       "\n",
       "       v_sum_comment_times_1  v_sum_collect_times_1  v_sum_share_times_1  \\\n",
       "17211                    0.0                    0.0                  0.0   \n",
       "12479                    0.0                    0.0                  0.0   \n",
       "2532                     2.0                   24.0                  4.0   \n",
       "28572                    0.0                    0.0                  0.0   \n",
       "19280                    2.0                   55.0                 35.0   \n",
       "...                      ...                    ...                  ...   \n",
       "11302                    0.0                    0.0                  1.0   \n",
       "4584                     1.0                    5.0                  1.0   \n",
       "26296                   11.0                  245.0                 55.0   \n",
       "9816                    34.0                  174.0                 46.0   \n",
       "10186                    7.0                   22.0                  1.0   \n",
       "\n",
       "       v_sum_quit_times_1  v_sum_skip_times_1  v_sum_watch_days_1  \\\n",
       "17211                 4.0                 0.0                 1.0   \n",
       "12479                 0.0                 0.0                 0.0   \n",
       "2532               1866.0                 0.0                 1.0   \n",
       "28572                 0.0                 0.0                 0.0   \n",
       "19280              3082.0                 0.0                 1.0   \n",
       "...                   ...                 ...                 ...   \n",
       "11302                88.0                 0.0                 1.0   \n",
       "4584                531.0                 0.0                 1.0   \n",
       "26296             22996.0                 0.0                 1.0   \n",
       "9816               8865.0                 0.0                 1.0   \n",
       "10186              1044.0                 0.0                 1.0   \n",
       "\n",
       "       v_avg_watch_label_3  ...   class_6   class_7   class_8   class_9  \\\n",
       "17211             0.958333  ...  0.041922  0.041925  0.041925  0.041923   \n",
       "12479             0.000000  ...  0.306212  0.037078  0.037070  0.037070   \n",
       "2532              0.213834  ...  0.350094  0.041778  0.041778  0.041771   \n",
       "28572             0.000000  ...  0.041922  0.041925  0.041925  0.041923   \n",
       "19280             0.548684  ...  0.050000  0.050003  0.050009  0.050000   \n",
       "...                    ...  ...       ...       ...       ...       ...   \n",
       "11302             1.124863  ...  0.270403  0.036901  0.036893  0.271428   \n",
       "4584              1.040830  ...  0.212644  0.031771  0.031764  0.409137   \n",
       "26296             0.725334  ...  0.037483  0.037487  0.395917  0.037484   \n",
       "9816              1.619248  ...  0.248009  0.036880  0.036881  0.270608   \n",
       "10186             1.158740  ...  0.036693  0.528539  0.036671  0.178063   \n",
       "\n",
       "           da_0      da_1      da_2      da_3      da_4  watch_label  \n",
       "17211  0.084701  0.084811  0.661892  0.084126  0.084470          2.0  \n",
       "12479  0.074563  0.075237  0.316654  0.074140  0.459407          0.0  \n",
       "2532   0.324958  0.083541  0.083541  0.083541  0.424418          0.0  \n",
       "28572  0.084701  0.084811  0.661892  0.084126  0.084470          0.0  \n",
       "19280  0.600000  0.100000  0.100000  0.100000  0.100000          0.0  \n",
       "...         ...       ...       ...       ...       ...          ...  \n",
       "11302  0.073786  0.074915  0.078284  0.073786  0.699228          9.0  \n",
       "4584   0.065150  0.328750  0.068145  0.064165  0.473790          3.0  \n",
       "26296  0.075851  0.075966  0.388781  0.383735  0.075666          0.0  \n",
       "9816   0.238563  0.073745  0.073745  0.073745  0.540203          0.0  \n",
       "10186  0.074926  0.074954  0.124523  0.075064  0.650534          0.0  \n",
       "\n",
       "[5728 rows x 128 columns]"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = inference_dataset\n",
    "test_sh = inference_dataset.copy()\n",
    "test = xgb.DMatrix(test.values, enable_categorical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2822180, 127), 127)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inference_dataset.shape, test.num_col()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "bst_wl = wl_bst_sm  # gridsearch_results[opt_idx]['model']  \n",
    "bst_sh = sh_bst_sm  # gridsearch_results_sh[opt_idx_sh]['model']  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Counter({1.0: 519693,\n",
       "          0.0: 1004985,\n",
       "          9.0: 1032586,\n",
       "          2.0: 216270,\n",
       "          3.0: 20171,\n",
       "          4.0: 6656,\n",
       "          6.0: 3961,\n",
       "          5.0: 4722,\n",
       "          7.0: 3072,\n",
       "          8.0: 10064}),\n",
       " Counter({1.0: 970873, 0.0: 1851307}))"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wl = bst_wl.predict(test)\n",
    "test_sh['watch_label'] = wl\n",
    "test_sh = xgb.DMatrix(test_sh.values, enable_categorical=True)\n",
    "sh = bst_sh.predict(test_sh)\n",
    "Counter(wl), Counter(Counter(sh))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2822180, 61)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df['watch_label'] = wl.astype(np.uint8)\n",
    "test_df['is_share'] = sh.astype(np.uint8)\n",
    "test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>video_id</th>\n",
       "      <th>watch_label</th>\n",
       "      <th>is_share</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1688013</td>\n",
       "      <td>32645</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4502598</td>\n",
       "      <td>41270</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5585629</td>\n",
       "      <td>16345</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1635520</td>\n",
       "      <td>28149</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4160191</td>\n",
       "      <td>40554</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2822175</th>\n",
       "      <td>5019057</td>\n",
       "      <td>18766</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2822176</th>\n",
       "      <td>5019057</td>\n",
       "      <td>12968</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2822177</th>\n",
       "      <td>4255762</td>\n",
       "      <td>21794</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2822178</th>\n",
       "      <td>171497</td>\n",
       "      <td>21578</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2822179</th>\n",
       "      <td>5642580</td>\n",
       "      <td>28914</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2822180 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         user_id  video_id  watch_label  is_share\n",
       "0        1688013     32645            1         1\n",
       "1        4502598     41270            0         1\n",
       "2        5585629     16345            9         0\n",
       "3        1635520     28149            2         1\n",
       "4        4160191     40554            1         1\n",
       "...          ...       ...          ...       ...\n",
       "2822175  5019057     18766            0         0\n",
       "2822176  5019057     12968            0         0\n",
       "2822177  4255762     21794            2         0\n",
       "2822178   171497     21578            3         0\n",
       "2822179  5642580     28914            9         0\n",
       "\n",
       "[2822180 rows x 4 columns]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission = pd.DataFrame(test_df[['user_id', 'video_id', 'watch_label', 'is_share']])\n",
    "submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new submission saved to ../submission-1629622213.csv\n"
     ]
    }
   ],
   "source": [
    "fn = f'../submission-{int(time())}.csv'\n",
    "submission.to_csv(fn, index=False, sep=\",\")\n",
    "print(f\"new submission saved to {fn}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 保存模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "version = 19\n",
    "wl_model_name = f'wl_model_v{version}'\n",
    "sh_model_name = f'sh_model_v{version}'\n",
    "bst_wl.save_model(wl_model_name)\n",
    "bst_sh.save_model(sh_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_log(log_name, info, log_path=\"./\"):\n",
    "    import datetime\n",
    "    with open(os.path.join(log_path, log_name), 'w') as log:\n",
    "        log.write(f\"# {datetime.datetime.now().__str__()}\\n\")\n",
    "        if info.get('comment', False):\n",
    "            log.write(f\"\\n## Comment: \\n\")\n",
    "            log.write(f\"{info['comment']}\\n\")\n",
    "            \n",
    "        log.write(f\"\\n## model name: {info['model_name']}\\n\")\n",
    "        log.write(f\"- model save path : {info['model_save_path']}\\n\")\n",
    "        \n",
    "        log.write(f\"\\n## Data setup\\n\")\n",
    "        log.write(f\"- dataset.shape : {dataset.shape}\\n\")\n",
    "        log.write(f\"- dataset.columns : {dataset.columns}\\n\")\n",
    "        log.write(f\"- is resample : {info['is_resample']}\\n\")\n",
    "        log.write(f\"- Traing_Data.shape (watch_label)  : {X_train.shape}\\n\")\n",
    "        log.write(f\"- Testing_Data.shape (watch_label) : {X_test.shape}\\n\")\n",
    "        log.write(f\"- Traing_Data.shape (is_share)  : {X_train_sh.shape}\\n\")\n",
    "        log.write(f\"- Testing_Data.shape (is_share) : {X_test_sh.shape}\\n\")\n",
    "        if info.get('is_resample', False):\n",
    "            log.write(f\"- Resampled class distribution (watch_label): \\n{Counter(resampled_wl)}\\n\")\n",
    "            log.write(f\"- Resampled class distribution (is_share): \\n{Counter(resampled_sh)}\\n\")\n",
    "            \n",
    "        log.write(f\"\\n## Model Params\\n\")\n",
    "        log.write(f\"- model params (watch_label) : \\n{info['param_wl']}\\n\")\n",
    "        log.write(f\"- model params (is_share) : \\n{info['param_sh']}\\n\")\n",
    "        \n",
    "        log.write(f\"\\n## Model's Performance\\n\")\n",
    "        log.write(f\"- Aucs (watch_label) : {info['aucs']}\\n\")\n",
    "        log.write(f\"- Weighted Aucs (watch_label) : {info['w_auc']}\\n\")\n",
    "        log.write(f\"- Aucs (is_share) : {info['aucs_sh']}\\n\")\n",
    "        \n",
    "        log.write(f\"- Classification Report (watch_label) : \\n\\n{info['report']}\\n\")\n",
    "        log.write(f\"- Classification Report (is_share) : \\n\\n{info['report_sh']}\\n\")\n",
    "        \n",
    "        log.flush()\n",
    "        \n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_wl = param  # all_params[opt_idx]\n",
    "param_sh = param_sh  # all_params[opt_idx_sh]\n",
    "\n",
    "aucs = aucs  # gridsearch_results[opt_idx]['aucs']\n",
    "w_auc = w_auc  # gridsearch_results[opt_idx]['w_auc']\n",
    "aucs_sh = aucs_sh  # gridsearch_results_sh[opt_idx]['aucs']\n",
    "\n",
    "report = report  # gridsearch_results[opt_idx]['report']\n",
    "report_sh = report_sh  # gridsearch_results_sh[opt_idx]['report']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_name = f\"log_v{version}.md\"\n",
    "info = {'is_resample': True, 'model_name': [wl_model_name, sh_model_name], 'model_save_path': os.getcwd(),\n",
    "        'comment': f\"特征：基础特征+用户和视频的统计量特征，除此之外，is_share的特征还加上了watch_label，训练集使用的真实的watch_label，测试集使用预测的watch_label。\\n数据集划分：watch_label的测试集为.15，is_share的测试集为.2。\\nwatch_label训练300rounds，is_share训练600rounds。\\n此次生成的提交是：{fn}。官方测评得分：xxx😐\",\n",
    "        'param_wl': param_wl, 'param_sh': param_sh, 'aucs': aucs, 'w_auc': w_auc, 'aucs_sh': aucs_sh, \n",
    "        'report': report, 'report_sh': report_sh}\n",
    "write_log(log_name, info, log_path=\"./logs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 服务器间同步文件"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 推向Digix服务器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ssh: connect to host 49.123.120.71 port 22: No route to host\n",
      "lost connection\n"
     ]
    }
   ],
   "source": [
    "!scp ./models.ipynb digix@49.123.120.71:/home/digix/digix/Models/models.ipynb "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "digix@49.123.120.71's password: "
     ]
    }
   ],
   "source": [
    "!scp ./ensemble.ipynb digix@49.123.120.71:/home/digix/digix/Models/ensemble_from_gzy.ipynb "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 599,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ssh: connect to host 49.123.120.71 port 22: No route to host\n",
      "lost connection\n"
     ]
    }
   ],
   "source": [
    "!scp ./log_*.md digix@49.123.120.71:/home/digix/digix/Models/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "explore-data.ipynb                            100%  306KB  10.6MB/s   00:00    \n"
     ]
    }
   ],
   "source": [
    "!scp ../explore-data.ipynb digix@49.123.120.71:/home/digix/digix/explore-data.ipynb "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 650,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "video_features.jay                            100% 9035KB  11.1MB/s   00:00    \n"
     ]
    }
   ],
   "source": [
    "!scp ../2021_3_data/traindata/video_features_data/video_features.jay digix@49.123.120.71:/home/digix/digix/dataset/new_video_features.jay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 从Digix服务器拉数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "digix@49.123.120.71's password: \n"
     ]
    }
   ],
   "source": [
    "!scp  digix@49.123.120.71:/home/digix/digix/Models/LightGBM.ipynb ./LightGBM.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scp: /home/digix/digix/Models/feature_engineering.ipynb: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!scp  digix@49.123.120.71:/home/digix/digix/Models/feature_engineering.ipynb ./feature_engineering.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "utils.py                                      100% 3860     2.6MB/s   00:00    \n",
      "data_analysis.ipynb                           100% 6566KB  11.2MB/s   00:00    \n",
      "__init__.py                                   100%    0     0.0KB/s   00:00    \n",
      "__init__.cpython-36.pyc                       100%  139   128.3KB/s   00:00    \n",
      "utils.cpython-36.pyc                          100% 4120     2.6MB/s   00:00    \n",
      "video_data.ipynb                              100%   55KB   1.7MB/s   00:00    \n",
      "user_data-checkpoint.ipynb                    100%  202KB  10.1MB/s   00:00    \n",
      "data_analysis-checkpoint.ipynb                100% 6554KB  11.0MB/s   00:00    \n",
      "utils-checkpoint.py                           100% 3860     2.4MB/s   00:00    \n",
      "video_data-checkpoint.ipynb                   100%   17KB   1.4MB/s   00:00    \n",
      "user_data.ipynb                               100%  202KB  10.3MB/s   00:00    \n"
     ]
    }
   ],
   "source": [
    "!scp -r digix@49.123.120.71:/home/digix/digix/Models/Feature_Engineering/  ./"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "video_status.csv                              100% 2008KB   9.1MB/s   00:00    \n",
      "user_status.csv                               100%  138MB   9.3MB/s   00:14    \n"
     ]
    }
   ],
   "source": [
    "!scp -r digix@49.123.120.71:/home/digix/digix/dataset/traindata/video_features_data/video_status.csv ../2021_3_data/traindata/video_features_data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user_status.csv                               100%  168MB  11.2MB/s   00:14    \n"
     ]
    }
   ],
   "source": [
    "!scp -r digix@49.123.120.71:/home/digix/digix/dataset/traindata/user_features_data/user_status.csv ../2021_3_data/traindata/user_features_data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "digix@49.123.120.71's password: \n"
     ]
    }
   ],
   "source": [
    "!scp -r digix@49.123.120.71:/home/digix/digix/Models/MMoE/MMoe_DouLoss.ipynb  ./MMoe_DouLoss.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
