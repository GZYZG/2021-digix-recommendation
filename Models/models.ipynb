{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type='text/css'>\n",
       ".datatable table.frame { margin-bottom: 0; }\n",
       ".datatable table.frame thead { border-bottom: none; }\n",
       ".datatable table.frame tr.coltypes td {  color: #FFFFFF;  line-height: 6px;  padding: 0 0.5em;}\n",
       ".datatable .bool    { background: #DDDD99; }\n",
       ".datatable .object  { background: #565656; }\n",
       ".datatable .int     { background: #5D9E5D; }\n",
       ".datatable .float   { background: #4040CC; }\n",
       ".datatable .str     { background: #CC4040; }\n",
       ".datatable .time    { background: #40CC40; }\n",
       ".datatable .row_index {  background: var(--jp-border-color3);  border-right: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  font-size: 9px;}\n",
       ".datatable .frame tbody td { text-align: left; }\n",
       ".datatable .frame tr.coltypes .row_index {  background: var(--jp-border-color0);}\n",
       ".datatable th:nth-child(2) { padding-left: 12px; }\n",
       ".datatable .hellipsis {  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .vellipsis {  background: var(--jp-layout-color0);  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .na {  color: var(--jp-cell-editor-border-color);  font-size: 80%;}\n",
       ".datatable .sp {  opacity: 0.25;}\n",
       ".datatable .footer { font-size: 9px; }\n",
       ".datatable .frame_dimensions {  background: var(--jp-border-color3);  border-top: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  display: inline-block;  opacity: 0.6;  padding: 1px 10px 1px 5px;}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from time import time\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from collections import Counter\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import NearMiss, TomekLinks\n",
    "import datatable as dt\n",
    "import warnings\n",
    "import json\n",
    "import pickle\n",
    "import datetime\n",
    "from chinese_calendar import is_workday, is_holiday\n",
    "\n",
    "from tools import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 给定预测标签，计算AUC\n",
    "使用OVR的策略计算每个类别的AUC\n",
    "过程：\n",
    "- 选择类别i作为正类，其他类别作为负类\n",
    "- 将真实标签中不等于i的标记为0，等于i的标记为1\n",
    "- 将预测标签中不等于i的标记为0，等于ide标记为1\n",
    "- 计算混淆矩阵\n",
    "- 计算(fpr, tpr)\n",
    "- 计算AUC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 加载数据\n",
    "训练数据加载过程：\n",
    "1. 分别加载处理好的用户特征和视频特征，以及整合的用户历史行为数据；\n",
    "2. 从用户历史行为数据中筛掉在视频特征中没出现过的video_id；\n",
    "3. 将行为数据中的user_id、video_id替换为对应用户/视频的特征\n",
    "4. 根据不同的任务划分为`watch_label`、`is_share`的数据集\n",
    "\n",
    "推断时，类似于上述过程拼接数据。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 读取数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = \"../2021_3_data\"\n",
    "test_data_dir  = os.path.join(base_dir, \"testdata\")\n",
    "train_data_dir = os.path.join(base_dir, \"traindata\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 基础特征与附加特征合并"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_status = dt.fread(os.path.join(train_data_dir, \"video_features_data/video_status.csv\"))\n",
    "user_status = dt.fread(os.path.join(train_data_dir, \"user_features_data/user_status.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "tab_user = dt.fread(os.path.join(train_data_dir, \"user_features_data/user_features.jay\"))\n",
    "tab_video = dt.fread(os.path.join(train_data_dir, \"video_features_data/video_features.jay\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_status.key = 'video_id'\n",
    "video_ws = tab_video[:, :, join(video_status)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_status.key = 'user_id'\n",
    "user_ws = tab_user[:, :, join(user_status)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_ws.to_jay(os.path.join(train_data_dir, \"video_features_data/video_features_with_status.jay\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_ws.to_jay(os.path.join(train_data_dir, \"user_features_data/user_features_with_status.jay\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class='datatable'>\n",
       "  <table class='frame'>\n",
       "  <thead>\n",
       "    <tr class='colnames'><td class='row_index'></td><th>user_id</th><th>age_0</th><th>age_1</th><th>age_2</th><th>age_3</th><th>age_4</th><th>age_5</th><th>age_6</th><th>age_7</th><th>gender_0</th><th class='vellipsis'>&hellip;</th><th>average_watch_label</th><th>sum_watch_times</th><th>sum_comment_times</th><th>sum_collect_times</th><th>sum_share_times</th></tr>\n",
       "    <tr class='coltypes'><td class='row_index'></td><td class='float' title='float64'>&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;</td><td class='float' title='float64'>&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;</td><td class='float' title='float64'>&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;</td><td class='float' title='float64'>&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;</td><td class='float' title='float64'>&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;</td><td class='float' title='float64'>&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;</td><td class='float' title='float64'>&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;</td><td class='float' title='float64'>&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;</td><td class='float' title='float64'>&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;</td><td class='float' title='float64'>&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;</td><td></td><td class='float' title='float64'>&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;</td><td class='float' title='float64'>&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;</td><td class='float' title='float64'>&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;</td><td class='float' title='float64'>&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;</td><td class='float' title='float64'>&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;</td></tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr><td class='row_index'>0</td><td>1.757e+06</td><td>0</td><td>0</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td class=vellipsis>&hellip;</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr>\n",
       "    <tr><td class='row_index'>1</td><td>17938</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td><td class=vellipsis>&hellip;</td><td>0.0967742</td><td>3</td><td>0</td><td>0</td><td>0</td></tr>\n",
       "    <tr><td class='row_index'>2</td><td>4.26352e+06</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td><td class=vellipsis>&hellip;</td><td>0.204545</td><td>2</td><td>0</td><td>0</td><td>0</td></tr>\n",
       "    <tr><td class='row_index'>3</td><td>1.4116e+06</td><td>0</td><td>0</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td><td class=vellipsis>&hellip;</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr>\n",
       "    <tr><td class='row_index'>4</td><td>3.99224e+06</td><td>0</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td><td class=vellipsis>&hellip;</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr>\n",
       "    <tr><td class='row_index'>5</td><td>4.0116e+06</td><td>0</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td><td class=vellipsis>&hellip;</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr>\n",
       "    <tr><td class='row_index'>6</td><td>4.78556e+06</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td><td class=vellipsis>&hellip;</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr>\n",
       "    <tr><td class='row_index'>7</td><td>5.11036e+06</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td><td class=vellipsis>&hellip;</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr>\n",
       "    <tr><td class='row_index'>8</td><td>1.3212e+06</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td><td class=vellipsis>&hellip;</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr>\n",
       "    <tr><td class='row_index'>9</td><td>3.20698e+06</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td class=vellipsis>&hellip;</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr>\n",
       "    <tr><td class='row_index'>10</td><td>5.18172e+06</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td class=vellipsis>&hellip;</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td></tr>\n",
       "    <tr><td class='row_index'>11</td><td>1.878e+06</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td><td class=vellipsis>&hellip;</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td></tr>\n",
       "    <tr><td class='row_index'>12</td><td>3.04464e+06</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td class=vellipsis>&hellip;</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td></tr>\n",
       "    <tr><td class='row_index'>13</td><td>108273</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td class=vellipsis>&hellip;</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr>\n",
       "    <tr><td class='row_index'>14</td><td>5.64838e+06</td><td>0</td><td>0</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td class=vellipsis>&hellip;</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr>\n",
       "    <tr><td class='row_index'>&#x22EE;</td><td class='hellipsis'>&#x22EE;</td><td class='hellipsis'>&#x22EE;</td><td class='hellipsis'>&#x22EE;</td><td class='hellipsis'>&#x22EE;</td><td class='hellipsis'>&#x22EE;</td><td class='hellipsis'>&#x22EE;</td><td class='hellipsis'>&#x22EE;</td><td class='hellipsis'>&#x22EE;</td><td class='hellipsis'>&#x22EE;</td><td class='hellipsis'>&#x22EE;</td><td class='hellipsis'>&#x22F1;</td><td class='hellipsis'>&#x22EE;</td><td class='hellipsis'>&#x22EE;</td><td class='hellipsis'>&#x22EE;</td><td class='hellipsis'>&#x22EE;</td><td class='hellipsis'>&#x22EE;</td></tr>\n",
       "    <tr><td class='row_index'>5,910,795</td><td>3.22343e+06</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td><td>1</td><td class=vellipsis>&hellip;</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr>\n",
       "    <tr><td class='row_index'>5,910,796</td><td>4.70783e+06</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td><td>1</td><td class=vellipsis>&hellip;</td><td>0.142857</td><td>3</td><td>0</td><td>0</td><td>0</td></tr>\n",
       "    <tr><td class='row_index'>5,910,797</td><td>5.90765e+06</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td><td class=vellipsis>&hellip;</td><td>0</td><td>2</td><td>0</td><td>0</td><td>0</td></tr>\n",
       "    <tr><td class='row_index'>5,910,798</td><td>3.63322e+06</td><td>0</td><td>0</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td><td class=vellipsis>&hellip;</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr>\n",
       "    <tr><td class='row_index'>5,910,799</td><td>782537</td><td>0</td><td>0</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td><td class=vellipsis>&hellip;</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr>\n",
       "  </tbody>\n",
       "  </table>\n",
       "  <div class='footer'>\n",
       "    <div class='frame_dimensions'>5,910,800 rows &times; 36 columns</div>\n",
       "  </div>\n",
       "</div>\n"
      ],
      "text/plain": [
       "<Frame#7f315506be10 5910800x36>"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_ws"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 通过 .npz 读取数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 单表读取后合并"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.6 s, sys: 2.4 s, total: 6 s\n",
      "Wall time: 12.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# 单独读取每个文件再进行合并\n",
    "user_df = read_npz_to_df(os.path.join(train_data_dir, \"user_features_data/user_features.npz\"), data_name='features', column_name='columns')\n",
    "video_df = read_npz_to_df(os.path.join(train_data_dir, \"video_features_data/video_features.npz\"), data_name='features')\n",
    "action_df = read_npz_to_df(os.path.join(train_data_dir, \"all_actions.npz\"), data_name='data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 797,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 因为将字符串保存到 .npz时会使dtype为object，重新读回DataFrame时各个列的数据类型均为 object，所以先转换类型\n",
    "dtypes = dict(zip(video_df.columns, [np.float32] * video_df.shape[1]))\n",
    "dtypes.update({'video_name': np.str})\n",
    "video_df = video_df.astype(dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 802,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 29s, sys: 5.88 s, total: 1min 35s\n",
      "Wall time: 40.1 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(7353024, 76)"
      ]
     },
     "execution_count": 802,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# 合并各个表\n",
    "df_train = merge_user_video_action(user_df, video_df, action_df)\n",
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 805,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez(os.path.join(train_data_dir, \"train.npz\"), data=df_train.to_pandas().values, columns=df_train.to_pandas().columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 777,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 626 ms, sys: 0 ns, total: 626 ms\n",
      "Wall time: 721 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "test_df = load_table(os.path.join(test_data_dir, \"test.csv\"), ftype=\"csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 779,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 8s, sys: 12.8 s, total: 3min 20s\n",
      "Wall time: 51.2 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2822180, 73)"
      ]
     },
     "execution_count": 779,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "df_test = merge_user_video_action(user_df, video_df, test_df)\n",
    "df_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 读取合并好的数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# 直接读取保存好的合并后的训练数据\n",
    "path = os.path.join(train_data_dir, \"train.npz\")\n",
    "df_train = read_npz_to_df(path, data_name='data')\n",
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 810,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 37s, sys: 36.9 s, total: 2min 14s\n",
      "Wall time: 5min 49s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2822180, 73)"
      ]
     },
     "execution_count": 810,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# 直接读取保存好的合并后的测试数据\n",
    "path = os.path.join(test_data_dir, \"test.npz\")\n",
    "df_test = read_npz_to_df(path, data_name='data')\n",
    "df_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 通过 .jay 文件读取"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 单表读取后合并"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "with_status = False\n",
    "if with_status:\n",
    "    user_features_name = \"user_features_with_status\"\n",
    "    video_features_name = \"video_features_with_status\"\n",
    "else:\n",
    "    user_features_name = \"user_features_v1\"\n",
    "    video_features_name = \"video_features_v1\"\n",
    "    \n",
    "p_user = os.path.join(train_data_dir, f\"user_features_data/{user_features_name}.jay\")\n",
    "p_video = os.path.join(train_data_dir, f\"video_features_data/{video_features_name}.jay\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 21.1 s, sys: 804 ms, total: 21.9 s\n",
      "Wall time: 1.22 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(7353655, 176)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "## 使用datatable 加载训练数据\n",
    "p_act = os.path.join(train_data_dir, \"all_actions_with_status_v1.jay\")\n",
    "\n",
    "df_train, others = load_train_test_data(None, pre_merged=False, return_others=True,\n",
    "                           **{\"p_user\": p_user, \"p_video\": p_video, \"p_action\": p_act})\n",
    "user_df = others['user']\n",
    "video_df = others['video']\n",
    "action_df = others['action']\n",
    "df_train.shape"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# 给训练数据生成is_happy_day列\n",
    "tt = dt.fread(os.path.join(train_data_dir, \"all_actions_with_ptd_v1.jay\")).to_pandas()\n",
    "tt['pt_d'] = tt['pt_d'].apply(lambda x: datetime.date(x//10000, x%20210000//100, x%100))\n",
    "dates = set(tt['pt_d'])\n",
    "happy_dict = {d: int((not is_workday(d)) or is_holiday(d)) for d in dates}\n",
    "is_happy_day = tt['pt_d'].apply(lambda x: happy_dict[x])\n",
    "is_happy_day"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# action_date = dt.fread(os.path.join(train_data_dir, f\"all_actions_with_ptd.jay\"))\n",
    "video_status = dt.fread(os.path.join(train_data_dir, f\"video_features_data/video_137days_status.csv\"))\n",
    "user_status = dt.fread(os.path.join(train_data_dir, f\"user_features_data/user_137days_status.csv\"))\n",
    "video_status.key = ('video_id', 'pt_d')\n",
    "user_status.key = ('user_id', 'pt_d')\n",
    "video_status"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "tt = df_train.to_pandas()\n",
    "np.savez(os.path.join(train_data_dir, \"train\"), data=tt.values, columns=tt.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 23.4 s, sys: 2.7 s, total: 26.1 s\n",
      "Wall time: 3.86 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2822180, 173)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# p_user = os.path.join(train_data_dir, \"user_features_data/user_features.jay\")\n",
    "# p_video = os.path.join(train_data_dir, \"video_features_data/video_features.jay\")\n",
    "p_act = os.path.join(test_data_dir, \"test_with_status.jay\")\n",
    "\n",
    "#path = os.path.join(test_data_dir, \"test.jay\")\n",
    "kwargs = {\"p_user\": p_user, \"p_video\": p_video, \"p_action\": p_act}\n",
    "\n",
    "df_test, others = load_train_test_data(None, pre_merged=False, return_others=True, **kwargs)\n",
    "test_df = others['action']\n",
    "df_test.shape"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "% 为测试数据加上is_happy_day特征列\n",
    "d = datetime.date(2021,5,3)\n",
    "v = int((not is_workday(d)) or is_holiday(d))\n",
    "test_df['is_happy_day'] = v"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "act = dt.fread(os.path.join(test_data_dir, \"test.csv\"))\n",
    "act['pt_d'] = 20210503\n",
    "# act = dt.fread(os.path.join(train_data_dir, \"all_actions_with_ptd_v1.jay\"))\n",
    "video_status = dt.fread(os.path.join(train_data_dir, f\"video_features_data/video_137days_status.csv\"))\n",
    "user_status = dt.fread(os.path.join(train_data_dir, f\"user_features_data/user_137days_status.csv\"))\n",
    "video_status.key = ('video_id', 'pt_d')\n",
    "user_status.key = ('user_id', 'pt_d')\n",
    "\n",
    "act = act[:, :, dt.join(video_status)]\n",
    "act = act[:, :, dt.join(user_status)]\n",
    "act"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "del act['pt_d']\n",
    "act.to_jay(os.path.join(train_data_dir, \"all_actions_with_status_v1.jay\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_df = action_df.to_pandas()\n",
    "user_df = user_df.to_pandas()\n",
    "video_df = video_df.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = test_df.to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 读取合并好后的数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_test.to_jay(os.path.join(test_data_dir, \"test_with_status.jay\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.to_jay(os.path.join(train_data_dir, \"train_with_status.jay\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 177 ms, total: 177 ms\n",
      "Wall time: 184 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(7353024, 76)"
      ]
     },
     "execution_count": 468,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# 直接读取保存好的合并后的训练数据\n",
    "path = os.path.join(train_data_dir, \"train.jay\")\n",
    "df_train = load_train_test_data(path, pre_merged=True)\n",
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 649 µs, sys: 46 µs, total: 695 µs\n",
      "Wall time: 684 µs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2822180, 72)"
      ]
     },
     "execution_count": 454,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# 直接读取保存好的合并后的测试数据\n",
    "path = os.path.join(test_data_dir, \"test.jay\")\n",
    "df_test = load_train_test_data(path, pre_merged=True)\n",
    "df_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 处理训练数据\n",
    "可在此做一些预处理：\n",
    "- 从用户历史行为数据中筛掉在视频特征中没出现过的video_id\n",
    "- 删除多余的列\n",
    "- 调整列的顺序\n",
    "- 改变列的数据类型\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 节约内存的一个标配函数\n",
    "def reduce_mem(df):\n",
    "    starttime = time()\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if pd.isnull(c_min) or pd.isnull(c_max):\n",
    "                continue\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)\n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('-- Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction),time spend:{:2.2f} min'.format(end_mem,\n",
    "                                                                                                           100*(start_mem-end_mem)/start_mem,\n",
    "                                                                                                           (time()-starttime)/60))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 34.8 s, sys: 10.7 s, total: 45.5 s\n",
      "Wall time: 12.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if isinstance(df_train, dt.Frame):\n",
    "    df_train = df_train.to_pandas()\n",
    "if isinstance(df_test, dt.Frame):\n",
    "    df_test = df_test.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = reduce_mem(df_train)\n",
    "df_test  = reduce_mem(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7353655 entries, 0 to 7353654\n",
      "Columns: 176 entries, user_id to da_7\n",
      "dtypes: bool(53), float16(105), float32(6), int16(1), int32(2), int8(4), object(5)\n",
      "memory usage: 2.3+ GB\n"
     ]
    }
   ],
   "source": [
    "df_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 删除 video_name、is_watch 列\n",
    "df_train.drop(['video_name', 'is_watch'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "if 'action_df' not in dir():\n",
    "    action_df = load_table(os.path.join(train_data_dir, \"all_actions.jay\")).to_pandas()\n",
    "if 'video_df' not in dir():\n",
    "    video_df = load_table(os.path.join(train_data_dir, \"video_features_data/video_features.jay\")).to_pandas()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# 从用户历史行为数据中筛掉在视频特征中没出现过的video_id\n",
    "idx1 = pd.Index(action_df['video_id'].unique())\n",
    "idx2 = pd.Index(video_df['video_id'])\n",
    "not_exists = idx1.difference(idx2)\n",
    "not_exists"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "%%time\n",
    "# 将训练数据中未出现的视频剔除\n",
    "n = 0\n",
    "for vid in not_exists:\n",
    "    tn = (df_train['video_id'] == vid).sum()\n",
    "    df_train['video_id'].replace(vid, np.nan, inplace=True)\n",
    "    n += tn\n",
    "\n",
    "if n > 0:\n",
    "    df_train.dropna(axis=0, inplace=True)\n",
    "    df_train.reset_index(drop=True, inplace=True)\n",
    "print(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 删除 video_id、user_id列\n",
    "df_train.drop(['user_id', 'video_id'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7353655, 172)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = df_train\n",
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((7353655,), (7353655,), (7353655, 170))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 准备数据\n",
    "watch_label = dataset.pop('watch_label').astype(np.uint8)\n",
    "is_share = dataset.pop('is_share').astype(np.uint8)\n",
    "watch_label.shape, is_share.shape, dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "del action_df, user_df, video_df, test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 处理测试数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'test_df' not in dir():\n",
    "    test_df = pd.read_csv(os.path.join(test_data_dir, \"test.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2822180 entries, 0 to 2822179\n",
      "Data columns (total 60 columns):\n",
      " #   Column                 Dtype  \n",
      "---  ------                 -----  \n",
      " 0   user_id                int32  \n",
      " 1   video_id               int32  \n",
      " 2   v_avg_watch_label_1    float64\n",
      " 3   v_sum_watch_times_1    float64\n",
      " 4   v_sum_watch_overs_1    float64\n",
      " 5   v_sum_comment_times_1  float64\n",
      " 6   v_sum_collect_times_1  float64\n",
      " 7   v_sum_share_times_1    float64\n",
      " 8   v_sum_quit_times_1     float64\n",
      " 9   v_sum_skip_times_1     float64\n",
      " 10  v_sum_watch_days_1     float64\n",
      " 11  v_avg_watch_label_3    float64\n",
      " 12  v_sum_watch_times_3    float64\n",
      " 13  v_sum_watch_overs_3    float64\n",
      " 14  v_sum_comment_times_3  float64\n",
      " 15  v_sum_collect_times_3  float64\n",
      " 16  v_sum_share_times_3    float64\n",
      " 17  v_sum_quit_times_3     float64\n",
      " 18  v_sum_skip_times_3     float64\n",
      " 19  v_sum_watch_days_3     float64\n",
      " 20  v_avg_watch_label_7    float64\n",
      " 21  v_sum_watch_times_7    float64\n",
      " 22  v_sum_watch_overs_7    float64\n",
      " 23  v_sum_comment_times_7  float64\n",
      " 24  v_sum_collect_times_7  float64\n",
      " 25  v_sum_share_times_7    float64\n",
      " 26  v_sum_quit_times_7     float64\n",
      " 27  v_sum_skip_times_7     float64\n",
      " 28  v_sum_watch_days_7     float64\n",
      " 29  u_avg_watch_label_1    float64\n",
      " 30  u_sum_watch_times_1    float64\n",
      " 31  u_sum_watch_overs_1    float64\n",
      " 32  u_sum_quit_times_1     float64\n",
      " 33  u_sum_skip_times_1     object \n",
      " 34  u_sum_comment_times_1  float64\n",
      " 35  u_sum_collect_times_1  float64\n",
      " 36  u_sum_share_times_1    float64\n",
      " 37  u_sum_watch_time_1     float64\n",
      " 38  u_sum_watch_days_1     object \n",
      " 39  u_avg_watch_label_3    float64\n",
      " 40  u_sum_watch_times_3    float64\n",
      " 41  u_sum_watch_overs_3    float64\n",
      " 42  u_sum_quit_times_3     float64\n",
      " 43  u_sum_skip_times_3     object \n",
      " 44  u_sum_comment_times_3  float64\n",
      " 45  u_sum_collect_times_3  float64\n",
      " 46  u_sum_share_times_3    float64\n",
      " 47  u_sum_watch_time_3     float64\n",
      " 48  u_sum_watch_days_3     float64\n",
      " 49  u_avg_watch_label_7    float64\n",
      " 50  u_sum_watch_times_7    float64\n",
      " 51  u_sum_watch_overs_7    float64\n",
      " 52  u_sum_quit_times_7     float64\n",
      " 53  u_sum_skip_times_7     object \n",
      " 54  u_sum_comment_times_7  float64\n",
      " 55  u_sum_collect_times_7  float64\n",
      " 56  u_sum_share_times_7    float64\n",
      " 57  u_sum_watch_time_7     float64\n",
      " 58  u_sum_watch_days_7     float64\n",
      " 59  is_happy_day           int32  \n",
      "dtypes: float64(53), int32(3), object(4)\n",
      "memory usage: 1.2+ GB\n"
     ]
    }
   ],
   "source": [
    "test_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7353655 entries, 0 to 7353654\n",
      "Columns: 170 entries, v_avg_watch_label_1 to da_7\n",
      "dtypes: bool(53), float16(105), float32(6), int16(1), int8(1), object(4)\n",
      "memory usage: 2.2+ GB\n"
     ]
    }
   ],
   "source": [
    "df_test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 删除 video_name 列\n",
    "if 'video_name' in df_test.columns:\n",
    "    df_test.drop('video_name', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# 测试数据集中存在video_id没有在视频特征中出现\n",
    "idx1 = pd.Index(test_df['video_id'].unique())\n",
    "idx2 = pd.Index(video_df['video_id'].unique())\n",
    "non_exists = idx1.difference(idx2)\n",
    "non_exists"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "%%time\n",
    "t0 = time()\n",
    "n = 0\n",
    "for vid in not_exists:\n",
    "    tn = (test_df['video_id'] == vid).sum()\n",
    "#     df_test = action_df[action_df['video_id'] != vid]\n",
    "    n += tn\n",
    "\n",
    "print(f\"在视频特征中不存在的video_id在测试数据集中出现的次数 = {n}\\t\\t(cost {time() - t0:.3f}s)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 删除 video_id、user_id 列\n",
    "df_test.drop(['user_id', 'video_id'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2822180, 170)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inference_dataset = df_test\n",
    "inference_dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 判断训练集和测试集的特征列是对齐的\n",
    "all([e1 == e2 for e1, e2 in zip(inference_dataset.columns.to_list(), dataset.columns.to_list())])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# watch_label 预测"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 处理数据不均衡问题"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 5177374), (1, 557421), (2, 314107), (3, 219188), (4, 172404), (5, 143001), (6, 125092), (7, 117749), (8, 138798), (9, 388521)]\n",
      "[[0.         0.70405451]\n",
      " [1.         0.0758019 ]\n",
      " [2.         0.04271441]\n",
      " [3.         0.02980667]\n",
      " [4.         0.02344467]\n",
      " [5.         0.01944625]\n",
      " [6.         0.01701086]\n",
      " [7.         0.01601231]\n",
      " [8.         0.0188747 ]\n",
      " [9.         0.05283373]]\n"
     ]
    }
   ],
   "source": [
    "items = list(Counter(watch_label).items())\n",
    "items.sort(key=lambda x: x[0])\n",
    "print(items)\n",
    "\n",
    "dist = np.array(items, dtype=np.float)\n",
    "dist[:, 1] = dist[:, 1] / watch_label.shape[0]\n",
    "print(dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "under_ss = np.array(items)\n",
    "under_ss_thresh = under_ss[9, 1]  # 设置每个类别样本数目的上限\n",
    "under_ss[:, 1] = np.clip(under_ss[:, 1], a_min=None, a_max=under_ss_thresh)\n",
    "\n",
    "over_ss = under_ss.copy()\n",
    "over_ss_thresh = under_ss[2, 1]  # 设置每个类别样本数据的下限\n",
    "over_ss[:, 1] = np.clip(over_ss[:, 1], a_min=over_ss_thresh, a_max=None)\n",
    "\n",
    "under_ss = dict(under_ss)\n",
    "over_ss = dict(over_ss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({0: 388521,\n",
       "  1: 388521,\n",
       "  2: 314107,\n",
       "  3: 219188,\n",
       "  4: 172404,\n",
       "  5: 143001,\n",
       "  6: 125092,\n",
       "  7: 117749,\n",
       "  8: 138798,\n",
       "  9: 388521},\n",
       " {0: 388521,\n",
       "  1: 388521,\n",
       "  2: 314107,\n",
       "  3: 314107,\n",
       "  4: 314107,\n",
       "  5: 314107,\n",
       "  6: 314107,\n",
       "  7: 314107,\n",
       "  8: 314107,\n",
       "  9: 388521})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "under_ss, over_ss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({0: 4788853})\n",
      "Counter({1: 168900})\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Int64Index([      0,       1,       2,       5,       7,       8,      11,\n",
       "                 12,      13,      14,\n",
       "            ...\n",
       "            7353642, 7353643, 7353646, 7353647, 7353648, 7353650, 7353651,\n",
       "            7353652, 7353653, 7353654],\n",
       "           dtype='int64', length=4957753)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del_idxs = pd.Index([], dtype=int)\n",
    "for l, n in items:\n",
    "    if n > under_ss_thresh:\n",
    "        t_idxs = watch_label == l\n",
    "        t_idxs = t_idxs.replace(False, np.nan).dropna().index  # 保留watch_label=l的行索引\n",
    "        t_left_idxs = np.random.choice(t_idxs, under_ss_thresh, replace=False)  # 选择一部分保留，注意replace参数，为True时会重复采样\n",
    "        t_del_idxs = t_idxs.difference(t_left_idxs)\n",
    "        print(Counter(watch_label[t_del_idxs]))\n",
    "                \n",
    "        del_idxs = del_idxs.union(t_del_idxs)\n",
    "del_idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({2: 314107,\n",
       "         0: 5176743,\n",
       "         5: 143001,\n",
       "         4: 172404,\n",
       "         1: 557421,\n",
       "         9: 388521,\n",
       "         3: 219188,\n",
       "         8: 138798,\n",
       "         7: 117749,\n",
       "         6: 125092})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(watch_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2395902, 170), (2395902,))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resampled_data = np.delete(dataset.values, del_idxs, axis=0)\n",
    "resampled_wl = np.delete(watch_label.values, del_idxs, axis=0)\n",
    "resampled_data.shape, resampled_wl.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({2: 314107,\n",
       "         5: 143001,\n",
       "         4: 172404,\n",
       "         1: 388521,\n",
       "         0: 388521,\n",
       "         9: 388521,\n",
       "         3: 219188,\n",
       "         8: 138798,\n",
       "         7: 117749,\n",
       "         6: 125092})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(resampled_wl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2395902, 170), (2395902,))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 将采样后的数据重装回 DataFrame\n",
    "data = pd.DataFrame(resampled_data, columns=dataset.columns)\n",
    "watch_label_res = pd.Series(resampled_wl)\n",
    "data.shape, watch_label_res.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "del resampled_wl, resampled_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_data = data[(watch_label_res==5) | (watch_label_res == 6) | (watch_label_res == 7) | (watch_label_res == 8)]\n",
    "tmp_wl = watch_label_res[(watch_label_res==5) | (watch_label_res == 6) | (watch_label_res == 7) | (watch_label_res == 8)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({5: 143001, 8: 138798, 7: 117749, 6: 125092})"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(tmp_wl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# 对几个数量较少的类别进行上采样\n",
    "smt = SMOTE(sampling_strategy={5: 150000, 6: 130000, 7: 120000, 8: 140000}, n_jobs=8)\n",
    "X_r, y_r = smt.fit_resample(tmp_data, tmp_wl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(tmp_data.index, axis=0, inplace=True)\n",
    "watch_label_res.drop(tmp_wl.index, axis=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2441262, 127), (2441262,))"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data.append(X_r, ignore_index=True)\n",
    "watch_label_res = watch_label_res.append(y_r, ignore_index=True)\n",
    "data.shape, watch_label_res.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1964639,), (431263,))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_rate = 0.18\n",
    "train_idx, test_idx = train_test_split(data.index, test_size=test_rate, random_state=0, shuffle=True, stratify=watch_label_res)\n",
    "train_idx.shape, test_idx.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = data.iloc[train_idx]\n",
    "X_test  = data.iloc[test_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = watch_label_res.iloc[train_idx]\n",
    "y_test  = watch_label_res.iloc[test_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1964639, 170), (1964639,), (431263, 170), (431263,))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, y_train.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# 为每个样本赋予权重，值为样本类别所占比例\n",
    "t = list(Counter(y_train).items())\n",
    "t.sort(key=lambda x: x[0])\n",
    "t = np.array(t)[:, 1]\n",
    "t = t / sum(t)\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data preparing finished ...\t\t(66.874s)\n"
     ]
    }
   ],
   "source": [
    "t0 = time()\n",
    "# train_weight = [t[i] for i in y_train]\n",
    "# test_weight = [t[i] for i in y_test]\n",
    "xg_train = xgb.DMatrix(X_train.values, label=y_train.values, enable_categorical=True)\n",
    "xg_test = xgb.DMatrix(X_test.values, label=y_test.values, enable_categorical=True)\n",
    "print(f\"Data preparing finished ...\\t\\t({time()-t0:.3f}s)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'xg_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1384/1340533697.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m }\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mwatchlist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxg_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mxg_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'test'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'xg_train' is not defined"
     ]
    }
   ],
   "source": [
    "# setup parameters for xgboost\n",
    "param = {\n",
    "    'objective': 'multi:softmax',\n",
    "    'eta': 0.1,\n",
    "    'nthread': 8,\n",
    "    'num_class': 10,\n",
    "    'gpu_id': 0,\n",
    "    'tree_method': 'gpu_hist',\n",
    "    'eval_metric': ['mlogloss', 'auc', 'merror'],\n",
    "    'max_depth': 9,\n",
    "    'min_child_weight': 9,\n",
    "    'gamma': 0.2,\n",
    "    'subsample': 0.9,\n",
    "    'colsample_bytree': 0.6,\n",
    "    'reg_alpha': 0\n",
    "}\n",
    "\n",
    "watchlist = [(xg_train, 'train'), (xg_test, 'test')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-mlogloss:2.27473\ttrain-auc:0.62858\ttrain-merror:0.74215\ttest-mlogloss:2.27543\ttest-auc:0.61998\ttest-merror:0.74538\n",
      "[1]\ttrain-mlogloss:2.25109\ttrain-auc:0.63735\ttrain-merror:0.73863\ttest-mlogloss:2.25244\ttest-auc:0.62716\ttest-merror:0.74258\n",
      "[2]\ttrain-mlogloss:2.23110\ttrain-auc:0.64097\ttrain-merror:0.73768\ttest-mlogloss:2.23313\ttest-auc:0.62943\ttest-merror:0.74156\n",
      "[3]\ttrain-mlogloss:2.21299\ttrain-auc:0.64413\ttrain-merror:0.73598\ttest-mlogloss:2.21566\ttest-auc:0.63189\ttest-merror:0.73981\n",
      "[4]\ttrain-mlogloss:2.19732\ttrain-auc:0.64656\ttrain-merror:0.73476\ttest-mlogloss:2.20064\ttest-auc:0.63364\ttest-merror:0.73877\n",
      "[5]\ttrain-mlogloss:2.18377\ttrain-auc:0.64818\ttrain-merror:0.73405\ttest-mlogloss:2.18773\ttest-auc:0.63469\ttest-merror:0.73793\n",
      "[6]\ttrain-mlogloss:2.17177\ttrain-auc:0.64948\ttrain-merror:0.73314\ttest-mlogloss:2.17630\ttest-auc:0.63546\ttest-merror:0.73713\n",
      "[7]\ttrain-mlogloss:2.16095\ttrain-auc:0.65055\ttrain-merror:0.73275\ttest-mlogloss:2.16608\ttest-auc:0.63622\ttest-merror:0.73704\n",
      "[8]\ttrain-mlogloss:2.15145\ttrain-auc:0.65154\ttrain-merror:0.73223\ttest-mlogloss:2.15717\ttest-auc:0.63679\ttest-merror:0.73662\n",
      "[9]\ttrain-mlogloss:2.14305\ttrain-auc:0.65232\ttrain-merror:0.73181\ttest-mlogloss:2.14935\ttest-auc:0.63718\ttest-merror:0.73641\n",
      "[10]\ttrain-mlogloss:2.13543\ttrain-auc:0.65307\ttrain-merror:0.73138\ttest-mlogloss:2.14232\ttest-auc:0.63754\ttest-merror:0.73607\n",
      "[11]\ttrain-mlogloss:2.12852\ttrain-auc:0.65385\ttrain-merror:0.73108\ttest-mlogloss:2.13599\ttest-auc:0.63788\ttest-merror:0.73594\n",
      "[12]\ttrain-mlogloss:2.12219\ttrain-auc:0.65460\ttrain-merror:0.73090\ttest-mlogloss:2.13025\ttest-auc:0.63822\ttest-merror:0.73588\n",
      "[13]\ttrain-mlogloss:2.11639\ttrain-auc:0.65524\ttrain-merror:0.73054\ttest-mlogloss:2.12502\ttest-auc:0.63859\ttest-merror:0.73547\n",
      "[14]\ttrain-mlogloss:2.11119\ttrain-auc:0.65588\ttrain-merror:0.73020\ttest-mlogloss:2.12039\ttest-auc:0.63883\ttest-merror:0.73533\n",
      "[15]\ttrain-mlogloss:2.10644\ttrain-auc:0.65648\ttrain-merror:0.72988\ttest-mlogloss:2.11619\ttest-auc:0.63912\ttest-merror:0.73522\n",
      "[16]\ttrain-mlogloss:2.10212\ttrain-auc:0.65713\ttrain-merror:0.72976\ttest-mlogloss:2.11245\ttest-auc:0.63938\ttest-merror:0.73495\n",
      "[17]\ttrain-mlogloss:2.09817\ttrain-auc:0.65773\ttrain-merror:0.72934\ttest-mlogloss:2.10906\ttest-auc:0.63963\ttest-merror:0.73482\n",
      "[18]\ttrain-mlogloss:2.09458\ttrain-auc:0.65829\ttrain-merror:0.72908\ttest-mlogloss:2.10606\ttest-auc:0.63981\ttest-merror:0.73450\n",
      "[19]\ttrain-mlogloss:2.09121\ttrain-auc:0.65888\ttrain-merror:0.72877\ttest-mlogloss:2.10324\ttest-auc:0.64009\ttest-merror:0.73426\n",
      "[20]\ttrain-mlogloss:2.08815\ttrain-auc:0.65942\ttrain-merror:0.72846\ttest-mlogloss:2.10073\ttest-auc:0.64030\ttest-merror:0.73412\n",
      "[21]\ttrain-mlogloss:2.08533\ttrain-auc:0.65994\ttrain-merror:0.72822\ttest-mlogloss:2.09845\ttest-auc:0.64050\ttest-merror:0.73394\n",
      "[22]\ttrain-mlogloss:2.08275\ttrain-auc:0.66042\ttrain-merror:0.72791\ttest-mlogloss:2.09638\ttest-auc:0.64068\ttest-merror:0.73384\n",
      "[23]\ttrain-mlogloss:2.08035\ttrain-auc:0.66089\ttrain-merror:0.72770\ttest-mlogloss:2.09451\ttest-auc:0.64084\ttest-merror:0.73389\n",
      "[24]\ttrain-mlogloss:2.07801\ttrain-auc:0.66143\ttrain-merror:0.72745\ttest-mlogloss:2.09270\ttest-auc:0.64107\ttest-merror:0.73360\n",
      "[25]\ttrain-mlogloss:2.07581\ttrain-auc:0.66194\ttrain-merror:0.72711\ttest-mlogloss:2.09104\ttest-auc:0.64127\ttest-merror:0.73352\n",
      "[26]\ttrain-mlogloss:2.07378\ttrain-auc:0.66249\ttrain-merror:0.72683\ttest-mlogloss:2.08958\ttest-auc:0.64144\ttest-merror:0.73337\n",
      "[27]\ttrain-mlogloss:2.07191\ttrain-auc:0.66298\ttrain-merror:0.72662\ttest-mlogloss:2.08825\ttest-auc:0.64161\ttest-merror:0.73317\n",
      "[28]\ttrain-mlogloss:2.07008\ttrain-auc:0.66351\ttrain-merror:0.72632\ttest-mlogloss:2.08696\ttest-auc:0.64179\ttest-merror:0.73298\n",
      "[29]\ttrain-mlogloss:2.06840\ttrain-auc:0.66399\ttrain-merror:0.72607\ttest-mlogloss:2.08581\ttest-auc:0.64196\ttest-merror:0.73279\n",
      "[30]\ttrain-mlogloss:2.06678\ttrain-auc:0.66445\ttrain-merror:0.72575\ttest-mlogloss:2.08471\ttest-auc:0.64214\ttest-merror:0.73279\n",
      "[31]\ttrain-mlogloss:2.06524\ttrain-auc:0.66500\ttrain-merror:0.72547\ttest-mlogloss:2.08371\ttest-auc:0.64232\ttest-merror:0.73268\n",
      "[32]\ttrain-mlogloss:2.06374\ttrain-auc:0.66554\ttrain-merror:0.72508\ttest-mlogloss:2.08278\ttest-auc:0.64248\ttest-merror:0.73246\n",
      "[33]\ttrain-mlogloss:2.06236\ttrain-auc:0.66605\ttrain-merror:0.72482\ttest-mlogloss:2.08191\ttest-auc:0.64267\ttest-merror:0.73226\n",
      "[34]\ttrain-mlogloss:2.06107\ttrain-auc:0.66650\ttrain-merror:0.72453\ttest-mlogloss:2.08113\ttest-auc:0.64281\ttest-merror:0.73210\n",
      "[35]\ttrain-mlogloss:2.05988\ttrain-auc:0.66691\ttrain-merror:0.72436\ttest-mlogloss:2.08040\ttest-auc:0.64296\ttest-merror:0.73201\n",
      "[36]\ttrain-mlogloss:2.05873\ttrain-auc:0.66740\ttrain-merror:0.72404\ttest-mlogloss:2.07975\ttest-auc:0.64311\ttest-merror:0.73186\n",
      "[37]\ttrain-mlogloss:2.05759\ttrain-auc:0.66788\ttrain-merror:0.72381\ttest-mlogloss:2.07916\ttest-auc:0.64321\ttest-merror:0.73173\n",
      "[38]\ttrain-mlogloss:2.05642\ttrain-auc:0.66840\ttrain-merror:0.72345\ttest-mlogloss:2.07853\ttest-auc:0.64336\ttest-merror:0.73157\n",
      "[39]\ttrain-mlogloss:2.05539\ttrain-auc:0.66888\ttrain-merror:0.72320\ttest-mlogloss:2.07803\ttest-auc:0.64348\ttest-merror:0.73134\n",
      "[40]\ttrain-mlogloss:2.05436\ttrain-auc:0.66934\ttrain-merror:0.72299\ttest-mlogloss:2.07751\ttest-auc:0.64361\ttest-merror:0.73123\n",
      "[41]\ttrain-mlogloss:2.05331\ttrain-auc:0.66987\ttrain-merror:0.72267\ttest-mlogloss:2.07698\ttest-auc:0.64376\ttest-merror:0.73113\n",
      "[42]\ttrain-mlogloss:2.05235\ttrain-auc:0.67031\ttrain-merror:0.72242\ttest-mlogloss:2.07653\ttest-auc:0.64389\ttest-merror:0.73107\n",
      "[43]\ttrain-mlogloss:2.05141\ttrain-auc:0.67080\ttrain-merror:0.72217\ttest-mlogloss:2.07610\ttest-auc:0.64400\ttest-merror:0.73110\n",
      "[44]\ttrain-mlogloss:2.05050\ttrain-auc:0.67121\ttrain-merror:0.72191\ttest-mlogloss:2.07567\ttest-auc:0.64412\ttest-merror:0.73099\n",
      "[45]\ttrain-mlogloss:2.04962\ttrain-auc:0.67165\ttrain-merror:0.72172\ttest-mlogloss:2.07531\ttest-auc:0.64421\ttest-merror:0.73088\n",
      "[46]\ttrain-mlogloss:2.04871\ttrain-auc:0.67214\ttrain-merror:0.72141\ttest-mlogloss:2.07494\ttest-auc:0.64431\ttest-merror:0.73070\n",
      "[47]\ttrain-mlogloss:2.04790\ttrain-auc:0.67254\ttrain-merror:0.72112\ttest-mlogloss:2.07460\ttest-auc:0.64441\ttest-merror:0.73060\n",
      "[48]\ttrain-mlogloss:2.04710\ttrain-auc:0.67296\ttrain-merror:0.72087\ttest-mlogloss:2.07430\ttest-auc:0.64450\ttest-merror:0.73056\n",
      "[49]\ttrain-mlogloss:2.04629\ttrain-auc:0.67342\ttrain-merror:0.72059\ttest-mlogloss:2.07398\ttest-auc:0.64460\ttest-merror:0.73050\n",
      "[50]\ttrain-mlogloss:2.04553\ttrain-auc:0.67384\ttrain-merror:0.72040\ttest-mlogloss:2.07368\ttest-auc:0.64469\ttest-merror:0.73047\n",
      "[51]\ttrain-mlogloss:2.04479\ttrain-auc:0.67424\ttrain-merror:0.72014\ttest-mlogloss:2.07345\ttest-auc:0.64475\ttest-merror:0.73037\n",
      "[52]\ttrain-mlogloss:2.04398\ttrain-auc:0.67472\ttrain-merror:0.71988\ttest-mlogloss:2.07317\ttest-auc:0.64483\ttest-merror:0.73015\n",
      "[53]\ttrain-mlogloss:2.04325\ttrain-auc:0.67515\ttrain-merror:0.71971\ttest-mlogloss:2.07293\ttest-auc:0.64492\ttest-merror:0.73005\n",
      "[54]\ttrain-mlogloss:2.04253\ttrain-auc:0.67554\ttrain-merror:0.71944\ttest-mlogloss:2.07269\ttest-auc:0.64500\ttest-merror:0.72999\n",
      "[55]\ttrain-mlogloss:2.04186\ttrain-auc:0.67592\ttrain-merror:0.71924\ttest-mlogloss:2.07246\ttest-auc:0.64508\ttest-merror:0.72996\n",
      "[56]\ttrain-mlogloss:2.04116\ttrain-auc:0.67637\ttrain-merror:0.71899\ttest-mlogloss:2.07223\ttest-auc:0.64517\ttest-merror:0.72990\n",
      "[57]\ttrain-mlogloss:2.04042\ttrain-auc:0.67682\ttrain-merror:0.71877\ttest-mlogloss:2.07199\ttest-auc:0.64527\ttest-merror:0.72989\n",
      "[58]\ttrain-mlogloss:2.03976\ttrain-auc:0.67721\ttrain-merror:0.71857\ttest-mlogloss:2.07178\ttest-auc:0.64534\ttest-merror:0.72977\n",
      "[59]\ttrain-mlogloss:2.03905\ttrain-auc:0.67764\ttrain-merror:0.71825\ttest-mlogloss:2.07160\ttest-auc:0.64541\ttest-merror:0.72968\n",
      "[60]\ttrain-mlogloss:2.03837\ttrain-auc:0.67805\ttrain-merror:0.71798\ttest-mlogloss:2.07141\ttest-auc:0.64548\ttest-merror:0.72957\n",
      "[61]\ttrain-mlogloss:2.03770\ttrain-auc:0.67845\ttrain-merror:0.71778\ttest-mlogloss:2.07122\ttest-auc:0.64553\ttest-merror:0.72952\n",
      "[62]\ttrain-mlogloss:2.03712\ttrain-auc:0.67878\ttrain-merror:0.71758\ttest-mlogloss:2.07104\ttest-auc:0.64560\ttest-merror:0.72944\n",
      "[63]\ttrain-mlogloss:2.03645\ttrain-auc:0.67922\ttrain-merror:0.71731\ttest-mlogloss:2.07084\ttest-auc:0.64570\ttest-merror:0.72954\n",
      "[64]\ttrain-mlogloss:2.03582\ttrain-auc:0.67964\ttrain-merror:0.71706\ttest-mlogloss:2.07068\ttest-auc:0.64577\ttest-merror:0.72944\n",
      "[65]\ttrain-mlogloss:2.03524\ttrain-auc:0.67999\ttrain-merror:0.71688\ttest-mlogloss:2.07053\ttest-auc:0.64583\ttest-merror:0.72936\n",
      "[66]\ttrain-mlogloss:2.03456\ttrain-auc:0.68043\ttrain-merror:0.71667\ttest-mlogloss:2.07036\ttest-auc:0.64592\ttest-merror:0.72937\n",
      "[67]\ttrain-mlogloss:2.03387\ttrain-auc:0.68087\ttrain-merror:0.71641\ttest-mlogloss:2.07021\ttest-auc:0.64599\ttest-merror:0.72920\n",
      "[68]\ttrain-mlogloss:2.03332\ttrain-auc:0.68121\ttrain-merror:0.71621\ttest-mlogloss:2.07008\ttest-auc:0.64604\ttest-merror:0.72931\n",
      "[69]\ttrain-mlogloss:2.03273\ttrain-auc:0.68157\ttrain-merror:0.71600\ttest-mlogloss:2.06994\ttest-auc:0.64610\ttest-merror:0.72923\n",
      "[70]\ttrain-mlogloss:2.03217\ttrain-auc:0.68194\ttrain-merror:0.71583\ttest-mlogloss:2.06981\ttest-auc:0.64616\ttest-merror:0.72914\n",
      "[71]\ttrain-mlogloss:2.03159\ttrain-auc:0.68230\ttrain-merror:0.71561\ttest-mlogloss:2.06971\ttest-auc:0.64621\ttest-merror:0.72910\n",
      "[72]\ttrain-mlogloss:2.03106\ttrain-auc:0.68265\ttrain-merror:0.71547\ttest-mlogloss:2.06959\ttest-auc:0.64625\ttest-merror:0.72903\n",
      "[73]\ttrain-mlogloss:2.03048\ttrain-auc:0.68303\ttrain-merror:0.71521\ttest-mlogloss:2.06945\ttest-auc:0.64632\ttest-merror:0.72904\n",
      "[74]\ttrain-mlogloss:2.02994\ttrain-auc:0.68337\ttrain-merror:0.71506\ttest-mlogloss:2.06935\ttest-auc:0.64635\ttest-merror:0.72903\n",
      "[75]\ttrain-mlogloss:2.02942\ttrain-auc:0.68370\ttrain-merror:0.71483\ttest-mlogloss:2.06923\ttest-auc:0.64640\ttest-merror:0.72906\n",
      "[76]\ttrain-mlogloss:2.02887\ttrain-auc:0.68406\ttrain-merror:0.71462\ttest-mlogloss:2.06915\ttest-auc:0.64642\ttest-merror:0.72896\n",
      "[77]\ttrain-mlogloss:2.02835\ttrain-auc:0.68440\ttrain-merror:0.71440\ttest-mlogloss:2.06905\ttest-auc:0.64647\ttest-merror:0.72895\n",
      "[78]\ttrain-mlogloss:2.02788\ttrain-auc:0.68468\ttrain-merror:0.71422\ttest-mlogloss:2.06898\ttest-auc:0.64649\ttest-merror:0.72894\n",
      "[79]\ttrain-mlogloss:2.02738\ttrain-auc:0.68500\ttrain-merror:0.71404\ttest-mlogloss:2.06890\ttest-auc:0.64653\ttest-merror:0.72899\n",
      "[80]\ttrain-mlogloss:2.02684\ttrain-auc:0.68533\ttrain-merror:0.71386\ttest-mlogloss:2.06878\ttest-auc:0.64658\ttest-merror:0.72885\n",
      "[81]\ttrain-mlogloss:2.02629\ttrain-auc:0.68568\ttrain-merror:0.71363\ttest-mlogloss:2.06868\ttest-auc:0.64663\ttest-merror:0.72877\n",
      "[82]\ttrain-mlogloss:2.02578\ttrain-auc:0.68603\ttrain-merror:0.71346\ttest-mlogloss:2.06861\ttest-auc:0.64665\ttest-merror:0.72877\n",
      "[83]\ttrain-mlogloss:2.02526\ttrain-auc:0.68637\ttrain-merror:0.71320\ttest-mlogloss:2.06852\ttest-auc:0.64669\ttest-merror:0.72857\n",
      "[84]\ttrain-mlogloss:2.02473\ttrain-auc:0.68669\ttrain-merror:0.71290\ttest-mlogloss:2.06841\ttest-auc:0.64675\ttest-merror:0.72852\n",
      "[85]\ttrain-mlogloss:2.02428\ttrain-auc:0.68699\ttrain-merror:0.71275\ttest-mlogloss:2.06834\ttest-auc:0.64679\ttest-merror:0.72850\n",
      "[86]\ttrain-mlogloss:2.02386\ttrain-auc:0.68725\ttrain-merror:0.71257\ttest-mlogloss:2.06828\ttest-auc:0.64682\ttest-merror:0.72852\n",
      "[87]\ttrain-mlogloss:2.02339\ttrain-auc:0.68756\ttrain-merror:0.71229\ttest-mlogloss:2.06822\ttest-auc:0.64685\ttest-merror:0.72848\n",
      "[88]\ttrain-mlogloss:2.02292\ttrain-auc:0.68787\ttrain-merror:0.71214\ttest-mlogloss:2.06817\ttest-auc:0.64688\ttest-merror:0.72850\n",
      "[89]\ttrain-mlogloss:2.02243\ttrain-auc:0.68818\ttrain-merror:0.71196\ttest-mlogloss:2.06807\ttest-auc:0.64692\ttest-merror:0.72850\n",
      "[90]\ttrain-mlogloss:2.02199\ttrain-auc:0.68849\ttrain-merror:0.71183\ttest-mlogloss:2.06801\ttest-auc:0.64695\ttest-merror:0.72850\n",
      "[91]\ttrain-mlogloss:2.02150\ttrain-auc:0.68881\ttrain-merror:0.71161\ttest-mlogloss:2.06794\ttest-auc:0.64699\ttest-merror:0.72847\n",
      "[92]\ttrain-mlogloss:2.02106\ttrain-auc:0.68910\ttrain-merror:0.71139\ttest-mlogloss:2.06786\ttest-auc:0.64703\ttest-merror:0.72841\n",
      "[93]\ttrain-mlogloss:2.02063\ttrain-auc:0.68939\ttrain-merror:0.71120\ttest-mlogloss:2.06783\ttest-auc:0.64705\ttest-merror:0.72840\n",
      "[94]\ttrain-mlogloss:2.02014\ttrain-auc:0.68972\ttrain-merror:0.71098\ttest-mlogloss:2.06776\ttest-auc:0.64708\ttest-merror:0.72844\n",
      "[95]\ttrain-mlogloss:2.01968\ttrain-auc:0.69002\ttrain-merror:0.71085\ttest-mlogloss:2.06773\ttest-auc:0.64708\ttest-merror:0.72830\n",
      "[96]\ttrain-mlogloss:2.01926\ttrain-auc:0.69029\ttrain-merror:0.71067\ttest-mlogloss:2.06767\ttest-auc:0.64711\ttest-merror:0.72833\n",
      "[97]\ttrain-mlogloss:2.01878\ttrain-auc:0.69061\ttrain-merror:0.71045\ttest-mlogloss:2.06761\ttest-auc:0.64715\ttest-merror:0.72831\n",
      "[98]\ttrain-mlogloss:2.01836\ttrain-auc:0.69087\ttrain-merror:0.71029\ttest-mlogloss:2.06754\ttest-auc:0.64717\ttest-merror:0.72820\n",
      "[99]\ttrain-mlogloss:2.01795\ttrain-auc:0.69114\ttrain-merror:0.71008\ttest-mlogloss:2.06748\ttest-auc:0.64721\ttest-merror:0.72816\n",
      "[100]\ttrain-mlogloss:2.01749\ttrain-auc:0.69144\ttrain-merror:0.70990\ttest-mlogloss:2.06740\ttest-auc:0.64724\ttest-merror:0.72822\n",
      "[101]\ttrain-mlogloss:2.01701\ttrain-auc:0.69175\ttrain-merror:0.70968\ttest-mlogloss:2.06731\ttest-auc:0.64730\ttest-merror:0.72813\n",
      "[102]\ttrain-mlogloss:2.01663\ttrain-auc:0.69201\ttrain-merror:0.70945\ttest-mlogloss:2.06725\ttest-auc:0.64732\ttest-merror:0.72808\n",
      "[103]\ttrain-mlogloss:2.01616\ttrain-auc:0.69229\ttrain-merror:0.70930\ttest-mlogloss:2.06720\ttest-auc:0.64734\ttest-merror:0.72795\n",
      "[104]\ttrain-mlogloss:2.01574\ttrain-auc:0.69255\ttrain-merror:0.70914\ttest-mlogloss:2.06716\ttest-auc:0.64735\ttest-merror:0.72793\n",
      "[105]\ttrain-mlogloss:2.01527\ttrain-auc:0.69285\ttrain-merror:0.70898\ttest-mlogloss:2.06712\ttest-auc:0.64735\ttest-merror:0.72788\n",
      "[106]\ttrain-mlogloss:2.01479\ttrain-auc:0.69313\ttrain-merror:0.70878\ttest-mlogloss:2.06705\ttest-auc:0.64737\ttest-merror:0.72793\n",
      "[107]\ttrain-mlogloss:2.01434\ttrain-auc:0.69342\ttrain-merror:0.70859\ttest-mlogloss:2.06700\ttest-auc:0.64740\ttest-merror:0.72790\n",
      "[108]\ttrain-mlogloss:2.01394\ttrain-auc:0.69370\ttrain-merror:0.70842\ttest-mlogloss:2.06696\ttest-auc:0.64741\ttest-merror:0.72791\n",
      "[109]\ttrain-mlogloss:2.01351\ttrain-auc:0.69398\ttrain-merror:0.70830\ttest-mlogloss:2.06692\ttest-auc:0.64743\ttest-merror:0.72775\n",
      "[110]\ttrain-mlogloss:2.01309\ttrain-auc:0.69427\ttrain-merror:0.70812\ttest-mlogloss:2.06688\ttest-auc:0.64746\ttest-merror:0.72779\n",
      "[111]\ttrain-mlogloss:2.01265\ttrain-auc:0.69455\ttrain-merror:0.70792\ttest-mlogloss:2.06683\ttest-auc:0.64748\ttest-merror:0.72782\n",
      "[112]\ttrain-mlogloss:2.01224\ttrain-auc:0.69480\ttrain-merror:0.70776\ttest-mlogloss:2.06676\ttest-auc:0.64752\ttest-merror:0.72775\n",
      "[113]\ttrain-mlogloss:2.01180\ttrain-auc:0.69509\ttrain-merror:0.70751\ttest-mlogloss:2.06672\ttest-auc:0.64754\ttest-merror:0.72764\n",
      "[114]\ttrain-mlogloss:2.01145\ttrain-auc:0.69531\ttrain-merror:0.70734\ttest-mlogloss:2.06669\ttest-auc:0.64755\ttest-merror:0.72759\n",
      "[115]\ttrain-mlogloss:2.01104\ttrain-auc:0.69556\ttrain-merror:0.70720\ttest-mlogloss:2.06666\ttest-auc:0.64756\ttest-merror:0.72766\n",
      "[116]\ttrain-mlogloss:2.01068\ttrain-auc:0.69580\ttrain-merror:0.70707\ttest-mlogloss:2.06664\ttest-auc:0.64756\ttest-merror:0.72761\n",
      "[117]\ttrain-mlogloss:2.01027\ttrain-auc:0.69605\ttrain-merror:0.70691\ttest-mlogloss:2.06658\ttest-auc:0.64759\ttest-merror:0.72758\n",
      "[118]\ttrain-mlogloss:2.00986\ttrain-auc:0.69632\ttrain-merror:0.70676\ttest-mlogloss:2.06655\ttest-auc:0.64760\ttest-merror:0.72751\n",
      "[119]\ttrain-mlogloss:2.00947\ttrain-auc:0.69658\ttrain-merror:0.70666\ttest-mlogloss:2.06652\ttest-auc:0.64761\ttest-merror:0.72752\n",
      "[120]\ttrain-mlogloss:2.00901\ttrain-auc:0.69688\ttrain-merror:0.70648\ttest-mlogloss:2.06648\ttest-auc:0.64763\ttest-merror:0.72740\n",
      "[121]\ttrain-mlogloss:2.00856\ttrain-auc:0.69716\ttrain-merror:0.70625\ttest-mlogloss:2.06644\ttest-auc:0.64765\ttest-merror:0.72738\n",
      "[122]\ttrain-mlogloss:2.00819\ttrain-auc:0.69739\ttrain-merror:0.70609\ttest-mlogloss:2.06641\ttest-auc:0.64766\ttest-merror:0.72740\n",
      "[123]\ttrain-mlogloss:2.00784\ttrain-auc:0.69762\ttrain-merror:0.70591\ttest-mlogloss:2.06638\ttest-auc:0.64767\ttest-merror:0.72742\n",
      "[124]\ttrain-mlogloss:2.00744\ttrain-auc:0.69787\ttrain-merror:0.70578\ttest-mlogloss:2.06632\ttest-auc:0.64771\ttest-merror:0.72739\n",
      "[125]\ttrain-mlogloss:2.00713\ttrain-auc:0.69807\ttrain-merror:0.70567\ttest-mlogloss:2.06630\ttest-auc:0.64772\ttest-merror:0.72743\n",
      "[126]\ttrain-mlogloss:2.00671\ttrain-auc:0.69833\ttrain-merror:0.70549\ttest-mlogloss:2.06628\ttest-auc:0.64773\ttest-merror:0.72737\n",
      "[127]\ttrain-mlogloss:2.00627\ttrain-auc:0.69861\ttrain-merror:0.70533\ttest-mlogloss:2.06621\ttest-auc:0.64776\ttest-merror:0.72732\n",
      "[128]\ttrain-mlogloss:2.00592\ttrain-auc:0.69884\ttrain-merror:0.70517\ttest-mlogloss:2.06620\ttest-auc:0.64775\ttest-merror:0.72733\n",
      "[129]\ttrain-mlogloss:2.00555\ttrain-auc:0.69909\ttrain-merror:0.70503\ttest-mlogloss:2.06616\ttest-auc:0.64777\ttest-merror:0.72729\n",
      "[130]\ttrain-mlogloss:2.00515\ttrain-auc:0.69932\ttrain-merror:0.70487\ttest-mlogloss:2.06612\ttest-auc:0.64779\ttest-merror:0.72728\n",
      "[131]\ttrain-mlogloss:2.00478\ttrain-auc:0.69956\ttrain-merror:0.70472\ttest-mlogloss:2.06611\ttest-auc:0.64779\ttest-merror:0.72735\n",
      "[132]\ttrain-mlogloss:2.00444\ttrain-auc:0.69978\ttrain-merror:0.70459\ttest-mlogloss:2.06607\ttest-auc:0.64781\ttest-merror:0.72733\n",
      "[133]\ttrain-mlogloss:2.00411\ttrain-auc:0.69998\ttrain-merror:0.70442\ttest-mlogloss:2.06605\ttest-auc:0.64782\ttest-merror:0.72735\n",
      "[134]\ttrain-mlogloss:2.00372\ttrain-auc:0.70024\ttrain-merror:0.70425\ttest-mlogloss:2.06602\ttest-auc:0.64783\ttest-merror:0.72731\n",
      "[135]\ttrain-mlogloss:2.00336\ttrain-auc:0.70047\ttrain-merror:0.70408\ttest-mlogloss:2.06598\ttest-auc:0.64784\ttest-merror:0.72732\n",
      "[136]\ttrain-mlogloss:2.00302\ttrain-auc:0.70069\ttrain-merror:0.70397\ttest-mlogloss:2.06597\ttest-auc:0.64785\ttest-merror:0.72732\n",
      "[137]\ttrain-mlogloss:2.00263\ttrain-auc:0.70094\ttrain-merror:0.70379\ttest-mlogloss:2.06595\ttest-auc:0.64786\ttest-merror:0.72726\n",
      "[138]\ttrain-mlogloss:2.00228\ttrain-auc:0.70115\ttrain-merror:0.70365\ttest-mlogloss:2.06591\ttest-auc:0.64788\ttest-merror:0.72724\n",
      "[139]\ttrain-mlogloss:2.00191\ttrain-auc:0.70139\ttrain-merror:0.70354\ttest-mlogloss:2.06588\ttest-auc:0.64789\ttest-merror:0.72719\n",
      "[140]\ttrain-mlogloss:2.00156\ttrain-auc:0.70161\ttrain-merror:0.70336\ttest-mlogloss:2.06586\ttest-auc:0.64790\ttest-merror:0.72715\n",
      "[141]\ttrain-mlogloss:2.00115\ttrain-auc:0.70188\ttrain-merror:0.70322\ttest-mlogloss:2.06585\ttest-auc:0.64790\ttest-merror:0.72718\n",
      "[142]\ttrain-mlogloss:2.00080\ttrain-auc:0.70209\ttrain-merror:0.70304\ttest-mlogloss:2.06584\ttest-auc:0.64790\ttest-merror:0.72713\n",
      "[143]\ttrain-mlogloss:2.00047\ttrain-auc:0.70231\ttrain-merror:0.70291\ttest-mlogloss:2.06584\ttest-auc:0.64789\ttest-merror:0.72702\n",
      "[144]\ttrain-mlogloss:2.00011\ttrain-auc:0.70255\ttrain-merror:0.70269\ttest-mlogloss:2.06581\ttest-auc:0.64790\ttest-merror:0.72708\n",
      "[145]\ttrain-mlogloss:1.99969\ttrain-auc:0.70282\ttrain-merror:0.70254\ttest-mlogloss:2.06580\ttest-auc:0.64791\ttest-merror:0.72700\n",
      "[146]\ttrain-mlogloss:1.99938\ttrain-auc:0.70302\ttrain-merror:0.70242\ttest-mlogloss:2.06578\ttest-auc:0.64792\ttest-merror:0.72699\n",
      "[147]\ttrain-mlogloss:1.99900\ttrain-auc:0.70324\ttrain-merror:0.70228\ttest-mlogloss:2.06575\ttest-auc:0.64794\ttest-merror:0.72689\n",
      "[148]\ttrain-mlogloss:1.99865\ttrain-auc:0.70346\ttrain-merror:0.70212\ttest-mlogloss:2.06572\ttest-auc:0.64795\ttest-merror:0.72688\n",
      "[149]\ttrain-mlogloss:1.99834\ttrain-auc:0.70366\ttrain-merror:0.70200\ttest-mlogloss:2.06570\ttest-auc:0.64796\ttest-merror:0.72682\n",
      "[150]\ttrain-mlogloss:1.99792\ttrain-auc:0.70392\ttrain-merror:0.70183\ttest-mlogloss:2.06568\ttest-auc:0.64798\ttest-merror:0.72680\n",
      "[151]\ttrain-mlogloss:1.99760\ttrain-auc:0.70412\ttrain-merror:0.70171\ttest-mlogloss:2.06566\ttest-auc:0.64798\ttest-merror:0.72678\n",
      "[152]\ttrain-mlogloss:1.99727\ttrain-auc:0.70432\ttrain-merror:0.70155\ttest-mlogloss:2.06563\ttest-auc:0.64799\ttest-merror:0.72674\n",
      "[153]\ttrain-mlogloss:1.99692\ttrain-auc:0.70453\ttrain-merror:0.70142\ttest-mlogloss:2.06561\ttest-auc:0.64801\ttest-merror:0.72670\n",
      "[154]\ttrain-mlogloss:1.99662\ttrain-auc:0.70472\ttrain-merror:0.70132\ttest-mlogloss:2.06559\ttest-auc:0.64801\ttest-merror:0.72674\n",
      "[155]\ttrain-mlogloss:1.99631\ttrain-auc:0.70491\ttrain-merror:0.70120\ttest-mlogloss:2.06557\ttest-auc:0.64802\ttest-merror:0.72676\n",
      "[156]\ttrain-mlogloss:1.99599\ttrain-auc:0.70509\ttrain-merror:0.70105\ttest-mlogloss:2.06555\ttest-auc:0.64803\ttest-merror:0.72675\n",
      "[157]\ttrain-mlogloss:1.99565\ttrain-auc:0.70529\ttrain-merror:0.70089\ttest-mlogloss:2.06553\ttest-auc:0.64804\ttest-merror:0.72674\n",
      "[158]\ttrain-mlogloss:1.99528\ttrain-auc:0.70553\ttrain-merror:0.70069\ttest-mlogloss:2.06551\ttest-auc:0.64804\ttest-merror:0.72674\n",
      "[159]\ttrain-mlogloss:1.99491\ttrain-auc:0.70576\ttrain-merror:0.70059\ttest-mlogloss:2.06549\ttest-auc:0.64806\ttest-merror:0.72676\n",
      "[160]\ttrain-mlogloss:1.99454\ttrain-auc:0.70600\ttrain-merror:0.70046\ttest-mlogloss:2.06545\ttest-auc:0.64808\ttest-merror:0.72671\n",
      "[161]\ttrain-mlogloss:1.99422\ttrain-auc:0.70619\ttrain-merror:0.70030\ttest-mlogloss:2.06544\ttest-auc:0.64807\ttest-merror:0.72672\n",
      "[162]\ttrain-mlogloss:1.99388\ttrain-auc:0.70641\ttrain-merror:0.70016\ttest-mlogloss:2.06543\ttest-auc:0.64808\ttest-merror:0.72672\n",
      "[163]\ttrain-mlogloss:1.99355\ttrain-auc:0.70661\ttrain-merror:0.70003\ttest-mlogloss:2.06542\ttest-auc:0.64808\ttest-merror:0.72672\n",
      "[164]\ttrain-mlogloss:1.99321\ttrain-auc:0.70683\ttrain-merror:0.69992\ttest-mlogloss:2.06541\ttest-auc:0.64809\ttest-merror:0.72670\n",
      "[165]\ttrain-mlogloss:1.99289\ttrain-auc:0.70704\ttrain-merror:0.69979\ttest-mlogloss:2.06540\ttest-auc:0.64810\ttest-merror:0.72663\n",
      "[166]\ttrain-mlogloss:1.99254\ttrain-auc:0.70725\ttrain-merror:0.69968\ttest-mlogloss:2.06539\ttest-auc:0.64810\ttest-merror:0.72660\n",
      "[167]\ttrain-mlogloss:1.99213\ttrain-auc:0.70751\ttrain-merror:0.69949\ttest-mlogloss:2.06537\ttest-auc:0.64811\ttest-merror:0.72665\n",
      "[168]\ttrain-mlogloss:1.99179\ttrain-auc:0.70772\ttrain-merror:0.69929\ttest-mlogloss:2.06535\ttest-auc:0.64812\ttest-merror:0.72664\n",
      "[169]\ttrain-mlogloss:1.99148\ttrain-auc:0.70790\ttrain-merror:0.69916\ttest-mlogloss:2.06534\ttest-auc:0.64812\ttest-merror:0.72663\n",
      "[170]\ttrain-mlogloss:1.99116\ttrain-auc:0.70810\ttrain-merror:0.69902\ttest-mlogloss:2.06532\ttest-auc:0.64814\ttest-merror:0.72667\n",
      "[171]\ttrain-mlogloss:1.99085\ttrain-auc:0.70829\ttrain-merror:0.69889\ttest-mlogloss:2.06532\ttest-auc:0.64813\ttest-merror:0.72660\n",
      "[172]\ttrain-mlogloss:1.99059\ttrain-auc:0.70846\ttrain-merror:0.69876\ttest-mlogloss:2.06530\ttest-auc:0.64814\ttest-merror:0.72662\n",
      "[173]\ttrain-mlogloss:1.99026\ttrain-auc:0.70867\ttrain-merror:0.69863\ttest-mlogloss:2.06528\ttest-auc:0.64815\ttest-merror:0.72668\n",
      "[174]\ttrain-mlogloss:1.98993\ttrain-auc:0.70887\ttrain-merror:0.69850\ttest-mlogloss:2.06526\ttest-auc:0.64816\ttest-merror:0.72656\n",
      "[175]\ttrain-mlogloss:1.98959\ttrain-auc:0.70906\ttrain-merror:0.69834\ttest-mlogloss:2.06523\ttest-auc:0.64818\ttest-merror:0.72652\n",
      "[176]\ttrain-mlogloss:1.98929\ttrain-auc:0.70923\ttrain-merror:0.69823\ttest-mlogloss:2.06524\ttest-auc:0.64817\ttest-merror:0.72652\n",
      "[177]\ttrain-mlogloss:1.98896\ttrain-auc:0.70942\ttrain-merror:0.69814\ttest-mlogloss:2.06522\ttest-auc:0.64818\ttest-merror:0.72654\n",
      "[178]\ttrain-mlogloss:1.98864\ttrain-auc:0.70961\ttrain-merror:0.69799\ttest-mlogloss:2.06523\ttest-auc:0.64817\ttest-merror:0.72653\n",
      "[179]\ttrain-mlogloss:1.98828\ttrain-auc:0.70983\ttrain-merror:0.69781\ttest-mlogloss:2.06518\ttest-auc:0.64820\ttest-merror:0.72646\n",
      "[180]\ttrain-mlogloss:1.98796\ttrain-auc:0.71003\ttrain-merror:0.69767\ttest-mlogloss:2.06518\ttest-auc:0.64820\ttest-merror:0.72647\n",
      "[181]\ttrain-mlogloss:1.98765\ttrain-auc:0.71021\ttrain-merror:0.69751\ttest-mlogloss:2.06515\ttest-auc:0.64821\ttest-merror:0.72641\n",
      "[182]\ttrain-mlogloss:1.98728\ttrain-auc:0.71043\ttrain-merror:0.69735\ttest-mlogloss:2.06512\ttest-auc:0.64823\ttest-merror:0.72639\n",
      "[183]\ttrain-mlogloss:1.98693\ttrain-auc:0.71065\ttrain-merror:0.69720\ttest-mlogloss:2.06512\ttest-auc:0.64823\ttest-merror:0.72640\n",
      "[184]\ttrain-mlogloss:1.98663\ttrain-auc:0.71083\ttrain-merror:0.69707\ttest-mlogloss:2.06509\ttest-auc:0.64825\ttest-merror:0.72636\n",
      "[185]\ttrain-mlogloss:1.98627\ttrain-auc:0.71104\ttrain-merror:0.69691\ttest-mlogloss:2.06507\ttest-auc:0.64826\ttest-merror:0.72632\n",
      "[186]\ttrain-mlogloss:1.98593\ttrain-auc:0.71125\ttrain-merror:0.69677\ttest-mlogloss:2.06505\ttest-auc:0.64826\ttest-merror:0.72631\n",
      "[187]\ttrain-mlogloss:1.98562\ttrain-auc:0.71144\ttrain-merror:0.69668\ttest-mlogloss:2.06505\ttest-auc:0.64827\ttest-merror:0.72627\n",
      "[188]\ttrain-mlogloss:1.98535\ttrain-auc:0.71161\ttrain-merror:0.69653\ttest-mlogloss:2.06504\ttest-auc:0.64826\ttest-merror:0.72626\n",
      "[189]\ttrain-mlogloss:1.98507\ttrain-auc:0.71178\ttrain-merror:0.69645\ttest-mlogloss:2.06503\ttest-auc:0.64826\ttest-merror:0.72623\n",
      "[190]\ttrain-mlogloss:1.98472\ttrain-auc:0.71199\ttrain-merror:0.69629\ttest-mlogloss:2.06503\ttest-auc:0.64826\ttest-merror:0.72630\n",
      "[191]\ttrain-mlogloss:1.98446\ttrain-auc:0.71214\ttrain-merror:0.69618\ttest-mlogloss:2.06503\ttest-auc:0.64827\ttest-merror:0.72625\n",
      "[192]\ttrain-mlogloss:1.98418\ttrain-auc:0.71231\ttrain-merror:0.69607\ttest-mlogloss:2.06502\ttest-auc:0.64827\ttest-merror:0.72625\n",
      "[193]\ttrain-mlogloss:1.98385\ttrain-auc:0.71252\ttrain-merror:0.69596\ttest-mlogloss:2.06500\ttest-auc:0.64829\ttest-merror:0.72627\n",
      "[194]\ttrain-mlogloss:1.98355\ttrain-auc:0.71270\ttrain-merror:0.69585\ttest-mlogloss:2.06499\ttest-auc:0.64830\ttest-merror:0.72630\n",
      "[195]\ttrain-mlogloss:1.98327\ttrain-auc:0.71287\ttrain-merror:0.69573\ttest-mlogloss:2.06497\ttest-auc:0.64830\ttest-merror:0.72634\n",
      "[196]\ttrain-mlogloss:1.98294\ttrain-auc:0.71307\ttrain-merror:0.69559\ttest-mlogloss:2.06497\ttest-auc:0.64831\ttest-merror:0.72634\n",
      "[197]\ttrain-mlogloss:1.98263\ttrain-auc:0.71325\ttrain-merror:0.69546\ttest-mlogloss:2.06495\ttest-auc:0.64831\ttest-merror:0.72633\n",
      "[198]\ttrain-mlogloss:1.98226\ttrain-auc:0.71348\ttrain-merror:0.69530\ttest-mlogloss:2.06495\ttest-auc:0.64830\ttest-merror:0.72635\n",
      "[199]\ttrain-mlogloss:1.98189\ttrain-auc:0.71370\ttrain-merror:0.69519\ttest-mlogloss:2.06495\ttest-auc:0.64829\ttest-merror:0.72630\n",
      "[200]\ttrain-mlogloss:1.98158\ttrain-auc:0.71387\ttrain-merror:0.69506\ttest-mlogloss:2.06494\ttest-auc:0.64831\ttest-merror:0.72630\n",
      "[201]\ttrain-mlogloss:1.98122\ttrain-auc:0.71408\ttrain-merror:0.69489\ttest-mlogloss:2.06493\ttest-auc:0.64831\ttest-merror:0.72626\n",
      "[202]\ttrain-mlogloss:1.98089\ttrain-auc:0.71429\ttrain-merror:0.69479\ttest-mlogloss:2.06492\ttest-auc:0.64831\ttest-merror:0.72631\n",
      "[203]\ttrain-mlogloss:1.98051\ttrain-auc:0.71451\ttrain-merror:0.69463\ttest-mlogloss:2.06489\ttest-auc:0.64833\ttest-merror:0.72632\n",
      "[204]\ttrain-mlogloss:1.98025\ttrain-auc:0.71467\ttrain-merror:0.69448\ttest-mlogloss:2.06489\ttest-auc:0.64833\ttest-merror:0.72634\n",
      "[205]\ttrain-mlogloss:1.97992\ttrain-auc:0.71488\ttrain-merror:0.69434\ttest-mlogloss:2.06489\ttest-auc:0.64833\ttest-merror:0.72634\n",
      "[206]\ttrain-mlogloss:1.97964\ttrain-auc:0.71504\ttrain-merror:0.69424\ttest-mlogloss:2.06488\ttest-auc:0.64833\ttest-merror:0.72622\n",
      "[207]\ttrain-mlogloss:1.97932\ttrain-auc:0.71524\ttrain-merror:0.69413\ttest-mlogloss:2.06487\ttest-auc:0.64834\ttest-merror:0.72625\n",
      "[208]\ttrain-mlogloss:1.97902\ttrain-auc:0.71542\ttrain-merror:0.69400\ttest-mlogloss:2.06486\ttest-auc:0.64834\ttest-merror:0.72625\n",
      "[209]\ttrain-mlogloss:1.97873\ttrain-auc:0.71559\ttrain-merror:0.69386\ttest-mlogloss:2.06485\ttest-auc:0.64835\ttest-merror:0.72623\n",
      "[210]\ttrain-mlogloss:1.97841\ttrain-auc:0.71579\ttrain-merror:0.69370\ttest-mlogloss:2.06485\ttest-auc:0.64835\ttest-merror:0.72628\n",
      "[211]\ttrain-mlogloss:1.97811\ttrain-auc:0.71597\ttrain-merror:0.69356\ttest-mlogloss:2.06486\ttest-auc:0.64834\ttest-merror:0.72631\n",
      "[212]\ttrain-mlogloss:1.97784\ttrain-auc:0.71613\ttrain-merror:0.69344\ttest-mlogloss:2.06485\ttest-auc:0.64834\ttest-merror:0.72622\n",
      "[213]\ttrain-mlogloss:1.97760\ttrain-auc:0.71628\ttrain-merror:0.69333\ttest-mlogloss:2.06484\ttest-auc:0.64834\ttest-merror:0.72628\n",
      "[214]\ttrain-mlogloss:1.97727\ttrain-auc:0.71647\ttrain-merror:0.69323\ttest-mlogloss:2.06484\ttest-auc:0.64834\ttest-merror:0.72628\n",
      "[215]\ttrain-mlogloss:1.97696\ttrain-auc:0.71665\ttrain-merror:0.69311\ttest-mlogloss:2.06483\ttest-auc:0.64834\ttest-merror:0.72621\n",
      "[216]\ttrain-mlogloss:1.97667\ttrain-auc:0.71682\ttrain-merror:0.69297\ttest-mlogloss:2.06482\ttest-auc:0.64835\ttest-merror:0.72618\n",
      "[217]\ttrain-mlogloss:1.97637\ttrain-auc:0.71700\ttrain-merror:0.69284\ttest-mlogloss:2.06482\ttest-auc:0.64834\ttest-merror:0.72618\n",
      "[218]\ttrain-mlogloss:1.97605\ttrain-auc:0.71718\ttrain-merror:0.69272\ttest-mlogloss:2.06480\ttest-auc:0.64835\ttest-merror:0.72622\n",
      "[219]\ttrain-mlogloss:1.97580\ttrain-auc:0.71733\ttrain-merror:0.69261\ttest-mlogloss:2.06480\ttest-auc:0.64834\ttest-merror:0.72619\n",
      "[220]\ttrain-mlogloss:1.97549\ttrain-auc:0.71752\ttrain-merror:0.69250\ttest-mlogloss:2.06480\ttest-auc:0.64834\ttest-merror:0.72619\n",
      "[221]\ttrain-mlogloss:1.97516\ttrain-auc:0.71772\ttrain-merror:0.69232\ttest-mlogloss:2.06479\ttest-auc:0.64834\ttest-merror:0.72612\n",
      "[222]\ttrain-mlogloss:1.97481\ttrain-auc:0.71792\ttrain-merror:0.69214\ttest-mlogloss:2.06479\ttest-auc:0.64834\ttest-merror:0.72611\n",
      "[223]\ttrain-mlogloss:1.97456\ttrain-auc:0.71807\ttrain-merror:0.69203\ttest-mlogloss:2.06480\ttest-auc:0.64834\ttest-merror:0.72610\n",
      "[224]\ttrain-mlogloss:1.97422\ttrain-auc:0.71826\ttrain-merror:0.69187\ttest-mlogloss:2.06478\ttest-auc:0.64834\ttest-merror:0.72607\n",
      "[225]\ttrain-mlogloss:1.97395\ttrain-auc:0.71842\ttrain-merror:0.69173\ttest-mlogloss:2.06478\ttest-auc:0.64834\ttest-merror:0.72606\n",
      "[226]\ttrain-mlogloss:1.97366\ttrain-auc:0.71859\ttrain-merror:0.69159\ttest-mlogloss:2.06480\ttest-auc:0.64832\ttest-merror:0.72613\n",
      "[227]\ttrain-mlogloss:1.97336\ttrain-auc:0.71878\ttrain-merror:0.69148\ttest-mlogloss:2.06479\ttest-auc:0.64832\ttest-merror:0.72615\n",
      "[228]\ttrain-mlogloss:1.97306\ttrain-auc:0.71895\ttrain-merror:0.69136\ttest-mlogloss:2.06478\ttest-auc:0.64832\ttest-merror:0.72616\n",
      "[229]\ttrain-mlogloss:1.97277\ttrain-auc:0.71911\ttrain-merror:0.69124\ttest-mlogloss:2.06477\ttest-auc:0.64833\ttest-merror:0.72619\n",
      "[230]\ttrain-mlogloss:1.97248\ttrain-auc:0.71927\ttrain-merror:0.69111\ttest-mlogloss:2.06478\ttest-auc:0.64832\ttest-merror:0.72618\n",
      "[231]\ttrain-mlogloss:1.97218\ttrain-auc:0.71943\ttrain-merror:0.69099\ttest-mlogloss:2.06479\ttest-auc:0.64831\ttest-merror:0.72610\n",
      "[232]\ttrain-mlogloss:1.97191\ttrain-auc:0.71959\ttrain-merror:0.69084\ttest-mlogloss:2.06479\ttest-auc:0.64831\ttest-merror:0.72606\n",
      "[233]\ttrain-mlogloss:1.97163\ttrain-auc:0.71975\ttrain-merror:0.69072\ttest-mlogloss:2.06480\ttest-auc:0.64830\ttest-merror:0.72604\n",
      "[234]\ttrain-mlogloss:1.97129\ttrain-auc:0.71995\ttrain-merror:0.69059\ttest-mlogloss:2.06480\ttest-auc:0.64830\ttest-merror:0.72603\n",
      "[235]\ttrain-mlogloss:1.97099\ttrain-auc:0.72013\ttrain-merror:0.69044\ttest-mlogloss:2.06479\ttest-auc:0.64831\ttest-merror:0.72610\n",
      "[236]\ttrain-mlogloss:1.97071\ttrain-auc:0.72028\ttrain-merror:0.69029\ttest-mlogloss:2.06478\ttest-auc:0.64831\ttest-merror:0.72610\n",
      "[237]\ttrain-mlogloss:1.97041\ttrain-auc:0.72045\ttrain-merror:0.69013\ttest-mlogloss:2.06479\ttest-auc:0.64830\ttest-merror:0.72597\n",
      "[238]\ttrain-mlogloss:1.97014\ttrain-auc:0.72060\ttrain-merror:0.69002\ttest-mlogloss:2.06478\ttest-auc:0.64830\ttest-merror:0.72602\n",
      "[239]\ttrain-mlogloss:1.96984\ttrain-auc:0.72077\ttrain-merror:0.68990\ttest-mlogloss:2.06477\ttest-auc:0.64831\ttest-merror:0.72607\n",
      "[240]\ttrain-mlogloss:1.96952\ttrain-auc:0.72092\ttrain-merror:0.68976\ttest-mlogloss:2.06475\ttest-auc:0.64832\ttest-merror:0.72597\n",
      "[241]\ttrain-mlogloss:1.96921\ttrain-auc:0.72110\ttrain-merror:0.68963\ttest-mlogloss:2.06475\ttest-auc:0.64832\ttest-merror:0.72596\n",
      "[242]\ttrain-mlogloss:1.96893\ttrain-auc:0.72126\ttrain-merror:0.68953\ttest-mlogloss:2.06474\ttest-auc:0.64832\ttest-merror:0.72596\n",
      "[243]\ttrain-mlogloss:1.96866\ttrain-auc:0.72140\ttrain-merror:0.68940\ttest-mlogloss:2.06475\ttest-auc:0.64832\ttest-merror:0.72591\n",
      "[244]\ttrain-mlogloss:1.96836\ttrain-auc:0.72159\ttrain-merror:0.68928\ttest-mlogloss:2.06475\ttest-auc:0.64831\ttest-merror:0.72591\n",
      "[245]\ttrain-mlogloss:1.96808\ttrain-auc:0.72175\ttrain-merror:0.68918\ttest-mlogloss:2.06475\ttest-auc:0.64831\ttest-merror:0.72594\n",
      "[246]\ttrain-mlogloss:1.96777\ttrain-auc:0.72192\ttrain-merror:0.68906\ttest-mlogloss:2.06474\ttest-auc:0.64831\ttest-merror:0.72594\n",
      "[247]\ttrain-mlogloss:1.96752\ttrain-auc:0.72205\ttrain-merror:0.68897\ttest-mlogloss:2.06473\ttest-auc:0.64832\ttest-merror:0.72591\n",
      "[248]\ttrain-mlogloss:1.96723\ttrain-auc:0.72221\ttrain-merror:0.68887\ttest-mlogloss:2.06472\ttest-auc:0.64832\ttest-merror:0.72590\n",
      "[249]\ttrain-mlogloss:1.96693\ttrain-auc:0.72238\ttrain-merror:0.68874\ttest-mlogloss:2.06472\ttest-auc:0.64832\ttest-merror:0.72589\n",
      "250-rounds Training finished ...\t\t(487.358s)\n"
     ]
    }
   ],
   "source": [
    "num_round = 250\n",
    "t0 = time()\n",
    "eval_result = {}\n",
    "def decay_eta(nth):\n",
    "    etas = [.1, .05, .03, .01]\n",
    "    return etas[(nth // 60) % len(etas)]\n",
    "\n",
    "wl_bst_sm = xgb.train(param, xg_train, num_round, watchlist, evals_result=eval_result, early_stopping_rounds=30)\n",
    "#                       callbacks=[xgb.callback.LearningRateScheduler(decay_eta)])\n",
    "print(f\"{num_round}-rounds Training finished ...\\t\\t({time()-t0:.3f}s)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': OrderedDict([('mlogloss',\n",
       "               [2.273391,\n",
       "                2.249271,\n",
       "                2.228938,\n",
       "                2.211438,\n",
       "                2.196323,\n",
       "                2.183143,\n",
       "                2.171514,\n",
       "                2.16128,\n",
       "                2.15201,\n",
       "                2.14378,\n",
       "                2.136391,\n",
       "                2.129838,\n",
       "                2.12384,\n",
       "                2.118412,\n",
       "                2.113522,\n",
       "                2.109029,\n",
       "                2.105014,\n",
       "                2.101309,\n",
       "                2.0979,\n",
       "                2.094819,\n",
       "                2.092041,\n",
       "                2.089248,\n",
       "                2.086848,\n",
       "                2.084486,\n",
       "                2.082175,\n",
       "                2.080239,\n",
       "                2.078318,\n",
       "                2.076628,\n",
       "                2.074873,\n",
       "                2.073293,\n",
       "                2.071798,\n",
       "                2.070462,\n",
       "                2.069056,\n",
       "                2.067713,\n",
       "                2.066565,\n",
       "                2.065228,\n",
       "                2.064006,\n",
       "                2.062921,\n",
       "                2.061723,\n",
       "                2.060604,\n",
       "                2.059523,\n",
       "                2.058563,\n",
       "                2.057618,\n",
       "                2.056739,\n",
       "                2.055834,\n",
       "                2.055038,\n",
       "                2.054273,\n",
       "                2.053414,\n",
       "                2.052641,\n",
       "                2.051878,\n",
       "                2.051152,\n",
       "                2.050514,\n",
       "                2.049806,\n",
       "                2.049126,\n",
       "                2.04851,\n",
       "                2.047696,\n",
       "                2.047043,\n",
       "                2.046361,\n",
       "                2.045739,\n",
       "                2.045126,\n",
       "                2.044484,\n",
       "                2.043822,\n",
       "                2.043189,\n",
       "                2.042589,\n",
       "                2.041953,\n",
       "                2.041371,\n",
       "                2.040852,\n",
       "                2.040214,\n",
       "                2.039633,\n",
       "                2.039045,\n",
       "                2.038459,\n",
       "                2.037893,\n",
       "                2.037293,\n",
       "                2.03677,\n",
       "                2.036205,\n",
       "                2.035654,\n",
       "                2.035119,\n",
       "                2.034608,\n",
       "                2.034042,\n",
       "                2.033438,\n",
       "                2.033002,\n",
       "                2.032466,\n",
       "                2.031891,\n",
       "                2.031407,\n",
       "                2.030922,\n",
       "                2.030501,\n",
       "                2.029937,\n",
       "                2.029474,\n",
       "                2.028983,\n",
       "                2.028555,\n",
       "                2.028012,\n",
       "                2.027574,\n",
       "                2.027126,\n",
       "                2.026658,\n",
       "                2.026212,\n",
       "                2.025816,\n",
       "                2.025496,\n",
       "                2.025096,\n",
       "                2.024753,\n",
       "                2.024403,\n",
       "                2.024139,\n",
       "                2.023821,\n",
       "                2.023507,\n",
       "                2.023234,\n",
       "                2.022821,\n",
       "                2.022518,\n",
       "                2.022208,\n",
       "                2.022001,\n",
       "                2.021776,\n",
       "                2.021562,\n",
       "                2.021295,\n",
       "                2.021012,\n",
       "                2.020731,\n",
       "                2.020529,\n",
       "                2.020328,\n",
       "                2.020141,\n",
       "                2.01988,\n",
       "                2.019671,\n",
       "                2.01945,\n",
       "                2.019143,\n",
       "                2.018958,\n",
       "                2.018705,\n",
       "                2.018505,\n",
       "                2.018282,\n",
       "                2.018093,\n",
       "                2.017933,\n",
       "                2.017817,\n",
       "                2.017704,\n",
       "                2.017591,\n",
       "                2.017471,\n",
       "                2.017397,\n",
       "                2.017253,\n",
       "                2.017146,\n",
       "                2.017033,\n",
       "                2.016887,\n",
       "                2.016766,\n",
       "                2.016657,\n",
       "                2.01646,\n",
       "                2.016353,\n",
       "                2.016211,\n",
       "                2.016126,\n",
       "                2.016019,\n",
       "                2.015876,\n",
       "                2.015805,\n",
       "                2.015669,\n",
       "                2.015573,\n",
       "                2.015421,\n",
       "                2.015299,\n",
       "                2.015163,\n",
       "                2.015013,\n",
       "                2.014934,\n",
       "                2.014853,\n",
       "                2.014726,\n",
       "                2.014677,\n",
       "                2.014621,\n",
       "                2.014551,\n",
       "                2.014474,\n",
       "                2.014271,\n",
       "                2.01423,\n",
       "                2.014152,\n",
       "                2.014093,\n",
       "                2.014045,\n",
       "                2.013975,\n",
       "                2.013928,\n",
       "                2.013889,\n",
       "                2.013806,\n",
       "                2.013786,\n",
       "                2.013756,\n",
       "                2.013702,\n",
       "                2.013679,\n",
       "                2.013618,\n",
       "                2.013508,\n",
       "                2.013389,\n",
       "                2.013267,\n",
       "                2.01323,\n",
       "                2.013192,\n",
       "                2.01314,\n",
       "                2.013098,\n",
       "                2.01299,\n",
       "                2.012877,\n",
       "                2.012784,\n",
       "                2.012739,\n",
       "                2.012705,\n",
       "                2.012635,\n",
       "                2.012612,\n",
       "                2.012561,\n",
       "                2.012506,\n",
       "                2.012436,\n",
       "                2.012393,\n",
       "                2.012362,\n",
       "                2.012309,\n",
       "                2.012259,\n",
       "                2.012213,\n",
       "                2.012111,\n",
       "                2.012066,\n",
       "                2.011981,\n",
       "                2.011952,\n",
       "                2.011911,\n",
       "                2.011884,\n",
       "                2.011832]),\n",
       "              ('auc',\n",
       "               [0.609166,\n",
       "                0.613574,\n",
       "                0.615673,\n",
       "                0.617684,\n",
       "                0.618899,\n",
       "                0.620377,\n",
       "                0.621478,\n",
       "                0.62236,\n",
       "                0.623596,\n",
       "                0.624718,\n",
       "                0.625693,\n",
       "                0.626546,\n",
       "                0.627477,\n",
       "                0.628391,\n",
       "                0.629221,\n",
       "                0.630098,\n",
       "                0.630813,\n",
       "                0.631575,\n",
       "                0.632343,\n",
       "                0.633113,\n",
       "                0.633773,\n",
       "                0.634675,\n",
       "                0.635358,\n",
       "                0.636146,\n",
       "                0.637049,\n",
       "                0.637714,\n",
       "                0.638476,\n",
       "                0.63913,\n",
       "                0.639907,\n",
       "                0.64057,\n",
       "                0.641302,\n",
       "                0.641907,\n",
       "                0.642584,\n",
       "                0.643294,\n",
       "                0.64389,\n",
       "                0.644677,\n",
       "                0.645397,\n",
       "                0.646051,\n",
       "                0.646797,\n",
       "                0.64751,\n",
       "                0.648158,\n",
       "                0.648724,\n",
       "                0.649365,\n",
       "                0.649896,\n",
       "                0.650452,\n",
       "                0.650985,\n",
       "                0.651553,\n",
       "                0.652133,\n",
       "                0.652763,\n",
       "                0.653332,\n",
       "                0.653814,\n",
       "                0.654313,\n",
       "                0.654877,\n",
       "                0.655356,\n",
       "                0.655828,\n",
       "                0.656415,\n",
       "                0.656954,\n",
       "                0.65745,\n",
       "                0.657979,\n",
       "                0.658472,\n",
       "                0.658947,\n",
       "                0.659518,\n",
       "                0.660011,\n",
       "                0.66048,\n",
       "                0.661035,\n",
       "                0.661515,\n",
       "                0.661939,\n",
       "                0.66243,\n",
       "                0.662903,\n",
       "                0.663358,\n",
       "                0.663827,\n",
       "                0.664305,\n",
       "                0.664799,\n",
       "                0.665215,\n",
       "                0.665669,\n",
       "                0.66609,\n",
       "                0.666529,\n",
       "                0.666907,\n",
       "                0.66736,\n",
       "                0.667831,\n",
       "                0.668144,\n",
       "                0.668582,\n",
       "                0.669065,\n",
       "                0.66944,\n",
       "                0.669814,\n",
       "                0.67014,\n",
       "                0.670602,\n",
       "                0.670932,\n",
       "                0.671322,\n",
       "                0.671626,\n",
       "                0.672037,\n",
       "                0.672368,\n",
       "                0.672707,\n",
       "                0.673066,\n",
       "                0.673372,\n",
       "                0.673655,\n",
       "                0.673886,\n",
       "                0.674167,\n",
       "                0.674412,\n",
       "                0.67468,\n",
       "                0.674849,\n",
       "                0.675083,\n",
       "                0.675328,\n",
       "                0.675503,\n",
       "                0.675813,\n",
       "                0.676021,\n",
       "                0.676225,\n",
       "                0.67635,\n",
       "                0.676503,\n",
       "                0.676644,\n",
       "                0.676819,\n",
       "                0.677004,\n",
       "                0.677191,\n",
       "                0.677305,\n",
       "                0.677427,\n",
       "                0.677553,\n",
       "                0.677731,\n",
       "                0.677853,\n",
       "                0.678005,\n",
       "                0.67821,\n",
       "                0.678332,\n",
       "                0.678514,\n",
       "                0.678642,\n",
       "                0.678797,\n",
       "                0.678916,\n",
       "                0.67901,\n",
       "                0.679068,\n",
       "                0.679116,\n",
       "                0.679174,\n",
       "                0.679233,\n",
       "                0.679268,\n",
       "                0.679347,\n",
       "                0.679393,\n",
       "                0.679453,\n",
       "                0.679532,\n",
       "                0.679592,\n",
       "                0.679641,\n",
       "                0.679756,\n",
       "                0.679805,\n",
       "                0.679884,\n",
       "                0.67992,\n",
       "                0.679976,\n",
       "                0.680058,\n",
       "                0.680091,\n",
       "                0.680166,\n",
       "                0.680201,\n",
       "                0.680297,\n",
       "                0.680364,\n",
       "                0.680429,\n",
       "                0.680511,\n",
       "                0.680554,\n",
       "                0.680594,\n",
       "                0.68066,\n",
       "                0.680675,\n",
       "                0.680696,\n",
       "                0.680724,\n",
       "                0.680755,\n",
       "                0.680916,\n",
       "                0.68093,\n",
       "                0.680963,\n",
       "                0.680986,\n",
       "                0.681005,\n",
       "                0.681047,\n",
       "                0.681063,\n",
       "                0.681076,\n",
       "                0.681109,\n",
       "                0.681114,\n",
       "                0.681125,\n",
       "                0.681142,\n",
       "                0.681148,\n",
       "                0.681176,\n",
       "                0.681243,\n",
       "                0.681297,\n",
       "                0.68136,\n",
       "                0.681373,\n",
       "                0.681389,\n",
       "                0.681404,\n",
       "                0.681419,\n",
       "                0.681469,\n",
       "                0.681542,\n",
       "                0.681581,\n",
       "                0.681599,\n",
       "                0.681611,\n",
       "                0.681642,\n",
       "                0.681648,\n",
       "                0.681666,\n",
       "                0.681681,\n",
       "                0.681711,\n",
       "                0.681724,\n",
       "                0.681731,\n",
       "                0.681748,\n",
       "                0.681768,\n",
       "                0.681781,\n",
       "                0.681825,\n",
       "                0.681841,\n",
       "                0.681887,\n",
       "                0.681898,\n",
       "                0.681912,\n",
       "                0.681922,\n",
       "                0.681937]),\n",
       "              ('merror',\n",
       "               [0.736567,\n",
       "                0.734123,\n",
       "                0.733287,\n",
       "                0.732624,\n",
       "                0.732093,\n",
       "                0.731596,\n",
       "                0.731194,\n",
       "                0.730924,\n",
       "                0.730526,\n",
       "                0.730136,\n",
       "                0.729627,\n",
       "                0.729457,\n",
       "                0.729307,\n",
       "                0.729097,\n",
       "                0.72865,\n",
       "                0.728462,\n",
       "                0.728281,\n",
       "                0.728087,\n",
       "                0.727865,\n",
       "                0.72766,\n",
       "                0.727516,\n",
       "                0.727328,\n",
       "                0.727033,\n",
       "                0.72682,\n",
       "                0.726528,\n",
       "                0.726331,\n",
       "                0.726146,\n",
       "                0.725933,\n",
       "                0.725733,\n",
       "                0.725498,\n",
       "                0.725218,\n",
       "                0.725064,\n",
       "                0.724871,\n",
       "                0.724663,\n",
       "                0.724455,\n",
       "                0.724101,\n",
       "                0.723866,\n",
       "                0.723614,\n",
       "                0.723313,\n",
       "                0.723139,\n",
       "                0.722898,\n",
       "                0.722674,\n",
       "                0.72244,\n",
       "                0.722268,\n",
       "                0.722085,\n",
       "                0.721878,\n",
       "                0.721729,\n",
       "                0.721451,\n",
       "                0.721261,\n",
       "                0.721111,\n",
       "                0.720938,\n",
       "                0.720817,\n",
       "                0.72056,\n",
       "                0.72037,\n",
       "                0.720195,\n",
       "                0.719901,\n",
       "                0.719704,\n",
       "                0.719465,\n",
       "                0.719327,\n",
       "                0.719086,\n",
       "                0.71886,\n",
       "                0.718676,\n",
       "                0.718485,\n",
       "                0.718345,\n",
       "                0.718081,\n",
       "                0.717925,\n",
       "                0.717798,\n",
       "                0.717584,\n",
       "                0.717402,\n",
       "                0.717194,\n",
       "                0.716954,\n",
       "                0.716775,\n",
       "                0.716569,\n",
       "                0.71638,\n",
       "                0.716216,\n",
       "                0.716011,\n",
       "                0.715823,\n",
       "                0.715645,\n",
       "                0.715486,\n",
       "                0.715216,\n",
       "                0.71509,\n",
       "                0.714905,\n",
       "                0.714731,\n",
       "                0.714552,\n",
       "                0.714427,\n",
       "                0.714291,\n",
       "                0.714078,\n",
       "                0.71394,\n",
       "                0.71374,\n",
       "                0.713625,\n",
       "                0.713416,\n",
       "                0.71326,\n",
       "                0.713118,\n",
       "                0.71294,\n",
       "                0.712771,\n",
       "                0.712584,\n",
       "                0.712466,\n",
       "                0.712347,\n",
       "                0.712194,\n",
       "                0.712007,\n",
       "                0.711866,\n",
       "                0.711762,\n",
       "                0.711646,\n",
       "                0.711515,\n",
       "                0.711271,\n",
       "                0.71117,\n",
       "                0.711008,\n",
       "                0.710895,\n",
       "                0.710786,\n",
       "                0.710699,\n",
       "                0.710572,\n",
       "                0.710429,\n",
       "                0.710301,\n",
       "                0.710233,\n",
       "                0.710098,\n",
       "                0.709965,\n",
       "                0.709799,\n",
       "                0.709727,\n",
       "                0.709592,\n",
       "                0.709422,\n",
       "                0.709292,\n",
       "                0.709139,\n",
       "                0.708986,\n",
       "                0.708837,\n",
       "                0.708743,\n",
       "                0.70863,\n",
       "                0.708564,\n",
       "                0.708518,\n",
       "                0.708465,\n",
       "                0.708408,\n",
       "                0.708354,\n",
       "                0.708295,\n",
       "                0.708238,\n",
       "                0.708167,\n",
       "                0.708065,\n",
       "                0.708008,\n",
       "                0.70796,\n",
       "                0.707828,\n",
       "                0.707759,\n",
       "                0.707672,\n",
       "                0.707609,\n",
       "                0.707545,\n",
       "                0.707455,\n",
       "                0.707402,\n",
       "                0.707346,\n",
       "                0.707296,\n",
       "                0.707199,\n",
       "                0.707134,\n",
       "                0.707047,\n",
       "                0.706961,\n",
       "                0.706909,\n",
       "                0.706842,\n",
       "                0.706767,\n",
       "                0.706737,\n",
       "                0.706718,\n",
       "                0.706687,\n",
       "                0.706645,\n",
       "                0.70648,\n",
       "                0.706468,\n",
       "                0.706433,\n",
       "                0.706415,\n",
       "                0.706409,\n",
       "                0.706365,\n",
       "                0.706351,\n",
       "                0.706338,\n",
       "                0.706305,\n",
       "                0.706298,\n",
       "                0.706279,\n",
       "                0.706264,\n",
       "                0.706252,\n",
       "                0.706232,\n",
       "                0.706168,\n",
       "                0.706121,\n",
       "                0.706088,\n",
       "                0.70608,\n",
       "                0.706062,\n",
       "                0.706038,\n",
       "                0.70602,\n",
       "                0.705938,\n",
       "                0.705859,\n",
       "                0.705835,\n",
       "                0.705819,\n",
       "                0.705809,\n",
       "                0.705772,\n",
       "                0.705756,\n",
       "                0.705754,\n",
       "                0.705724,\n",
       "                0.705688,\n",
       "                0.705675,\n",
       "                0.705644,\n",
       "                0.705621,\n",
       "                0.705604,\n",
       "                0.705576,\n",
       "                0.705528,\n",
       "                0.705507,\n",
       "                0.705469,\n",
       "                0.705457,\n",
       "                0.705441,\n",
       "                0.705429,\n",
       "                0.705403])]),\n",
       " 'test': OrderedDict([('mlogloss',\n",
       "               [2.273966,\n",
       "                2.250428,\n",
       "                2.230677,\n",
       "                2.213754,\n",
       "                2.199225,\n",
       "                2.186585,\n",
       "                2.175527,\n",
       "                2.165833,\n",
       "                2.157122,\n",
       "                2.149473,\n",
       "                2.142651,\n",
       "                2.136662,\n",
       "                2.131266,\n",
       "                2.126448,\n",
       "                2.122146,\n",
       "                2.118271,\n",
       "                2.114859,\n",
       "                2.111758,\n",
       "                2.108978,\n",
       "                2.106493,\n",
       "                2.104311,\n",
       "                2.102141,\n",
       "                2.100348,\n",
       "                2.098594,\n",
       "                2.096948,\n",
       "                2.095634,\n",
       "                2.094353,\n",
       "                2.093259,\n",
       "                2.092155,\n",
       "                2.091199,\n",
       "                2.090338,\n",
       "                2.089609,\n",
       "                2.088794,\n",
       "                2.088084,\n",
       "                2.087566,\n",
       "                2.086879,\n",
       "                2.086292,\n",
       "                2.085867,\n",
       "                2.085336,\n",
       "                2.08488,\n",
       "                2.084435,\n",
       "                2.084051,\n",
       "                2.083765,\n",
       "                2.083475,\n",
       "                2.083183,\n",
       "                2.083005,\n",
       "                2.08284,\n",
       "                2.082611,\n",
       "                2.082492,\n",
       "                2.082344,\n",
       "                2.082157,\n",
       "                2.08208,\n",
       "                2.082,\n",
       "                2.081881,\n",
       "                2.081804,\n",
       "                2.081594,\n",
       "                2.081518,\n",
       "                2.081397,\n",
       "                2.081355,\n",
       "                2.081302,\n",
       "                2.081173,\n",
       "                2.081138,\n",
       "                2.081072,\n",
       "                2.08103,\n",
       "                2.080996,\n",
       "                2.080967,\n",
       "                2.080953,\n",
       "                2.080883,\n",
       "                2.08084,\n",
       "                2.080801,\n",
       "                2.080752,\n",
       "                2.080732,\n",
       "                2.080695,\n",
       "                2.080673,\n",
       "                2.080658,\n",
       "                2.080602,\n",
       "                2.080593,\n",
       "                2.080565,\n",
       "                2.080539,\n",
       "                2.080512,\n",
       "                2.080488,\n",
       "                2.080473,\n",
       "                2.080469,\n",
       "                2.08046,\n",
       "                2.080437,\n",
       "                2.080427,\n",
       "                2.080432,\n",
       "                2.080418,\n",
       "                2.080394,\n",
       "                2.080377,\n",
       "                2.080375,\n",
       "                2.080362,\n",
       "                2.080354,\n",
       "                2.080343,\n",
       "                2.080349,\n",
       "                2.08034,\n",
       "                2.080347,\n",
       "                2.080342,\n",
       "                2.080329,\n",
       "                2.080311,\n",
       "                2.080294,\n",
       "                2.080276,\n",
       "                2.080289,\n",
       "                2.080271,\n",
       "                2.08027,\n",
       "                2.080249,\n",
       "                2.080216,\n",
       "                2.080196,\n",
       "                2.080184,\n",
       "                2.080182,\n",
       "                2.080157,\n",
       "                2.080133,\n",
       "                2.08011,\n",
       "                2.080096,\n",
       "                2.08008,\n",
       "                2.080082,\n",
       "                2.080074,\n",
       "                2.080047,\n",
       "                2.080029,\n",
       "                2.080013,\n",
       "                2.079996,\n",
       "                2.079986,\n",
       "                2.079977,\n",
       "                2.079978,\n",
       "                2.079966,\n",
       "                2.079957,\n",
       "                2.07994,\n",
       "                2.079918,\n",
       "                2.079911,\n",
       "                2.07991,\n",
       "                2.079897,\n",
       "                2.079887,\n",
       "                2.07987,\n",
       "                2.079848,\n",
       "                2.079819,\n",
       "                2.079782,\n",
       "                2.079779,\n",
       "                2.079745,\n",
       "                2.07973,\n",
       "                2.079714,\n",
       "                2.07969,\n",
       "                2.079686,\n",
       "                2.079694,\n",
       "                2.079683,\n",
       "                2.079683,\n",
       "                2.079665,\n",
       "                2.079645,\n",
       "                2.079633,\n",
       "                2.079602,\n",
       "                2.079573,\n",
       "                2.079575,\n",
       "                2.079566,\n",
       "                2.079545,\n",
       "                2.079539,\n",
       "                2.079537,\n",
       "                2.079527,\n",
       "                2.07952,\n",
       "                2.079513,\n",
       "                2.079507,\n",
       "                2.079505,\n",
       "                2.0795,\n",
       "                2.079493,\n",
       "                2.079496,\n",
       "                2.079484,\n",
       "                2.079475,\n",
       "                2.07947,\n",
       "                2.079468,\n",
       "                2.079465,\n",
       "                2.079451,\n",
       "                2.07945,\n",
       "                2.079444,\n",
       "                2.079444,\n",
       "                2.079428,\n",
       "                2.079413,\n",
       "                2.079404,\n",
       "                2.079399,\n",
       "                2.079379,\n",
       "                2.079372,\n",
       "                2.079349,\n",
       "                2.079342,\n",
       "                2.079335,\n",
       "                2.079337,\n",
       "                2.079336,\n",
       "                2.079337,\n",
       "                2.079331,\n",
       "                2.079327,\n",
       "                2.079313,\n",
       "                2.079311,\n",
       "                2.079301,\n",
       "                2.079287,\n",
       "                2.079275,\n",
       "                2.079274,\n",
       "                2.079258,\n",
       "                2.079248,\n",
       "                2.079233,\n",
       "                2.07923,\n",
       "                2.07923,\n",
       "                2.079229,\n",
       "                2.079225,\n",
       "                2.079212]),\n",
       "              ('auc',\n",
       "               [0.598253,\n",
       "                0.601141,\n",
       "                0.602342,\n",
       "                0.603543,\n",
       "                0.604139,\n",
       "                0.60494,\n",
       "                0.605443,\n",
       "                0.605851,\n",
       "                0.606437,\n",
       "                0.606949,\n",
       "                0.607375,\n",
       "                0.607739,\n",
       "                0.608041,\n",
       "                0.608347,\n",
       "                0.60867,\n",
       "                0.608922,\n",
       "                0.609089,\n",
       "                0.60928,\n",
       "                0.609503,\n",
       "                0.609693,\n",
       "                0.60982,\n",
       "                0.61015,\n",
       "                0.610302,\n",
       "                0.610542,\n",
       "                0.610849,\n",
       "                0.61098,\n",
       "                0.611182,\n",
       "                0.611318,\n",
       "                0.611519,\n",
       "                0.61164,\n",
       "                0.611776,\n",
       "                0.611853,\n",
       "                0.612054,\n",
       "                0.61219,\n",
       "                0.612242,\n",
       "                0.612456,\n",
       "                0.61262,\n",
       "                0.612678,\n",
       "                0.612842,\n",
       "                0.612986,\n",
       "                0.613119,\n",
       "                0.61323,\n",
       "                0.613288,\n",
       "                0.613347,\n",
       "                0.613418,\n",
       "                0.613423,\n",
       "                0.613461,\n",
       "                0.613496,\n",
       "                0.61352,\n",
       "                0.61354,\n",
       "                0.6136,\n",
       "                0.613591,\n",
       "                0.613589,\n",
       "                0.613613,\n",
       "                0.613626,\n",
       "                0.61372,\n",
       "                0.613748,\n",
       "                0.613798,\n",
       "                0.613788,\n",
       "                0.6138,\n",
       "                0.613858,\n",
       "                0.613856,\n",
       "                0.613858,\n",
       "                0.613861,\n",
       "                0.613865,\n",
       "                0.613865,\n",
       "                0.613868,\n",
       "                0.613893,\n",
       "                0.613916,\n",
       "                0.613913,\n",
       "                0.613942,\n",
       "                0.613944,\n",
       "                0.613963,\n",
       "                0.613981,\n",
       "                0.61398,\n",
       "                0.614018,\n",
       "                0.614012,\n",
       "                0.61403,\n",
       "                0.614051,\n",
       "                0.614065,\n",
       "                0.614085,\n",
       "                0.614102,\n",
       "                0.614087,\n",
       "                0.614089,\n",
       "                0.614105,\n",
       "                0.614113,\n",
       "                0.6141,\n",
       "                0.61411,\n",
       "                0.614128,\n",
       "                0.614128,\n",
       "                0.614123,\n",
       "                0.614124,\n",
       "                0.614132,\n",
       "                0.61414,\n",
       "                0.614134,\n",
       "                0.614134,\n",
       "                0.614128,\n",
       "                0.614134,\n",
       "                0.614138,\n",
       "                0.614145,\n",
       "                0.614153,\n",
       "                0.614162,\n",
       "                0.61415,\n",
       "                0.614162,\n",
       "                0.614155,\n",
       "                0.614178,\n",
       "                0.614195,\n",
       "                0.614212,\n",
       "                0.614221,\n",
       "                0.614221,\n",
       "                0.614237,\n",
       "                0.614244,\n",
       "                0.61426,\n",
       "                0.61427,\n",
       "                0.614282,\n",
       "                0.614284,\n",
       "                0.61429,\n",
       "                0.614312,\n",
       "                0.614321,\n",
       "                0.614324,\n",
       "                0.614341,\n",
       "                0.614347,\n",
       "                0.614351,\n",
       "                0.61435,\n",
       "                0.614353,\n",
       "                0.614354,\n",
       "                0.614365,\n",
       "                0.61438,\n",
       "                0.614384,\n",
       "                0.614381,\n",
       "                0.614389,\n",
       "                0.614395,\n",
       "                0.614405,\n",
       "                0.61442,\n",
       "                0.614442,\n",
       "                0.61446,\n",
       "                0.614462,\n",
       "                0.614475,\n",
       "                0.614485,\n",
       "                0.614492,\n",
       "                0.614502,\n",
       "                0.614504,\n",
       "                0.614497,\n",
       "                0.614501,\n",
       "                0.614498,\n",
       "                0.614504,\n",
       "                0.614515,\n",
       "                0.614524,\n",
       "                0.614536,\n",
       "                0.614551,\n",
       "                0.61455,\n",
       "                0.614554,\n",
       "                0.614567,\n",
       "                0.614571,\n",
       "                0.614571,\n",
       "                0.614577,\n",
       "                0.61458,\n",
       "                0.614577,\n",
       "                0.61458,\n",
       "                0.61458,\n",
       "                0.614584,\n",
       "                0.614586,\n",
       "                0.614584,\n",
       "                0.61459,\n",
       "                0.614594,\n",
       "                0.614598,\n",
       "                0.6146,\n",
       "                0.614601,\n",
       "                0.614609,\n",
       "                0.61461,\n",
       "                0.614611,\n",
       "                0.614604,\n",
       "                0.614612,\n",
       "                0.614622,\n",
       "                0.614628,\n",
       "                0.61463,\n",
       "                0.61464,\n",
       "                0.614643,\n",
       "                0.614659,\n",
       "                0.614665,\n",
       "                0.614668,\n",
       "                0.614668,\n",
       "                0.614669,\n",
       "                0.614669,\n",
       "                0.614672,\n",
       "                0.614676,\n",
       "                0.614683,\n",
       "                0.614684,\n",
       "                0.614688,\n",
       "                0.614696,\n",
       "                0.614702,\n",
       "                0.614704,\n",
       "                0.614712,\n",
       "                0.614716,\n",
       "                0.614726,\n",
       "                0.614729,\n",
       "                0.61473,\n",
       "                0.61473,\n",
       "                0.614731,\n",
       "                0.614737]),\n",
       "              ('merror',\n",
       "               [0.73782,\n",
       "                0.736012,\n",
       "                0.734923,\n",
       "                0.734415,\n",
       "                0.734394,\n",
       "                0.733806,\n",
       "                0.733643,\n",
       "                0.733631,\n",
       "                0.73342,\n",
       "                0.732988,\n",
       "                0.732547,\n",
       "                0.732556,\n",
       "                0.732391,\n",
       "                0.732311,\n",
       "                0.732046,\n",
       "                0.73203,\n",
       "                0.731683,\n",
       "                0.731566,\n",
       "                0.731479,\n",
       "                0.731341,\n",
       "                0.73133,\n",
       "                0.731284,\n",
       "                0.731188,\n",
       "                0.731063,\n",
       "                0.730976,\n",
       "                0.730907,\n",
       "                0.730913,\n",
       "                0.730882,\n",
       "                0.730976,\n",
       "                0.730836,\n",
       "                0.730779,\n",
       "                0.730742,\n",
       "                0.730683,\n",
       "                0.730523,\n",
       "                0.730479,\n",
       "                0.730354,\n",
       "                0.730351,\n",
       "                0.730258,\n",
       "                0.730009,\n",
       "                0.729901,\n",
       "                0.729949,\n",
       "                0.729951,\n",
       "                0.729876,\n",
       "                0.729736,\n",
       "                0.729702,\n",
       "                0.72969,\n",
       "                0.729656,\n",
       "                0.729569,\n",
       "                0.729602,\n",
       "                0.729469,\n",
       "                0.7294,\n",
       "                0.729448,\n",
       "                0.729412,\n",
       "                0.729385,\n",
       "                0.729333,\n",
       "                0.729258,\n",
       "                0.729235,\n",
       "                0.729224,\n",
       "                0.729268,\n",
       "                0.729203,\n",
       "                0.729218,\n",
       "                0.72917,\n",
       "                0.729254,\n",
       "                0.729312,\n",
       "                0.729208,\n",
       "                0.729199,\n",
       "                0.729228,\n",
       "                0.729258,\n",
       "                0.72926,\n",
       "                0.72926,\n",
       "                0.729176,\n",
       "                0.729158,\n",
       "                0.729145,\n",
       "                0.729137,\n",
       "                0.729116,\n",
       "                0.729064,\n",
       "                0.729074,\n",
       "                0.728995,\n",
       "                0.728963,\n",
       "                0.728934,\n",
       "                0.728982,\n",
       "                0.728997,\n",
       "                0.729076,\n",
       "                0.729028,\n",
       "                0.729053,\n",
       "                0.729068,\n",
       "                0.729001,\n",
       "                0.729036,\n",
       "                0.728955,\n",
       "                0.72892,\n",
       "                0.728886,\n",
       "                0.72892,\n",
       "                0.728799,\n",
       "                0.728817,\n",
       "                0.728861,\n",
       "                0.728884,\n",
       "                0.728847,\n",
       "                0.728849,\n",
       "                0.728865,\n",
       "                0.728867,\n",
       "                0.728805,\n",
       "                0.728799,\n",
       "                0.728834,\n",
       "                0.72879,\n",
       "                0.728803,\n",
       "                0.728744,\n",
       "                0.728688,\n",
       "                0.728663,\n",
       "                0.728609,\n",
       "                0.72864,\n",
       "                0.728581,\n",
       "                0.728598,\n",
       "                0.728586,\n",
       "                0.72855,\n",
       "                0.728525,\n",
       "                0.728571,\n",
       "                0.728538,\n",
       "                0.728569,\n",
       "                0.728561,\n",
       "                0.728542,\n",
       "                0.728563,\n",
       "                0.728604,\n",
       "                0.728617,\n",
       "                0.728682,\n",
       "                0.728667,\n",
       "                0.728623,\n",
       "                0.728665,\n",
       "                0.728632,\n",
       "                0.728638,\n",
       "                0.728625,\n",
       "                0.728669,\n",
       "                0.728565,\n",
       "                0.728506,\n",
       "                0.728554,\n",
       "                0.728504,\n",
       "                0.728462,\n",
       "                0.728475,\n",
       "                0.72846,\n",
       "                0.72841,\n",
       "                0.728364,\n",
       "                0.728392,\n",
       "                0.728348,\n",
       "                0.728398,\n",
       "                0.728406,\n",
       "                0.7284,\n",
       "                0.728389,\n",
       "                0.728379,\n",
       "                0.728398,\n",
       "                0.728362,\n",
       "                0.728312,\n",
       "                0.728314,\n",
       "                0.728258,\n",
       "                0.72827,\n",
       "                0.72825,\n",
       "                0.728233,\n",
       "                0.728225,\n",
       "                0.728222,\n",
       "                0.728225,\n",
       "                0.728212,\n",
       "                0.728254,\n",
       "                0.728247,\n",
       "                0.728237,\n",
       "                0.72825,\n",
       "                0.728252,\n",
       "                0.728258,\n",
       "                0.728306,\n",
       "                0.728293,\n",
       "                0.728279,\n",
       "                0.728268,\n",
       "                0.728266,\n",
       "                0.728277,\n",
       "                0.728266,\n",
       "                0.728235,\n",
       "                0.72826,\n",
       "                0.728245,\n",
       "                0.728235,\n",
       "                0.72822,\n",
       "                0.728212,\n",
       "                0.728179,\n",
       "                0.728131,\n",
       "                0.728166,\n",
       "                0.728193,\n",
       "                0.728218,\n",
       "                0.728222,\n",
       "                0.728206,\n",
       "                0.728197,\n",
       "                0.728195,\n",
       "                0.728195,\n",
       "                0.728195,\n",
       "                0.728183,\n",
       "                0.728183,\n",
       "                0.728197,\n",
       "                0.728202,\n",
       "                0.728214,\n",
       "                0.728229,\n",
       "                0.728227,\n",
       "                0.728225,\n",
       "                0.728239,\n",
       "                0.728229,\n",
       "                0.728218])])}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test error using softmax = 0.725886524000436\n"
     ]
    }
   ],
   "source": [
    "# get prediction\n",
    "pred = wl_bst_sm.predict(xg_test)\n",
    "# pred = pred.astype(np.uint8)\n",
    "error_rate = np.sum(pred != y_test) / y_test.shape[0]\n",
    "print('Test error using softmax = {}'.format(error_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[0]\\teval-mlogloss:2.064721\\teval-auc:0.648317\\teval-merror:0.725887'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_s = wl_bst_sm.eval(xg_test)\n",
    "# eval_dict = eval_str_2_dict(eval_s)\n",
    "eval_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.66875439, 0.59651881, 0.51571881, 0.50515382, 0.50375124,\n",
       "        0.50399863, 0.50695963, 0.50318814, 0.51005315, 0.63726772]),\n",
       " 2.4058325431151597)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = np.arange(0, 1, 0.1)\n",
    "aucs = auc(y_test.astype(np.uint8), pred.astype(np.uint8), np.arange(param['num_class']))\n",
    "# aucs[aucs == 0.5] = 0\n",
    "w_auc = (aucs * weights).sum()\n",
    "aucs, w_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "report = metrics.classification_report(list(y_test), list(pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.36      0.51      0.42     69934\n",
      "           1       0.26      0.43      0.32     69934\n",
      "           2       0.18      0.10      0.13     56539\n",
      "           3       0.17      0.02      0.04     39454\n",
      "           4       0.19      0.01      0.02     31033\n",
      "           5       0.18      0.01      0.02     25740\n",
      "           6       0.25      0.02      0.03     22516\n",
      "           7       0.21      0.01      0.02     21195\n",
      "           8       0.26      0.02      0.04     24984\n",
      "           9       0.25      0.63      0.36     69934\n",
      "\n",
      "    accuracy                           0.27    431263\n",
      "   macro avg       0.23      0.18      0.14    431263\n",
      "weighted avg       0.24      0.27      0.21    431263\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "del X_train, y_train, X_test, y_test, xg_train, xg_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 调参"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn import svm, datasets\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from collections.abc import Iterable\n",
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "base_param = {  # 基本参数，不需要调参\n",
    "    'objective': 'multi:softmax',\n",
    "    'eta': 0.1,\n",
    "    'nthread': 8,\n",
    "    'num_class': 10,\n",
    "    'gpu_id': 0,\n",
    "    'tree_method': 'gpu_hist',\n",
    "    'eval_metric': ['mlogloss', 'auc', 'merror']\n",
    "} \n",
    "# 需要调参的参数\n",
    "ps1 = {  \n",
    "    'max_depth': list(range(5, 14, 2)),\n",
    "    'min_child_weight': list(range(1, 10, 2)),\n",
    "}\n",
    "\n",
    "ps2 = {\n",
    "    'gamma': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6]\n",
    "}\n",
    "\n",
    "ps3 = {\n",
    "    'subsample':  [i/10.0 for i in range(6,11,1)], \n",
    "    'colsample_bytree':  [i/10.0 for i in range(6,11,1)] \n",
    "}\n",
    "\n",
    "ps4 = {'reg_alpha': [0, 0.1, 0.2, 0.5, 1, 1.5, 2, 4]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 529,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data preparing finished ...\t\t(5.092s)\n"
     ]
    }
   ],
   "source": [
    "t0 = time()\n",
    "xg_train = xgb.DMatrix(X_train.values, label=y_train.values, enable_categorical=True)\n",
    "xg_test = xgb.DMatrix(X_test.values, label=y_test.values, enable_categorical=True)\n",
    "print(f\"Data preparing finished ...\\t\\t({time()-t0:.3f}s)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def myproduct(*iterables):\n",
    "    n = len(iterables)\n",
    "#     print(iterables)\n",
    "    if n == 0:\n",
    "        return None \n",
    "    \n",
    "    ret = []\n",
    "    ret.extend([[e] for e in iterables[0].copy()])\n",
    "    if n == 1:\n",
    "        return ret\n",
    "\n",
    "    # 将需要调参的参数进行组合，即笛卡尔乘积。类似于sklearn中的 ParameterGrid\n",
    "    for k in range(1, n):\n",
    "        v = iterables[k].copy()\n",
    "        l = len(ret)\n",
    "        ret = [ret[i%l].copy() for i in range(len(v) * len(ret))]\n",
    "        for i, e in enumerate(ret):\n",
    "            e.append(v[i // l])\n",
    "    return ret\n",
    "\n",
    "def compose_param_grid(grid, base):\n",
    "    items = list(grid.items())\n",
    "    iterables = [item[1] for item in items]\n",
    "    keys = [item[0] for item in items]\n",
    "\n",
    "    ret = myproduct(*iterables)\n",
    "    com_ps = [dict(zip(keys, e)) for e in ret]\n",
    "\n",
    "\n",
    "    all_params = [base.copy() for _ in range(len(com_ps))] \n",
    "    for i in range(len(com_ps)):\n",
    "        all_params[i].update(com_ps[i])\n",
    "        \n",
    "    return all_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始搜索：colsample_bytree=[0.6, 0.7, 0.8, 0.9, 1.0], subsample=[0.6, 0.7, 0.8, 0.9, 1.0]\n",
      "待搜索的参数组合数量：25\n",
      "1 / 25: 200-rounds Training finished param={'objective': 'multi:softmax', 'eta': 0.1, 'nthread': 8, 'num_class': 10, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['mlogloss', 'auc', 'merror'], 'max_depth': 9, 'min_child_weight': 9, 'gamma': 0.2, 'subsample': 0.6, 'colsample_bytree': 0.6} ...\t\t(820.946s)\n",
      "2 / 25: 200-rounds Training finished param={'objective': 'multi:softmax', 'eta': 0.1, 'nthread': 8, 'num_class': 10, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['mlogloss', 'auc', 'merror'], 'max_depth': 9, 'min_child_weight': 9, 'gamma': 0.2, 'subsample': 0.7, 'colsample_bytree': 0.6} ...\t\t(816.416s)\n",
      "3 / 25: 200-rounds Training finished param={'objective': 'multi:softmax', 'eta': 0.1, 'nthread': 8, 'num_class': 10, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['mlogloss', 'auc', 'merror'], 'max_depth': 9, 'min_child_weight': 9, 'gamma': 0.2, 'subsample': 0.8, 'colsample_bytree': 0.6} ...\t\t(828.957s)\n",
      "4 / 25: 200-rounds Training finished param={'objective': 'multi:softmax', 'eta': 0.1, 'nthread': 8, 'num_class': 10, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['mlogloss', 'auc', 'merror'], 'max_depth': 9, 'min_child_weight': 9, 'gamma': 0.2, 'subsample': 0.9, 'colsample_bytree': 0.6} ...\t\t(838.505s)\n",
      "5 / 25: 200-rounds Training finished param={'objective': 'multi:softmax', 'eta': 0.1, 'nthread': 8, 'num_class': 10, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['mlogloss', 'auc', 'merror'], 'max_depth': 9, 'min_child_weight': 9, 'gamma': 0.2, 'subsample': 1.0, 'colsample_bytree': 0.6} ...\t\t(823.276s)\n",
      "6 / 25: 200-rounds Training finished param={'objective': 'multi:softmax', 'eta': 0.1, 'nthread': 8, 'num_class': 10, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['mlogloss', 'auc', 'merror'], 'max_depth': 9, 'min_child_weight': 9, 'gamma': 0.2, 'subsample': 0.6, 'colsample_bytree': 0.7} ...\t\t(828.692s)\n",
      "7 / 25: 200-rounds Training finished param={'objective': 'multi:softmax', 'eta': 0.1, 'nthread': 8, 'num_class': 10, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['mlogloss', 'auc', 'merror'], 'max_depth': 9, 'min_child_weight': 9, 'gamma': 0.2, 'subsample': 0.7, 'colsample_bytree': 0.7} ...\t\t(848.865s)\n",
      "8 / 25: 200-rounds Training finished param={'objective': 'multi:softmax', 'eta': 0.1, 'nthread': 8, 'num_class': 10, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['mlogloss', 'auc', 'merror'], 'max_depth': 9, 'min_child_weight': 9, 'gamma': 0.2, 'subsample': 0.8, 'colsample_bytree': 0.7} ...\t\t(854.009s)\n",
      "9 / 25: 200-rounds Training finished param={'objective': 'multi:softmax', 'eta': 0.1, 'nthread': 8, 'num_class': 10, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['mlogloss', 'auc', 'merror'], 'max_depth': 9, 'min_child_weight': 9, 'gamma': 0.2, 'subsample': 0.9, 'colsample_bytree': 0.7} ...\t\t(863.061s)\n",
      "10 / 25: 200-rounds Training finished param={'objective': 'multi:softmax', 'eta': 0.1, 'nthread': 8, 'num_class': 10, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['mlogloss', 'auc', 'merror'], 'max_depth': 9, 'min_child_weight': 9, 'gamma': 0.2, 'subsample': 1.0, 'colsample_bytree': 0.7} ...\t\t(854.728s)\n",
      "11 / 25: 200-rounds Training finished param={'objective': 'multi:softmax', 'eta': 0.1, 'nthread': 8, 'num_class': 10, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['mlogloss', 'auc', 'merror'], 'max_depth': 9, 'min_child_weight': 9, 'gamma': 0.2, 'subsample': 0.6, 'colsample_bytree': 0.8} ...\t\t(833.519s)\n",
      "12 / 25: 200-rounds Training finished param={'objective': 'multi:softmax', 'eta': 0.1, 'nthread': 8, 'num_class': 10, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['mlogloss', 'auc', 'merror'], 'max_depth': 9, 'min_child_weight': 9, 'gamma': 0.2, 'subsample': 0.7, 'colsample_bytree': 0.8} ...\t\t(836.241s)\n",
      "13 / 25: 200-rounds Training finished param={'objective': 'multi:softmax', 'eta': 0.1, 'nthread': 8, 'num_class': 10, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['mlogloss', 'auc', 'merror'], 'max_depth': 9, 'min_child_weight': 9, 'gamma': 0.2, 'subsample': 0.8, 'colsample_bytree': 0.8} ...\t\t(852.569s)\n",
      "14 / 25: 200-rounds Training finished param={'objective': 'multi:softmax', 'eta': 0.1, 'nthread': 8, 'num_class': 10, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['mlogloss', 'auc', 'merror'], 'max_depth': 9, 'min_child_weight': 9, 'gamma': 0.2, 'subsample': 0.9, 'colsample_bytree': 0.8} ...\t\t(867.649s)\n",
      "15 / 25: 200-rounds Training finished param={'objective': 'multi:softmax', 'eta': 0.1, 'nthread': 8, 'num_class': 10, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['mlogloss', 'auc', 'merror'], 'max_depth': 9, 'min_child_weight': 9, 'gamma': 0.2, 'subsample': 1.0, 'colsample_bytree': 0.8} ...\t\t(837.901s)\n",
      "16 / 25: 200-rounds Training finished param={'objective': 'multi:softmax', 'eta': 0.1, 'nthread': 8, 'num_class': 10, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['mlogloss', 'auc', 'merror'], 'max_depth': 9, 'min_child_weight': 9, 'gamma': 0.2, 'subsample': 0.6, 'colsample_bytree': 0.9} ...\t\t(827.339s)\n",
      "17 / 25: 200-rounds Training finished param={'objective': 'multi:softmax', 'eta': 0.1, 'nthread': 8, 'num_class': 10, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['mlogloss', 'auc', 'merror'], 'max_depth': 9, 'min_child_weight': 9, 'gamma': 0.2, 'subsample': 0.7, 'colsample_bytree': 0.9} ...\t\t(829.429s)\n",
      "18 / 25: 200-rounds Training finished param={'objective': 'multi:softmax', 'eta': 0.1, 'nthread': 8, 'num_class': 10, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['mlogloss', 'auc', 'merror'], 'max_depth': 9, 'min_child_weight': 9, 'gamma': 0.2, 'subsample': 0.8, 'colsample_bytree': 0.9} ...\t\t(854.584s)\n",
      "19 / 25: 200-rounds Training finished param={'objective': 'multi:softmax', 'eta': 0.1, 'nthread': 8, 'num_class': 10, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['mlogloss', 'auc', 'merror'], 'max_depth': 9, 'min_child_weight': 9, 'gamma': 0.2, 'subsample': 0.9, 'colsample_bytree': 0.9} ...\t\t(865.975s)\n",
      "20 / 25: 200-rounds Training finished param={'objective': 'multi:softmax', 'eta': 0.1, 'nthread': 8, 'num_class': 10, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['mlogloss', 'auc', 'merror'], 'max_depth': 9, 'min_child_weight': 9, 'gamma': 0.2, 'subsample': 1.0, 'colsample_bytree': 0.9} ...\t\t(830.648s)\n",
      "21 / 25: 200-rounds Training finished param={'objective': 'multi:softmax', 'eta': 0.1, 'nthread': 8, 'num_class': 10, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['mlogloss', 'auc', 'merror'], 'max_depth': 9, 'min_child_weight': 9, 'gamma': 0.2, 'subsample': 0.6, 'colsample_bytree': 1.0} ...\t\t(846.784s)\n",
      "22 / 25: 200-rounds Training finished param={'objective': 'multi:softmax', 'eta': 0.1, 'nthread': 8, 'num_class': 10, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['mlogloss', 'auc', 'merror'], 'max_depth': 9, 'min_child_weight': 9, 'gamma': 0.2, 'subsample': 0.7, 'colsample_bytree': 1.0} ...\t\t(852.846s)\n",
      "23 / 25: 200-rounds Training finished param={'objective': 'multi:softmax', 'eta': 0.1, 'nthread': 8, 'num_class': 10, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['mlogloss', 'auc', 'merror'], 'max_depth': 9, 'min_child_weight': 9, 'gamma': 0.2, 'subsample': 0.8, 'colsample_bytree': 1.0} ...\t\t(850.433s)\n",
      "24 / 25: 200-rounds Training finished param={'objective': 'multi:softmax', 'eta': 0.1, 'nthread': 8, 'num_class': 10, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['mlogloss', 'auc', 'merror'], 'max_depth': 9, 'min_child_weight': 9, 'gamma': 0.2, 'subsample': 0.9, 'colsample_bytree': 1.0} ...\t\t(865.453s)\n",
      "25 / 25: 200-rounds Training finished param={'objective': 'multi:softmax', 'eta': 0.1, 'nthread': 8, 'num_class': 10, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['mlogloss', 'auc', 'merror'], 'max_depth': 9, 'min_child_weight': 9, 'gamma': 0.2, 'subsample': 1.0, 'colsample_bytree': 1.0} ...\t\t(833.121s)\n",
      "colsample_bytree=0.6, eta=0.1, eval_metric=['mlogloss', 'auc', 'merror'], gamma=0.2, gpu_id=0, max_depth=9, min_child_weight=9, nthread=8, num_class=10, objective=multi:softmax, subsample=0.9, tree_method=gpu_hist\n",
      "colsample_bytree=1.0, eta=0.1, eval_metric=['mlogloss', 'auc', 'merror'], gamma=0.2, gpu_id=0, max_depth=9, min_child_weight=9, nthread=8, num_class=10, objective=multi:softmax, subsample=0.6, tree_method=gpu_hist\n",
      "colsample_bytree=1.0, eta=0.1, eval_metric=['mlogloss', 'auc', 'merror'], gamma=0.2, gpu_id=0, max_depth=9, min_child_weight=9, nthread=8, num_class=10, objective=multi:softmax, subsample=0.9, tree_method=gpu_hist\n",
      "开始搜索：reg_alpha=[0, 0.1, 0.2, 0.5, 1, 1.5, 2, 4]\n",
      "待搜索的参数组合数量：8\n",
      "1 / 8: 200-rounds Training finished param={'objective': 'multi:softmax', 'eta': 0.1, 'nthread': 8, 'num_class': 10, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['mlogloss', 'auc', 'merror'], 'max_depth': 9, 'min_child_weight': 9, 'gamma': 0.2, 'subsample': 0.9, 'colsample_bytree': 0.6, 'reg_alpha': 0} ...\t\t(864.738s)\n",
      "2 / 8: 200-rounds Training finished param={'objective': 'multi:softmax', 'eta': 0.1, 'nthread': 8, 'num_class': 10, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['mlogloss', 'auc', 'merror'], 'max_depth': 9, 'min_child_weight': 9, 'gamma': 0.2, 'subsample': 0.9, 'colsample_bytree': 0.6, 'reg_alpha': 0.1} ...\t\t(871.016s)\n",
      "3 / 8: 200-rounds Training finished param={'objective': 'multi:softmax', 'eta': 0.1, 'nthread': 8, 'num_class': 10, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['mlogloss', 'auc', 'merror'], 'max_depth': 9, 'min_child_weight': 9, 'gamma': 0.2, 'subsample': 0.9, 'colsample_bytree': 0.6, 'reg_alpha': 0.2} ...\t\t(854.900s)\n",
      "4 / 8: 200-rounds Training finished param={'objective': 'multi:softmax', 'eta': 0.1, 'nthread': 8, 'num_class': 10, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['mlogloss', 'auc', 'merror'], 'max_depth': 9, 'min_child_weight': 9, 'gamma': 0.2, 'subsample': 0.9, 'colsample_bytree': 0.6, 'reg_alpha': 0.5} ...\t\t(846.531s)\n",
      "5 / 8: 200-rounds Training finished param={'objective': 'multi:softmax', 'eta': 0.1, 'nthread': 8, 'num_class': 10, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['mlogloss', 'auc', 'merror'], 'max_depth': 9, 'min_child_weight': 9, 'gamma': 0.2, 'subsample': 0.9, 'colsample_bytree': 0.6, 'reg_alpha': 1} ...\t\t(857.497s)\n",
      "6 / 8: 200-rounds Training finished param={'objective': 'multi:softmax', 'eta': 0.1, 'nthread': 8, 'num_class': 10, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['mlogloss', 'auc', 'merror'], 'max_depth': 9, 'min_child_weight': 9, 'gamma': 0.2, 'subsample': 0.9, 'colsample_bytree': 0.6, 'reg_alpha': 1.5} ...\t\t(874.460s)\n",
      "7 / 8: 200-rounds Training finished param={'objective': 'multi:softmax', 'eta': 0.1, 'nthread': 8, 'num_class': 10, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['mlogloss', 'auc', 'merror'], 'max_depth': 9, 'min_child_weight': 9, 'gamma': 0.2, 'subsample': 0.9, 'colsample_bytree': 0.6, 'reg_alpha': 2} ...\t\t(875.624s)\n",
      "8 / 8: 200-rounds Training finished param={'objective': 'multi:softmax', 'eta': 0.1, 'nthread': 8, 'num_class': 10, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['mlogloss', 'auc', 'merror'], 'max_depth': 9, 'min_child_weight': 9, 'gamma': 0.2, 'subsample': 0.9, 'colsample_bytree': 0.6, 'reg_alpha': 4} ...\t\t(902.339s)\n",
      "colsample_bytree=0.6, eta=0.1, eval_metric=['mlogloss', 'auc', 'merror'], gamma=0.2, gpu_id=0, max_depth=9, min_child_weight=9, nthread=8, num_class=10, objective=multi:softmax, reg_alpha=0, subsample=0.9, tree_method=gpu_hist\n",
      "colsample_bytree=0.6, eta=0.1, eval_metric=['mlogloss', 'auc', 'merror'], gamma=0.2, gpu_id=0, max_depth=9, min_child_weight=9, nthread=8, num_class=10, objective=multi:softmax, reg_alpha=1, subsample=0.9, tree_method=gpu_hist\n",
      "colsample_bytree=0.6, eta=0.1, eval_metric=['mlogloss', 'auc', 'merror'], gamma=0.2, gpu_id=0, max_depth=9, min_child_weight=9, nthread=8, num_class=10, objective=multi:softmax, reg_alpha=4, subsample=0.9, tree_method=gpu_hist\n",
      "找到的最棒的参数是：\n",
      "colsample_bytree=0.6, eta=0.1, eval_metric=['mlogloss', 'auc', 'merror'], gamma=0.2, gpu_id=0, max_depth=9, min_child_weight=9, nthread=8, num_class=10, objective=multi:softmax, reg_alpha=0, subsample=0.9, tree_method=gpu_hist\n"
     ]
    }
   ],
   "source": [
    "base = base_param.copy()\n",
    "base.update({'max_depth': 9, 'min_child_weight': 9})\n",
    "base.update({'gamma': .2})\n",
    "grids = [ps3, ps4]\n",
    "\n",
    "rets = []\n",
    "for grid in grids:\n",
    "    params = compose_param_grid(grid, base)\n",
    "    print(f\"开始搜索：{dict_2_str(grid)}\\n待搜索的参数组合数量：{len(params)}\")\n",
    "    ret = gridsearch_cv_xgb(data.values, watch_label_res.values, params, n_round=200, verbose_eval=False, n_class=10)\n",
    "    arr = np.array([[-e['eval-merror'] for e in ret], \n",
    "                    [-e['eval-mlogloss'] for e in ret],\n",
    "                    [e['eval-auc'] for e in ret], \n",
    "                    [e['w_auc'] for e in ret]], dtype=np.float32)\n",
    "    opt_idxs = list(set(arr.argmax(axis=1)))\n",
    "    for i in opt_idxs:\n",
    "        print(dict_2_str(ret[i]['param']))\n",
    "    opt_idx = opt_idxs[0]\n",
    "    opt_param = ret[opt_idx]['param']\n",
    "    base.update(opt_param)\n",
    "    rets.append(ret)\n",
    "    \n",
    "    ks = list(grid.keys())\n",
    "    ks = '-'.join(ks)\n",
    "    with open(f\"./logs/{ks}-watch_label-{int(time())}.pkl\", \"wb\") as f:\n",
    "        pickle.dump(ret, f)\n",
    "    with open(f\"./logs/{ks}-watch_label-{int(time())}.md\", \"w\") as f:\n",
    "        lines = []\n",
    "        for e in ret:\n",
    "            line = metric_2_str(e)\n",
    "            lines.append(line)\n",
    "            lines.append('\\n')\n",
    "        f.write(\"\\n\".join(lines))\n",
    "\n",
    "print(f\"找到的最棒的参数是：\\n{dict_2_str(base)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'objective': 'multi:softmax',\n",
       " 'eta': 0.1,\n",
       " 'nthread': 8,\n",
       " 'num_class': 10,\n",
       " 'gpu_id': 0,\n",
       " 'tree_method': 'gpu_hist',\n",
       " 'eval_metric': ['mlogloss', 'auc', 'merror'],\n",
       " 'max_depth': 9,\n",
       " 'min_child_weight': 9,\n",
       " 'gamma': 0.2,\n",
       " 'subsample': 0.9,\n",
       " 'colsample_bytree': 0.6,\n",
       " 'reg_alpha': 0}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gridsearch_results = gridsearch_xgb(all_params, xg_train, xg_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_cv_results_sh = gridsearch_cv_xgb(data.values, watch_label_res.values, all_params, n_round=200, verbose_eval=False, n_class=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 584,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "最小误差与最大AUC对应的模型不一致 : [93 19]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-584-646c9ffaa26f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mopt_idxs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mopt_idxs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mopt_idxs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"最小误差与最大AUC对应的模型不一致 : {opt_idxs}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mopt_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopt_idxs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: 最小误差与最大AUC对应的模型不一致 : [93 19]"
     ]
    }
   ],
   "source": [
    "arr = np.array([[-e['test_error'] for e in gridsearch_results], [e['w_auc'] for e in gridsearch_results]], dtype=np.float32)\n",
    "opt_idxs = arr.argmax(axis=1)\n",
    "if opt_idxs[0] != opt_idxs[1]:\n",
    "     warnings.warn(f\"最小误差与最大AUC对应的模型不一致 : {opt_idxs}。选择误差最小的模型 : {opt_idxs[0]}\")\n",
    "\n",
    "opt_idx = opt_idxs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 583,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'test_error': 0.7289070620796754,\n",
       "  'aucs': array([0.57555597, 0.58567653, 0.50347593, 0.50017473, 0.50017414,\n",
       "         0.5000983 , 0.50292636, 0.50040085, 0.50878295, 0.60767447]),\n",
       "  'w_auc': 2.365403849959146,\n",
       "  'report': '              precision    recall  f1-score   support\\n\\n           0       0.33      0.19      0.24     43749\\n           1       0.29      0.68      0.41    111616\\n           2       0.23      0.01      0.03     62829\\n           3       0.13      0.00      0.00     43872\\n           4       0.15      0.00      0.00     34228\\n           5       0.18      0.00      0.00     28926\\n           6       0.33      0.01      0.01     25061\\n           7       0.16      0.00      0.00     23400\\n           8       0.31      0.02      0.04     27750\\n           9       0.24      0.56      0.34     77663\\n\\n    accuracy                           0.27    479094\\n   macro avg       0.23      0.15      0.11    479094\\nweighted avg       0.24      0.27      0.18    479094\\n',\n",
       "  'model': <xgboost.core.Booster at 0x7f35ba8b00d0>},\n",
       " {'test_error': 0.73068750600091,\n",
       "  'aucs': array([0.57882633, 0.58668642, 0.50378179, 0.50067885, 0.50046282,\n",
       "         0.50010955, 0.50312644, 0.50043677, 0.50916784, 0.60848382]),\n",
       "  'w_auc': 2.367019878746503,\n",
       "  'report': '              precision    recall  f1-score   support\\n\\n           0       0.32      0.20      0.25     43749\\n           1       0.29      0.66      0.40    111616\\n           2       0.18      0.03      0.04     62829\\n           3       0.12      0.01      0.01     43872\\n           4       0.12      0.00      0.00     34228\\n           5       0.08      0.00      0.00     28926\\n           6       0.25      0.01      0.01     25061\\n           7       0.11      0.00      0.00     23400\\n           8       0.27      0.02      0.04     27750\\n           9       0.24      0.57      0.34     77663\\n\\n    accuracy                           0.27    479094\\n   macro avg       0.20      0.15      0.11    479094\\nweighted avg       0.22      0.27      0.18    479094\\n',\n",
       "  'model': <xgboost.core.Booster at 0x7f3584f94e50>})"
      ]
     },
     "execution_count": 583,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gridsearch_results[93]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 588,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.33      0.19      0.24     43749\n",
      "           1       0.29      0.68      0.41    111616\n",
      "           2       0.23      0.01      0.03     62829\n",
      "           3       0.13      0.00      0.00     43872\n",
      "           4       0.15      0.00      0.00     34228\n",
      "           5       0.18      0.00      0.00     28926\n",
      "           6       0.33      0.01      0.01     25061\n",
      "           7       0.16      0.00      0.00     23400\n",
      "           8       0.31      0.02      0.04     27750\n",
      "           9       0.24      0.56      0.34     77663\n",
      "\n",
      "    accuracy                           0.27    479094\n",
      "   macro avg       0.23      0.15      0.11    479094\n",
      "weighted avg       0.24      0.27      0.18    479094\n",
      "\n"
     ]
    }
   ],
   "source": [
    "opt_idx = opt_idxs[0]\n",
    "opt_param = all_params[opt_idx]\n",
    "print(gridsearch_results[opt_idx]['report'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 557,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(479094,)"
      ]
     },
     "execution_count": 557,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gridsearch_cv_xgb(data.values, watch_label_res, all_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {}\n",
    "# use softmax multi-class classification\n",
    "param['objective'] = 'multi:softmax'\n",
    "# scale weight of positive examples\n",
    "param['eta'] = 0.1\n",
    "param['max_depth'] = 8\n",
    "param['nthread'] = 8\n",
    "param['num_class'] = 10\n",
    "# param['gpu_id'] = 0\n",
    "# param['tree_method'] = 'gpu_hist'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "cv_res= xgb.cv(param, cv_data, num_boost_round=200,early_stopping_rounds=30,nfold=3, metrics='auc',show_stdv=True)\n",
    "print(cv_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB \n",
    "from mlxtend.classifier import StackingClassifier\n",
    "from sklearn.ensemble import (RandomForestClassifier, AdaBoostClassifier, \n",
    "                              GradientBoostingClassifier, ExtraTreesClassifier)\n",
    "from xgboost import XGBClassifier, XGBRFClassifier\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 第一层模型参数\n",
    "first_layer_params = {\n",
    "#     LogisticRegression : {\n",
    "#         'C' : 10,\n",
    "#         'random_state': 0\n",
    "#     },\n",
    "    XGBRFClassifier : {\n",
    "        'n_jobs': 4,\n",
    "        'n_estimators': 200,\n",
    "         #'max_features': 0.2,\n",
    "        'colsample_bytree': 0.6,\n",
    "        'max_depth': 8,\n",
    "        'learning_rate': .1,\n",
    "        'verbosity': 0,\n",
    "        'gpu_id': 1,\n",
    "    },\n",
    "    ExtraTreesClassifier : {\n",
    "        'n_jobs': -1,\n",
    "        'n_estimators':200,\n",
    "        #'max_features': 0.5,\n",
    "        'max_depth': 8,\n",
    "        'min_samples_leaf': 2,\n",
    "        'verbose': 0\n",
    "    },\n",
    "    AdaBoostClassifier : {\n",
    "        'n_estimators': 200,\n",
    "        'learning_rate' : 0.75\n",
    "    },\n",
    "    XGBClassifier : {\n",
    "        'objective': 'multi:softmax',\n",
    "        'eta': 0.1,\n",
    "        'nthread': 8,\n",
    "        'num_class': 10,\n",
    "        'gpu_id': 0,\n",
    "        'tree_method': 'gpu_hist',\n",
    "        'eval_metric': ['mlogloss', 'auc', 'merror'],\n",
    "        'max_depth': 7,\n",
    "        'min_child_weight': 9,\n",
    "        'gamma': 0.2,\n",
    "        'subsample': 0.9,\n",
    "        'colsample_bytree': 0.6,\n",
    "        'reg_alpha': 0,\n",
    "        'n_estimators': 200,\n",
    "    }\n",
    "#     GradientBoostingClassifier : { # 太慢了\n",
    "#         'n_estimators': 200,\n",
    "#          #'max_features': 0.2,\n",
    "#         'max_depth': 5,\n",
    "#         'min_samples_leaf': 2,\n",
    "#         'verbose': 0\n",
    "#     },\n",
    "#     SVC : { # 太慢了\n",
    "#         'kernel' : 'rbf',\n",
    "#         'C' : 0.025,\n",
    "#         'probability': True\n",
    "#     } \n",
    "}\n",
    "\n",
    "\n",
    "second_layer_params = {\n",
    "    XGBClassifier : {\n",
    "        'objective': 'multi:softmax',\n",
    "        'eta': 0.1,\n",
    "        'nthread': 8,\n",
    "        'num_class': 10,\n",
    "        'gpu_id': 0,\n",
    "        'tree_method': 'gpu_hist',\n",
    "        'eval_metric': ['mlogloss', 'auc', 'merror'],\n",
    "        'max_depth': 9,\n",
    "        'min_child_weight': 9,\n",
    "        'gamma': 0.2,\n",
    "        'subsample': 0.9,\n",
    "        'colsample_bytree': 0.6,\n",
    "        'reg_alpha': 0,\n",
    "        'n_estimators': 200,\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "clfs = []\n",
    "for c, p in first_layer_params.items():\n",
    "    clfs.append(c(**p))\n",
    "\n",
    "meta = XGBClassifier(**second_layer_params[XGBClassifier])\n",
    "sclf = StackingClassifier(classifiers=clfs, \n",
    "                          meta_classifier=meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gzy/.local/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/home/gzy/.local/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "sclf.fit(X_train, y_train)\n",
    "sclf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sclf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_19427/2239363165.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtmp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'sclf' is not defined"
     ]
    }
   ],
   "source": [
    "tmp = sclf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter(tmp), Counter(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gzy/.local/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "clf = XGBRFClassifier(**first_layer_params[XGBRFClassifier])\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.10100084, 0.10091538, 0.10042646, 0.09997695, 0.09962477,\n",
       "        0.09947018, 0.09939709, 0.09926751, 0.09943457, 0.10048625]])"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# is_share 预测"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 处理数据不均衡问题"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 7338705), (1, 14950)]\n",
      "[[0.       0.997967]\n",
      " [1.       0.002033]]\n"
     ]
    }
   ],
   "source": [
    "items = list(Counter(is_share).items())\n",
    "items.sort(key=lambda x: x[0])\n",
    "print(items)\n",
    "\n",
    "dist = np.array(items, dtype=np.float)\n",
    "dist[:, 1] = dist[:, 1] / is_share.shape[0]\n",
    "print(dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "under_ss = np.array(items)\n",
    "under_ss_thresh = under_ss[1, 1]\n",
    "under_ss[:, 1] = np.clip(under_ss[:, 1], a_min=None, a_max=under_ss_thresh)\n",
    "\n",
    "over_ss = under_ss.copy()\n",
    "over_ss_thresh = under_ss[1, 1]\n",
    "over_ss[:, 1] = np.clip(over_ss[:, 1], a_min=over_ss_thresh, a_max=None)\n",
    "\n",
    "under_ss = dict(under_ss)\n",
    "over_ss = dict(over_ss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({0: 14950, 1: 14950}, {0: 14950, 1: 14950})"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "under_ss, over_ss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7338705,)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idxs = is_share == 0\n",
    "idxs = idxs.replace(False, np.nan).dropna().index  # 保留watch_label=0的行索引\n",
    "idxs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((7323755,), (14950,))"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "left_idxs = np.random.choice(idxs, under_ss_thresh, replace=False)  # 选择一部分保留\n",
    "del_idxs = idxs.difference(left_idxs)\n",
    "del_idxs.shape, left_idxs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((29900, 170), (29900,))"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dataset['watch_label'] = watch_label\n",
    "resampled_data = np.delete(dataset.values, del_idxs, axis=0)\n",
    "resampled_sh = np.delete(is_share.values, del_idxs, axis=0)\n",
    "resampled_data.shape, resampled_sh.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((29900, 170), (29900,))"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 将采样后的数据重装会DataFrame\n",
    "data_sh = pd.DataFrame(resampled_data, columns=dataset.columns)\n",
    "# del dataset['watch_label']\n",
    "is_share_res = pd.Series(resampled_sh)\n",
    "data_sh.shape, is_share_res.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "del resampled_data, resampled_sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9.06 s, sys: 37.7 s, total: 46.8 s\n",
      "Wall time: 4.95 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# 对几个数量较少的类别进行上采样\n",
    "smt_sh = SMOTE(sampling_strategy={1: 15000, 0: 15000}, n_jobs=8)\n",
    "X_sh_r, y_sh_r = smt_sh.fit_resample(data_sh, is_share_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((30000, 128), (30000,))"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_sh = X_sh_r\n",
    "is_share_res = y_sh_r\n",
    "data_sh.shape, is_share_res.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((24518,), (5382,))"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_rate = .18\n",
    "train_idx, test_idx = train_test_split(data_sh.index, test_size=test_rate, random_state=1)\n",
    "train_idx.shape, test_idx.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_sh = data_sh.iloc[train_idx]\n",
    "X_test_sh  = data_sh.iloc[test_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_sh = is_share_res.iloc[train_idx]\n",
    "y_test_sh  = is_share_res.iloc[test_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data preparing finished ...\t\t(0.823s)\n"
     ]
    }
   ],
   "source": [
    "t0 = time()\n",
    "xg_train_sh = xgb.DMatrix(X_train_sh.values, label=y_train_sh.values, enable_categorical=True)\n",
    "xg_test_sh = xgb.DMatrix(X_test_sh.values, label=y_test_sh.values, enable_categorical=True)\n",
    "print(f\"Data preparing finished ...\\t\\t({time()-t0:.3f}s)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'xg_train_sh' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1384/3363818105.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mwatchlist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxg_train_sh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mxg_test_sh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'test'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'xg_train_sh' is not defined"
     ]
    }
   ],
   "source": [
    "# setup parameters for xgboost\n",
    "param_sh = {\n",
    "    'objective': 'binary:hinge',\n",
    "    'eta': 0.1,\n",
    "    'nthread': 8,\n",
    "    'gpu_id': 0,\n",
    "    'tree_method': 'gpu_hist',\n",
    "    'eval_metric': ['logloss', 'auc', 'error'],\n",
    "    'max_depth': 5,\n",
    "    'min_child_weight': 1,\n",
    "    'gamma': 0.1,\n",
    "    'subsample': 0.6,\n",
    "    'colsample_bytree': 0.6,\n",
    "    'reg_alpha': 0\n",
    "}\n",
    "\n",
    "\n",
    "watchlist = [(xg_train_sh, 'train'), (xg_test_sh, 'test')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-logloss:18.32301\ttrain-auc:0.50000\ttrain-error:0.49735\ttest-logloss:18.86563\ttest-auc:0.50000\ttest-error:0.51208\n",
      "[1]\ttrain-logloss:18.32301\ttrain-auc:0.50000\ttrain-error:0.49735\ttest-logloss:18.86563\ttest-auc:0.50000\ttest-error:0.51208\n",
      "[2]\ttrain-logloss:18.32301\ttrain-auc:0.50000\ttrain-error:0.49735\ttest-logloss:18.86563\ttest-auc:0.50000\ttest-error:0.51208\n",
      "[3]\ttrain-logloss:18.32301\ttrain-auc:0.50000\ttrain-error:0.49735\ttest-logloss:18.86563\ttest-auc:0.50000\ttest-error:0.51208\n",
      "[4]\ttrain-logloss:18.32301\ttrain-auc:0.50000\ttrain-error:0.49735\ttest-logloss:18.86563\ttest-auc:0.50000\ttest-error:0.51208\n",
      "[5]\ttrain-logloss:18.32301\ttrain-auc:0.50000\ttrain-error:0.49735\ttest-logloss:18.86563\ttest-auc:0.50000\ttest-error:0.51208\n",
      "[6]\ttrain-logloss:17.95637\ttrain-auc:0.51001\ttrain-error:0.48740\ttest-logloss:18.50283\ttest-auc:0.50961\ttest-error:0.50223\n",
      "[7]\ttrain-logloss:17.60476\ttrain-auc:0.51962\ttrain-error:0.47785\ttest-logloss:18.16056\ttest-auc:0.51864\ttest-error:0.49294\n",
      "[8]\ttrain-logloss:16.88500\ttrain-auc:0.53931\ttrain-error:0.45832\ttest-logloss:17.36651\ttest-auc:0.53944\ttest-error:0.47139\n",
      "[9]\ttrain-logloss:16.10063\ttrain-auc:0.56079\ttrain-error:0.43703\ttest-logloss:16.51769\ttest-auc:0.56158\ttest-error:0.44835\n",
      "[10]\ttrain-logloss:15.66036\ttrain-auc:0.57288\ttrain-error:0.42507\ttest-logloss:16.19596\ttest-auc:0.56973\ttest-error:0.43961\n",
      "[11]\ttrain-logloss:15.35533\ttrain-auc:0.58123\ttrain-error:0.41680\ttest-logloss:15.93584\ttest-auc:0.57649\ttest-error:0.43255\n",
      "[12]\ttrain-logloss:15.15097\ttrain-auc:0.58684\ttrain-error:0.41125\ttest-logloss:15.76471\ttest-auc:0.58086\ttest-error:0.42791\n",
      "[13]\ttrain-logloss:15.06382\ttrain-auc:0.58923\ttrain-error:0.40888\ttest-logloss:15.60042\ttest-auc:0.58520\ttest-error:0.42345\n",
      "[14]\ttrain-logloss:15.00071\ttrain-auc:0.59097\ttrain-error:0.40717\ttest-logloss:15.54566\ttest-auc:0.58657\ttest-error:0.42196\n",
      "[15]\ttrain-logloss:14.95563\ttrain-auc:0.59222\ttrain-error:0.40595\ttest-logloss:15.51143\ttest-auc:0.58739\ttest-error:0.42103\n",
      "[16]\ttrain-logloss:14.88200\ttrain-auc:0.59424\ttrain-error:0.40395\ttest-logloss:15.40875\ttest-auc:0.59007\ttest-error:0.41825\n",
      "[17]\ttrain-logloss:14.86397\ttrain-auc:0.59474\ttrain-error:0.40346\ttest-logloss:15.34714\ttest-auc:0.59166\ttest-error:0.41657\n",
      "[18]\ttrain-logloss:14.82189\ttrain-auc:0.59591\ttrain-error:0.40232\ttest-logloss:15.30607\ttest-auc:0.59271\ttest-error:0.41546\n",
      "[19]\ttrain-logloss:14.76479\ttrain-auc:0.59747\ttrain-error:0.40077\ttest-logloss:15.21024\ttest-auc:0.59521\ttest-error:0.41286\n",
      "[20]\ttrain-logloss:14.73324\ttrain-auc:0.59834\ttrain-error:0.39991\ttest-logloss:15.19655\ttest-auc:0.59547\ttest-error:0.41249\n",
      "[21]\ttrain-logloss:14.68816\ttrain-auc:0.59959\ttrain-error:0.39869\ttest-logloss:15.16917\ttest-auc:0.59612\ttest-error:0.41174\n",
      "[22]\ttrain-logloss:14.65360\ttrain-auc:0.60055\ttrain-error:0.39775\ttest-logloss:15.12125\ttest-auc:0.59732\ttest-error:0.41044\n",
      "[23]\ttrain-logloss:14.57997\ttrain-auc:0.60257\ttrain-error:0.39575\ttest-logloss:15.06649\ttest-auc:0.59870\ttest-error:0.40896\n",
      "[24]\ttrain-logloss:14.50334\ttrain-auc:0.60467\ttrain-error:0.39367\ttest-logloss:15.03226\ttest-auc:0.59950\ttest-error:0.40803\n",
      "[25]\ttrain-logloss:14.40567\ttrain-auc:0.60735\ttrain-error:0.39102\ttest-logloss:14.95012\ttest-auc:0.60161\ttest-error:0.40580\n",
      "[26]\ttrain-logloss:14.34857\ttrain-auc:0.60891\ttrain-error:0.38947\ttest-logloss:14.86113\ttest-auc:0.60397\ttest-error:0.40338\n",
      "[27]\ttrain-logloss:14.27494\ttrain-auc:0.61094\ttrain-error:0.38747\ttest-logloss:14.79268\ttest-auc:0.60570\ttest-error:0.40152\n",
      "[28]\ttrain-logloss:14.25991\ttrain-auc:0.61136\ttrain-error:0.38706\ttest-logloss:14.75160\ttest-auc:0.60671\ttest-error:0.40041\n",
      "[29]\ttrain-logloss:14.21784\ttrain-auc:0.61253\ttrain-error:0.38592\ttest-logloss:14.71053\ttest-auc:0.60772\ttest-error:0.39929\n",
      "[30]\ttrain-logloss:14.12768\ttrain-auc:0.61500\ttrain-error:0.38347\ttest-logloss:14.64893\ttest-auc:0.60927\ttest-error:0.39762\n",
      "[31]\ttrain-logloss:14.07208\ttrain-auc:0.61653\ttrain-error:0.38196\ttest-logloss:14.57363\ttest-auc:0.61124\ttest-error:0.39558\n",
      "[32]\ttrain-logloss:14.04654\ttrain-auc:0.61723\ttrain-error:0.38127\ttest-logloss:14.55994\ttest-auc:0.61160\ttest-error:0.39521\n",
      "[33]\ttrain-logloss:13.97892\ttrain-auc:0.61908\ttrain-error:0.37944\ttest-logloss:14.56678\ttest-auc:0.61134\ttest-error:0.39539\n",
      "[34]\ttrain-logloss:13.93084\ttrain-auc:0.62040\ttrain-error:0.37813\ttest-logloss:14.55309\ttest-auc:0.61164\ttest-error:0.39502\n",
      "[35]\ttrain-logloss:13.89778\ttrain-auc:0.62131\ttrain-error:0.37723\ttest-logloss:14.53940\ttest-auc:0.61194\ttest-error:0.39465\n",
      "[36]\ttrain-logloss:13.88726\ttrain-auc:0.62161\ttrain-error:0.37695\ttest-logloss:14.52571\ttest-auc:0.61226\ttest-error:0.39428\n",
      "[37]\ttrain-logloss:13.86472\ttrain-auc:0.62223\ttrain-error:0.37634\ttest-logloss:14.52571\ttest-auc:0.61218\ttest-error:0.39428\n",
      "[38]\ttrain-logloss:13.82114\ttrain-auc:0.62344\ttrain-error:0.37515\ttest-logloss:14.49833\ttest-auc:0.61284\ttest-error:0.39353\n",
      "[39]\ttrain-logloss:13.77456\ttrain-auc:0.62472\ttrain-error:0.37389\ttest-logloss:14.48464\ttest-auc:0.61315\ttest-error:0.39316\n",
      "[40]\ttrain-logloss:13.72197\ttrain-auc:0.62616\ttrain-error:0.37246\ttest-logloss:14.44357\ttest-auc:0.61418\ttest-error:0.39205\n",
      "[41]\ttrain-logloss:13.68441\ttrain-auc:0.62719\ttrain-error:0.37144\ttest-logloss:14.38196\ttest-auc:0.61581\ttest-error:0.39038\n",
      "[42]\ttrain-logloss:13.63933\ttrain-auc:0.62842\ttrain-error:0.37022\ttest-logloss:14.38880\ttest-auc:0.61559\ttest-error:0.39056\n",
      "[43]\ttrain-logloss:13.61679\ttrain-auc:0.62904\ttrain-error:0.36961\ttest-logloss:14.33404\ttest-auc:0.61700\ttest-error:0.38908\n",
      "[44]\ttrain-logloss:13.60026\ttrain-auc:0.62950\ttrain-error:0.36916\ttest-logloss:14.35458\ttest-auc:0.61643\ttest-error:0.38963\n",
      "[45]\ttrain-logloss:13.56870\ttrain-auc:0.63037\ttrain-error:0.36830\ttest-logloss:14.34773\ttest-auc:0.61654\ttest-error:0.38945\n",
      "[46]\ttrain-logloss:13.51311\ttrain-auc:0.63190\ttrain-error:0.36679\ttest-logloss:14.32720\ttest-auc:0.61698\ttest-error:0.38889\n",
      "[47]\ttrain-logloss:13.50109\ttrain-auc:0.63224\ttrain-error:0.36646\ttest-logloss:14.33404\ttest-auc:0.61672\ttest-error:0.38908\n",
      "[48]\ttrain-logloss:13.45751\ttrain-auc:0.63343\ttrain-error:0.36528\ttest-logloss:14.27243\ttest-auc:0.61831\ttest-error:0.38740\n",
      "[49]\ttrain-logloss:13.41995\ttrain-auc:0.63446\ttrain-error:0.36426\ttest-logloss:14.27243\ttest-auc:0.61828\ttest-error:0.38740\n",
      "[50]\ttrain-logloss:13.35533\ttrain-auc:0.63624\ttrain-error:0.36251\ttest-logloss:14.22452\ttest-auc:0.61947\ttest-error:0.38610\n",
      "[51]\ttrain-logloss:13.29072\ttrain-auc:0.63801\ttrain-error:0.36075\ttest-logloss:14.17660\ttest-auc:0.62069\ttest-error:0.38480\n",
      "[52]\ttrain-logloss:13.25015\ttrain-auc:0.63912\ttrain-error:0.35965\ttest-logloss:14.12184\ttest-auc:0.62212\ttest-error:0.38332\n",
      "[53]\ttrain-logloss:13.20357\ttrain-auc:0.64040\ttrain-error:0.35839\ttest-logloss:14.14237\ttest-auc:0.62151\ttest-error:0.38387\n",
      "[54]\ttrain-logloss:13.20507\ttrain-auc:0.64037\ttrain-error:0.35843\ttest-logloss:14.12184\ttest-auc:0.62203\ttest-error:0.38332\n",
      "[55]\ttrain-logloss:13.16901\ttrain-auc:0.64136\ttrain-error:0.35745\ttest-logloss:14.08761\ttest-auc:0.62288\ttest-error:0.38239\n",
      "[56]\ttrain-logloss:13.09388\ttrain-auc:0.64342\ttrain-error:0.35541\ttest-logloss:14.01916\ttest-auc:0.62466\ttest-error:0.38053\n",
      "[57]\ttrain-logloss:13.02926\ttrain-auc:0.64520\ttrain-error:0.35366\ttest-logloss:13.96440\ttest-auc:0.62605\ttest-error:0.37904\n",
      "[58]\ttrain-logloss:13.01874\ttrain-auc:0.64549\ttrain-error:0.35337\ttest-logloss:13.95071\ttest-auc:0.62640\ttest-error:0.37867\n",
      "[59]\ttrain-logloss:12.98719\ttrain-auc:0.64636\ttrain-error:0.35252\ttest-logloss:13.94386\ttest-auc:0.62656\ttest-error:0.37848\n",
      "[60]\ttrain-logloss:12.95714\ttrain-auc:0.64719\ttrain-error:0.35170\ttest-logloss:13.94386\ttest-auc:0.62646\ttest-error:0.37848\n",
      "[61]\ttrain-logloss:12.91206\ttrain-auc:0.64842\ttrain-error:0.35048\ttest-logloss:13.91648\ttest-auc:0.62715\ttest-error:0.37774\n",
      "[62]\ttrain-logloss:12.87149\ttrain-auc:0.64954\ttrain-error:0.34938\ttest-logloss:13.84803\ttest-auc:0.62896\ttest-error:0.37588\n",
      "[63]\ttrain-logloss:12.83242\ttrain-auc:0.65062\ttrain-error:0.34832\ttest-logloss:13.79326\ttest-auc:0.63036\ttest-error:0.37440\n",
      "[64]\ttrain-logloss:12.80086\ttrain-auc:0.65150\ttrain-error:0.34746\ttest-logloss:13.72481\ttest-auc:0.63213\ttest-error:0.37254\n",
      "[65]\ttrain-logloss:12.74977\ttrain-auc:0.65289\ttrain-error:0.34607\ttest-logloss:13.67689\ttest-auc:0.63337\ttest-error:0.37124\n",
      "[66]\ttrain-logloss:12.69718\ttrain-auc:0.65434\ttrain-error:0.34464\ttest-logloss:13.68374\ttest-auc:0.63314\ttest-error:0.37142\n",
      "[67]\ttrain-logloss:12.68216\ttrain-auc:0.65476\ttrain-error:0.34424\ttest-logloss:13.60160\ttest-auc:0.63532\ttest-error:0.36919\n",
      "[68]\ttrain-logloss:12.67614\ttrain-auc:0.65493\ttrain-error:0.34407\ttest-logloss:13.60160\ttest-auc:0.63531\ttest-error:0.36919\n",
      "[69]\ttrain-logloss:12.66262\ttrain-auc:0.65531\ttrain-error:0.34371\ttest-logloss:13.57421\ttest-auc:0.63601\ttest-error:0.36845\n",
      "[70]\ttrain-logloss:12.62205\ttrain-auc:0.65643\ttrain-error:0.34260\ttest-logloss:13.59475\ttest-auc:0.63538\ttest-error:0.36901\n",
      "[71]\ttrain-logloss:12.61303\ttrain-auc:0.65668\ttrain-error:0.34236\ttest-logloss:13.56052\ttest-auc:0.63624\ttest-error:0.36808\n",
      "[72]\ttrain-logloss:12.57096\ttrain-auc:0.65783\ttrain-error:0.34122\ttest-logloss:13.59475\ttest-auc:0.63527\ttest-error:0.36901\n",
      "[73]\ttrain-logloss:12.53490\ttrain-auc:0.65883\ttrain-error:0.34024\ttest-logloss:13.57421\ttest-auc:0.63579\ttest-error:0.36845\n",
      "[74]\ttrain-logloss:12.51536\ttrain-auc:0.65938\ttrain-error:0.33971\ttest-logloss:13.52630\ttest-auc:0.63702\ttest-error:0.36715\n",
      "[75]\ttrain-logloss:12.48080\ttrain-auc:0.66033\ttrain-error:0.33877\ttest-logloss:13.52630\ttest-auc:0.63694\ttest-error:0.36715\n",
      "[76]\ttrain-logloss:12.45826\ttrain-auc:0.66096\ttrain-error:0.33816\ttest-logloss:13.49207\ttest-auc:0.63780\ttest-error:0.36622\n",
      "[77]\ttrain-logloss:12.43873\ttrain-auc:0.66150\ttrain-error:0.33763\ttest-logloss:13.45784\ttest-auc:0.63866\ttest-error:0.36529\n",
      "[78]\ttrain-logloss:12.41469\ttrain-auc:0.66216\ttrain-error:0.33698\ttest-logloss:13.45784\ttest-auc:0.63860\ttest-error:0.36529\n",
      "[79]\ttrain-logloss:12.40417\ttrain-auc:0.66245\ttrain-error:0.33669\ttest-logloss:13.45100\ttest-auc:0.63877\ttest-error:0.36511\n",
      "[80]\ttrain-logloss:12.36660\ttrain-auc:0.66348\ttrain-error:0.33567\ttest-logloss:13.42362\ttest-auc:0.63948\ttest-error:0.36436\n",
      "[81]\ttrain-logloss:12.36059\ttrain-auc:0.66365\ttrain-error:0.33551\ttest-logloss:13.34832\ttest-auc:0.64146\ttest-error:0.36232\n",
      "[82]\ttrain-logloss:12.35458\ttrain-auc:0.66382\ttrain-error:0.33535\ttest-logloss:13.31409\ttest-auc:0.64236\ttest-error:0.36139\n",
      "[83]\ttrain-logloss:12.34707\ttrain-auc:0.66403\ttrain-error:0.33514\ttest-logloss:13.32094\ttest-auc:0.64209\ttest-error:0.36158\n",
      "[84]\ttrain-logloss:12.33806\ttrain-auc:0.66429\ttrain-error:0.33490\ttest-logloss:13.32778\ttest-auc:0.64189\ttest-error:0.36176\n",
      "[85]\ttrain-logloss:12.29598\ttrain-auc:0.66544\ttrain-error:0.33376\ttest-logloss:13.30725\ttest-auc:0.64241\ttest-error:0.36120\n",
      "[86]\ttrain-logloss:12.26893\ttrain-auc:0.66618\ttrain-error:0.33302\ttest-logloss:13.26618\ttest-auc:0.64353\ttest-error:0.36009\n",
      "[87]\ttrain-logloss:12.27494\ttrain-auc:0.66602\ttrain-error:0.33318\ttest-logloss:13.27987\ttest-auc:0.64312\ttest-error:0.36046\n",
      "[88]\ttrain-logloss:12.27044\ttrain-auc:0.66614\ttrain-error:0.33306\ttest-logloss:13.23879\ttest-auc:0.64420\ttest-error:0.35935\n",
      "[89]\ttrain-logloss:12.24489\ttrain-auc:0.66685\ttrain-error:0.33237\ttest-logloss:13.25933\ttest-auc:0.64361\ttest-error:0.35990\n",
      "[90]\ttrain-logloss:12.23738\ttrain-auc:0.66705\ttrain-error:0.33216\ttest-logloss:13.22510\ttest-auc:0.64445\ttest-error:0.35897\n",
      "[91]\ttrain-logloss:12.22386\ttrain-auc:0.66743\ttrain-error:0.33180\ttest-logloss:13.18403\ttest-auc:0.64555\ttest-error:0.35786\n",
      "[92]\ttrain-logloss:12.18779\ttrain-auc:0.66841\ttrain-error:0.33082\ttest-logloss:13.22510\ttest-auc:0.64445\ttest-error:0.35897\n",
      "[93]\ttrain-logloss:12.17427\ttrain-auc:0.66878\ttrain-error:0.33045\ttest-logloss:13.23879\ttest-auc:0.64406\ttest-error:0.35935\n",
      "[94]\ttrain-logloss:12.16676\ttrain-auc:0.66900\ttrain-error:0.33025\ttest-logloss:13.20457\ttest-auc:0.64494\ttest-error:0.35842\n",
      "[95]\ttrain-logloss:12.13520\ttrain-auc:0.66986\ttrain-error:0.32939\ttest-logloss:13.20457\ttest-auc:0.64490\ttest-error:0.35842\n",
      "[96]\ttrain-logloss:12.14872\ttrain-auc:0.66950\ttrain-error:0.32976\ttest-logloss:13.19772\ttest-auc:0.64505\ttest-error:0.35823\n",
      "[97]\ttrain-logloss:12.10364\ttrain-auc:0.67073\ttrain-error:0.32853\ttest-logloss:13.20457\ttest-auc:0.64485\ttest-error:0.35842\n",
      "[98]\ttrain-logloss:12.09463\ttrain-auc:0.67098\ttrain-error:0.32829\ttest-logloss:13.19088\ttest-auc:0.64520\ttest-error:0.35805\n",
      "[99]\ttrain-logloss:12.08712\ttrain-auc:0.67119\ttrain-error:0.32809\ttest-logloss:13.19772\ttest-auc:0.64499\ttest-error:0.35823\n",
      "[100]\ttrain-logloss:12.06157\ttrain-auc:0.67188\ttrain-error:0.32739\ttest-logloss:13.18403\ttest-auc:0.64533\ttest-error:0.35786\n",
      "[101]\ttrain-logloss:12.03302\ttrain-auc:0.67266\ttrain-error:0.32662\ttest-logloss:13.19772\ttest-auc:0.64495\ttest-error:0.35823\n",
      "[102]\ttrain-logloss:12.03302\ttrain-auc:0.67267\ttrain-error:0.32662\ttest-logloss:13.17719\ttest-auc:0.64547\ttest-error:0.35767\n",
      "[103]\ttrain-logloss:12.02851\ttrain-auc:0.67280\ttrain-error:0.32649\ttest-logloss:13.17034\ttest-auc:0.64563\ttest-error:0.35749\n",
      "[104]\ttrain-logloss:12.00447\ttrain-auc:0.67346\ttrain-error:0.32584\ttest-logloss:13.17034\ttest-auc:0.64564\ttest-error:0.35749\n",
      "[105]\ttrain-logloss:11.99696\ttrain-auc:0.67366\ttrain-error:0.32564\ttest-logloss:13.14981\ttest-auc:0.64622\ttest-error:0.35693\n",
      "[106]\ttrain-logloss:11.99546\ttrain-auc:0.67370\ttrain-error:0.32560\ttest-logloss:13.13612\ttest-auc:0.64653\ttest-error:0.35656\n",
      "[107]\ttrain-logloss:11.98043\ttrain-auc:0.67412\ttrain-error:0.32519\ttest-logloss:13.11558\ttest-auc:0.64710\ttest-error:0.35600\n",
      "[108]\ttrain-logloss:11.97893\ttrain-auc:0.67417\ttrain-error:0.32515\ttest-logloss:13.12242\ttest-auc:0.64688\ttest-error:0.35619\n",
      "[109]\ttrain-logloss:11.98344\ttrain-auc:0.67405\ttrain-error:0.32527\ttest-logloss:13.09504\ttest-auc:0.64760\ttest-error:0.35544\n",
      "[110]\ttrain-logloss:11.97292\ttrain-auc:0.67434\ttrain-error:0.32499\ttest-logloss:13.09504\ttest-auc:0.64758\ttest-error:0.35544\n",
      "[111]\ttrain-logloss:11.95639\ttrain-auc:0.67479\ttrain-error:0.32454\ttest-logloss:13.07451\ttest-auc:0.64812\ttest-error:0.35489\n",
      "[112]\ttrain-logloss:11.94887\ttrain-auc:0.67499\ttrain-error:0.32433\ttest-logloss:13.06766\ttest-auc:0.64829\ttest-error:0.35470\n",
      "[113]\ttrain-logloss:11.94437\ttrain-auc:0.67512\ttrain-error:0.32421\ttest-logloss:13.06082\ttest-auc:0.64847\ttest-error:0.35452\n",
      "[114]\ttrain-logloss:11.91882\ttrain-auc:0.67582\ttrain-error:0.32352\ttest-logloss:13.09504\ttest-auc:0.64751\ttest-error:0.35544\n",
      "[115]\ttrain-logloss:11.90229\ttrain-auc:0.67627\ttrain-error:0.32307\ttest-logloss:13.08820\ttest-auc:0.64771\ttest-error:0.35526\n",
      "[116]\ttrain-logloss:11.91431\ttrain-auc:0.67595\ttrain-error:0.32339\ttest-logloss:13.07451\ttest-auc:0.64806\ttest-error:0.35489\n",
      "[117]\ttrain-logloss:11.90229\ttrain-auc:0.67627\ttrain-error:0.32307\ttest-logloss:13.08135\ttest-auc:0.64786\ttest-error:0.35507\n",
      "[118]\ttrain-logloss:11.87374\ttrain-auc:0.67705\ttrain-error:0.32229\ttest-logloss:13.05397\ttest-auc:0.64858\ttest-error:0.35433\n",
      "[119]\ttrain-logloss:11.86923\ttrain-auc:0.67718\ttrain-error:0.32217\ttest-logloss:13.08135\ttest-auc:0.64781\ttest-error:0.35507\n",
      "[120]\ttrain-logloss:11.82265\ttrain-auc:0.67844\ttrain-error:0.32091\ttest-logloss:13.06766\ttest-auc:0.64817\ttest-error:0.35470\n",
      "[121]\ttrain-logloss:11.81514\ttrain-auc:0.67865\ttrain-error:0.32070\ttest-logloss:13.06082\ttest-auc:0.64836\ttest-error:0.35452\n",
      "[122]\ttrain-logloss:11.81815\ttrain-auc:0.67857\ttrain-error:0.32078\ttest-logloss:13.04713\ttest-auc:0.64872\ttest-error:0.35414\n",
      "[123]\ttrain-logloss:11.81965\ttrain-auc:0.67853\ttrain-error:0.32083\ttest-logloss:13.01975\ttest-auc:0.64942\ttest-error:0.35340\n",
      "[124]\ttrain-logloss:11.81364\ttrain-auc:0.67870\ttrain-error:0.32066\ttest-logloss:13.01290\ttest-auc:0.64961\ttest-error:0.35321\n",
      "[125]\ttrain-logloss:11.80913\ttrain-auc:0.67882\ttrain-error:0.32054\ttest-logloss:12.99236\ttest-auc:0.65016\ttest-error:0.35266\n",
      "[126]\ttrain-logloss:11.80462\ttrain-auc:0.67895\ttrain-error:0.32042\ttest-logloss:12.96498\ttest-auc:0.65088\ttest-error:0.35191\n",
      "[127]\ttrain-logloss:11.79410\ttrain-auc:0.67924\ttrain-error:0.32013\ttest-logloss:12.94445\ttest-auc:0.65147\ttest-error:0.35136\n",
      "[128]\ttrain-logloss:11.80011\ttrain-auc:0.67908\ttrain-error:0.32029\ttest-logloss:12.95129\ttest-auc:0.65127\ttest-error:0.35154\n",
      "[129]\ttrain-logloss:11.78509\ttrain-auc:0.67949\ttrain-error:0.31989\ttest-logloss:12.93760\ttest-auc:0.65164\ttest-error:0.35117\n",
      "[130]\ttrain-logloss:11.78509\ttrain-auc:0.67949\ttrain-error:0.31989\ttest-logloss:12.91707\ttest-auc:0.65217\ttest-error:0.35061\n",
      "[131]\ttrain-logloss:11.75503\ttrain-auc:0.68031\ttrain-error:0.31907\ttest-logloss:12.89653\ttest-auc:0.65270\ttest-error:0.35006\n",
      "[132]\ttrain-logloss:11.75353\ttrain-auc:0.68036\ttrain-error:0.31903\ttest-logloss:12.89653\ttest-auc:0.65267\ttest-error:0.35006\n",
      "[133]\ttrain-logloss:11.74752\ttrain-auc:0.68052\ttrain-error:0.31887\ttest-logloss:12.92391\ttest-auc:0.65192\ttest-error:0.35080\n",
      "[134]\ttrain-logloss:11.75053\ttrain-auc:0.68044\ttrain-error:0.31895\ttest-logloss:12.93760\ttest-auc:0.65155\ttest-error:0.35117\n",
      "[135]\ttrain-logloss:11.74752\ttrain-auc:0.68052\ttrain-error:0.31887\ttest-logloss:12.93076\ttest-auc:0.65172\ttest-error:0.35098\n",
      "[136]\ttrain-logloss:11.73099\ttrain-auc:0.68097\ttrain-error:0.31842\ttest-logloss:12.94445\ttest-auc:0.65134\ttest-error:0.35136\n",
      "[137]\ttrain-logloss:11.71897\ttrain-auc:0.68131\ttrain-error:0.31809\ttest-logloss:12.97183\ttest-auc:0.65054\ttest-error:0.35210\n",
      "[138]\ttrain-logloss:11.69944\ttrain-auc:0.68184\ttrain-error:0.31756\ttest-logloss:12.95129\ttest-auc:0.65109\ttest-error:0.35154\n",
      "[139]\ttrain-logloss:11.68742\ttrain-auc:0.68216\ttrain-error:0.31724\ttest-logloss:12.95814\ttest-auc:0.65091\ttest-error:0.35173\n",
      "[140]\ttrain-logloss:11.67690\ttrain-auc:0.68245\ttrain-error:0.31695\ttest-logloss:12.93760\ttest-auc:0.65146\ttest-error:0.35117\n",
      "[141]\ttrain-logloss:11.68742\ttrain-auc:0.68217\ttrain-error:0.31724\ttest-logloss:12.94445\ttest-auc:0.65124\ttest-error:0.35136\n",
      "[142]\ttrain-logloss:11.68892\ttrain-auc:0.68213\ttrain-error:0.31728\ttest-logloss:12.93076\ttest-auc:0.65162\ttest-error:0.35098\n",
      "[143]\ttrain-logloss:11.66788\ttrain-auc:0.68270\ttrain-error:0.31671\ttest-logloss:12.95814\ttest-auc:0.65088\ttest-error:0.35173\n",
      "[144]\ttrain-logloss:11.64685\ttrain-auc:0.68327\ttrain-error:0.31613\ttest-logloss:12.97867\ttest-auc:0.65029\ttest-error:0.35229\n",
      "[145]\ttrain-logloss:11.63332\ttrain-auc:0.68364\ttrain-error:0.31577\ttest-logloss:12.94445\ttest-auc:0.65122\ttest-error:0.35136\n",
      "[146]\ttrain-logloss:11.62130\ttrain-auc:0.68397\ttrain-error:0.31544\ttest-logloss:12.93760\ttest-auc:0.65141\ttest-error:0.35117\n",
      "[147]\ttrain-logloss:11.60628\ttrain-auc:0.68438\ttrain-error:0.31503\ttest-logloss:12.93760\ttest-auc:0.65140\ttest-error:0.35117\n",
      "[148]\ttrain-logloss:11.60928\ttrain-auc:0.68430\ttrain-error:0.31511\ttest-logloss:12.94445\ttest-auc:0.65120\ttest-error:0.35136\n",
      "[149]\ttrain-logloss:11.59576\ttrain-auc:0.68467\ttrain-error:0.31475\ttest-logloss:12.93760\ttest-auc:0.65137\ttest-error:0.35117\n",
      "[150]\ttrain-logloss:11.57021\ttrain-auc:0.68536\ttrain-error:0.31405\ttest-logloss:12.93076\ttest-auc:0.65155\ttest-error:0.35098\n",
      "[151]\ttrain-logloss:11.55368\ttrain-auc:0.68581\ttrain-error:0.31361\ttest-logloss:12.89653\ttest-auc:0.65248\ttest-error:0.35006\n",
      "[152]\ttrain-logloss:11.54767\ttrain-auc:0.68598\ttrain-error:0.31344\ttest-logloss:12.89653\ttest-auc:0.65248\ttest-error:0.35006\n",
      "[153]\ttrain-logloss:11.54767\ttrain-auc:0.68599\ttrain-error:0.31344\ttest-logloss:12.86915\ttest-auc:0.65321\ttest-error:0.34931\n",
      "[154]\ttrain-logloss:11.53565\ttrain-auc:0.68631\ttrain-error:0.31312\ttest-logloss:12.87599\ttest-auc:0.65301\ttest-error:0.34950\n",
      "[155]\ttrain-logloss:11.51912\ttrain-auc:0.68676\ttrain-error:0.31267\ttest-logloss:12.88969\ttest-auc:0.65264\ttest-error:0.34987\n",
      "[156]\ttrain-logloss:11.51011\ttrain-auc:0.68701\ttrain-error:0.31242\ttest-logloss:12.88284\ttest-auc:0.65282\ttest-error:0.34968\n",
      "[157]\ttrain-logloss:11.51161\ttrain-auc:0.68697\ttrain-error:0.31246\ttest-logloss:12.88284\ttest-auc:0.65281\ttest-error:0.34968\n",
      "[158]\ttrain-logloss:11.50109\ttrain-auc:0.68725\ttrain-error:0.31218\ttest-logloss:12.89653\ttest-auc:0.65244\ttest-error:0.35006\n",
      "[159]\ttrain-logloss:11.51011\ttrain-auc:0.68701\ttrain-error:0.31242\ttest-logloss:12.84861\ttest-auc:0.65370\ttest-error:0.34875\n",
      "[160]\ttrain-logloss:11.48907\ttrain-auc:0.68759\ttrain-error:0.31185\ttest-logloss:12.81439\ttest-auc:0.65460\ttest-error:0.34783\n",
      "[161]\ttrain-logloss:11.48006\ttrain-auc:0.68783\ttrain-error:0.31161\ttest-logloss:12.81439\ttest-auc:0.65458\ttest-error:0.34783\n",
      "[162]\ttrain-logloss:11.47104\ttrain-auc:0.68808\ttrain-error:0.31136\ttest-logloss:12.81439\ttest-auc:0.65459\ttest-error:0.34783\n",
      "[163]\ttrain-logloss:11.46653\ttrain-auc:0.68820\ttrain-error:0.31124\ttest-logloss:12.82123\ttest-auc:0.65438\ttest-error:0.34801\n",
      "[164]\ttrain-logloss:11.45000\ttrain-auc:0.68865\ttrain-error:0.31079\ttest-logloss:12.82123\ttest-auc:0.65437\ttest-error:0.34801\n",
      "[165]\ttrain-logloss:11.43047\ttrain-auc:0.68919\ttrain-error:0.31026\ttest-logloss:12.85546\ttest-auc:0.65343\ttest-error:0.34894\n",
      "[166]\ttrain-logloss:11.42296\ttrain-auc:0.68939\ttrain-error:0.31006\ttest-logloss:12.83492\ttest-auc:0.65397\ttest-error:0.34838\n",
      "[167]\ttrain-logloss:11.41845\ttrain-auc:0.68952\ttrain-error:0.30994\ttest-logloss:12.80069\ttest-auc:0.65489\ttest-error:0.34745\n",
      "[168]\ttrain-logloss:11.39290\ttrain-auc:0.69022\ttrain-error:0.30924\ttest-logloss:12.81439\ttest-auc:0.65450\ttest-error:0.34783\n",
      "[169]\ttrain-logloss:11.39290\ttrain-auc:0.69022\ttrain-error:0.30924\ttest-logloss:12.80754\ttest-auc:0.65471\ttest-error:0.34764\n",
      "[170]\ttrain-logloss:11.37788\ttrain-auc:0.69063\ttrain-error:0.30883\ttest-logloss:12.82123\ttest-auc:0.65431\ttest-error:0.34801\n",
      "[171]\ttrain-logloss:11.37036\ttrain-auc:0.69083\ttrain-error:0.30863\ttest-logloss:12.82123\ttest-auc:0.65431\ttest-error:0.34801\n",
      "[172]\ttrain-logloss:11.36886\ttrain-auc:0.69087\ttrain-error:0.30859\ttest-logloss:12.80754\ttest-auc:0.65468\ttest-error:0.34764\n",
      "[173]\ttrain-logloss:11.36135\ttrain-auc:0.69107\ttrain-error:0.30839\ttest-logloss:12.82808\ttest-auc:0.65411\ttest-error:0.34820\n",
      "[174]\ttrain-logloss:11.35384\ttrain-auc:0.69128\ttrain-error:0.30818\ttest-logloss:12.83492\ttest-auc:0.65394\ttest-error:0.34838\n",
      "[175]\ttrain-logloss:11.33881\ttrain-auc:0.69169\ttrain-error:0.30777\ttest-logloss:12.84177\ttest-auc:0.65374\ttest-error:0.34857\n",
      "[176]\ttrain-logloss:11.32679\ttrain-auc:0.69201\ttrain-error:0.30745\ttest-logloss:12.84177\ttest-auc:0.65373\ttest-error:0.34857\n",
      "[177]\ttrain-logloss:11.31477\ttrain-auc:0.69234\ttrain-error:0.30712\ttest-logloss:12.85546\ttest-auc:0.65336\ttest-error:0.34894\n",
      "[178]\ttrain-logloss:11.29223\ttrain-auc:0.69296\ttrain-error:0.30651\ttest-logloss:12.84861\ttest-auc:0.65355\ttest-error:0.34875\n",
      "[179]\ttrain-logloss:11.28171\ttrain-auc:0.69324\ttrain-error:0.30622\ttest-logloss:12.83492\ttest-auc:0.65391\ttest-error:0.34838\n",
      "[180]\ttrain-logloss:11.25767\ttrain-auc:0.69390\ttrain-error:0.30557\ttest-logloss:12.82808\ttest-auc:0.65410\ttest-error:0.34820\n",
      "[181]\ttrain-logloss:11.24264\ttrain-auc:0.69431\ttrain-error:0.30516\ttest-logloss:12.80754\ttest-auc:0.65463\ttest-error:0.34764\n",
      "[182]\ttrain-logloss:11.22611\ttrain-auc:0.69476\ttrain-error:0.30472\ttest-logloss:12.78016\ttest-auc:0.65539\ttest-error:0.34690\n",
      "[183]\ttrain-logloss:11.21409\ttrain-auc:0.69508\ttrain-error:0.30439\ttest-logloss:12.78701\ttest-auc:0.65520\ttest-error:0.34708\n",
      "[184]\ttrain-logloss:11.21860\ttrain-auc:0.69496\ttrain-error:0.30451\ttest-logloss:12.79385\ttest-auc:0.65500\ttest-error:0.34727\n",
      "[185]\ttrain-logloss:11.20508\ttrain-auc:0.69533\ttrain-error:0.30414\ttest-logloss:12.80069\ttest-auc:0.65482\ttest-error:0.34745\n",
      "[186]\ttrain-logloss:11.19756\ttrain-auc:0.69554\ttrain-error:0.30394\ttest-logloss:12.78701\ttest-auc:0.65514\ttest-error:0.34708\n",
      "[187]\ttrain-logloss:11.19756\ttrain-auc:0.69554\ttrain-error:0.30394\ttest-logloss:12.82123\ttest-auc:0.65421\ttest-error:0.34801\n",
      "[188]\ttrain-logloss:11.19005\ttrain-auc:0.69574\ttrain-error:0.30374\ttest-logloss:12.80754\ttest-auc:0.65458\ttest-error:0.34764\n",
      "[189]\ttrain-logloss:11.18855\ttrain-auc:0.69579\ttrain-error:0.30369\ttest-logloss:12.82123\ttest-auc:0.65421\ttest-error:0.34801\n",
      "[190]\ttrain-logloss:11.18404\ttrain-auc:0.69591\ttrain-error:0.30357\ttest-logloss:12.81439\ttest-auc:0.65441\ttest-error:0.34783\n",
      "[191]\ttrain-logloss:11.17352\ttrain-auc:0.69619\ttrain-error:0.30329\ttest-logloss:12.80069\ttest-auc:0.65480\ttest-error:0.34745\n",
      "[192]\ttrain-logloss:11.16000\ttrain-auc:0.69656\ttrain-error:0.30292\ttest-logloss:12.81439\ttest-auc:0.65441\ttest-error:0.34783\n",
      "[193]\ttrain-logloss:11.15248\ttrain-auc:0.69677\ttrain-error:0.30272\ttest-logloss:12.81439\ttest-auc:0.65441\ttest-error:0.34783\n",
      "[194]\ttrain-logloss:11.14948\ttrain-auc:0.69685\ttrain-error:0.30263\ttest-logloss:12.81439\ttest-auc:0.65440\ttest-error:0.34783\n",
      "[195]\ttrain-logloss:11.14497\ttrain-auc:0.69698\ttrain-error:0.30251\ttest-logloss:12.81439\ttest-auc:0.65440\ttest-error:0.34783\n",
      "[196]\ttrain-logloss:11.14197\ttrain-auc:0.69706\ttrain-error:0.30243\ttest-logloss:12.80754\ttest-auc:0.65458\ttest-error:0.34764\n",
      "[197]\ttrain-logloss:11.13445\ttrain-auc:0.69726\ttrain-error:0.30223\ttest-logloss:12.76647\ttest-auc:0.65568\ttest-error:0.34653\n",
      "[198]\ttrain-logloss:11.11792\ttrain-auc:0.69771\ttrain-error:0.30178\ttest-logloss:12.78701\ttest-auc:0.65510\ttest-error:0.34708\n",
      "[199]\ttrain-logloss:11.07886\ttrain-auc:0.69877\ttrain-error:0.30072\ttest-logloss:12.82808\ttest-auc:0.65399\ttest-error:0.34820\n",
      "[200]\ttrain-logloss:11.06533\ttrain-auc:0.69914\ttrain-error:0.30035\ttest-logloss:12.82123\ttest-auc:0.65415\ttest-error:0.34801\n",
      "[201]\ttrain-logloss:11.07134\ttrain-auc:0.69898\ttrain-error:0.30051\ttest-logloss:12.86230\ttest-auc:0.65302\ttest-error:0.34913\n",
      "[202]\ttrain-logloss:11.06383\ttrain-auc:0.69919\ttrain-error:0.30031\ttest-logloss:12.86230\ttest-auc:0.65304\ttest-error:0.34913\n",
      "[203]\ttrain-logloss:11.04279\ttrain-auc:0.69976\ttrain-error:0.29974\ttest-logloss:12.85546\ttest-auc:0.65324\ttest-error:0.34894\n",
      "[204]\ttrain-logloss:11.03077\ttrain-auc:0.70009\ttrain-error:0.29941\ttest-logloss:12.86915\ttest-auc:0.65285\ttest-error:0.34931\n",
      "[205]\ttrain-logloss:11.02777\ttrain-auc:0.70017\ttrain-error:0.29933\ttest-logloss:12.85546\ttest-auc:0.65323\ttest-error:0.34894\n",
      "[206]\ttrain-logloss:11.01124\ttrain-auc:0.70062\ttrain-error:0.29888\ttest-logloss:12.84861\ttest-auc:0.65340\ttest-error:0.34875\n",
      "[207]\ttrain-logloss:10.98569\ttrain-auc:0.70132\ttrain-error:0.29819\ttest-logloss:12.83492\ttest-auc:0.65376\ttest-error:0.34838\n",
      "[208]\ttrain-logloss:10.97818\ttrain-auc:0.70152\ttrain-error:0.29798\ttest-logloss:12.86230\ttest-auc:0.65301\ttest-error:0.34913\n",
      "[209]\ttrain-logloss:10.98118\ttrain-auc:0.70144\ttrain-error:0.29807\ttest-logloss:12.88284\ttest-auc:0.65244\ttest-error:0.34968\n",
      "[210]\ttrain-logloss:10.97367\ttrain-auc:0.70165\ttrain-error:0.29786\ttest-logloss:12.84177\ttest-auc:0.65354\ttest-error:0.34857\n",
      "[211]\ttrain-logloss:10.95865\ttrain-auc:0.70206\ttrain-error:0.29746\ttest-logloss:12.83492\ttest-auc:0.65371\ttest-error:0.34838\n",
      "[212]\ttrain-logloss:10.94061\ttrain-auc:0.70254\ttrain-error:0.29696\ttest-logloss:12.82123\ttest-auc:0.65409\ttest-error:0.34801\n",
      "[213]\ttrain-logloss:10.92859\ttrain-auc:0.70288\ttrain-error:0.29664\ttest-logloss:12.80754\ttest-auc:0.65445\ttest-error:0.34764\n",
      "[214]\ttrain-logloss:10.91958\ttrain-auc:0.70312\ttrain-error:0.29639\ttest-logloss:12.84177\ttest-auc:0.65353\ttest-error:0.34857\n",
      "[215]\ttrain-logloss:10.89854\ttrain-auc:0.70369\ttrain-error:0.29582\ttest-logloss:12.81439\ttest-auc:0.65427\ttest-error:0.34783\n",
      "[216]\ttrain-logloss:10.89704\ttrain-auc:0.70373\ttrain-error:0.29578\ttest-logloss:12.81439\ttest-auc:0.65425\ttest-error:0.34783\n",
      "[217]\ttrain-logloss:10.90455\ttrain-auc:0.70353\ttrain-error:0.29599\ttest-logloss:12.82808\ttest-auc:0.65388\ttest-error:0.34820\n",
      "[218]\ttrain-logloss:10.88652\ttrain-auc:0.70402\ttrain-error:0.29550\ttest-logloss:12.88284\ttest-auc:0.65237\ttest-error:0.34968\n",
      "[219]\ttrain-logloss:10.87750\ttrain-auc:0.70427\ttrain-error:0.29525\ttest-logloss:12.86915\ttest-auc:0.65274\ttest-error:0.34931\n",
      "[220]\ttrain-logloss:10.86999\ttrain-auc:0.70447\ttrain-error:0.29505\ttest-logloss:12.82808\ttest-auc:0.65388\ttest-error:0.34820\n",
      "[221]\ttrain-logloss:10.88652\ttrain-auc:0.70402\ttrain-error:0.29550\ttest-logloss:12.84177\ttest-auc:0.65350\ttest-error:0.34857\n",
      "[222]\ttrain-logloss:10.86548\ttrain-auc:0.70460\ttrain-error:0.29493\ttest-logloss:12.82123\ttest-auc:0.65405\ttest-error:0.34801\n",
      "[223]\ttrain-logloss:10.85496\ttrain-auc:0.70488\ttrain-error:0.29464\ttest-logloss:12.78701\ttest-auc:0.65499\ttest-error:0.34708\n",
      "[224]\ttrain-logloss:10.85346\ttrain-auc:0.70492\ttrain-error:0.29460\ttest-logloss:12.78701\ttest-auc:0.65499\ttest-error:0.34708\n",
      "[225]\ttrain-logloss:10.84595\ttrain-auc:0.70512\ttrain-error:0.29440\ttest-logloss:12.80754\ttest-auc:0.65442\ttest-error:0.34764\n",
      "[226]\ttrain-logloss:10.83543\ttrain-auc:0.70541\ttrain-error:0.29411\ttest-logloss:12.82123\ttest-auc:0.65405\ttest-error:0.34801\n",
      "[227]\ttrain-logloss:10.82942\ttrain-auc:0.70558\ttrain-error:0.29395\ttest-logloss:12.80069\ttest-auc:0.65461\ttest-error:0.34745\n",
      "[228]\ttrain-logloss:10.81289\ttrain-auc:0.70603\ttrain-error:0.29350\ttest-logloss:12.80069\ttest-auc:0.65461\ttest-error:0.34745\n",
      "[229]\ttrain-logloss:10.81139\ttrain-auc:0.70607\ttrain-error:0.29346\ttest-logloss:12.79385\ttest-auc:0.65480\ttest-error:0.34727\n",
      "[230]\ttrain-logloss:10.79486\ttrain-auc:0.70652\ttrain-error:0.29301\ttest-logloss:12.78701\ttest-auc:0.65499\ttest-error:0.34708\n",
      "[231]\ttrain-logloss:10.76180\ttrain-auc:0.70742\ttrain-error:0.29211\ttest-logloss:12.77332\ttest-auc:0.65536\ttest-error:0.34671\n",
      "[232]\ttrain-logloss:10.75429\ttrain-auc:0.70762\ttrain-error:0.29191\ttest-logloss:12.76647\ttest-auc:0.65553\ttest-error:0.34653\n",
      "[233]\ttrain-logloss:10.73776\ttrain-auc:0.70807\ttrain-error:0.29146\ttest-logloss:12.74593\ttest-auc:0.65607\ttest-error:0.34597\n",
      "[234]\ttrain-logloss:10.73926\ttrain-auc:0.70803\ttrain-error:0.29150\ttest-logloss:12.74593\ttest-auc:0.65606\ttest-error:0.34597\n",
      "[235]\ttrain-logloss:10.73325\ttrain-auc:0.70820\ttrain-error:0.29134\ttest-logloss:12.74593\ttest-auc:0.65605\ttest-error:0.34597\n",
      "[236]\ttrain-logloss:10.73025\ttrain-auc:0.70828\ttrain-error:0.29125\ttest-logloss:12.73224\ttest-auc:0.65642\ttest-error:0.34560\n",
      "[237]\ttrain-logloss:10.74076\ttrain-auc:0.70799\ttrain-error:0.29154\ttest-logloss:12.78016\ttest-auc:0.65512\ttest-error:0.34690\n",
      "[238]\ttrain-logloss:10.73475\ttrain-auc:0.70816\ttrain-error:0.29138\ttest-logloss:12.76647\ttest-auc:0.65550\ttest-error:0.34653\n",
      "[239]\ttrain-logloss:10.73776\ttrain-auc:0.70808\ttrain-error:0.29146\ttest-logloss:12.77332\ttest-auc:0.65533\ttest-error:0.34671\n",
      "[240]\ttrain-logloss:10.72273\ttrain-auc:0.70849\ttrain-error:0.29105\ttest-logloss:12.78016\ttest-auc:0.65514\ttest-error:0.34690\n",
      "[241]\ttrain-logloss:10.71071\ttrain-auc:0.70881\ttrain-error:0.29073\ttest-logloss:12.80754\ttest-auc:0.65439\ttest-error:0.34764\n",
      "[242]\ttrain-logloss:10.69568\ttrain-auc:0.70922\ttrain-error:0.29032\ttest-logloss:12.78016\ttest-auc:0.65514\ttest-error:0.34690\n",
      "[243]\ttrain-logloss:10.68066\ttrain-auc:0.70963\ttrain-error:0.28991\ttest-logloss:12.76647\ttest-auc:0.65550\ttest-error:0.34653\n",
      "[244]\ttrain-logloss:10.66864\ttrain-auc:0.70996\ttrain-error:0.28958\ttest-logloss:12.78016\ttest-auc:0.65511\ttest-error:0.34690\n",
      "[245]\ttrain-logloss:10.66714\ttrain-auc:0.71000\ttrain-error:0.28954\ttest-logloss:12.80069\ttest-auc:0.65455\ttest-error:0.34745\n",
      "[246]\ttrain-logloss:10.65662\ttrain-auc:0.71029\ttrain-error:0.28926\ttest-logloss:12.79385\ttest-auc:0.65473\ttest-error:0.34727\n",
      "[247]\ttrain-logloss:10.65061\ttrain-auc:0.71045\ttrain-error:0.28909\ttest-logloss:12.78701\ttest-auc:0.65491\ttest-error:0.34708\n",
      "[248]\ttrain-logloss:10.64610\ttrain-auc:0.71057\ttrain-error:0.28897\ttest-logloss:12.78701\ttest-auc:0.65491\ttest-error:0.34708\n",
      "[249]\ttrain-logloss:10.63859\ttrain-auc:0.71077\ttrain-error:0.28877\ttest-logloss:12.79385\ttest-auc:0.65473\ttest-error:0.34727\n",
      "250-rounds Training finished ...\t\t(1.365s)\n"
     ]
    }
   ],
   "source": [
    "num_round = 250\n",
    "t0 = time()\n",
    "sh_bst_sm = xgb.train(param_sh, xg_train_sh, num_round, watchlist, early_stopping_rounds=60)\n",
    "print(f\"{num_round}-rounds Training finished ...\\t\\t({time()-t0:.3f}s)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test error using softmax = 0.3472686733556299\n"
     ]
    }
   ],
   "source": [
    "# get prediction\n",
    "pred_sh = sh_bst_sm.predict(xg_test_sh)\n",
    "error_rate = np.sum(pred_sh != y_test_sh) / y_test_sh.shape[0]\n",
    "print('Test error using softmax = {}'.format(error_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Counter({1.0: 3115, 0.0: 2267}), Counter({0: 2756, 1: 2626}))"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(pred_sh), Counter(y_test_sh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.57      0.63      2756\n",
      "           1       0.62      0.74      0.67      2626\n",
      "\n",
      "    accuracy                           0.65      5382\n",
      "   macro avg       0.66      0.65      0.65      5382\n",
      "weighted avg       0.66      0.65      0.65      5382\n",
      "\n"
     ]
    }
   ],
   "source": [
    "report_sh = metrics.classification_report(list(y_test_sh), list(pred_sh))\n",
    "print(report_sh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.65472453, 0.65472453])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aucs_sh = auc(y_test_sh.astype(np.uint8), pred_sh.astype(np.uint8), [0, 1])\n",
    "aucs_sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "del X_train_sh, y_train_sh, X_test_sh, y_test_sh, xg_train_sh, xg_test_sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 调参"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "base_param_sh = {  # 基本参数，不需要调参\n",
    "    'objective': 'binary:hinge',\n",
    "    'eta': 0.1,\n",
    "    'nthread': 8,\n",
    "#     'num_class': 10,\n",
    "    'gpu_id': 0,\n",
    "    'tree_method': 'gpu_hist',\n",
    "    'eval_metric': ['logloss', 'auc', 'error']\n",
    "} \n",
    "# 需要调参的参数\n",
    "ps1 = {  \n",
    "    'max_depth': list(range(5, 10, 1)),\n",
    "    'min_child_weight': list(range(1, 10, 2)),\n",
    "}\n",
    "\n",
    "ps2 = {\n",
    "    'gamma': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6]\n",
    "}\n",
    "\n",
    "ps3 = {\n",
    "    'subsample':  [i/10.0 for i in range(6,11,1)], \n",
    "    'colsample_bytree':  [i/10.0 for i in range(6,11,1)] \n",
    "}\n",
    "\n",
    "ps4 = {'reg_alpha': [0, 0.1, 0.2, 0.5, 1, 1.5, 2, 4]}\n",
    "\n",
    "\n",
    "# com_ps_sh = list(ParameterGrid(ps_sh))\n",
    "\n",
    "\n",
    "# all_params_sh = [base_param_sh.copy() for _ in range(len(com_ps_sh))] \n",
    "# for i in range(len(com_ps_sh)):\n",
    "#     all_params_sh[i].update(com_ps_sh[i])\n",
    "\n",
    "# # print(com_ps_sh)\n",
    "# print(all_params_sh.__len__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 550,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data preparing finished ...\t\t(24.924s)\n"
     ]
    }
   ],
   "source": [
    "t0 = time()\n",
    "xg_train_sh = xgb.DMatrix(X_train_sh.values, label=y_train_sh.values, enable_categorical=True)\n",
    "xg_test_sh = xgb.DMatrix(X_test_sh.values, label=y_test_sh.values, enable_categorical=True)\n",
    "print(f\"Data preparing finished ...\\t\\t({time()-t0:.3f}s)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始搜索：max_depth=[5, 6, 7, 8, 9], min_child_weight=[1, 3, 5, 7, 9]\n",
      "待搜索的参数组合数量：25\n",
      "1 / 25: 150-rounds Training finished param={'objective': 'binary:hinge', 'eta': 0.1, 'nthread': 8, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['logloss', 'auc', 'error'], 'max_depth': 5, 'min_child_weight': 1} ...\t\t(3.035s)\n",
      "2 / 25: 150-rounds Training finished param={'objective': 'binary:hinge', 'eta': 0.1, 'nthread': 8, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['logloss', 'auc', 'error'], 'max_depth': 6, 'min_child_weight': 1} ...\t\t(4.271s)\n",
      "3 / 25: 150-rounds Training finished param={'objective': 'binary:hinge', 'eta': 0.1, 'nthread': 8, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['logloss', 'auc', 'error'], 'max_depth': 7, 'min_child_weight': 1} ...\t\t(6.333s)\n",
      "4 / 25: 150-rounds Training finished param={'objective': 'binary:hinge', 'eta': 0.1, 'nthread': 8, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['logloss', 'auc', 'error'], 'max_depth': 8, 'min_child_weight': 1} ...\t\t(9.294s)\n",
      "5 / 25: 150-rounds Training finished param={'objective': 'binary:hinge', 'eta': 0.1, 'nthread': 8, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['logloss', 'auc', 'error'], 'max_depth': 9, 'min_child_weight': 1} ...\t\t(12.788s)\n",
      "6 / 25: 150-rounds Training finished param={'objective': 'binary:hinge', 'eta': 0.1, 'nthread': 8, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['logloss', 'auc', 'error'], 'max_depth': 5, 'min_child_weight': 3} ...\t\t(2.993s)\n",
      "7 / 25: 150-rounds Training finished param={'objective': 'binary:hinge', 'eta': 0.1, 'nthread': 8, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['logloss', 'auc', 'error'], 'max_depth': 6, 'min_child_weight': 3} ...\t\t(4.183s)\n",
      "8 / 25: 150-rounds Training finished param={'objective': 'binary:hinge', 'eta': 0.1, 'nthread': 8, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['logloss', 'auc', 'error'], 'max_depth': 7, 'min_child_weight': 3} ...\t\t(6.127s)\n",
      "9 / 25: 150-rounds Training finished param={'objective': 'binary:hinge', 'eta': 0.1, 'nthread': 8, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['logloss', 'auc', 'error'], 'max_depth': 8, 'min_child_weight': 3} ...\t\t(9.131s)\n",
      "10 / 25: 150-rounds Training finished param={'objective': 'binary:hinge', 'eta': 0.1, 'nthread': 8, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['logloss', 'auc', 'error'], 'max_depth': 9, 'min_child_weight': 3} ...\t\t(12.637s)\n",
      "11 / 25: 150-rounds Training finished param={'objective': 'binary:hinge', 'eta': 0.1, 'nthread': 8, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['logloss', 'auc', 'error'], 'max_depth': 5, 'min_child_weight': 5} ...\t\t(2.996s)\n",
      "12 / 25: 150-rounds Training finished param={'objective': 'binary:hinge', 'eta': 0.1, 'nthread': 8, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['logloss', 'auc', 'error'], 'max_depth': 6, 'min_child_weight': 5} ...\t\t(4.244s)\n",
      "13 / 25: 150-rounds Training finished param={'objective': 'binary:hinge', 'eta': 0.1, 'nthread': 8, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['logloss', 'auc', 'error'], 'max_depth': 7, 'min_child_weight': 5} ...\t\t(6.028s)\n",
      "14 / 25: 150-rounds Training finished param={'objective': 'binary:hinge', 'eta': 0.1, 'nthread': 8, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['logloss', 'auc', 'error'], 'max_depth': 8, 'min_child_weight': 5} ...\t\t(8.756s)\n",
      "15 / 25: 150-rounds Training finished param={'objective': 'binary:hinge', 'eta': 0.1, 'nthread': 8, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['logloss', 'auc', 'error'], 'max_depth': 9, 'min_child_weight': 5} ...\t\t(12.128s)\n",
      "16 / 25: 150-rounds Training finished param={'objective': 'binary:hinge', 'eta': 0.1, 'nthread': 8, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['logloss', 'auc', 'error'], 'max_depth': 5, 'min_child_weight': 7} ...\t\t(2.959s)\n",
      "17 / 25: 150-rounds Training finished param={'objective': 'binary:hinge', 'eta': 0.1, 'nthread': 8, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['logloss', 'auc', 'error'], 'max_depth': 6, 'min_child_weight': 7} ...\t\t(4.105s)\n",
      "18 / 25: 150-rounds Training finished param={'objective': 'binary:hinge', 'eta': 0.1, 'nthread': 8, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['logloss', 'auc', 'error'], 'max_depth': 7, 'min_child_weight': 7} ...\t\t(5.897s)\n",
      "19 / 25: 150-rounds Training finished param={'objective': 'binary:hinge', 'eta': 0.1, 'nthread': 8, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['logloss', 'auc', 'error'], 'max_depth': 8, 'min_child_weight': 7} ...\t\t(8.433s)\n",
      "20 / 25: 150-rounds Training finished param={'objective': 'binary:hinge', 'eta': 0.1, 'nthread': 8, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['logloss', 'auc', 'error'], 'max_depth': 9, 'min_child_weight': 7} ...\t\t(11.471s)\n",
      "21 / 25: 150-rounds Training finished param={'objective': 'binary:hinge', 'eta': 0.1, 'nthread': 8, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['logloss', 'auc', 'error'], 'max_depth': 5, 'min_child_weight': 9} ...\t\t(2.935s)\n",
      "22 / 25: 150-rounds Training finished param={'objective': 'binary:hinge', 'eta': 0.1, 'nthread': 8, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['logloss', 'auc', 'error'], 'max_depth': 6, 'min_child_weight': 9} ...\t\t(4.050s)\n",
      "23 / 25: 150-rounds Training finished param={'objective': 'binary:hinge', 'eta': 0.1, 'nthread': 8, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['logloss', 'auc', 'error'], 'max_depth': 7, 'min_child_weight': 9} ...\t\t(5.802s)\n",
      "24 / 25: 150-rounds Training finished param={'objective': 'binary:hinge', 'eta': 0.1, 'nthread': 8, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['logloss', 'auc', 'error'], 'max_depth': 8, 'min_child_weight': 9} ...\t\t(8.152s)\n",
      "25 / 25: 150-rounds Training finished param={'objective': 'binary:hinge', 'eta': 0.1, 'nthread': 8, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['logloss', 'auc', 'error'], 'max_depth': 9, 'min_child_weight': 9} ...\t\t(11.286s)\n",
      "eta=0.1, eval_metric=['logloss', 'auc', 'error'], gpu_id=0, max_depth=5, min_child_weight=1, nthread=8, objective=binary:hinge, tree_method=gpu_hist\n",
      "eta=0.1, eval_metric=['logloss', 'auc', 'error'], gpu_id=0, max_depth=7, min_child_weight=9, nthread=8, objective=binary:hinge, tree_method=gpu_hist\n",
      "开始搜索：gamma=[0.1, 0.2, 0.3, 0.4, 0.5, 0.6]\n",
      "待搜索的参数组合数量：6\n",
      "1 / 6: 150-rounds Training finished param={'objective': 'binary:hinge', 'eta': 0.1, 'nthread': 8, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['logloss', 'auc', 'error'], 'max_depth': 5, 'min_child_weight': 1, 'gamma': 0.1} ...\t\t(3.033s)\n",
      "2 / 6: 150-rounds Training finished param={'objective': 'binary:hinge', 'eta': 0.1, 'nthread': 8, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['logloss', 'auc', 'error'], 'max_depth': 5, 'min_child_weight': 1, 'gamma': 0.2} ...\t\t(3.015s)\n",
      "3 / 6: 150-rounds Training finished param={'objective': 'binary:hinge', 'eta': 0.1, 'nthread': 8, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['logloss', 'auc', 'error'], 'max_depth': 5, 'min_child_weight': 1, 'gamma': 0.3} ...\t\t(3.015s)\n",
      "4 / 6: 150-rounds Training finished param={'objective': 'binary:hinge', 'eta': 0.1, 'nthread': 8, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['logloss', 'auc', 'error'], 'max_depth': 5, 'min_child_weight': 1, 'gamma': 0.4} ...\t\t(3.057s)\n",
      "5 / 6: 150-rounds Training finished param={'objective': 'binary:hinge', 'eta': 0.1, 'nthread': 8, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['logloss', 'auc', 'error'], 'max_depth': 5, 'min_child_weight': 1, 'gamma': 0.5} ...\t\t(3.028s)\n",
      "6 / 6: 150-rounds Training finished param={'objective': 'binary:hinge', 'eta': 0.1, 'nthread': 8, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['logloss', 'auc', 'error'], 'max_depth': 5, 'min_child_weight': 1, 'gamma': 0.6} ...\t\t(3.047s)\n",
      "eta=0.1, eval_metric=['logloss', 'auc', 'error'], gamma=0.1, gpu_id=0, max_depth=5, min_child_weight=1, nthread=8, objective=binary:hinge, tree_method=gpu_hist\n",
      "eta=0.1, eval_metric=['logloss', 'auc', 'error'], gamma=0.3, gpu_id=0, max_depth=5, min_child_weight=1, nthread=8, objective=binary:hinge, tree_method=gpu_hist\n",
      "开始搜索：colsample_bytree=[0.6, 0.7, 0.8, 0.9, 1.0], subsample=[0.6, 0.7, 0.8, 0.9, 1.0]\n",
      "待搜索的参数组合数量：25\n",
      "1 / 25: 150-rounds Training finished param={'objective': 'binary:hinge', 'eta': 0.1, 'nthread': 8, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['logloss', 'auc', 'error'], 'max_depth': 5, 'min_child_weight': 1, 'gamma': 0.1, 'subsample': 0.6, 'colsample_bytree': 0.6} ...\t\t(2.884s)\n",
      "2 / 25: 150-rounds Training finished param={'objective': 'binary:hinge', 'eta': 0.1, 'nthread': 8, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['logloss', 'auc', 'error'], 'max_depth': 5, 'min_child_weight': 1, 'gamma': 0.1, 'subsample': 0.7, 'colsample_bytree': 0.6} ...\t\t(2.922s)\n",
      "3 / 25: 150-rounds Training finished param={'objective': 'binary:hinge', 'eta': 0.1, 'nthread': 8, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['logloss', 'auc', 'error'], 'max_depth': 5, 'min_child_weight': 1, 'gamma': 0.1, 'subsample': 0.8, 'colsample_bytree': 0.6} ...\t\t(2.947s)\n",
      "4 / 25: 150-rounds Training finished param={'objective': 'binary:hinge', 'eta': 0.1, 'nthread': 8, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['logloss', 'auc', 'error'], 'max_depth': 5, 'min_child_weight': 1, 'gamma': 0.1, 'subsample': 0.9, 'colsample_bytree': 0.6} ...\t\t(2.897s)\n",
      "5 / 25: 150-rounds Training finished param={'objective': 'binary:hinge', 'eta': 0.1, 'nthread': 8, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['logloss', 'auc', 'error'], 'max_depth': 5, 'min_child_weight': 1, 'gamma': 0.1, 'subsample': 1.0, 'colsample_bytree': 0.6} ...\t\t(2.930s)\n",
      "6 / 25: 150-rounds Training finished param={'objective': 'binary:hinge', 'eta': 0.1, 'nthread': 8, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['logloss', 'auc', 'error'], 'max_depth': 5, 'min_child_weight': 1, 'gamma': 0.1, 'subsample': 0.6, 'colsample_bytree': 0.7} ...\t\t(2.918s)\n",
      "7 / 25: 150-rounds Training finished param={'objective': 'binary:hinge', 'eta': 0.1, 'nthread': 8, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['logloss', 'auc', 'error'], 'max_depth': 5, 'min_child_weight': 1, 'gamma': 0.1, 'subsample': 0.7, 'colsample_bytree': 0.7} ...\t\t(2.911s)\n",
      "8 / 25: 150-rounds Training finished param={'objective': 'binary:hinge', 'eta': 0.1, 'nthread': 8, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['logloss', 'auc', 'error'], 'max_depth': 5, 'min_child_weight': 1, 'gamma': 0.1, 'subsample': 0.8, 'colsample_bytree': 0.7} ...\t\t(2.913s)\n",
      "9 / 25: 150-rounds Training finished param={'objective': 'binary:hinge', 'eta': 0.1, 'nthread': 8, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['logloss', 'auc', 'error'], 'max_depth': 5, 'min_child_weight': 1, 'gamma': 0.1, 'subsample': 0.9, 'colsample_bytree': 0.7} ...\t\t(2.894s)\n",
      "10 / 25: 150-rounds Training finished param={'objective': 'binary:hinge', 'eta': 0.1, 'nthread': 8, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['logloss', 'auc', 'error'], 'max_depth': 5, 'min_child_weight': 1, 'gamma': 0.1, 'subsample': 1.0, 'colsample_bytree': 0.7} ...\t\t(2.918s)\n",
      "11 / 25: 150-rounds Training finished param={'objective': 'binary:hinge', 'eta': 0.1, 'nthread': 8, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['logloss', 'auc', 'error'], 'max_depth': 5, 'min_child_weight': 1, 'gamma': 0.1, 'subsample': 0.6, 'colsample_bytree': 0.8} ...\t\t(2.878s)\n",
      "12 / 25: 150-rounds Training finished param={'objective': 'binary:hinge', 'eta': 0.1, 'nthread': 8, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['logloss', 'auc', 'error'], 'max_depth': 5, 'min_child_weight': 1, 'gamma': 0.1, 'subsample': 0.7, 'colsample_bytree': 0.8} ...\t\t(2.942s)\n",
      "13 / 25: 150-rounds Training finished param={'objective': 'binary:hinge', 'eta': 0.1, 'nthread': 8, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['logloss', 'auc', 'error'], 'max_depth': 5, 'min_child_weight': 1, 'gamma': 0.1, 'subsample': 0.8, 'colsample_bytree': 0.8} ...\t\t(2.914s)\n",
      "14 / 25: 150-rounds Training finished param={'objective': 'binary:hinge', 'eta': 0.1, 'nthread': 8, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['logloss', 'auc', 'error'], 'max_depth': 5, 'min_child_weight': 1, 'gamma': 0.1, 'subsample': 0.9, 'colsample_bytree': 0.8} ...\t\t(2.922s)\n",
      "15 / 25: 150-rounds Training finished param={'objective': 'binary:hinge', 'eta': 0.1, 'nthread': 8, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['logloss', 'auc', 'error'], 'max_depth': 5, 'min_child_weight': 1, 'gamma': 0.1, 'subsample': 1.0, 'colsample_bytree': 0.8} ...\t\t(2.936s)\n",
      "16 / 25: 150-rounds Training finished param={'objective': 'binary:hinge', 'eta': 0.1, 'nthread': 8, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['logloss', 'auc', 'error'], 'max_depth': 5, 'min_child_weight': 1, 'gamma': 0.1, 'subsample': 0.6, 'colsample_bytree': 0.9} ...\t\t(2.921s)\n",
      "17 / 25: 150-rounds Training finished param={'objective': 'binary:hinge', 'eta': 0.1, 'nthread': 8, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['logloss', 'auc', 'error'], 'max_depth': 5, 'min_child_weight': 1, 'gamma': 0.1, 'subsample': 0.7, 'colsample_bytree': 0.9} ...\t\t(2.938s)\n",
      "18 / 25: 150-rounds Training finished param={'objective': 'binary:hinge', 'eta': 0.1, 'nthread': 8, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['logloss', 'auc', 'error'], 'max_depth': 5, 'min_child_weight': 1, 'gamma': 0.1, 'subsample': 0.8, 'colsample_bytree': 0.9} ...\t\t(2.965s)\n",
      "19 / 25: 150-rounds Training finished param={'objective': 'binary:hinge', 'eta': 0.1, 'nthread': 8, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['logloss', 'auc', 'error'], 'max_depth': 5, 'min_child_weight': 1, 'gamma': 0.1, 'subsample': 0.9, 'colsample_bytree': 0.9} ...\t\t(3.075s)\n",
      "20 / 25: 150-rounds Training finished param={'objective': 'binary:hinge', 'eta': 0.1, 'nthread': 8, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['logloss', 'auc', 'error'], 'max_depth': 5, 'min_child_weight': 1, 'gamma': 0.1, 'subsample': 1.0, 'colsample_bytree': 0.9} ...\t\t(2.903s)\n",
      "21 / 25: 150-rounds Training finished param={'objective': 'binary:hinge', 'eta': 0.1, 'nthread': 8, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['logloss', 'auc', 'error'], 'max_depth': 5, 'min_child_weight': 1, 'gamma': 0.1, 'subsample': 0.6, 'colsample_bytree': 1.0} ...\t\t(2.902s)\n",
      "22 / 25: 150-rounds Training finished param={'objective': 'binary:hinge', 'eta': 0.1, 'nthread': 8, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['logloss', 'auc', 'error'], 'max_depth': 5, 'min_child_weight': 1, 'gamma': 0.1, 'subsample': 0.7, 'colsample_bytree': 1.0} ...\t\t(2.910s)\n",
      "23 / 25: 150-rounds Training finished param={'objective': 'binary:hinge', 'eta': 0.1, 'nthread': 8, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['logloss', 'auc', 'error'], 'max_depth': 5, 'min_child_weight': 1, 'gamma': 0.1, 'subsample': 0.8, 'colsample_bytree': 1.0} ...\t\t(2.951s)\n",
      "24 / 25: 150-rounds Training finished param={'objective': 'binary:hinge', 'eta': 0.1, 'nthread': 8, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['logloss', 'auc', 'error'], 'max_depth': 5, 'min_child_weight': 1, 'gamma': 0.1, 'subsample': 0.9, 'colsample_bytree': 1.0} ...\t\t(2.914s)\n",
      "25 / 25: 150-rounds Training finished param={'objective': 'binary:hinge', 'eta': 0.1, 'nthread': 8, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['logloss', 'auc', 'error'], 'max_depth': 5, 'min_child_weight': 1, 'gamma': 0.1, 'subsample': 1.0, 'colsample_bytree': 1.0} ...\t\t(2.935s)\n",
      "colsample_bytree=0.6, eta=0.1, eval_metric=['logloss', 'auc', 'error'], gamma=0.1, gpu_id=0, max_depth=5, min_child_weight=1, nthread=8, objective=binary:hinge, subsample=0.6, tree_method=gpu_hist\n",
      "colsample_bytree=0.6, eta=0.1, eval_metric=['logloss', 'auc', 'error'], gamma=0.1, gpu_id=0, max_depth=5, min_child_weight=1, nthread=8, objective=binary:hinge, subsample=0.8, tree_method=gpu_hist\n",
      "开始搜索：reg_alpha=[0, 0.1, 0.2, 0.5, 1, 1.5, 2, 4]\n",
      "待搜索的参数组合数量：8\n",
      "1 / 8: 150-rounds Training finished param={'objective': 'binary:hinge', 'eta': 0.1, 'nthread': 8, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['logloss', 'auc', 'error'], 'max_depth': 5, 'min_child_weight': 1, 'gamma': 0.1, 'subsample': 0.6, 'colsample_bytree': 0.6, 'reg_alpha': 0} ...\t\t(2.778s)\n",
      "2 / 8: 150-rounds Training finished param={'objective': 'binary:hinge', 'eta': 0.1, 'nthread': 8, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['logloss', 'auc', 'error'], 'max_depth': 5, 'min_child_weight': 1, 'gamma': 0.1, 'subsample': 0.6, 'colsample_bytree': 0.6, 'reg_alpha': 0.1} ...\t\t(2.773s)\n",
      "3 / 8: 150-rounds Training finished param={'objective': 'binary:hinge', 'eta': 0.1, 'nthread': 8, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['logloss', 'auc', 'error'], 'max_depth': 5, 'min_child_weight': 1, 'gamma': 0.1, 'subsample': 0.6, 'colsample_bytree': 0.6, 'reg_alpha': 0.2} ...\t\t(2.782s)\n",
      "4 / 8: 150-rounds Training finished param={'objective': 'binary:hinge', 'eta': 0.1, 'nthread': 8, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['logloss', 'auc', 'error'], 'max_depth': 5, 'min_child_weight': 1, 'gamma': 0.1, 'subsample': 0.6, 'colsample_bytree': 0.6, 'reg_alpha': 0.5} ...\t\t(2.783s)\n",
      "5 / 8: 150-rounds Training finished param={'objective': 'binary:hinge', 'eta': 0.1, 'nthread': 8, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['logloss', 'auc', 'error'], 'max_depth': 5, 'min_child_weight': 1, 'gamma': 0.1, 'subsample': 0.6, 'colsample_bytree': 0.6, 'reg_alpha': 1} ...\t\t(2.803s)\n",
      "6 / 8: 150-rounds Training finished param={'objective': 'binary:hinge', 'eta': 0.1, 'nthread': 8, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['logloss', 'auc', 'error'], 'max_depth': 5, 'min_child_weight': 1, 'gamma': 0.1, 'subsample': 0.6, 'colsample_bytree': 0.6, 'reg_alpha': 1.5} ...\t\t(2.803s)\n",
      "7 / 8: 150-rounds Training finished param={'objective': 'binary:hinge', 'eta': 0.1, 'nthread': 8, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['logloss', 'auc', 'error'], 'max_depth': 5, 'min_child_weight': 1, 'gamma': 0.1, 'subsample': 0.6, 'colsample_bytree': 0.6, 'reg_alpha': 2} ...\t\t(2.817s)\n",
      "8 / 8: 150-rounds Training finished param={'objective': 'binary:hinge', 'eta': 0.1, 'nthread': 8, 'gpu_id': 0, 'tree_method': 'gpu_hist', 'eval_metric': ['logloss', 'auc', 'error'], 'max_depth': 5, 'min_child_weight': 1, 'gamma': 0.1, 'subsample': 0.6, 'colsample_bytree': 0.6, 'reg_alpha': 4} ...\t\t(2.815s)\n",
      "colsample_bytree=0.6, eta=0.1, eval_metric=['logloss', 'auc', 'error'], gamma=0.1, gpu_id=0, max_depth=5, min_child_weight=1, nthread=8, objective=binary:hinge, reg_alpha=0, subsample=0.6, tree_method=gpu_hist\n",
      "colsample_bytree=0.6, eta=0.1, eval_metric=['logloss', 'auc', 'error'], gamma=0.1, gpu_id=0, max_depth=5, min_child_weight=1, nthread=8, objective=binary:hinge, reg_alpha=0.5, subsample=0.6, tree_method=gpu_hist\n",
      "找到的最棒的参数是：\n",
      "colsample_bytree=0.6, eta=0.1, eval_metric=['logloss', 'auc', 'error'], gamma=0.1, gpu_id=0, max_depth=5, min_child_weight=1, nthread=8, objective=binary:hinge, reg_alpha=0, subsample=0.6, tree_method=gpu_hist\n"
     ]
    }
   ],
   "source": [
    "base_sh = base_param_sh.copy()\n",
    "grids_sh = [ps1, ps2, ps3, ps4]\n",
    "\n",
    "rets_sh = []\n",
    "for grid in grids_sh:\n",
    "    params = compose_param_grid(grid, base_sh)\n",
    "    print(f\"开始搜索：{dict_2_str(grid)}\\n待搜索的参数组合数量：{len(params)}\")\n",
    "    ret = gridsearch_cv_xgb(data_sh.values, is_share_res.values, params, n_round=150, verbose_eval=False, n_class=2)\n",
    "    arr = np.array([[-e['eval-error'] for e in ret], \n",
    "                    [-e['eval-logloss'] for e in ret],\n",
    "                    [e['eval-auc'] for e in ret], \n",
    "                    [e['w_auc'] for e in ret]], dtype=np.float32)\n",
    "    opt_idxs = list(set(arr.argmax(axis=1)))\n",
    "    for i in opt_idxs:\n",
    "        print(dict_2_str(ret[i]['param']))\n",
    "    opt_idx = opt_idxs[0]\n",
    "    opt_param = ret[opt_idx]['param']\n",
    "    base_sh.update(opt_param)\n",
    "    rets_sh.append(ret)\n",
    "    \n",
    "    ks = list(grid.keys())\n",
    "    ks = '-'.join(ks)\n",
    "    with open(f\"./logs/{ks}-is_share-{int(time())}.pkl\", \"wb\") as f:\n",
    "        pickle.dump(ret, f)\n",
    "    with open(f\"./logs/{ks}-is_share-{int(time())}.md\", \"w\") as f:\n",
    "        lines = []\n",
    "        for e in ret:\n",
    "            line = metric_2_str(e)\n",
    "            lines.append(line)\n",
    "            lines.append('\\n')\n",
    "        f.write(\"\\n\".join(lines))\n",
    "\n",
    "print(f\"找到的最棒的参数是：\\n{dict_2_str(base_sh)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'objective': 'binary:hinge',\n",
       " 'eta': 0.1,\n",
       " 'nthread': 8,\n",
       " 'gpu_id': 0,\n",
       " 'tree_method': 'gpu_hist',\n",
       " 'eval_metric': ['logloss', 'auc', 'error'],\n",
       " 'max_depth': 5,\n",
       " 'min_child_weight': 1,\n",
       " 'gamma': 0.1,\n",
       " 'subsample': 0.6,\n",
       " 'colsample_bytree': 0.6,\n",
       " 'reg_alpha': 0}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gridsearch_results_sh = gridsearch_xgb(all_params_sh, xg_train_sh, xg_test_sh, num_round=150, n_class=2, verbose_eval=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_cv_results_sh = gridsearch_cv_xgb(data_sh.values, is_share_res.values, all_params_sh, n_round=150, verbose_eval=False, n_class=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 753,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('greadsearch-cv-is_share.md', 'w') as f:\n",
    "    for ret in performance:\n",
    "        f.write(f\"# {', '.join([f'{k}={v}' for k, v in ret[0].items()])}\\n\")\n",
    "        for k, v in ret[1].items():\n",
    "            is_break = '\\n' if '\\n' in str(df) else ''\n",
    "            f.write(f\"- {k} :{is_break} {v}\\n\\n\")\n",
    "        f.write(f\"{'-'*50}\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 765,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48"
      ]
     },
     "execution_count": 765,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_test_error = np.array([e[1]['mean_test_error'] for e in performance])\n",
    "mean_test_error.argmin()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 617,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'objective': 'binary:hinge',\n",
       " 'eta': 0.1,\n",
       " 'nthread': 8,\n",
       " 'gpu_id': 0,\n",
       " 'tree_method': 'gpu_hist',\n",
       " 'gamma': 0.3,\n",
       " 'max_depth': 11,\n",
       " 'min_child_weight': 9}"
      ]
     },
     "execution_count": 617,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr = np.array([[-e['test_error'] for e in gridsearch_results_sh], [e['aucs'][1] for e in gridsearch_results_sh]], dtype=np.float32)\n",
    "opt_idxs_sh = arr.argmax(axis=1)\n",
    "if opt_idxs_sh[0] != opt_idxs_sh[1]:\n",
    "    warnings.warn(f\"最小误差与最大AUC对应的模型不一致 : {opt_idxs_sh}。选择误差最小的模型 : {opt_idxs_sh[0]}\")\n",
    "\n",
    "opt_idx_sh = opt_idxs_sh[0]\n",
    "all_params_sh[opt_idx_sh]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 603,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "146"
      ]
     },
     "execution_count": 603,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opt_idx_sh = opt_idxs_sh[0]\n",
    "opt_idx_sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 619,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.54      0.44      0.49      3053\n",
      "         1.0       0.50      0.60      0.54      2835\n",
      "\n",
      "    accuracy                           0.52      5888\n",
      "   macro avg       0.52      0.52      0.51      5888\n",
      "weighted avg       0.52      0.52      0.51      5888\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(gridsearch_results_sh[opt_idx_sh]['report'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 620,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>test_error</th>\n",
       "      <th>aucs</th>\n",
       "      <th>w_auc</th>\n",
       "      <th>report</th>\n",
       "      <th>model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.503057</td>\n",
       "      <td>[0.50379590202715, 0.50379590202715]</td>\n",
       "      <td>None</td>\n",
       "      <td>precision    recall  f1-score   ...</td>\n",
       "      <td>&lt;xgboost.core.Booster object at 0x7f35ba907130&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.496943</td>\n",
       "      <td>[0.508734635779073, 0.508734635779073]</td>\n",
       "      <td>None</td>\n",
       "      <td>precision    recall  f1-score   ...</td>\n",
       "      <td>&lt;xgboost.core.Booster object at 0x7f32e4009580&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.501189</td>\n",
       "      <td>[0.5048543919272165, 0.5048543919272165]</td>\n",
       "      <td>None</td>\n",
       "      <td>precision    recall  f1-score   ...</td>\n",
       "      <td>&lt;xgboost.core.Booster object at 0x7f3583f4f4f0&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.502717</td>\n",
       "      <td>[0.502360357955947, 0.502360357955947]</td>\n",
       "      <td>None</td>\n",
       "      <td>precision    recall  f1-score   ...</td>\n",
       "      <td>&lt;xgboost.core.Booster object at 0x7f3574279550&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.499321</td>\n",
       "      <td>[0.5061017844072763, 0.5061017844072763]</td>\n",
       "      <td>None</td>\n",
       "      <td>precision    recall  f1-score   ...</td>\n",
       "      <td>&lt;xgboost.core.Booster object at 0x7f32e4082730&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>0.498132</td>\n",
       "      <td>[0.5057747576472328, 0.5057747576472328]</td>\n",
       "      <td>None</td>\n",
       "      <td>precision    recall  f1-score   ...</td>\n",
       "      <td>&lt;xgboost.core.Booster object at 0x7f32468a4640&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>0.497962</td>\n",
       "      <td>[0.5046665869463117, 0.5046665869463118]</td>\n",
       "      <td>None</td>\n",
       "      <td>precision    recall  f1-score   ...</td>\n",
       "      <td>&lt;xgboost.core.Booster object at 0x7f32468a4850&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>0.499490</td>\n",
       "      <td>[0.5026636996830249, 0.5026636996830249]</td>\n",
       "      <td>None</td>\n",
       "      <td>precision    recall  f1-score   ...</td>\n",
       "      <td>&lt;xgboost.core.Booster object at 0x7f32468a46a0&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>0.491678</td>\n",
       "      <td>[0.5107513874519006, 0.5107513874519005]</td>\n",
       "      <td>None</td>\n",
       "      <td>precision    recall  f1-score   ...</td>\n",
       "      <td>&lt;xgboost.core.Booster object at 0x7f32468a4670&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>0.499151</td>\n",
       "      <td>[0.5034068320344114, 0.5034068320344114]</td>\n",
       "      <td>None</td>\n",
       "      <td>precision    recall  f1-score   ...</td>\n",
       "      <td>&lt;xgboost.core.Booster object at 0x7f32468a4820&gt;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>120 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     test_error                                      aucs w_auc  \\\n",
       "0      0.503057      [0.50379590202715, 0.50379590202715]  None   \n",
       "1      0.496943    [0.508734635779073, 0.508734635779073]  None   \n",
       "2      0.501189  [0.5048543919272165, 0.5048543919272165]  None   \n",
       "3      0.502717    [0.502360357955947, 0.502360357955947]  None   \n",
       "4      0.499321  [0.5061017844072763, 0.5061017844072763]  None   \n",
       "..          ...                                       ...   ...   \n",
       "115    0.498132  [0.5057747576472328, 0.5057747576472328]  None   \n",
       "116    0.497962  [0.5046665869463117, 0.5046665869463118]  None   \n",
       "117    0.499490  [0.5026636996830249, 0.5026636996830249]  None   \n",
       "118    0.491678  [0.5107513874519006, 0.5107513874519005]  None   \n",
       "119    0.499151  [0.5034068320344114, 0.5034068320344114]  None   \n",
       "\n",
       "                                                report  \\\n",
       "0                  precision    recall  f1-score   ...   \n",
       "1                  precision    recall  f1-score   ...   \n",
       "2                  precision    recall  f1-score   ...   \n",
       "3                  precision    recall  f1-score   ...   \n",
       "4                  precision    recall  f1-score   ...   \n",
       "..                                                 ...   \n",
       "115                precision    recall  f1-score   ...   \n",
       "116                precision    recall  f1-score   ...   \n",
       "117                precision    recall  f1-score   ...   \n",
       "118                precision    recall  f1-score   ...   \n",
       "119                precision    recall  f1-score   ...   \n",
       "\n",
       "                                               model  \n",
       "0    <xgboost.core.Booster object at 0x7f35ba907130>  \n",
       "1    <xgboost.core.Booster object at 0x7f32e4009580>  \n",
       "2    <xgboost.core.Booster object at 0x7f3583f4f4f0>  \n",
       "3    <xgboost.core.Booster object at 0x7f3574279550>  \n",
       "4    <xgboost.core.Booster object at 0x7f32e4082730>  \n",
       "..                                               ...  \n",
       "115  <xgboost.core.Booster object at 0x7f32468a4640>  \n",
       "116  <xgboost.core.Booster object at 0x7f32468a4850>  \n",
       "117  <xgboost.core.Booster object at 0x7f32468a46a0>  \n",
       "118  <xgboost.core.Booster object at 0x7f32468a4670>  \n",
       "119  <xgboost.core.Booster object at 0x7f32468a4820>  \n",
       "\n",
       "[120 rows x 5 columns]"
      ]
     },
     "execution_count": 620,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(gridsearch_results_sh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB \n",
    "from mlxtend.classifier import StackingClassifier\n",
    "from sklearn.ensemble import (RandomForestClassifier, AdaBoostClassifier, \n",
    "                              GradientBoostingClassifier, ExtraTreesClassifier)\n",
    "from xgboost import XGBClassifier, XGBRFClassifier\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 第一层模型参数\n",
    "first_layer_params = {\n",
    "    LogisticRegression : {\n",
    "        'C' : 10,\n",
    "        'random_state': 0\n",
    "    },\n",
    "    RandomForestClassifier : {\n",
    "        'n_jobs': -1,\n",
    "        'n_estimators': 200,\n",
    "         'warm_start': True, \n",
    "         #'max_features': 0.2,\n",
    "        'max_depth': 8,\n",
    "        'min_samples_leaf': 2,\n",
    "        'max_features' : 'sqrt',\n",
    "        'verbose': 0\n",
    "    },\n",
    "    ExtraTreesClassifier : {\n",
    "        'n_jobs': -1,\n",
    "        'n_estimators':200,\n",
    "        #'max_features': 0.5,\n",
    "        'max_depth': 8,\n",
    "        'min_samples_leaf': 2,\n",
    "        'verbose': 0\n",
    "    },\n",
    "    AdaBoostClassifier : {\n",
    "        'n_estimators': 200,\n",
    "        'learning_rate' : 0.75\n",
    "    },\n",
    "#     GradientBoostingClassifier : { # 太慢了\n",
    "#         'n_estimators': 200,\n",
    "#          #'max_features': 0.2,\n",
    "#         'max_depth': 5,\n",
    "#         'min_samples_leaf': 2,\n",
    "#         'verbose': 0\n",
    "#     },\n",
    "#     SVC : { # 太慢了\n",
    "#         'kernel' : 'rbf',\n",
    "#         'C' : 0.025,\n",
    "#         'probability': True\n",
    "#     } \n",
    "}\n",
    "\n",
    "\n",
    "second_layer_params = {\n",
    "    XGBClassifier : {\n",
    "        'objective': 'multi:softmax',\n",
    "        'eta': 0.1,\n",
    "        'nthread': 8,\n",
    "        'num_class': 10,\n",
    "        'gpu_id': 0,\n",
    "        'tree_method': 'gpu_hist',\n",
    "        'eval_metric': ['mlogloss', 'auc', 'merror'],\n",
    "        'max_depth': 9,\n",
    "        'min_child_weight': 9,\n",
    "        'gamma': 0.2,\n",
    "        'subsample': 0.9,\n",
    "        'colsample_bytree': 0.6,\n",
    "        'reg_alpha': 0,\n",
    "        'n_estimators': 200,\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "clfs = []\n",
    "for c, p in first_layer_params.items():\n",
    "    clfs.append(c(**p))\n",
    "\n",
    "meta = XGBClassifier(**second_layer_params[XGBClassifier])\n",
    "sclf = StackingClassifier(classifiers=clfs, \n",
    "                          meta_classifier=meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gzy/.local/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "StackingClassifier(classifiers=[RandomForestClassifier(max_depth=8,\n",
       "                                                       max_features='sqrt',\n",
       "                                                       min_samples_leaf=2,\n",
       "                                                       n_estimators=200,\n",
       "                                                       n_jobs=-1,\n",
       "                                                       warm_start=True),\n",
       "                                ExtraTreesClassifier(max_depth=8,\n",
       "                                                     min_samples_leaf=2,\n",
       "                                                     n_estimators=200,\n",
       "                                                     n_jobs=-1),\n",
       "                                AdaBoostClassifier(learning_rate=0.75,\n",
       "                                                   n_estimators=200),\n",
       "                                GradientBoostingClassifier(max_depth=5,\n",
       "                                                           min_samples_leaf=2,\n",
       "                                                           n_esti...\n",
       "                                                 interaction_constraints=None,\n",
       "                                                 learning_rate=None,\n",
       "                                                 max_delta_step=None,\n",
       "                                                 max_depth=9,\n",
       "                                                 min_child_weight=9,\n",
       "                                                 missing=nan,\n",
       "                                                 monotone_constraints=None,\n",
       "                                                 n_estimators=200, n_jobs=None,\n",
       "                                                 nthread=8, num_class=10,\n",
       "                                                 num_parallel_tree=None,\n",
       "                                                 objective='multi:softmax',\n",
       "                                                 random_state=None, reg_alpha=0,\n",
       "                                                 reg_lambda=None,\n",
       "                                                 scale_pos_weight=None,\n",
       "                                                 subsample=0.9,\n",
       "                                                 tree_method='gpu_hist',\n",
       "                                                 validate_parameters=None, ...))"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sclf.fit(X_train_sh, y_train_sh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6674231843575419"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sclf.score(X_test_sh, y_test_sh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 13.1 s, sys: 112 ms, total: 13.2 s\n",
      "Wall time: 987 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(max_depth=8, max_features='sqrt', min_samples_leaf=2,\n",
       "                       n_estimators=200, n_jobs=-1, warm_start=True)"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "clf = RandomForestClassifier(**first_layer_params[RandomForestClassifier])\n",
    "clf.fit(X_train_sh, y_train_sh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.61      0.63      2845\n",
      "           1       0.64      0.69      0.66      2883\n",
      "\n",
      "    accuracy                           0.65      5728\n",
      "   macro avg       0.65      0.65      0.65      5728\n",
      "weighted avg       0.65      0.65      0.65      5728\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tmp = clf.predict(X_test_sh)\n",
    "print(metrics.classification_report(list(y_test_sh), list(tmp)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>v_avg_watch_label_1</th>\n",
       "      <th>v_sum_watch_times_1</th>\n",
       "      <th>v_sum_watch_overs_1</th>\n",
       "      <th>v_sum_comment_times_1</th>\n",
       "      <th>v_sum_collect_times_1</th>\n",
       "      <th>v_sum_share_times_1</th>\n",
       "      <th>v_sum_quit_times_1</th>\n",
       "      <th>v_sum_skip_times_1</th>\n",
       "      <th>v_sum_watch_days_1</th>\n",
       "      <th>v_avg_watch_label_3</th>\n",
       "      <th>...</th>\n",
       "      <th>class_6</th>\n",
       "      <th>class_7</th>\n",
       "      <th>class_8</th>\n",
       "      <th>class_9</th>\n",
       "      <th>da_0</th>\n",
       "      <th>da_1</th>\n",
       "      <th>da_2</th>\n",
       "      <th>da_3</th>\n",
       "      <th>da_4</th>\n",
       "      <th>watch_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>17211</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.958333</td>\n",
       "      <td>...</td>\n",
       "      <td>0.041922</td>\n",
       "      <td>0.041925</td>\n",
       "      <td>0.041925</td>\n",
       "      <td>0.041923</td>\n",
       "      <td>0.084701</td>\n",
       "      <td>0.084811</td>\n",
       "      <td>0.661892</td>\n",
       "      <td>0.084126</td>\n",
       "      <td>0.084470</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12479</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.306212</td>\n",
       "      <td>0.037078</td>\n",
       "      <td>0.037070</td>\n",
       "      <td>0.037070</td>\n",
       "      <td>0.074563</td>\n",
       "      <td>0.075237</td>\n",
       "      <td>0.316654</td>\n",
       "      <td>0.074140</td>\n",
       "      <td>0.459407</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2532</th>\n",
       "      <td>0.208611</td>\n",
       "      <td>1951.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1866.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.213834</td>\n",
       "      <td>...</td>\n",
       "      <td>0.350094</td>\n",
       "      <td>0.041778</td>\n",
       "      <td>0.041778</td>\n",
       "      <td>0.041771</td>\n",
       "      <td>0.324958</td>\n",
       "      <td>0.083541</td>\n",
       "      <td>0.083541</td>\n",
       "      <td>0.083541</td>\n",
       "      <td>0.424418</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28572</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.041922</td>\n",
       "      <td>0.041925</td>\n",
       "      <td>0.041925</td>\n",
       "      <td>0.041923</td>\n",
       "      <td>0.084701</td>\n",
       "      <td>0.084811</td>\n",
       "      <td>0.661892</td>\n",
       "      <td>0.084126</td>\n",
       "      <td>0.084470</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19280</th>\n",
       "      <td>0.509397</td>\n",
       "      <td>3565.0</td>\n",
       "      <td>106.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>3082.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.548684</td>\n",
       "      <td>...</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050003</td>\n",
       "      <td>0.050009</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11302</th>\n",
       "      <td>1.333333</td>\n",
       "      <td>123.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.124863</td>\n",
       "      <td>...</td>\n",
       "      <td>0.270403</td>\n",
       "      <td>0.036901</td>\n",
       "      <td>0.036893</td>\n",
       "      <td>0.271428</td>\n",
       "      <td>0.073786</td>\n",
       "      <td>0.074915</td>\n",
       "      <td>0.078284</td>\n",
       "      <td>0.073786</td>\n",
       "      <td>0.699228</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4584</th>\n",
       "      <td>0.902047</td>\n",
       "      <td>684.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>531.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.040830</td>\n",
       "      <td>...</td>\n",
       "      <td>0.212644</td>\n",
       "      <td>0.031771</td>\n",
       "      <td>0.031764</td>\n",
       "      <td>0.409137</td>\n",
       "      <td>0.065150</td>\n",
       "      <td>0.328750</td>\n",
       "      <td>0.068145</td>\n",
       "      <td>0.064165</td>\n",
       "      <td>0.473790</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26296</th>\n",
       "      <td>0.674253</td>\n",
       "      <td>32513.0</td>\n",
       "      <td>388.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>245.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>22996.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.725334</td>\n",
       "      <td>...</td>\n",
       "      <td>0.037483</td>\n",
       "      <td>0.037487</td>\n",
       "      <td>0.395917</td>\n",
       "      <td>0.037484</td>\n",
       "      <td>0.075851</td>\n",
       "      <td>0.075966</td>\n",
       "      <td>0.388781</td>\n",
       "      <td>0.383735</td>\n",
       "      <td>0.075666</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9816</th>\n",
       "      <td>1.617686</td>\n",
       "      <td>12903.0</td>\n",
       "      <td>1328.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>174.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>8865.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.619248</td>\n",
       "      <td>...</td>\n",
       "      <td>0.248009</td>\n",
       "      <td>0.036880</td>\n",
       "      <td>0.036881</td>\n",
       "      <td>0.270608</td>\n",
       "      <td>0.238563</td>\n",
       "      <td>0.073745</td>\n",
       "      <td>0.073745</td>\n",
       "      <td>0.073745</td>\n",
       "      <td>0.540203</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10186</th>\n",
       "      <td>1.143805</td>\n",
       "      <td>1356.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1044.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.158740</td>\n",
       "      <td>...</td>\n",
       "      <td>0.036693</td>\n",
       "      <td>0.528539</td>\n",
       "      <td>0.036671</td>\n",
       "      <td>0.178063</td>\n",
       "      <td>0.074926</td>\n",
       "      <td>0.074954</td>\n",
       "      <td>0.124523</td>\n",
       "      <td>0.075064</td>\n",
       "      <td>0.650534</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5728 rows × 128 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       v_avg_watch_label_1  v_sum_watch_times_1  v_sum_watch_overs_1  \\\n",
       "17211             0.000000                  4.0                  0.0   \n",
       "12479             0.000000                  0.0                  0.0   \n",
       "2532              0.208611               1951.0                 27.0   \n",
       "28572             0.000000                  0.0                  0.0   \n",
       "19280             0.509397               3565.0                106.0   \n",
       "...                    ...                  ...                  ...   \n",
       "11302             1.333333                123.0                 10.0   \n",
       "4584              0.902047                684.0                 30.0   \n",
       "26296             0.674253              32513.0                388.0   \n",
       "9816              1.617686              12903.0               1328.0   \n",
       "10186             1.143805               1356.0                100.0   \n",
       "\n",
       "       v_sum_comment_times_1  v_sum_collect_times_1  v_sum_share_times_1  \\\n",
       "17211                    0.0                    0.0                  0.0   \n",
       "12479                    0.0                    0.0                  0.0   \n",
       "2532                     2.0                   24.0                  4.0   \n",
       "28572                    0.0                    0.0                  0.0   \n",
       "19280                    2.0                   55.0                 35.0   \n",
       "...                      ...                    ...                  ...   \n",
       "11302                    0.0                    0.0                  1.0   \n",
       "4584                     1.0                    5.0                  1.0   \n",
       "26296                   11.0                  245.0                 55.0   \n",
       "9816                    34.0                  174.0                 46.0   \n",
       "10186                    7.0                   22.0                  1.0   \n",
       "\n",
       "       v_sum_quit_times_1  v_sum_skip_times_1  v_sum_watch_days_1  \\\n",
       "17211                 4.0                 0.0                 1.0   \n",
       "12479                 0.0                 0.0                 0.0   \n",
       "2532               1866.0                 0.0                 1.0   \n",
       "28572                 0.0                 0.0                 0.0   \n",
       "19280              3082.0                 0.0                 1.0   \n",
       "...                   ...                 ...                 ...   \n",
       "11302                88.0                 0.0                 1.0   \n",
       "4584                531.0                 0.0                 1.0   \n",
       "26296             22996.0                 0.0                 1.0   \n",
       "9816               8865.0                 0.0                 1.0   \n",
       "10186              1044.0                 0.0                 1.0   \n",
       "\n",
       "       v_avg_watch_label_3  ...   class_6   class_7   class_8   class_9  \\\n",
       "17211             0.958333  ...  0.041922  0.041925  0.041925  0.041923   \n",
       "12479             0.000000  ...  0.306212  0.037078  0.037070  0.037070   \n",
       "2532              0.213834  ...  0.350094  0.041778  0.041778  0.041771   \n",
       "28572             0.000000  ...  0.041922  0.041925  0.041925  0.041923   \n",
       "19280             0.548684  ...  0.050000  0.050003  0.050009  0.050000   \n",
       "...                    ...  ...       ...       ...       ...       ...   \n",
       "11302             1.124863  ...  0.270403  0.036901  0.036893  0.271428   \n",
       "4584              1.040830  ...  0.212644  0.031771  0.031764  0.409137   \n",
       "26296             0.725334  ...  0.037483  0.037487  0.395917  0.037484   \n",
       "9816              1.619248  ...  0.248009  0.036880  0.036881  0.270608   \n",
       "10186             1.158740  ...  0.036693  0.528539  0.036671  0.178063   \n",
       "\n",
       "           da_0      da_1      da_2      da_3      da_4  watch_label  \n",
       "17211  0.084701  0.084811  0.661892  0.084126  0.084470          2.0  \n",
       "12479  0.074563  0.075237  0.316654  0.074140  0.459407          0.0  \n",
       "2532   0.324958  0.083541  0.083541  0.083541  0.424418          0.0  \n",
       "28572  0.084701  0.084811  0.661892  0.084126  0.084470          0.0  \n",
       "19280  0.600000  0.100000  0.100000  0.100000  0.100000          0.0  \n",
       "...         ...       ...       ...       ...       ...          ...  \n",
       "11302  0.073786  0.074915  0.078284  0.073786  0.699228          9.0  \n",
       "4584   0.065150  0.328750  0.068145  0.064165  0.473790          3.0  \n",
       "26296  0.075851  0.075966  0.388781  0.383735  0.075666          0.0  \n",
       "9816   0.238563  0.073745  0.073745  0.073745  0.540203          0.0  \n",
       "10186  0.074926  0.074954  0.124523  0.075064  0.650534          0.0  \n",
       "\n",
       "[5728 rows x 128 columns]"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "del dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = inference_dataset\n",
    "# test_sh = inference_dataset.copy()\n",
    "test = xgb.DMatrix(test.values, enable_categorical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2822180, 170), 170)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inference_dataset.shape, test.num_col()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "bst_wl = wl_bst_sm  # gridsearch_results[opt_idx]['model']  \n",
    "bst_sh = sh_bst_sm  # gridsearch_results_sh[opt_idx_sh]['model']  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "bst_wl = xgb.Booster()\n",
    "bst_sh = xgb.Booster()\n",
    "bst_wl.load_model('wl_model_v22')\n",
    "bst_sh.load_model('sh_model_v22')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Counter({0.0: 1561540,\n",
       "          9.0: 677697,\n",
       "          1.0: 394149,\n",
       "          2.0: 146269,\n",
       "          3.0: 18642,\n",
       "          5.0: 4122,\n",
       "          6.0: 3580,\n",
       "          8.0: 9097,\n",
       "          4.0: 5699,\n",
       "          7.0: 1385}),\n",
       " Counter({1.0: 1395022, 0.0: 1427158}))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wl = bst_wl.predict(test)\n",
    "# test_sh['watch_label'] = wl\n",
    "# test_sh = xgb.DMatrix(test_sh.values, enable_categorical=True)\n",
    "sh = bst_sh.predict(test)\n",
    "Counter(wl), Counter(Counter(sh))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2822180, 62)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df['watch_label'] = wl.astype(np.uint8)\n",
    "test_df['is_share'] = sh.astype(np.uint8)\n",
    "test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>video_id</th>\n",
       "      <th>watch_label</th>\n",
       "      <th>is_share</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1688013</td>\n",
       "      <td>32645</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4502598</td>\n",
       "      <td>41270</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5585629</td>\n",
       "      <td>16345</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1635520</td>\n",
       "      <td>28149</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4160191</td>\n",
       "      <td>40554</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2822175</th>\n",
       "      <td>5019057</td>\n",
       "      <td>18766</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2822176</th>\n",
       "      <td>5019057</td>\n",
       "      <td>12968</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2822177</th>\n",
       "      <td>4255762</td>\n",
       "      <td>21794</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2822178</th>\n",
       "      <td>171497</td>\n",
       "      <td>21578</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2822179</th>\n",
       "      <td>5642580</td>\n",
       "      <td>28914</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2822180 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         user_id  video_id  watch_label  is_share\n",
       "0        1688013     32645            0         1\n",
       "1        4502598     41270            0         1\n",
       "2        5585629     16345            9         0\n",
       "3        1635520     28149            0         1\n",
       "4        4160191     40554            0         1\n",
       "...          ...       ...          ...       ...\n",
       "2822175  5019057     18766            0         0\n",
       "2822176  5019057     12968            0         1\n",
       "2822177  4255762     21794            9         0\n",
       "2822178   171497     21578            1         0\n",
       "2822179  5642580     28914            0         1\n",
       "\n",
       "[2822180 rows x 4 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission = pd.DataFrame(test_df[['user_id', 'video_id', 'watch_label', 'is_share']])\n",
    "submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new submission saved to ../submission-1630259438.csv\n"
     ]
    }
   ],
   "source": [
    "fn = f'../submission-{int(time())}.csv'\n",
    "submission.to_csv(fn, index=False, sep=\",\")\n",
    "print(f\"new submission saved to {fn}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 保存模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "version = 22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "wl_model_name = f'wl_model_v{version}'\n",
    "sh_model_name = f'sh_model_v{version}'\n",
    "bst_wl.save_model(wl_model_name)\n",
    "bst_sh.save_model(sh_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_log(log_name, info, log_path=\"./\"):\n",
    "    import datetime\n",
    "    with open(os.path.join(log_path, log_name), 'w') as log:\n",
    "        log.write(f\"# {datetime.datetime.now().__str__()}\\n\")\n",
    "        if info.get('comment', False):\n",
    "            log.write(f\"\\n## Comment: \\n\")\n",
    "            log.write(f\"{info['comment']}\\n\")\n",
    "            \n",
    "        log.write(f\"\\n## model name: {info['model_name']}\\n\")\n",
    "        log.write(f\"- model save path : {info['model_save_path']}\\n\")\n",
    "        \n",
    "        try:\n",
    "            log.write(f\"\\n## Data setup\\n\")\n",
    "            log.write(f\"- dataset.shape : {dataset.shape}\\n\")\n",
    "            log.write(f\"- dataset.columns : {dataset.columns}\\n\")\n",
    "            log.write(f\"- is resample : {info['is_resample']}\\n\")\n",
    "            log.write(f\"- Traing_Data.shape (watch_label)  : {X_train.shape}\\n\")\n",
    "            log.write(f\"- Testing_Data.shape (watch_label) : {X_test.shape}\\n\")\n",
    "            log.write(f\"- Traing_Data.shape (is_share)  : {X_train_sh.shape}\\n\")\n",
    "            log.write(f\"- Testing_Data.shape (is_share) : {X_test_sh.shape}\\n\")\n",
    "            if info.get('is_resample', False):\n",
    "                log.write(f\"- Resampled class distribution (watch_label): \\n{Counter(resampled_wl)}\\n\")\n",
    "                log.write(f\"- Resampled class distribution (is_share): \\n{Counter(resampled_sh)}\\n\")\n",
    "        except Exception as exp:\n",
    "            pass\n",
    "            \n",
    "        log.write(f\"\\n## Model Params\\n\")\n",
    "        log.write(f\"- model params (watch_label) : \\n{info['param_wl']}\\n\")\n",
    "        log.write(f\"- model params (is_share) : \\n{info['param_sh']}\\n\")\n",
    "        \n",
    "        log.write(f\"\\n## Model's Performance\\n\")\n",
    "        log.write(f\"- Aucs (watch_label) : {info['aucs']}\\n\")\n",
    "        log.write(f\"- Weighted Aucs (watch_label) : {info['w_auc']}\\n\")\n",
    "        log.write(f\"- Aucs (is_share) : {info['aucs_sh']}\\n\")\n",
    "        \n",
    "        log.write(f\"- Classification Report (watch_label) : \\n\\n{info['report']}\\n\")\n",
    "        log.write(f\"- Classification Report (is_share) : \\n\\n{info['report_sh']}\\n\")\n",
    "        \n",
    "        log.flush()\n",
    "        \n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_wl = param  # all_params[opt_idx]\n",
    "param_sh = param_sh  # all_params[opt_idx_sh]\n",
    "\n",
    "aucs = \"\" #aucs  # gridsearch_results[opt_idx]['aucs']\n",
    "w_auc = \"\" #w_auc  # gridsearch_results[opt_idx]['w_auc']\n",
    "aucs_sh = \"\" #aucs_sh  # gridsearch_results_sh[opt_idx]['aucs']\n",
    "\n",
    "report = \"\" #report  # gridsearch_results[opt_idx]['report']\n",
    "report_sh = \"\" #report_sh  # gridsearch_results_sh[opt_idx]['report']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_name = f\"log_v{version}.md\"\n",
    "info = {'is_resample': True, 'model_name': [wl_model_name, sh_model_name], 'model_save_path': os.getcwd(),\n",
    "        'comment': f\"特征：v1版基础特征+用户和视频的统计量特征，添加了is_happy_day特征，表示这天是否为周末或节假日。\\n数据集划分：watch_label的测试集为.18，is_share的测试集为.18。\\nwatch_label训练250rounds，早停=30，is_share训练250rounds，早停=60。\\n。数据集划分时进行了shuffle和stratified。\\n此次生成的提交是：{fn}。官方测评得分：xxx😐\",\n",
    "        'param_wl': param_wl, 'param_sh': param_sh, 'aucs': aucs, 'w_auc': w_auc, 'aucs_sh': aucs_sh, \n",
    "        'report': report, 'report_sh': report_sh}\n",
    "write_log(log_name, info, log_path=\"./logs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 服务器间同步文件"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 推向Digix服务器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ssh: connect to host 49.123.120.71 port 22: No route to host\n",
      "lost connection\n"
     ]
    }
   ],
   "source": [
    "!scp ./models.ipynb digix@49.123.120.71:/home/digix/digix/Models/models.ipynb "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "digix@49.123.120.71's password: "
     ]
    }
   ],
   "source": [
    "!scp ./ensemble.ipynb digix@49.123.120.71:/home/digix/digix/Models/ensemble_from_gzy.ipynb "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 599,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ssh: connect to host 49.123.120.71 port 22: No route to host\n",
      "lost connection\n"
     ]
    }
   ],
   "source": [
    "!scp ./log_*.md digix@49.123.120.71:/home/digix/digix/Models/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "explore-data.ipynb                            100%  306KB  10.6MB/s   00:00    \n"
     ]
    }
   ],
   "source": [
    "!scp ../explore-data.ipynb digix@49.123.120.71:/home/digix/digix/explore-data.ipynb "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 650,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "video_features.jay                            100% 9035KB  11.1MB/s   00:00    \n"
     ]
    }
   ],
   "source": [
    "!scp ../2021_3_data/traindata/video_features_data/video_features.jay digix@49.123.120.71:/home/digix/digix/dataset/new_video_features.jay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 从Digix服务器拉数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "digix@49.123.120.71's password: \n"
     ]
    }
   ],
   "source": [
    "!scp  digix@49.123.120.71:/home/digix/digix/Models/LightGBM.ipynb ./LightGBM.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scp: /home/digix/digix/Models/feature_engineering.ipynb: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!scp  digix@49.123.120.71:/home/digix/digix/Models/feature_engineering.ipynb ./feature_engineering.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "utils.py                                      100% 3860     2.6MB/s   00:00    \n",
      "data_analysis.ipynb                           100% 6566KB  11.2MB/s   00:00    \n",
      "__init__.py                                   100%    0     0.0KB/s   00:00    \n",
      "__init__.cpython-36.pyc                       100%  139   128.3KB/s   00:00    \n",
      "utils.cpython-36.pyc                          100% 4120     2.6MB/s   00:00    \n",
      "video_data.ipynb                              100%   55KB   1.7MB/s   00:00    \n",
      "user_data-checkpoint.ipynb                    100%  202KB  10.1MB/s   00:00    \n",
      "data_analysis-checkpoint.ipynb                100% 6554KB  11.0MB/s   00:00    \n",
      "utils-checkpoint.py                           100% 3860     2.4MB/s   00:00    \n",
      "video_data-checkpoint.ipynb                   100%   17KB   1.4MB/s   00:00    \n",
      "user_data.ipynb                               100%  202KB  10.3MB/s   00:00    \n"
     ]
    }
   ],
   "source": [
    "!scp -r digix@49.123.120.71:/home/digix/digix/Models/Feature_Engineering/  ./"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "video_status.csv                              100% 2008KB   9.1MB/s   00:00    \n",
      "user_status.csv                               100%  138MB   9.3MB/s   00:14    \n"
     ]
    }
   ],
   "source": [
    "!scp -r digix@49.123.120.71:/home/digix/digix/dataset/traindata/video_features_data/video_status.csv ../2021_3_data/traindata/video_features_data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user_status.csv                               100%  168MB  11.2MB/s   00:14    \n"
     ]
    }
   ],
   "source": [
    "!scp -r digix@49.123.120.71:/home/digix/digix/dataset/traindata/user_features_data/user_status.csv ../2021_3_data/traindata/user_features_data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "digix@49.123.120.71's password: \n"
     ]
    }
   ],
   "source": [
    "!scp -r digix@49.123.120.71:/home/digix/digix/Models/MMoE/MMoe_DouLoss.ipynb  ./MMoe_DouLoss.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
