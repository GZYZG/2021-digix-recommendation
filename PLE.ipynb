{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "34d8ad7a-2fb6-45cd-995a-737ac677c50c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type='text/css'>\n",
       ".datatable table.frame { margin-bottom: 0; }\n",
       ".datatable table.frame thead { border-bottom: none; }\n",
       ".datatable table.frame tr.coltypes td {  color: #FFFFFF;  line-height: 6px;  padding: 0 0.5em;}\n",
       ".datatable .bool    { background: #DDDD99; }\n",
       ".datatable .object  { background: #565656; }\n",
       ".datatable .int     { background: #5D9E5D; }\n",
       ".datatable .float   { background: #4040CC; }\n",
       ".datatable .str     { background: #CC4040; }\n",
       ".datatable .row_index {  background: var(--jp-border-color3);  border-right: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  font-size: 9px;}\n",
       ".datatable .frame tr.coltypes .row_index {  background: var(--jp-border-color0);}\n",
       ".datatable th:nth-child(2) { padding-left: 12px; }\n",
       ".datatable .hellipsis {  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .vellipsis {  background: var(--jp-layout-color0);  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .na {  color: var(--jp-cell-editor-border-color);  font-size: 80%;}\n",
       ".datatable .footer { font-size: 9px; }\n",
       ".datatable .frame_dimensions {  background: var(--jp-border-color3);  border-top: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  display: inline-block;  opacity: 0.6;  padding: 1px 10px 1px 5px;}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from data_process import *\n",
    "# from MMoE import MMOE\n",
    "from PLE import PLE\n",
    "import logging,sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff37c82b-00dd-4c25-bf02-e7fb7ea3b3fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 4096\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "out_f = './mmoe_biClass_regression_train_log.txt'\n",
    "logging.basicConfig(filename= out_f , level=logging.INFO, filemode='a',\n",
    "                        format='[%(asctime)s.%(msecs)03d] %(message)s', datefmt='%H:%M:%S')\n",
    "logging.getLogger().addHandler(logging.StreamHandler(sys.stdout))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3e0a4ffc-5ef4-4c42-a14a-d6f5c5fbdba6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Mem. usage decreased to 1928.41 Mb (65.5% reduction),time spend:0.45 min\n"
     ]
    }
   ],
   "source": [
    "# 加载数据\n",
    "dataset, watch_label, share_label = load_train_datas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3135c2a0-9388-4a0b-b8f0-30d99fcf00bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将分类label 变为回归label\n",
    "watch_label_ratio = watch_label / 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3c2a74e8-ab54-45e8-9fc5-b85d8c93ae75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 划分数据集，得到训练集和验证集\n",
    "train_data, train_label, validation_data, validation_label = split_dataset(dataset, watch_label_ratio, share_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a015ee64-8b4b-4e86-ad54-93742377ac43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 归一化数据\n",
    "train_data_QT = QuantileTransformer(output_distribution='uniform').fit_transform(train_data)\n",
    "validation_data_QT = QuantileTransformer(output_distribution='uniform').fit_transform(validation_data)\n",
    "\n",
    "# 拼接两个label\n",
    "train_label_tmp = np.column_stack([train_label[0],train_label[1]])\n",
    "validation_label_tmp = np.column_stack([validation_label[0], validation_label[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8d9bb279-8a8b-4a67-91c2-ecc77e02138f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 训练\n",
    "\n",
    "def train_model(model, batch_size=1024, lr=3e-4, n_epochs=50, patience=15):\n",
    "    # 封装数据为dataloader\n",
    "    train_loader = DataLoader(dataset=getTensorDataset(train_data_QT, train_label_tmp), batch_size=BATCH_SIZE)\n",
    "    val_loader = DataLoader(dataset=getTensorDataset(validation_data_QT, validation_label_tmp), batch_size=BATCH_SIZE)    \n",
    "\n",
    "    # Defines loss function and optimizer\n",
    "    loss_wh_fn = nn.MSELoss(reduction='mean')\n",
    "    loss_sh_fn = nn.BCELoss(reduction='mean')\n",
    "    early_stopping = EarlyStopping(patience, verbose=True)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    losses = []\n",
    "    val_losses = []\n",
    "#     watch_auc = []\n",
    "#     share_auc = []\n",
    "#     sum_auc = []\n",
    "\n",
    "    # model init\n",
    "    def init_weights(m):\n",
    "        if type(m) in [nn.Linear]:\n",
    "            nn.init.normal_(m.weight, std =0.01)\n",
    "    model.apply(init_weights)\n",
    "    \n",
    "    devices = [0, 1]\n",
    "    model = nn.DataParallel(model, device_ids=devices)\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(n_epochs):\n",
    "        model.train()\n",
    "\n",
    "        # Uses loader to fetch one mini-batch for training\n",
    "        epoch_loss = []\n",
    "        c = 0\n",
    "        print(\"\\nEpoch: {}/{}\".format(epoch, n_epochs)) \n",
    "        for x_batch, y_batch in train_loader:\n",
    "\n",
    "            # NOW, sends the mini-batch data to the device\n",
    "            # so it matches location of the MODEL\n",
    "            x_batch = x_batch.to(devices[0])\n",
    "            y_batch = y_batch.to(devices[0])\n",
    "\n",
    "            # One stpe of training\n",
    "            yhat = model(x_batch.float())\n",
    "\n",
    "            # compute Loss\n",
    "            wh_loss = loss_wh_fn(yhat[0].squeeze(1).float(), y_batch[:, 0].float())\n",
    "            sh_loss = loss_sh_fn(yhat[1].squeeze(1).float(), y_batch[:, 1].float())        \n",
    "            loss = wh_loss + sh_loss\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss.append(loss.item())\n",
    "            c += 1\n",
    "            if c % 100 == 0:     \n",
    "                mesg = f'[Batch: {c}]' + \\\n",
    "                         f'train_loss: {np.mean(loss.item()):.5f} '\n",
    "                logging.info(mesg)\n",
    "        losses.append(np.mean(epoch_loss))\n",
    "\n",
    "        # After finishing training steps for all mini-batches,\n",
    "        # it is time for evaluation!\n",
    "\n",
    "        # We tell PyTorch to NOT use autograd...\n",
    "        with torch.no_grad():\n",
    "            # Uses loader to fetch one mini-batch for validation\n",
    "            epoch_loss = []\n",
    "            epoch_watch_auc = []\n",
    "            epoch_share_auc = []\n",
    "            epoch_sum_auc = []\n",
    "            for x_val, y_val in val_loader:\n",
    "                # Again, sends data to same device as model\n",
    "                x_val = x_val.to(devices[0])\n",
    "                y_val = y_val.to(devices[0])\n",
    "\n",
    "                model.eval()\n",
    "                # Makes predictions\n",
    "                yhat = model(x_val.float()) # len=11, 一组batch的预测值\n",
    "\n",
    "                # Computes validation loss\n",
    "                wh_loss = loss_wh_fn(yhat[0].squeeze(1).float(), y_val[:, 0].float())\n",
    "                sh_loss = loss_sh_fn(yhat[1].squeeze(1).float(), y_val[:, 1].float())        \n",
    "                loss = wh_loss + sh_loss\n",
    "\n",
    "                epoch_loss.append(loss.item())\n",
    "\n",
    "        val_losses.append(np.mean(epoch_loss))\n",
    "\n",
    "        epoch_len = len(str(n_epochs))\n",
    "        mesg = f'[{epoch:>{epoch_len}}/{n_epochs:>{epoch_len}}] ' + \\\n",
    "                         f'train_loss: {losses[-1]:.5f} ' + \\\n",
    "                         f'valid_loss: {val_losses[-1]:.5f}'\n",
    "        logging.info(mesg)\n",
    "\n",
    "        # ************* Early Stopping ****************\n",
    "        # early_stopping needs the validation loss to check if it has decresed, \n",
    "        # and if it has, it will make a checkpoint of the current model\n",
    "        early_stopping(val_losses[-1], model)\n",
    "        if early_stopping.early_stop:\n",
    "            print(\"Early stopping\")\n",
    "            break\n",
    "\n",
    "    # print(model.state_dict())\n",
    "    print(\"loss: \", np.mean(losses))\n",
    "    print(\"val_loss: \", np.mean(val_losses))\n",
    "    \n",
    "    return losses, val_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "78d528de-46cc-402c-ac3a-225c3a3c7bd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 0/50\n",
      "[Batch: 100]train_loss: 0.83090 \n",
      "[Batch: 200]train_loss: 0.58753 \n",
      "[Batch: 300]train_loss: 0.18084 \n",
      "[Batch: 400]train_loss: 0.11744 \n",
      "[Batch: 500]train_loss: 0.11698 \n",
      "[Batch: 600]train_loss: 0.09923 \n",
      "[Batch: 700]train_loss: 0.09767 \n",
      "[ 0/50] train_loss: 0.33282 valid_loss: 0.08677\n",
      "Validation loss decreased (inf --> 0.086772).  Saving model ...\n",
      "\n",
      "Epoch: 1/50\n",
      "[Batch: 100]train_loss: 0.10019 \n",
      "[Batch: 200]train_loss: 0.09588 \n",
      "[Batch: 300]train_loss: 0.08051 \n",
      "[Batch: 400]train_loss: 0.09642 \n",
      "[Batch: 500]train_loss: 0.11371 \n",
      "[Batch: 600]train_loss: 0.09087 \n",
      "[Batch: 700]train_loss: 0.09182 \n",
      "[ 1/50] train_loss: 0.09691 valid_loss: 0.08397\n",
      "Validation loss decreased (0.086772 --> 0.083970).  Saving model ...\n",
      "\n",
      "Epoch: 2/50\n",
      "[Batch: 100]train_loss: 0.09668 \n",
      "[Batch: 200]train_loss: 0.09403 \n",
      "[Batch: 300]train_loss: 0.07620 \n",
      "[Batch: 400]train_loss: 0.09656 \n",
      "[Batch: 500]train_loss: 0.10712 \n",
      "[Batch: 600]train_loss: 0.08925 \n",
      "[Batch: 700]train_loss: 0.08494 \n",
      "[ 2/50] train_loss: 0.09330 valid_loss: 0.08025\n",
      "Validation loss decreased (0.083970 --> 0.080247).  Saving model ...\n",
      "\n",
      "Epoch: 3/50\n",
      "[Batch: 100]train_loss: 0.09178 \n",
      "[Batch: 200]train_loss: 0.08873 \n",
      "[Batch: 300]train_loss: 0.07417 \n",
      "[Batch: 400]train_loss: 0.09022 \n",
      "[Batch: 500]train_loss: 0.10160 \n",
      "[Batch: 600]train_loss: 0.08667 \n",
      "[Batch: 700]train_loss: 0.08092 \n",
      "[ 3/50] train_loss: 0.08918 valid_loss: 0.07933\n",
      "Validation loss decreased (0.080247 --> 0.079328).  Saving model ...\n",
      "\n",
      "Epoch: 4/50\n",
      "[Batch: 100]train_loss: 0.09026 \n",
      "[Batch: 200]train_loss: 0.08952 \n",
      "[Batch: 300]train_loss: 0.07460 \n",
      "[Batch: 400]train_loss: 0.08725 \n",
      "[Batch: 500]train_loss: 0.10018 \n",
      "[Batch: 600]train_loss: 0.08511 \n",
      "[Batch: 700]train_loss: 0.08192 \n",
      "[ 4/50] train_loss: 0.08802 valid_loss: 0.07878\n",
      "Validation loss decreased (0.079328 --> 0.078782).  Saving model ...\n",
      "\n",
      "Epoch: 5/50\n",
      "[Batch: 100]train_loss: 0.09211 \n",
      "[Batch: 200]train_loss: 0.08745 \n",
      "[Batch: 300]train_loss: 0.07307 \n",
      "[Batch: 400]train_loss: 0.08764 \n",
      "[Batch: 500]train_loss: 0.10092 \n",
      "[Batch: 600]train_loss: 0.08437 \n",
      "[Batch: 700]train_loss: 0.08171 \n",
      "[ 5/50] train_loss: 0.08732 valid_loss: 0.07827\n",
      "Validation loss decreased (0.078782 --> 0.078270).  Saving model ...\n",
      "\n",
      "Epoch: 6/50\n",
      "[Batch: 100]train_loss: 0.08992 \n",
      "[Batch: 200]train_loss: 0.08709 \n",
      "[Batch: 300]train_loss: 0.07508 \n",
      "[Batch: 400]train_loss: 0.08986 \n",
      "[Batch: 500]train_loss: 0.09679 \n",
      "[Batch: 600]train_loss: 0.08427 \n",
      "[Batch: 700]train_loss: 0.07863 \n",
      "[ 6/50] train_loss: 0.08673 valid_loss: 0.07787\n",
      "Validation loss decreased (0.078270 --> 0.077871).  Saving model ...\n",
      "\n",
      "Epoch: 7/50\n",
      "[Batch: 100]train_loss: 0.08892 \n",
      "[Batch: 200]train_loss: 0.08403 \n",
      "[Batch: 300]train_loss: 0.07154 \n",
      "[Batch: 400]train_loss: 0.08623 \n",
      "[Batch: 500]train_loss: 0.09860 \n",
      "[Batch: 600]train_loss: 0.08258 \n",
      "[Batch: 700]train_loss: 0.07835 \n",
      "[ 7/50] train_loss: 0.08590 valid_loss: 0.07751\n",
      "Validation loss decreased (0.077871 --> 0.077515).  Saving model ...\n",
      "\n",
      "Epoch: 8/50\n",
      "[Batch: 100]train_loss: 0.08596 \n",
      "[Batch: 200]train_loss: 0.08495 \n",
      "[Batch: 300]train_loss: 0.07069 \n",
      "[Batch: 400]train_loss: 0.08521 \n",
      "[Batch: 500]train_loss: 0.09608 \n",
      "[Batch: 600]train_loss: 0.08224 \n",
      "[Batch: 700]train_loss: 0.07833 \n",
      "[ 8/50] train_loss: 0.08471 valid_loss: 0.07687\n",
      "Validation loss decreased (0.077515 --> 0.076866).  Saving model ...\n",
      "\n",
      "Epoch: 9/50\n",
      "[Batch: 100]train_loss: 0.08965 \n",
      "[Batch: 200]train_loss: 0.08312 \n",
      "[Batch: 300]train_loss: 0.07191 \n",
      "[Batch: 400]train_loss: 0.08342 \n",
      "[Batch: 500]train_loss: 0.09433 \n",
      "[Batch: 600]train_loss: 0.08031 \n",
      "[Batch: 700]train_loss: 0.07796 \n",
      "[ 9/50] train_loss: 0.08385 valid_loss: 0.07654\n",
      "Validation loss decreased (0.076866 --> 0.076543).  Saving model ...\n",
      "\n",
      "Epoch: 10/50\n",
      "[Batch: 100]train_loss: 0.08717 \n",
      "[Batch: 200]train_loss: 0.08273 \n",
      "[Batch: 300]train_loss: 0.07117 \n",
      "[Batch: 400]train_loss: 0.08222 \n",
      "[Batch: 500]train_loss: 0.09681 \n",
      "[Batch: 600]train_loss: 0.08091 \n",
      "[Batch: 700]train_loss: 0.07657 \n",
      "[10/50] train_loss: 0.08295 valid_loss: 0.07641\n",
      "Validation loss decreased (0.076543 --> 0.076414).  Saving model ...\n",
      "\n",
      "Epoch: 11/50\n",
      "[Batch: 100]train_loss: 0.08727 \n",
      "[Batch: 200]train_loss: 0.08318 \n",
      "[Batch: 300]train_loss: 0.06880 \n",
      "[Batch: 400]train_loss: 0.08253 \n",
      "[Batch: 500]train_loss: 0.09370 \n",
      "[Batch: 600]train_loss: 0.07987 \n",
      "[Batch: 700]train_loss: 0.07600 \n",
      "[11/50] train_loss: 0.08227 valid_loss: 0.07606\n",
      "Validation loss decreased (0.076414 --> 0.076065).  Saving model ...\n",
      "\n",
      "Epoch: 12/50\n",
      "[Batch: 100]train_loss: 0.08649 \n",
      "[Batch: 200]train_loss: 0.07997 \n",
      "[Batch: 300]train_loss: 0.06810 \n",
      "[Batch: 400]train_loss: 0.08256 \n",
      "[Batch: 500]train_loss: 0.09486 \n",
      "[Batch: 600]train_loss: 0.07895 \n",
      "[Batch: 700]train_loss: 0.07461 \n",
      "[12/50] train_loss: 0.08150 valid_loss: 0.07580\n",
      "Validation loss decreased (0.076065 --> 0.075800).  Saving model ...\n",
      "\n",
      "Epoch: 13/50\n",
      "[Batch: 100]train_loss: 0.08626 \n",
      "[Batch: 200]train_loss: 0.08058 \n",
      "[Batch: 300]train_loss: 0.06789 \n",
      "[Batch: 400]train_loss: 0.08212 \n",
      "[Batch: 500]train_loss: 0.09358 \n",
      "[Batch: 600]train_loss: 0.07881 \n",
      "[Batch: 700]train_loss: 0.07413 \n",
      "[13/50] train_loss: 0.08081 valid_loss: 0.07553\n",
      "Validation loss decreased (0.075800 --> 0.075534).  Saving model ...\n",
      "\n",
      "Epoch: 14/50\n",
      "[Batch: 100]train_loss: 0.08626 \n",
      "[Batch: 200]train_loss: 0.07903 \n",
      "[Batch: 300]train_loss: 0.06804 \n",
      "[Batch: 400]train_loss: 0.08026 \n",
      "[Batch: 500]train_loss: 0.09295 \n",
      "[Batch: 600]train_loss: 0.07824 \n",
      "[Batch: 700]train_loss: 0.07556 \n",
      "[14/50] train_loss: 0.08013 valid_loss: 0.07535\n",
      "Validation loss decreased (0.075534 --> 0.075349).  Saving model ...\n",
      "\n",
      "Epoch: 15/50\n",
      "[Batch: 100]train_loss: 0.08370 \n",
      "[Batch: 200]train_loss: 0.07665 \n",
      "[Batch: 300]train_loss: 0.07043 \n",
      "[Batch: 400]train_loss: 0.07975 \n",
      "[Batch: 500]train_loss: 0.09074 \n",
      "[Batch: 600]train_loss: 0.07715 \n",
      "[Batch: 700]train_loss: 0.07201 \n",
      "[15/50] train_loss: 0.07958 valid_loss: 0.07509\n",
      "Validation loss decreased (0.075349 --> 0.075095).  Saving model ...\n",
      "\n",
      "Epoch: 16/50\n",
      "[Batch: 100]train_loss: 0.08436 \n",
      "[Batch: 200]train_loss: 0.07577 \n",
      "[Batch: 300]train_loss: 0.06818 \n",
      "[Batch: 400]train_loss: 0.07999 \n",
      "[Batch: 500]train_loss: 0.09040 \n",
      "[Batch: 600]train_loss: 0.07726 \n",
      "[Batch: 700]train_loss: 0.07283 \n",
      "[16/50] train_loss: 0.07901 valid_loss: 0.07497\n",
      "Validation loss decreased (0.075095 --> 0.074968).  Saving model ...\n",
      "\n",
      "Epoch: 17/50\n",
      "[Batch: 100]train_loss: 0.08349 \n",
      "[Batch: 200]train_loss: 0.07813 \n",
      "[Batch: 300]train_loss: 0.06673 \n",
      "[Batch: 400]train_loss: 0.07872 \n",
      "[Batch: 500]train_loss: 0.09195 \n",
      "[Batch: 600]train_loss: 0.07476 \n",
      "[Batch: 700]train_loss: 0.07330 \n",
      "[17/50] train_loss: 0.07854 valid_loss: 0.07476\n",
      "Validation loss decreased (0.074968 --> 0.074762).  Saving model ...\n",
      "\n",
      "Epoch: 18/50\n",
      "[Batch: 100]train_loss: 0.08170 \n",
      "[Batch: 200]train_loss: 0.07796 \n",
      "[Batch: 300]train_loss: 0.06754 \n",
      "[Batch: 400]train_loss: 0.07731 \n",
      "[Batch: 500]train_loss: 0.08907 \n",
      "[Batch: 600]train_loss: 0.07582 \n",
      "[Batch: 700]train_loss: 0.07161 \n",
      "[18/50] train_loss: 0.07810 valid_loss: 0.07459\n",
      "Validation loss decreased (0.074762 --> 0.074593).  Saving model ...\n",
      "\n",
      "Epoch: 19/50\n",
      "[Batch: 100]train_loss: 0.08327 \n",
      "[Batch: 200]train_loss: 0.07697 \n",
      "[Batch: 300]train_loss: 0.06667 \n",
      "[Batch: 400]train_loss: 0.07726 \n",
      "[Batch: 500]train_loss: 0.08899 \n",
      "[Batch: 600]train_loss: 0.07410 \n",
      "[Batch: 700]train_loss: 0.07120 \n",
      "[19/50] train_loss: 0.07772 valid_loss: 0.07450\n",
      "Validation loss decreased (0.074593 --> 0.074497).  Saving model ...\n",
      "\n",
      "Epoch: 20/50\n",
      "[Batch: 100]train_loss: 0.08011 \n",
      "[Batch: 200]train_loss: 0.07483 \n",
      "[Batch: 300]train_loss: 0.06706 \n",
      "[Batch: 400]train_loss: 0.07714 \n",
      "[Batch: 500]train_loss: 0.09068 \n",
      "[Batch: 600]train_loss: 0.07664 \n",
      "[Batch: 700]train_loss: 0.07158 \n",
      "[20/50] train_loss: 0.07745 valid_loss: 0.07439\n",
      "Validation loss decreased (0.074497 --> 0.074390).  Saving model ...\n",
      "\n",
      "Epoch: 21/50\n",
      "[Batch: 100]train_loss: 0.08252 \n",
      "[Batch: 200]train_loss: 0.07631 \n",
      "[Batch: 300]train_loss: 0.06662 \n",
      "[Batch: 400]train_loss: 0.07567 \n",
      "[Batch: 500]train_loss: 0.08911 \n",
      "[Batch: 600]train_loss: 0.07462 \n",
      "[Batch: 700]train_loss: 0.07214 \n",
      "[21/50] train_loss: 0.07711 valid_loss: 0.07435\n",
      "Validation loss decreased (0.074390 --> 0.074349).  Saving model ...\n",
      "\n",
      "Epoch: 22/50\n",
      "[Batch: 100]train_loss: 0.08044 \n",
      "[Batch: 200]train_loss: 0.07563 \n",
      "[Batch: 300]train_loss: 0.06601 \n",
      "[Batch: 400]train_loss: 0.07671 \n",
      "[Batch: 500]train_loss: 0.08780 \n",
      "[Batch: 600]train_loss: 0.07515 \n",
      "[Batch: 700]train_loss: 0.07165 \n",
      "[22/50] train_loss: 0.07689 valid_loss: 0.07423\n",
      "Validation loss decreased (0.074349 --> 0.074233).  Saving model ...\n",
      "\n",
      "Epoch: 23/50\n",
      "[Batch: 100]train_loss: 0.08047 \n",
      "[Batch: 200]train_loss: 0.07545 \n",
      "[Batch: 300]train_loss: 0.06547 \n",
      "[Batch: 400]train_loss: 0.07618 \n",
      "[Batch: 500]train_loss: 0.08754 \n",
      "[Batch: 600]train_loss: 0.07436 \n",
      "[Batch: 700]train_loss: 0.07094 \n",
      "[23/50] train_loss: 0.07667 valid_loss: 0.07417\n",
      "Validation loss decreased (0.074233 --> 0.074169).  Saving model ...\n",
      "\n",
      "Epoch: 24/50\n",
      "[Batch: 100]train_loss: 0.08114 \n",
      "[Batch: 200]train_loss: 0.07438 \n",
      "[Batch: 300]train_loss: 0.06440 \n",
      "[Batch: 400]train_loss: 0.07750 \n",
      "[Batch: 500]train_loss: 0.08908 \n",
      "[Batch: 600]train_loss: 0.07377 \n",
      "[Batch: 700]train_loss: 0.07084 \n",
      "[24/50] train_loss: 0.07646 valid_loss: 0.07409\n",
      "Validation loss decreased (0.074169 --> 0.074092).  Saving model ...\n",
      "\n",
      "Epoch: 25/50\n",
      "[Batch: 100]train_loss: 0.08093 \n",
      "[Batch: 200]train_loss: 0.07463 \n",
      "[Batch: 300]train_loss: 0.06528 \n",
      "[Batch: 400]train_loss: 0.07701 \n",
      "[Batch: 500]train_loss: 0.08668 \n",
      "[Batch: 600]train_loss: 0.07443 \n",
      "[Batch: 700]train_loss: 0.07122 \n",
      "[25/50] train_loss: 0.07625 valid_loss: 0.07407\n",
      "Validation loss decreased (0.074092 --> 0.074070).  Saving model ...\n",
      "\n",
      "Epoch: 26/50\n",
      "[Batch: 100]train_loss: 0.08127 \n",
      "[Batch: 200]train_loss: 0.07517 \n",
      "[Batch: 300]train_loss: 0.06449 \n",
      "[Batch: 400]train_loss: 0.07496 \n",
      "[Batch: 500]train_loss: 0.08773 \n",
      "[Batch: 600]train_loss: 0.07297 \n",
      "[Batch: 700]train_loss: 0.07118 \n",
      "[26/50] train_loss: 0.07611 valid_loss: 0.07402\n",
      "Validation loss decreased (0.074070 --> 0.074018).  Saving model ...\n",
      "\n",
      "Epoch: 27/50\n",
      "[Batch: 100]train_loss: 0.07974 \n",
      "[Batch: 200]train_loss: 0.07534 \n",
      "[Batch: 300]train_loss: 0.06503 \n",
      "[Batch: 400]train_loss: 0.07644 \n",
      "[Batch: 500]train_loss: 0.08719 \n",
      "[Batch: 600]train_loss: 0.07479 \n",
      "[Batch: 700]train_loss: 0.07149 \n",
      "[27/50] train_loss: 0.07595 valid_loss: 0.07397\n",
      "Validation loss decreased (0.074018 --> 0.073974).  Saving model ...\n",
      "\n",
      "Epoch: 28/50\n",
      "[Batch: 100]train_loss: 0.08105 \n",
      "[Batch: 200]train_loss: 0.07367 \n",
      "[Batch: 300]train_loss: 0.06424 \n",
      "[Batch: 400]train_loss: 0.07571 \n",
      "[Batch: 500]train_loss: 0.08670 \n",
      "[Batch: 600]train_loss: 0.07342 \n",
      "[Batch: 700]train_loss: 0.07101 \n",
      "[28/50] train_loss: 0.07578 valid_loss: 0.07394\n",
      "Validation loss decreased (0.073974 --> 0.073945).  Saving model ...\n",
      "\n",
      "Epoch: 29/50\n",
      "[Batch: 100]train_loss: 0.07961 \n",
      "[Batch: 200]train_loss: 0.07393 \n",
      "[Batch: 300]train_loss: 0.06458 \n",
      "[Batch: 400]train_loss: 0.07500 \n",
      "[Batch: 500]train_loss: 0.08770 \n",
      "[Batch: 600]train_loss: 0.07413 \n",
      "[Batch: 700]train_loss: 0.06981 \n",
      "[29/50] train_loss: 0.07562 valid_loss: 0.07389\n",
      "Validation loss decreased (0.073945 --> 0.073889).  Saving model ...\n",
      "\n",
      "Epoch: 30/50\n",
      "[Batch: 100]train_loss: 0.08031 \n",
      "[Batch: 200]train_loss: 0.07407 \n",
      "[Batch: 300]train_loss: 0.06481 \n",
      "[Batch: 400]train_loss: 0.07436 \n",
      "[Batch: 500]train_loss: 0.08774 \n",
      "[Batch: 600]train_loss: 0.07286 \n",
      "[Batch: 700]train_loss: 0.07058 \n",
      "[30/50] train_loss: 0.07551 valid_loss: 0.07386\n",
      "Validation loss decreased (0.073889 --> 0.073857).  Saving model ...\n",
      "\n",
      "Epoch: 31/50\n",
      "[Batch: 100]train_loss: 0.07733 \n",
      "[Batch: 200]train_loss: 0.07386 \n",
      "[Batch: 300]train_loss: 0.06424 \n",
      "[Batch: 400]train_loss: 0.07550 \n",
      "[Batch: 500]train_loss: 0.08611 \n",
      "[Batch: 600]train_loss: 0.07203 \n",
      "[Batch: 700]train_loss: 0.07053 \n",
      "[31/50] train_loss: 0.07539 valid_loss: 0.07386\n",
      "Validation loss decreased (0.073857 --> 0.073856).  Saving model ...\n",
      "\n",
      "Epoch: 32/50\n",
      "[Batch: 100]train_loss: 0.07932 \n",
      "[Batch: 200]train_loss: 0.07292 \n",
      "[Batch: 300]train_loss: 0.06381 \n",
      "[Batch: 400]train_loss: 0.07526 \n",
      "[Batch: 500]train_loss: 0.08582 \n",
      "[Batch: 600]train_loss: 0.07263 \n",
      "[Batch: 700]train_loss: 0.07040 \n",
      "[32/50] train_loss: 0.07521 valid_loss: 0.07380\n",
      "Validation loss decreased (0.073856 --> 0.073802).  Saving model ...\n",
      "\n",
      "Epoch: 33/50\n",
      "[Batch: 100]train_loss: 0.07872 \n",
      "[Batch: 200]train_loss: 0.07320 \n",
      "[Batch: 300]train_loss: 0.06454 \n",
      "[Batch: 400]train_loss: 0.07475 \n",
      "[Batch: 500]train_loss: 0.08511 \n",
      "[Batch: 600]train_loss: 0.07272 \n",
      "[Batch: 700]train_loss: 0.07021 \n",
      "[33/50] train_loss: 0.07513 valid_loss: 0.07376\n",
      "Validation loss decreased (0.073802 --> 0.073763).  Saving model ...\n",
      "\n",
      "Epoch: 34/50\n",
      "[Batch: 100]train_loss: 0.07848 \n",
      "[Batch: 200]train_loss: 0.07317 \n",
      "[Batch: 300]train_loss: 0.06398 \n",
      "[Batch: 400]train_loss: 0.07495 \n",
      "[Batch: 500]train_loss: 0.08719 \n",
      "[Batch: 600]train_loss: 0.07342 \n",
      "[Batch: 700]train_loss: 0.06962 \n",
      "[34/50] train_loss: 0.07503 valid_loss: 0.07372\n",
      "Validation loss decreased (0.073763 --> 0.073715).  Saving model ...\n",
      "\n",
      "Epoch: 35/50\n",
      "[Batch: 100]train_loss: 0.07894 \n",
      "[Batch: 200]train_loss: 0.07335 \n",
      "[Batch: 300]train_loss: 0.06394 \n",
      "[Batch: 400]train_loss: 0.07542 \n",
      "[Batch: 500]train_loss: 0.08553 \n",
      "[Batch: 600]train_loss: 0.07269 \n",
      "[Batch: 700]train_loss: 0.06968 \n",
      "[35/50] train_loss: 0.07490 valid_loss: 0.07370\n",
      "Validation loss decreased (0.073715 --> 0.073703).  Saving model ...\n",
      "\n",
      "Epoch: 36/50\n",
      "[Batch: 100]train_loss: 0.07856 \n",
      "[Batch: 200]train_loss: 0.07334 \n",
      "[Batch: 300]train_loss: 0.06364 \n",
      "[Batch: 400]train_loss: 0.07504 \n",
      "[Batch: 500]train_loss: 0.08542 \n",
      "[Batch: 600]train_loss: 0.07325 \n",
      "[Batch: 700]train_loss: 0.06995 \n",
      "[36/50] train_loss: 0.07482 valid_loss: 0.07370\n",
      "Validation loss decreased (0.073703 --> 0.073698).  Saving model ...\n",
      "\n",
      "Epoch: 37/50\n",
      "[Batch: 100]train_loss: 0.07831 \n",
      "[Batch: 200]train_loss: 0.07226 \n",
      "[Batch: 300]train_loss: 0.06328 \n",
      "[Batch: 400]train_loss: 0.07430 \n",
      "[Batch: 500]train_loss: 0.08601 \n",
      "[Batch: 600]train_loss: 0.07259 \n",
      "[Batch: 700]train_loss: 0.06923 \n",
      "[37/50] train_loss: 0.07468 valid_loss: 0.07363\n",
      "Validation loss decreased (0.073698 --> 0.073633).  Saving model ...\n",
      "\n",
      "Epoch: 38/50\n",
      "[Batch: 100]train_loss: 0.07885 \n",
      "[Batch: 200]train_loss: 0.07271 \n",
      "[Batch: 300]train_loss: 0.06383 \n",
      "[Batch: 400]train_loss: 0.07484 \n",
      "[Batch: 500]train_loss: 0.08499 \n",
      "[Batch: 600]train_loss: 0.07240 \n",
      "[Batch: 700]train_loss: 0.06971 \n",
      "[38/50] train_loss: 0.07460 valid_loss: 0.07362\n",
      "Validation loss decreased (0.073633 --> 0.073624).  Saving model ...\n",
      "\n",
      "Epoch: 39/50\n",
      "[Batch: 100]train_loss: 0.07766 \n",
      "[Batch: 200]train_loss: 0.07284 \n",
      "[Batch: 300]train_loss: 0.06332 \n",
      "[Batch: 400]train_loss: 0.07537 \n",
      "[Batch: 500]train_loss: 0.08597 \n",
      "[Batch: 600]train_loss: 0.07254 \n",
      "[Batch: 700]train_loss: 0.06906 \n",
      "[39/50] train_loss: 0.07454 valid_loss: 0.07361\n",
      "Validation loss decreased (0.073624 --> 0.073607).  Saving model ...\n",
      "\n",
      "Epoch: 40/50\n",
      "[Batch: 100]train_loss: 0.07805 \n",
      "[Batch: 200]train_loss: 0.07312 \n",
      "[Batch: 300]train_loss: 0.06336 \n",
      "[Batch: 400]train_loss: 0.07440 \n",
      "[Batch: 500]train_loss: 0.08554 \n",
      "[Batch: 600]train_loss: 0.07267 \n",
      "[Batch: 700]train_loss: 0.06937 \n",
      "[40/50] train_loss: 0.07445 valid_loss: 0.07359\n",
      "Validation loss decreased (0.073607 --> 0.073594).  Saving model ...\n",
      "\n",
      "Epoch: 41/50\n",
      "[Batch: 100]train_loss: 0.07808 \n",
      "[Batch: 200]train_loss: 0.07231 \n",
      "[Batch: 300]train_loss: 0.06317 \n",
      "[Batch: 400]train_loss: 0.07395 \n",
      "[Batch: 500]train_loss: 0.08520 \n",
      "[Batch: 600]train_loss: 0.07303 \n",
      "[Batch: 700]train_loss: 0.06914 \n",
      "[41/50] train_loss: 0.07437 valid_loss: 0.07356\n",
      "Validation loss decreased (0.073594 --> 0.073563).  Saving model ...\n",
      "\n",
      "Epoch: 42/50\n",
      "[Batch: 100]train_loss: 0.07822 \n",
      "[Batch: 200]train_loss: 0.07206 \n",
      "[Batch: 300]train_loss: 0.06313 \n",
      "[Batch: 400]train_loss: 0.07371 \n",
      "[Batch: 500]train_loss: 0.08550 \n",
      "[Batch: 600]train_loss: 0.07193 \n",
      "[Batch: 700]train_loss: 0.06964 \n",
      "[42/50] train_loss: 0.07428 valid_loss: 0.07349\n",
      "Validation loss decreased (0.073563 --> 0.073490).  Saving model ...\n",
      "\n",
      "Epoch: 43/50\n",
      "[Batch: 100]train_loss: 0.07824 \n",
      "[Batch: 200]train_loss: 0.07172 \n",
      "[Batch: 300]train_loss: 0.06320 \n",
      "[Batch: 400]train_loss: 0.07442 \n",
      "[Batch: 500]train_loss: 0.08590 \n",
      "[Batch: 600]train_loss: 0.07198 \n",
      "[Batch: 700]train_loss: 0.06992 \n",
      "[43/50] train_loss: 0.07415 valid_loss: 0.07343\n",
      "Validation loss decreased (0.073490 --> 0.073434).  Saving model ...\n",
      "\n",
      "Epoch: 44/50\n",
      "[Batch: 100]train_loss: 0.07755 \n",
      "[Batch: 200]train_loss: 0.07187 \n",
      "[Batch: 300]train_loss: 0.06352 \n",
      "[Batch: 400]train_loss: 0.07389 \n",
      "[Batch: 500]train_loss: 0.08513 \n",
      "[Batch: 600]train_loss: 0.07193 \n",
      "[Batch: 700]train_loss: 0.06936 \n",
      "[44/50] train_loss: 0.07409 valid_loss: 0.07343\n",
      "EarlyStopping counter: 1 out of 10\n",
      "\n",
      "Epoch: 45/50\n",
      "[Batch: 100]train_loss: 0.07675 \n",
      "[Batch: 200]train_loss: 0.07133 \n",
      "[Batch: 300]train_loss: 0.06319 \n",
      "[Batch: 400]train_loss: 0.07408 \n",
      "[Batch: 500]train_loss: 0.08524 \n",
      "[Batch: 600]train_loss: 0.07142 \n",
      "[Batch: 700]train_loss: 0.06915 \n",
      "[45/50] train_loss: 0.07401 valid_loss: 0.07341\n",
      "Validation loss decreased (0.073434 --> 0.073405).  Saving model ...\n",
      "\n",
      "Epoch: 46/50\n",
      "[Batch: 100]train_loss: 0.07758 \n",
      "[Batch: 200]train_loss: 0.07243 \n",
      "[Batch: 300]train_loss: 0.06316 \n",
      "[Batch: 400]train_loss: 0.07386 \n",
      "[Batch: 500]train_loss: 0.08455 \n",
      "[Batch: 600]train_loss: 0.07212 \n",
      "[Batch: 700]train_loss: 0.06890 \n",
      "[46/50] train_loss: 0.07394 valid_loss: 0.07338\n",
      "Validation loss decreased (0.073405 --> 0.073384).  Saving model ...\n",
      "\n",
      "Epoch: 47/50\n",
      "[Batch: 100]train_loss: 0.07707 \n",
      "[Batch: 200]train_loss: 0.07235 \n",
      "[Batch: 300]train_loss: 0.06259 \n",
      "[Batch: 400]train_loss: 0.07403 \n",
      "[Batch: 500]train_loss: 0.08408 \n",
      "[Batch: 600]train_loss: 0.07160 \n",
      "[Batch: 700]train_loss: 0.06898 \n",
      "[47/50] train_loss: 0.07385 valid_loss: 0.07337\n",
      "Validation loss decreased (0.073384 --> 0.073366).  Saving model ...\n",
      "\n",
      "Epoch: 48/50\n",
      "[Batch: 100]train_loss: 0.07750 \n",
      "[Batch: 200]train_loss: 0.07179 \n",
      "[Batch: 300]train_loss: 0.06294 \n",
      "[Batch: 400]train_loss: 0.07392 \n",
      "[Batch: 500]train_loss: 0.08478 \n",
      "[Batch: 600]train_loss: 0.07230 \n",
      "[Batch: 700]train_loss: 0.06938 \n",
      "[48/50] train_loss: 0.07382 valid_loss: 0.07335\n",
      "Validation loss decreased (0.073366 --> 0.073353).  Saving model ...\n",
      "\n",
      "Epoch: 49/50\n",
      "[Batch: 100]train_loss: 0.07670 \n",
      "[Batch: 200]train_loss: 0.07194 \n",
      "[Batch: 300]train_loss: 0.06302 \n",
      "[Batch: 400]train_loss: 0.07370 \n",
      "[Batch: 500]train_loss: 0.08487 \n",
      "[Batch: 600]train_loss: 0.07200 \n",
      "[Batch: 700]train_loss: 0.06855 \n",
      "[49/50] train_loss: 0.07376 valid_loss: 0.07333\n",
      "Validation loss decreased (0.073353 --> 0.073330).  Saving model ...\n",
      "loss:  0.08368315909033443\n",
      "val_loss:  0.07523964826486729\n"
     ]
    }
   ],
   "source": [
    "# 定义模型\n",
    "model = PLE(input_size=128, num_specific_experts=8, num_shared_experts=4, experts_out=16, experts_hidden=16, towers_hidden=12)\n",
    "model = model.to(device)\n",
    "\n",
    "# Sets hyper-parameters\n",
    "lr = 1e-4 * 1.5\n",
    "n_epochs = 50\n",
    "patience = 10\n",
    "BATCH_SIZE=4096 * 2\n",
    "losses, val_losses = train_model(model, batch_size=BATCH_SIZE, lr=lr, patience=patience)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c4a8d235-cc47-457f-bb5a-3a3dcedf23eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "epochs = range(1, len(losses)+1)\n",
    "plt.plot(epochs, losses, 'g', label='Training loss')\n",
    "plt.plot(epochs, val_losses, 'b', label='validation loss')\n",
    "plt.title('Training and Validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "28f4c6e0-5d34-48d6-a333-2ba2ebe53597",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Mem. usage decreased to 820.89 Mb (65.7% reduction),time spend:0.21 min\n"
     ]
    }
   ],
   "source": [
    "test_data = load_test_datas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6238329a-a940-4fd4-872a-49942d1d6fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将测试数据组织为 DaTaloader\n",
    "test_loader = DataLoader(dataset=torch.utils.data.TensorDataset(torch.tensor(test_data.astype(float).to_numpy())), batch_size=4096*2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f2aea087-54f8-449d-b334-d0cdfa55441f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict():\n",
    "    # 预测\n",
    "    watch_pred = []\n",
    "    share_pred = []\n",
    "    with torch.no_grad():\n",
    "        # Uses loader to fetch one mini-batch for testing\n",
    "        for x_test in test_loader:\n",
    "            # Again, sends data to same device as model\n",
    "            x_test = x_test[0]\n",
    "            x_test = x_test.to(device)\n",
    "\n",
    "            model.eval()\n",
    "            # Makes predictions\n",
    "            yhat = model(x_test.float())\n",
    "#             print(yhat)\n",
    "# #             print(yhat[0].shape)\n",
    "#             break\n",
    "#             print(yhat)\n",
    "            yhat_watch = yhat[0]\n",
    "#             print(yhat_watch.shape)\n",
    "            yhat_share = yhat[1]\n",
    "#             print(yhat_share.shape)\n",
    "            \n",
    "            \n",
    "            # save\n",
    "            watch_pred.append(yhat_watch)\n",
    "            share_pred.append(yhat_share)\n",
    "    return watch_pred, share_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f7cc4d72-57b9-4737-aa49-e523bf7b72f6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "wh_pred, sh_pred = predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7bd44fb6-c625-4795-9b71-b7caa91b3f4e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "wh_preds = torch.cat(wh_pred, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2665a29c-9e20-4b78-9e52-87c27d1c14eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2822180, 1])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wh_preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "479a3e6d-d08f-4ee8-9a6a-e3b39fba32d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = np.rint(wh_preds.squeeze(1).cpu().numpy() * 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e6f03530-b241-4396-88a3-b1e6da600f73",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({0.0: 2704515,\n",
       "         10.0: 115348,\n",
       "         3.0: 1039,\n",
       "         7.0: 105,\n",
       "         2.0: 479,\n",
       "         1.0: 80,\n",
       "         6.0: 104,\n",
       "         4.0: 162,\n",
       "         8.0: 107,\n",
       "         5.0: 131,\n",
       "         9.0: 110})"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e2e90f8c-dffd-4622-94d5-b311f641f353",
   "metadata": {},
   "outputs": [],
   "source": [
    "sh_preds = torch.cat(sh_pred, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "25a1860e-966b-4959-b6a5-e4170562f436",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2822180, 1])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sh_preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "76ed830c-c6a4-4f9b-97bd-1af7df0d900e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sh_preds = sh_preds.cpu().squeeze(1).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "dd12f4cb-c4b3-430f-854b-3ce6c883e402",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({0.0: 2822180})"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(np.rint(sh_preds))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
