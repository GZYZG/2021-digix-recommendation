{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "34d8ad7a-2fb6-45cd-995a-737ac677c50c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type='text/css'>\n",
       ".datatable table.frame { margin-bottom: 0; }\n",
       ".datatable table.frame thead { border-bottom: none; }\n",
       ".datatable table.frame tr.coltypes td {  color: #FFFFFF;  line-height: 6px;  padding: 0 0.5em;}\n",
       ".datatable .bool    { background: #DDDD99; }\n",
       ".datatable .object  { background: #565656; }\n",
       ".datatable .int     { background: #5D9E5D; }\n",
       ".datatable .float   { background: #4040CC; }\n",
       ".datatable .str     { background: #CC4040; }\n",
       ".datatable .row_index {  background: var(--jp-border-color3);  border-right: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  font-size: 9px;}\n",
       ".datatable .frame tr.coltypes .row_index {  background: var(--jp-border-color0);}\n",
       ".datatable th:nth-child(2) { padding-left: 12px; }\n",
       ".datatable .hellipsis {  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .vellipsis {  background: var(--jp-layout-color0);  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .na {  color: var(--jp-cell-editor-border-color);  font-size: 80%;}\n",
       ".datatable .footer { font-size: 9px; }\n",
       ".datatable .frame_dimensions {  background: var(--jp-border-color3);  border-top: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  display: inline-block;  opacity: 0.6;  padding: 1px 10px 1px 5px;}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from data_process import *\n",
    "from PLE2 import PLE2\n",
    "import logging,sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff37c82b-00dd-4c25-bf02-e7fb7ea3b3fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 4096\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "out_f = './train.txt'\n",
    "logging.basicConfig(filename= out_f , level=logging.INFO, filemode='a',\n",
    "                        format='[%(asctime)s.%(msecs)03d] %(message)s', datefmt='%H:%M:%S')\n",
    "logging.getLogger().addHandler(logging.StreamHandler(sys.stdout))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3e0a4ffc-5ef4-4c42-a14a-d6f5c5fbdba6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Mem. usage decreased to 1928.41 Mb (65.5% reduction),time spend:0.45 min\n"
     ]
    }
   ],
   "source": [
    "# 加载数据\n",
    "dataset, watch_label, share_label = load_train_datas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3135c2a0-9388-4a0b-b8f0-30d99fcf00bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将分类label 变为回归label\n",
    "watch_label_ratio = watch_label / 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3c2a74e8-ab54-45e8-9fc5-b85d8c93ae75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 划分数据集，得到训练集和验证集\n",
    "train_data, train_label, validation_data, validation_label = split_dataset(dataset, watch_label_ratio, share_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a015ee64-8b4b-4e86-ad54-93742377ac43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 归一化数据\n",
    "train_data_QT = QuantileTransformer(output_distribution='uniform').fit_transform(train_data)\n",
    "validation_data_QT = QuantileTransformer(output_distribution='uniform').fit_transform(validation_data)\n",
    "\n",
    "# 拼接两个label\n",
    "train_label_tmp = np.column_stack([train_label[0],train_label[1]])\n",
    "validation_label_tmp = np.column_stack([validation_label[0], validation_label[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "8d9bb279-8a8b-4a67-91c2-ecc77e02138f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 训练\n",
    "def train_model(model, batch_size, lr=3e-4, n_epochs=50, patience=15):\n",
    "    # 封装数据为dataloader\n",
    "    train_loader = DataLoader(dataset=getTensorDataset(train_data_QT, train_label_tmp), batch_size=batch_size)\n",
    "    val_loader = DataLoader(dataset=getTensorDataset(validation_data_QT, validation_label_tmp), batch_size=batch_size)    \n",
    "\n",
    "    # Defines loss function and optimizer\n",
    "    loss_wh_fn = nn.MSELoss(reduction='mean')\n",
    "    loss_sh_fn = nn.BCELoss(reduction='mean')\n",
    "    early_stopping = EarlyStopping(patience, verbose=True)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    losses = []\n",
    "    val_losses = []\n",
    "#     watch_auc = []\n",
    "#     share_auc = []\n",
    "#     sum_auc = []\n",
    "\n",
    "    # model init\n",
    "    def init_weights(m):\n",
    "        if type(m) in [nn.Linear]:\n",
    "            nn.init.normal_(m.weight, std =0.01)\n",
    "    model.apply(init_weights)\n",
    "    \n",
    "    devices = [0, 1]\n",
    "    model = nn.DataParallel(model, device_ids=devices)\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(n_epochs):\n",
    "        model.train()\n",
    "\n",
    "        # Uses loader to fetch one mini-batch for training\n",
    "        epoch_loss = []\n",
    "        c = 0\n",
    "        print(\"\\nEpoch: {}/{}\".format(epoch, n_epochs)) \n",
    "        for x_batch, y_batch in train_loader:\n",
    "\n",
    "            # NOW, sends the mini-batch data to the device\n",
    "            # so it matches location of the MODEL\n",
    "            x_batch = x_batch.to(devices[0])\n",
    "            y_batch = y_batch.to(devices[0])\n",
    "            \n",
    "#             print(y_batch)\n",
    "#             print(y_batch.shape)\n",
    "            \n",
    "            # One stpe of training\n",
    "            yhat = model(x_batch.float())\n",
    "#             print(len(yhat))\n",
    "#             print(yhat[0].shape)\n",
    "#             break\n",
    "            \n",
    "\n",
    "            # compute Loss\n",
    "            wh_loss = loss_wh_fn(yhat[0].squeeze(1).float(), y_batch[:, 0].float())\n",
    "            sh_loss = loss_sh_fn(yhat[1].squeeze(1).float(), y_batch[:, 1].float())        \n",
    "            loss = wh_loss + sh_loss\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss.append(loss.item())\n",
    "            c += 1\n",
    "            if c % 10 == 0:     \n",
    "                mesg = f'[Batch: {c}]' + \\\n",
    "                         f'train_loss: {np.mean(loss.item()):.5f} '\n",
    "                logging.info(mesg)\n",
    "        losses.append(np.mean(epoch_loss))\n",
    "\n",
    "        # After finishing training steps for all mini-batches,\n",
    "        # it is time for evaluation!\n",
    "\n",
    "        # We tell PyTorch to NOT use autograd...\n",
    "        with torch.no_grad():\n",
    "            # Uses loader to fetch one mini-batch for validation\n",
    "            epoch_loss = []\n",
    "            epoch_watch_auc = []\n",
    "            epoch_share_auc = []\n",
    "            epoch_sum_auc = []\n",
    "            for x_val, y_val in val_loader:\n",
    "                # Again, sends data to same device as model\n",
    "                x_val = x_val.to(devices[0])\n",
    "                y_val = y_val.to(devices[0])\n",
    "\n",
    "                model.eval()\n",
    "                # Makes predictions\n",
    "                yhat = model(x_val.float()) # len=11, 一组batch的预测值\n",
    "\n",
    "                # Computes validation loss\n",
    "                wh_loss = loss_wh_fn(yhat[0].squeeze(1).float(), y_val[:, 0].float())\n",
    "                sh_loss = loss_sh_fn(yhat[1].squeeze(1).float(), y_val[:, 1].float())        \n",
    "                loss = wh_loss + sh_loss\n",
    "\n",
    "                epoch_loss.append(loss.item())\n",
    "\n",
    "        val_losses.append(np.mean(epoch_loss))\n",
    "\n",
    "        epoch_len = len(str(n_epochs))\n",
    "        mesg = f'[{epoch:>{epoch_len}}/{n_epochs:>{epoch_len}}] ' + \\\n",
    "                         f'train_loss: {losses[-1]:.5f} ' + \\\n",
    "                         f'valid_loss: {val_losses[-1]:.5f}'\n",
    "        logging.info(mesg)\n",
    "\n",
    "        # ************* Early Stopping ****************\n",
    "        # early_stopping needs the validation loss to check if it has decresed, \n",
    "        # and if it has, it will make a checkpoint of the current model\n",
    "        early_stopping(val_losses[-1], model)\n",
    "        if early_stopping.early_stop:\n",
    "            print(\"Early stopping\")\n",
    "            break\n",
    "\n",
    "    # print(model.state_dict())\n",
    "    print(\"loss: \", np.mean(losses))\n",
    "    print(\"val_loss: \", np.mean(val_losses))\n",
    "    \n",
    "    return losses, val_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "259ead92-d80b-4226-9e8b-e2dcf7a747ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义模型\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "model = PLE2(num_feature=128, num_experts=16, num_tasks=2, units=256, towers_hidden=32, selectors=2)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "78d528de-46cc-402c-ac3a-225c3a3c7bd2",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 0/50\n",
      "[Batch: 10]train_loss: 0.91773 \n",
      "[Batch: 20]train_loss: 0.91363 \n",
      "[Batch: 30]train_loss: 0.90477 \n",
      "[Batch: 40]train_loss: 0.88065 \n",
      "[Batch: 50]train_loss: 0.82393 \n",
      "[Batch: 60]train_loss: 0.70198 \n",
      "[Batch: 70]train_loss: 0.47363 \n",
      "[Batch: 80]train_loss: 0.26671 \n",
      "[Batch: 90]train_loss: 0.25635 \n",
      "[Batch: 100]train_loss: 0.16114 \n",
      "[Batch: 110]train_loss: 0.08882 \n",
      "[Batch: 120]train_loss: 0.08961 \n",
      "[Batch: 130]train_loss: 0.08208 \n",
      "[Batch: 140]train_loss: 0.09473 \n",
      "[Batch: 150]train_loss: 0.09187 \n",
      "[Batch: 160]train_loss: 0.08589 \n",
      "[Batch: 170]train_loss: 0.08370 \n",
      "[Batch: 180]train_loss: 0.08452 \n",
      "[Batch: 190]train_loss: 0.08529 \n",
      "[Batch: 200]train_loss: 0.08417 \n",
      "[Batch: 210]train_loss: 0.08784 \n",
      "[Batch: 220]train_loss: 0.08804 \n",
      "[Batch: 230]train_loss: 0.08738 \n",
      "[Batch: 240]train_loss: 0.08243 \n",
      "[Batch: 250]train_loss: 0.08496 \n",
      "[Batch: 260]train_loss: 0.07753 \n",
      "[Batch: 270]train_loss: 0.08648 \n",
      "[Batch: 280]train_loss: 0.08017 \n",
      "[Batch: 290]train_loss: 0.09270 \n",
      "[Batch: 300]train_loss: 0.08926 \n",
      "[Batch: 310]train_loss: 0.08678 \n",
      "[ 0/50] train_loss: 0.27306 valid_loss: 0.08174\n",
      "Validation loss decreased (inf --> 0.081745).  Saving model ...\n",
      "\n",
      "Epoch: 1/50\n",
      "[Batch: 10]train_loss: 0.08082 \n",
      "[Batch: 20]train_loss: 0.09115 \n",
      "[Batch: 30]train_loss: 0.08544 \n",
      "[Batch: 40]train_loss: 0.08544 \n",
      "[Batch: 50]train_loss: 0.08552 \n",
      "[Batch: 60]train_loss: 0.08706 \n",
      "[Batch: 70]train_loss: 0.08309 \n",
      "[Batch: 80]train_loss: 0.08761 \n",
      "[Batch: 90]train_loss: 0.09658 \n",
      "[Batch: 100]train_loss: 0.09193 \n",
      "[Batch: 110]train_loss: 0.08258 \n",
      "[Batch: 120]train_loss: 0.08636 \n",
      "[Batch: 130]train_loss: 0.07682 \n",
      "[Batch: 140]train_loss: 0.09208 \n",
      "[Batch: 150]train_loss: 0.09082 \n",
      "[Batch: 160]train_loss: 0.08485 \n",
      "[Batch: 170]train_loss: 0.08314 \n",
      "[Batch: 180]train_loss: 0.08435 \n",
      "[Batch: 190]train_loss: 0.08508 \n",
      "[Batch: 200]train_loss: 0.08454 \n",
      "[Batch: 210]train_loss: 0.08857 \n",
      "[Batch: 220]train_loss: 0.08720 \n",
      "[Batch: 230]train_loss: 0.08690 \n",
      "[Batch: 240]train_loss: 0.08161 \n",
      "[Batch: 250]train_loss: 0.08311 \n",
      "[Batch: 260]train_loss: 0.07800 \n",
      "[Batch: 270]train_loss: 0.08626 \n",
      "[Batch: 280]train_loss: 0.08016 \n",
      "[Batch: 290]train_loss: 0.09245 \n",
      "[Batch: 300]train_loss: 0.08637 \n",
      "[Batch: 310]train_loss: 0.08731 \n",
      "[ 1/50] train_loss: 0.08600 valid_loss: 0.08167\n",
      "Validation loss decreased (0.081745 --> 0.081671).  Saving model ...\n",
      "\n",
      "Epoch: 2/50\n",
      "[Batch: 10]train_loss: 0.07964 \n",
      "[Batch: 20]train_loss: 0.09024 \n",
      "[Batch: 30]train_loss: 0.08480 \n",
      "[Batch: 40]train_loss: 0.08487 \n",
      "[Batch: 50]train_loss: 0.08517 \n",
      "[Batch: 60]train_loss: 0.08666 \n",
      "[Batch: 70]train_loss: 0.08288 \n",
      "[Batch: 80]train_loss: 0.08788 \n",
      "[Batch: 90]train_loss: 0.09372 \n",
      "[Batch: 100]train_loss: 0.09020 \n",
      "[Batch: 110]train_loss: 0.08103 \n",
      "[Batch: 120]train_loss: 0.08377 \n",
      "[Batch: 130]train_loss: 0.07588 \n",
      "[Batch: 140]train_loss: 0.09056 \n",
      "[Batch: 150]train_loss: 0.09037 \n",
      "[Batch: 160]train_loss: 0.08511 \n",
      "[Batch: 170]train_loss: 0.08363 \n",
      "[Batch: 180]train_loss: 0.08248 \n",
      "[Batch: 190]train_loss: 0.08417 \n",
      "[Batch: 200]train_loss: 0.08384 \n",
      "[Batch: 210]train_loss: 0.08701 \n",
      "[Batch: 220]train_loss: 0.08730 \n",
      "[Batch: 230]train_loss: 0.08702 \n",
      "[Batch: 240]train_loss: 0.08056 \n",
      "[Batch: 250]train_loss: 0.08217 \n",
      "[Batch: 260]train_loss: 0.07753 \n",
      "[Batch: 270]train_loss: 0.08651 \n",
      "[Batch: 280]train_loss: 0.08157 \n",
      "[Batch: 290]train_loss: 0.09179 \n",
      "[Batch: 300]train_loss: 0.08636 \n",
      "[Batch: 310]train_loss: 0.08619 \n",
      "[ 2/50] train_loss: 0.08554 valid_loss: 0.08162\n",
      "Validation loss decreased (0.081671 --> 0.081617).  Saving model ...\n",
      "\n",
      "Epoch: 3/50\n",
      "[Batch: 10]train_loss: 0.08076 \n",
      "[Batch: 20]train_loss: 0.08926 \n",
      "[Batch: 30]train_loss: 0.08553 \n",
      "[Batch: 40]train_loss: 0.08442 \n",
      "[Batch: 50]train_loss: 0.08503 \n",
      "[Batch: 60]train_loss: 0.08642 \n",
      "[Batch: 70]train_loss: 0.08310 \n",
      "[Batch: 80]train_loss: 0.08790 \n",
      "[Batch: 90]train_loss: 0.09223 \n",
      "[Batch: 100]train_loss: 0.08989 \n",
      "[Batch: 110]train_loss: 0.07984 \n",
      "[Batch: 120]train_loss: 0.08297 \n",
      "[Batch: 130]train_loss: 0.07602 \n",
      "[Batch: 140]train_loss: 0.09055 \n",
      "[Batch: 150]train_loss: 0.09056 \n",
      "[Batch: 160]train_loss: 0.08461 \n",
      "[Batch: 170]train_loss: 0.08246 \n",
      "[Batch: 180]train_loss: 0.08322 \n",
      "[Batch: 190]train_loss: 0.08548 \n",
      "[Batch: 200]train_loss: 0.08400 \n",
      "[Batch: 210]train_loss: 0.08775 \n",
      "[Batch: 220]train_loss: 0.08754 \n",
      "[Batch: 230]train_loss: 0.08745 \n",
      "[Batch: 240]train_loss: 0.08074 \n",
      "[Batch: 250]train_loss: 0.08265 \n",
      "[Batch: 260]train_loss: 0.07808 \n",
      "[Batch: 270]train_loss: 0.08495 \n",
      "[Batch: 280]train_loss: 0.08080 \n",
      "[Batch: 290]train_loss: 0.09150 \n",
      "[Batch: 300]train_loss: 0.08626 \n",
      "[Batch: 310]train_loss: 0.08578 \n",
      "[ 3/50] train_loss: 0.08525 valid_loss: 0.08158\n",
      "Validation loss decreased (0.081617 --> 0.081579).  Saving model ...\n",
      "\n",
      "Epoch: 4/50\n",
      "[Batch: 10]train_loss: 0.07966 \n",
      "[Batch: 20]train_loss: 0.08925 \n",
      "[Batch: 30]train_loss: 0.08574 \n",
      "[Batch: 40]train_loss: 0.08589 \n",
      "[Batch: 50]train_loss: 0.08540 \n",
      "[Batch: 60]train_loss: 0.08533 \n",
      "[Batch: 70]train_loss: 0.08351 \n",
      "[Batch: 80]train_loss: 0.08789 \n",
      "[Batch: 90]train_loss: 0.09021 \n",
      "[Batch: 100]train_loss: 0.08943 \n",
      "[Batch: 110]train_loss: 0.07916 \n",
      "[Batch: 120]train_loss: 0.08330 \n",
      "[Batch: 130]train_loss: 0.07746 \n",
      "[Batch: 140]train_loss: 0.08964 \n",
      "[Batch: 150]train_loss: 0.08923 \n",
      "[Batch: 160]train_loss: 0.08434 \n",
      "[Batch: 170]train_loss: 0.08200 \n",
      "[Batch: 180]train_loss: 0.08416 \n",
      "[Batch: 190]train_loss: 0.08539 \n",
      "[Batch: 200]train_loss: 0.08441 \n",
      "[Batch: 210]train_loss: 0.08805 \n",
      "[Batch: 220]train_loss: 0.08697 \n",
      "[Batch: 230]train_loss: 0.08638 \n",
      "[Batch: 240]train_loss: 0.08051 \n",
      "[Batch: 250]train_loss: 0.08228 \n",
      "[Batch: 260]train_loss: 0.07740 \n",
      "[Batch: 270]train_loss: 0.08551 \n",
      "[Batch: 280]train_loss: 0.08011 \n",
      "[Batch: 290]train_loss: 0.09210 \n",
      "[Batch: 300]train_loss: 0.08740 \n",
      "[Batch: 310]train_loss: 0.08626 \n",
      "[ 4/50] train_loss: 0.08511 valid_loss: 0.08154\n",
      "Validation loss decreased (0.081579 --> 0.081543).  Saving model ...\n",
      "\n",
      "Epoch: 5/50\n",
      "[Batch: 10]train_loss: 0.08108 \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-72-290074b14906>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mpatience\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10000\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mlosses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_losses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatience\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpatience\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-70-52a9c71f5ecb>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, batch_size, lr, n_epochs, patience)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0;31m# One stpe of training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m             \u001b[0myhat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0;31m#             print(len(yhat))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;31m#             print(yhat[0].shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/anaconda/envs/digix/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/anaconda/envs/digix/lib/python3.6/site-packages/torch/nn/parallel/data_parallel.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    150\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 152\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mchain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuffers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    153\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc_device_obj\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m                     raise RuntimeError(\"module must have its parameters and buffers \"\n",
      "\u001b[0;32m/home/anaconda/envs/digix/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mbuffers\u001b[0;34m(self, recurse)\u001b[0m\n\u001b[1;32m   1490\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1491\u001b[0m         \"\"\"\n\u001b[0;32m-> 1492\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnamed_buffers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecurse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrecurse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1493\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mbuf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1494\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/anaconda/envs/digix/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mnamed_buffers\u001b[0;34m(self, prefix, recurse)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mlambda\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_buffers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m             prefix=prefix, recurse=recurse)\n\u001b[0;32m-> 1518\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0melem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgen\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0melem\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/anaconda/envs/digix/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_named_members\u001b[0;34m(self, get_members_fn, prefix, recurse)\u001b[0m\n\u001b[1;32m   1412\u001b[0m         \u001b[0mmemo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1413\u001b[0m         \u001b[0mmodules\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnamed_modules\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprefix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprefix\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprefix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1414\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule_prefix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1415\u001b[0m             \u001b[0mmembers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_members_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1416\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmembers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/anaconda/envs/digix/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mnamed_modules\u001b[0;34m(self, memo, prefix, remove_duplicate)\u001b[0m\n\u001b[1;32m   1617\u001b[0m                     \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1618\u001b[0m                 \u001b[0msubmodule_prefix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprefix\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'.'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mprefix\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1619\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0mm\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnamed_modules\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmemo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubmodule_prefix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mremove_duplicate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1620\u001b[0m                     \u001b[0;32myield\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/anaconda/envs/digix/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mnamed_modules\u001b[0;34m(self, memo, prefix, remove_duplicate)\u001b[0m\n\u001b[1;32m   1617\u001b[0m                     \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1618\u001b[0m                 \u001b[0msubmodule_prefix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprefix\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'.'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mprefix\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1619\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0mm\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnamed_modules\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmemo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubmodule_prefix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mremove_duplicate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1620\u001b[0m                     \u001b[0;32myield\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/anaconda/envs/digix/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mnamed_modules\u001b[0;34m(self, memo, prefix, remove_duplicate)\u001b[0m\n\u001b[1;32m   1617\u001b[0m                     \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1618\u001b[0m                 \u001b[0msubmodule_prefix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprefix\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'.'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mprefix\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1619\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0mm\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnamed_modules\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmemo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubmodule_prefix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mremove_duplicate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1620\u001b[0m                     \u001b[0;32myield\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/anaconda/envs/digix/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mnamed_modules\u001b[0;34m(self, memo, prefix, remove_duplicate)\u001b[0m\n\u001b[1;32m   1617\u001b[0m                     \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1618\u001b[0m                 \u001b[0msubmodule_prefix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprefix\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'.'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mprefix\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1619\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0mm\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnamed_modules\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmemo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubmodule_prefix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mremove_duplicate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1620\u001b[0m                     \u001b[0;32myield\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Sets hyper-parameters\n",
    "lr = 1e-4 * 1.5\n",
    "n_epochs = 50\n",
    "patience = 10\n",
    "BATCH_SIZE = 4096 * 2\n",
    "losses, val_losses = train_model(model, batch_size=BATCH_SIZE, lr=lr, patience=patience)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c4a8d235-cc47-457f-bb5a-3a3dcedf23eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "epochs = range(1, len(losses)+1)\n",
    "plt.plot(epochs, losses, 'g', label='Training loss')\n",
    "plt.plot(epochs, val_losses, 'b', label='validation loss')\n",
    "plt.title('Training and Validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "28f4c6e0-5d34-48d6-a333-2ba2ebe53597",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Mem. usage decreased to 820.89 Mb (65.7% reduction),time spend:0.20 min\n"
     ]
    }
   ],
   "source": [
    "test_data = load_test_datas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "6238329a-a940-4fd4-872a-49942d1d6fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将测试数据组织为 DaTaloader\n",
    "test_loader = DataLoader(dataset=torch.utils.data.TensorDataset(torch.tensor(test_data.astype(float).to_numpy())), batch_size=4096*2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f2aea087-54f8-449d-b334-d0cdfa55441f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict():\n",
    "    # 预测\n",
    "    watch_pred = []\n",
    "    share_pred = []\n",
    "    with torch.no_grad():\n",
    "        # Uses loader to fetch one mini-batch for testing\n",
    "        for x_test in test_loader:\n",
    "            # Again, sends data to same device as model\n",
    "            x_test = x_test[0]\n",
    "            x_test = x_test.to(device)\n",
    "\n",
    "            model.eval()\n",
    "            # Makes predictions\n",
    "            yhat = model(x_test.float())\n",
    "#             print(yhat)\n",
    "# #             print(yhat[0].shape)\n",
    "#             break\n",
    "#             print(yhat)\n",
    "            yhat_watch = yhat[0]\n",
    "#             print(yhat_watch.shape)\n",
    "            yhat_share = yhat[1]\n",
    "#             print(yhat_share.shape)\n",
    "            \n",
    "            \n",
    "            # save\n",
    "            watch_pred.append(yhat_watch)\n",
    "            share_pred.append(yhat_share)\n",
    "    return watch_pred, share_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f7cc4d72-57b9-4737-aa49-e523bf7b72f6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "wh_pred, sh_pred = predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "7bd44fb6-c625-4795-9b71-b7caa91b3f4e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "wh_preds = torch.cat(wh_pred, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "2665a29c-9e20-4b78-9e52-87c27d1c14eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2822180, 1])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wh_preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "479a3e6d-d08f-4ee8-9a6a-e3b39fba32d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = np.rint(wh_preds.squeeze(1).cpu().numpy() * 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e6f03530-b241-4396-88a3-b1e6da600f73",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({0.0: 2710859, 3.0: 86139, 2.0: 9027, 1.0: 16155})"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e2e90f8c-dffd-4622-94d5-b311f641f353",
   "metadata": {},
   "outputs": [],
   "source": [
    "sh_preds = torch.cat(sh_pred, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "25a1860e-966b-4959-b6a5-e4170562f436",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2822180, 1])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sh_preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "76ed830c-c6a4-4f9b-97bd-1af7df0d900e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sh_preds = sh_preds.cpu().squeeze(1).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "dd12f4cb-c4b3-430f-854b-3ce6c883e402",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({0.0: 2822180})"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(np.rint(sh_preds))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
