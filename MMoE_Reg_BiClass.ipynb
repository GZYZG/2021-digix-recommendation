{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "34d8ad7a-2fb6-45cd-995a-737ac677c50c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type='text/css'>\n",
       ".datatable table.frame { margin-bottom: 0; }\n",
       ".datatable table.frame thead { border-bottom: none; }\n",
       ".datatable table.frame tr.coltypes td {  color: #FFFFFF;  line-height: 6px;  padding: 0 0.5em;}\n",
       ".datatable .bool    { background: #DDDD99; }\n",
       ".datatable .object  { background: #565656; }\n",
       ".datatable .int     { background: #5D9E5D; }\n",
       ".datatable .float   { background: #4040CC; }\n",
       ".datatable .str     { background: #CC4040; }\n",
       ".datatable .row_index {  background: var(--jp-border-color3);  border-right: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  font-size: 9px;}\n",
       ".datatable .frame tr.coltypes .row_index {  background: var(--jp-border-color0);}\n",
       ".datatable th:nth-child(2) { padding-left: 12px; }\n",
       ".datatable .hellipsis {  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .vellipsis {  background: var(--jp-layout-color0);  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .na {  color: var(--jp-cell-editor-border-color);  font-size: 80%;}\n",
       ".datatable .footer { font-size: 9px; }\n",
       ".datatable .frame_dimensions {  background: var(--jp-border-color3);  border-top: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  display: inline-block;  opacity: 0.6;  padding: 1px 10px 1px 5px;}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from data_process import *\n",
    "from MMoE import MMOE\n",
    "import logging,sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ff37c82b-00dd-4c25-bf02-e7fb7ea3b3fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 4096\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "out_f = './mmoe_biClass_regression_train_log.txt'\n",
    "logging.basicConfig(filename= out_f , level=logging.INFO, filemode='a',\n",
    "                        format='[%(asctime)s.%(msecs)03d] %(message)s', datefmt='%H:%M:%S')\n",
    "logging.getLogger().addHandler(logging.StreamHandler(sys.stdout))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3e0a4ffc-5ef4-4c42-a14a-d6f5c5fbdba6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Mem. usage decreased to 1928.41 Mb (65.5% reduction),time spend:0.45 min\n"
     ]
    }
   ],
   "source": [
    "# 加载数据\n",
    "dataset, watch_label, share_label = load_train_datas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3135c2a0-9388-4a0b-b8f0-30d99fcf00bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将分类label 变为回归label\n",
    "watch_label_ratio = watch_label / 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3c2a74e8-ab54-45e8-9fc5-b85d8c93ae75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 划分数据集，得到训练集和验证集\n",
    "train_data, train_label, validation_data, validation_label = split_dataset(dataset, watch_label_ratio, share_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a015ee64-8b4b-4e86-ad54-93742377ac43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 归一化数据\n",
    "train_data_QT = QuantileTransformer(output_distribution='uniform').fit_transform(train_data)\n",
    "validation_data_QT = QuantileTransformer(output_distribution='uniform').fit_transform(validation_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "90254812-648c-4801-af55-44557f47a25f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 拼接两个label，封装DataLoader\n",
    "train_label_tmp = np.column_stack([train_label[0],train_label[1]])\n",
    "train_loader = DataLoader(dataset=getTensorDataset(train_data_QT, train_label_tmp), batch_size=BATCH_SIZE)\n",
    "\n",
    "validation_label_tmp = np.column_stack([validation_label[0], validation_label[1]])\n",
    "val_loader = DataLoader(dataset=getTensorDataset(validation_data_QT, validation_label_tmp), batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "c178ba16-eb52-4315-b667-278315c4d0ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义模型\n",
    "model = MMOE(input_size=128, num_experts=12, experts_out=16, experts_hidden=16, towers_hidden=12, tasks=2)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d9bb279-8a8b-4a67-91c2-ecc77e02138f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 0/10\n",
      "[Batch: 100]train_loss: 0.60099 \n",
      "[Batch: 200]train_loss: 0.47077 \n",
      "[Batch: 300]train_loss: 0.32119 \n",
      "[Batch: 400]train_loss: 0.22096 \n",
      "[Batch: 500]train_loss: 0.20755 \n",
      "[Batch: 600]train_loss: 0.13001 \n",
      "[Batch: 700]train_loss: 0.11468 \n",
      "[Batch: 800]train_loss: 0.09456 \n",
      "[Batch: 900]train_loss: 0.08929 \n",
      "[Batch: 1000]train_loss: 0.10000 \n",
      "[Batch: 1100]train_loss: 0.06370 \n",
      "[Batch: 1200]train_loss: 0.06945 \n",
      "[Batch: 1300]train_loss: 0.07304 \n",
      "[Batch: 1400]train_loss: 0.06256 \n",
      "[Batch: 1500]train_loss: 0.06865 \n",
      "[ 0/10] train_loss: 0.19900 valid_loss: 0.05185\n",
      "Validation loss decreased (inf --> 0.051847).  Saving model ...\n",
      "\n",
      "Epoch: 1/10\n",
      "[Batch: 100]train_loss: 0.07526 \n",
      "[Batch: 200]train_loss: 0.06954 \n",
      "[Batch: 300]train_loss: 0.05283 \n",
      "[Batch: 400]train_loss: 0.05349 \n",
      "[Batch: 500]train_loss: 0.06741 \n",
      "[Batch: 600]train_loss: 0.05406 \n",
      "[Batch: 700]train_loss: 0.07566 \n",
      "[Batch: 800]train_loss: 0.06067 \n",
      "[Batch: 900]train_loss: 0.06648 \n",
      "[Batch: 1000]train_loss: 0.07285 \n",
      "[Batch: 1100]train_loss: 0.05074 \n",
      "[Batch: 1200]train_loss: 0.06259 \n",
      "[Batch: 1300]train_loss: 0.05237 \n",
      "[Batch: 1400]train_loss: 0.05349 \n",
      "[Batch: 1500]train_loss: 0.06429 \n",
      "[ 1/10] train_loss: 0.06362 valid_loss: 0.04967\n",
      "Validation loss decreased (0.051847 --> 0.049666).  Saving model ...\n",
      "\n",
      "Epoch: 2/10\n",
      "[Batch: 100]train_loss: 0.06866 \n",
      "[Batch: 200]train_loss: 0.06749 \n",
      "[Batch: 300]train_loss: 0.05345 \n",
      "[Batch: 400]train_loss: 0.05598 \n",
      "[Batch: 500]train_loss: 0.06519 \n",
      "[Batch: 600]train_loss: 0.05063 \n",
      "[Batch: 700]train_loss: 0.08404 \n",
      "[Batch: 800]train_loss: 0.05775 \n",
      "[Batch: 900]train_loss: 0.06464 \n",
      "[Batch: 1000]train_loss: 0.07504 \n",
      "[Batch: 1100]train_loss: 0.04738 \n",
      "[Batch: 1200]train_loss: 0.05899 \n",
      "[Batch: 1300]train_loss: 0.05006 \n",
      "[Batch: 1400]train_loss: 0.05410 \n",
      "[Batch: 1500]train_loss: 0.05841 \n",
      "[ 2/10] train_loss: 0.06039 valid_loss: 0.04864\n",
      "Validation loss decreased (0.049666 --> 0.048639).  Saving model ...\n",
      "\n",
      "Epoch: 3/10\n",
      "[Batch: 100]train_loss: 0.06914 \n",
      "[Batch: 200]train_loss: 0.06513 \n",
      "[Batch: 300]train_loss: 0.05001 \n",
      "[Batch: 400]train_loss: 0.05133 \n",
      "[Batch: 500]train_loss: 0.06573 \n",
      "[Batch: 600]train_loss: 0.05183 \n",
      "[Batch: 700]train_loss: 0.07469 \n",
      "[Batch: 800]train_loss: 0.05909 \n",
      "[Batch: 900]train_loss: 0.06514 \n",
      "[Batch: 1000]train_loss: 0.07387 \n",
      "[Batch: 1100]train_loss: 0.05005 \n",
      "[Batch: 1200]train_loss: 0.05683 \n",
      "[Batch: 1300]train_loss: 0.05041 \n",
      "[Batch: 1400]train_loss: 0.04770 \n",
      "[Batch: 1500]train_loss: 0.05914 \n",
      "[ 3/10] train_loss: 0.05908 valid_loss: 0.04791\n",
      "Validation loss decreased (0.048639 --> 0.047910).  Saving model ...\n",
      "\n",
      "Epoch: 4/10\n",
      "[Batch: 100]train_loss: 0.06612 \n",
      "[Batch: 200]train_loss: 0.06418 \n",
      "[Batch: 300]train_loss: 0.05136 \n",
      "[Batch: 400]train_loss: 0.05602 \n",
      "[Batch: 500]train_loss: 0.06136 \n",
      "[Batch: 600]train_loss: 0.05162 \n",
      "[Batch: 700]train_loss: 0.07432 \n",
      "[Batch: 800]train_loss: 0.05586 \n",
      "[Batch: 900]train_loss: 0.05733 \n",
      "[Batch: 1000]train_loss: 0.06962 \n",
      "[Batch: 1100]train_loss: 0.04552 \n",
      "[Batch: 1200]train_loss: 0.05752 \n",
      "[Batch: 1300]train_loss: 0.05108 \n",
      "[Batch: 1400]train_loss: 0.04888 \n",
      "[Batch: 1500]train_loss: 0.05599 \n",
      "[ 4/10] train_loss: 0.05784 valid_loss: 0.04724\n",
      "Validation loss decreased (0.047910 --> 0.047242).  Saving model ...\n",
      "\n",
      "Epoch: 5/10\n",
      "[Batch: 100]train_loss: 0.06469 \n",
      "[Batch: 200]train_loss: 0.06610 \n",
      "[Batch: 300]train_loss: 0.04814 \n",
      "[Batch: 400]train_loss: 0.05518 \n",
      "[Batch: 500]train_loss: 0.05646 \n",
      "[Batch: 600]train_loss: 0.05029 \n",
      "[Batch: 700]train_loss: 0.06875 \n",
      "[Batch: 800]train_loss: 0.05686 \n",
      "[Batch: 900]train_loss: 0.05826 \n",
      "[Batch: 1000]train_loss: 0.06879 \n",
      "[Batch: 1100]train_loss: 0.04711 \n",
      "[Batch: 1200]train_loss: 0.05260 \n",
      "[Batch: 1300]train_loss: 0.04726 \n",
      "[Batch: 1400]train_loss: 0.04867 \n",
      "[Batch: 1500]train_loss: 0.06079 \n",
      "[ 5/10] train_loss: 0.05664 valid_loss: 0.04669\n",
      "Validation loss decreased (0.047242 --> 0.046688).  Saving model ...\n",
      "\n",
      "Epoch: 6/10\n",
      "[Batch: 100]train_loss: 0.06097 \n",
      "[Batch: 200]train_loss: 0.05637 \n",
      "[Batch: 300]train_loss: 0.04707 \n",
      "[Batch: 400]train_loss: 0.05075 \n",
      "[Batch: 500]train_loss: 0.05787 \n",
      "[Batch: 600]train_loss: 0.04799 \n",
      "[Batch: 700]train_loss: 0.06821 \n",
      "[Batch: 800]train_loss: 0.05515 \n",
      "[Batch: 900]train_loss: 0.05768 \n",
      "[Batch: 1000]train_loss: 0.06755 \n",
      "[Batch: 1100]train_loss: 0.04611 \n",
      "[Batch: 1200]train_loss: 0.05387 \n",
      "[Batch: 1300]train_loss: 0.04697 \n",
      "[Batch: 1400]train_loss: 0.04837 \n",
      "[Batch: 1500]train_loss: 0.06110 \n",
      "[ 6/10] train_loss: 0.05508 valid_loss: 0.04625\n",
      "Validation loss decreased (0.046688 --> 0.046248).  Saving model ...\n",
      "\n",
      "Epoch: 7/10\n",
      "[Batch: 100]train_loss: 0.06379 \n",
      "[Batch: 200]train_loss: 0.05992 \n",
      "[Batch: 300]train_loss: 0.04833 \n",
      "[Batch: 400]train_loss: 0.05280 \n",
      "[Batch: 500]train_loss: 0.05561 \n",
      "[Batch: 600]train_loss: 0.04860 \n",
      "[Batch: 700]train_loss: 0.06483 \n",
      "[Batch: 800]train_loss: 0.05325 \n",
      "[Batch: 900]train_loss: 0.05749 \n",
      "[Batch: 1000]train_loss: 0.07084 \n",
      "[Batch: 1100]train_loss: 0.04254 \n",
      "[Batch: 1200]train_loss: 0.05509 \n",
      "[Batch: 1300]train_loss: 0.04513 \n",
      "[Batch: 1400]train_loss: 0.04902 \n",
      "[Batch: 1500]train_loss: 0.06117 \n",
      "[ 7/10] train_loss: 0.05387 valid_loss: 0.04594\n",
      "Validation loss decreased (0.046248 --> 0.045939).  Saving model ...\n",
      "\n",
      "Epoch: 8/10\n",
      "[Batch: 100]train_loss: 0.05992 \n",
      "[Batch: 200]train_loss: 0.05693 \n",
      "[Batch: 300]train_loss: 0.04613 \n",
      "[Batch: 400]train_loss: 0.05105 \n",
      "[Batch: 500]train_loss: 0.05261 \n",
      "[Batch: 600]train_loss: 0.05009 \n",
      "[Batch: 700]train_loss: 0.06838 \n",
      "[Batch: 800]train_loss: 0.05037 \n",
      "[Batch: 900]train_loss: 0.06033 \n",
      "[Batch: 1000]train_loss: 0.06550 \n",
      "[Batch: 1100]train_loss: 0.04452 \n",
      "[Batch: 1200]train_loss: 0.05133 \n",
      "[Batch: 1300]train_loss: 0.04328 \n",
      "[Batch: 1400]train_loss: 0.04585 \n",
      "[Batch: 1500]train_loss: 0.05673 \n"
     ]
    }
   ],
   "source": [
    "# 训练\n",
    "\n",
    "# Sets hyper-parameters\n",
    "lr = 1e-4\n",
    "n_epochs = 10\n",
    "tasks = 11\n",
    "patience = 5\n",
    "# BATCH_SIZE=4096\n",
    "\n",
    "# # Defines loss function and optimizer\n",
    "loss_wh_fn = nn.MSELoss(reduction='mean')\n",
    "loss_sh_fn = nn.BCELoss(reduction='mean')\n",
    "early_stopping = EarlyStopping(patience, verbose=True)\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "losses = []\n",
    "val_losses = []\n",
    "\n",
    "watch_auc = []\n",
    "share_auc = []\n",
    "sum_auc = []\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()\n",
    "\n",
    "    # Uses loader to fetch one mini-batch for training\n",
    "    epoch_loss = []\n",
    "    c = 0\n",
    "    print(\"\\nEpoch: {}/{}\".format(epoch, n_epochs)) \n",
    "    for x_batch, y_batch in train_loader:\n",
    "\n",
    "        # NOW, sends the mini-batch data to the device\n",
    "        # so it matches location of the MODEL\n",
    "        x_batch = x_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "\n",
    "        # One stpe of training\n",
    "        yhat = model(x_batch.float())\n",
    "        \n",
    "        wh_loss = loss_wh_fn(yhat[:,0].float(), y_batch[:, 0].view(-1, 1).float())\n",
    "        sh_loss = loss_sh_fn(yhat[:,1].float(), y_batch[:, 1].view(-1, 1).float())        \n",
    "        loss = wh_loss/2 + sh_loss\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss.append(loss.item())\n",
    "        c += 1\n",
    "        if c % 100 == 0:     \n",
    "            mesg = f'[Batch: {c}]' + \\\n",
    "                     f'train_loss: {np.mean(loss.item()):.5f} '\n",
    "            logging.info(mesg)\n",
    "    losses.append(np.mean(epoch_loss))\n",
    "\n",
    "    # After finishing training steps for all mini-batches,\n",
    "    # it is time for evaluation!\n",
    "\n",
    "    # We tell PyTorch to NOT use autograd...\n",
    "    with torch.no_grad():\n",
    "        # Uses loader to fetch one mini-batch for validation\n",
    "        epoch_loss = []\n",
    "        epoch_watch_auc = []\n",
    "        epoch_share_auc = []\n",
    "        epoch_sum_auc = []\n",
    "        for x_val, y_val in val_loader:\n",
    "            # Again, sends data to same device as model\n",
    "            x_val = x_val.to(device)\n",
    "            y_val = y_val.to(device)\n",
    "\n",
    "            model.eval()\n",
    "            # Makes predictions\n",
    "            yhat = model(x_val.float()) # len=11, 一组batch的预测值\n",
    "\n",
    "            # Computes validation loss\n",
    "            wh_loss = loss_wh_fn(yhat[:,0].float(), y_val[:, 0].view(-1, 1).float())\n",
    "            sh_loss = loss_sh_fn(yhat[:,1].float(), y_val[:, 1].view(-1, 1).float())        \n",
    "            loss = wh_loss/2 + sh_loss\n",
    "\n",
    "            epoch_loss.append(loss.item())\n",
    "\n",
    "    val_losses.append(np.mean(epoch_loss))\n",
    "\n",
    "    epoch_len = len(str(n_epochs))\n",
    "    mesg = f'[{epoch:>{epoch_len}}/{n_epochs:>{epoch_len}}] ' + \\\n",
    "                     f'train_loss: {losses[-1]:.5f} ' + \\\n",
    "                     f'valid_loss: {val_losses[-1]:.5f}'\n",
    "    logging.info(mesg)\n",
    "\n",
    "    # ************* Early Stopping ****************\n",
    "    # early_stopping needs the validation loss to check if it has decresed, \n",
    "    # and if it has, it will make a checkpoint of the current model\n",
    "    early_stopping(val_losses[-1], model)\n",
    "\n",
    "    if early_stopping.early_stop:\n",
    "        print(\"Early stopping\")\n",
    "        break\n",
    "\n",
    "# print(model.state_dict())\n",
    "print(\"loss: \", np.mean(losses))\n",
    "print(\"val_loss: \", np.mean(val_losses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a8d235-cc47-457f-bb5a-3a3dcedf23eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = range(1, len(losses)+1)\n",
    "plt.plot(epochs, losses, 'g', label='Training loss')\n",
    "plt.plot(epochs, val_losses, 'b', label='validation loss')\n",
    "plt.title('Training and Validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "321461f1-1c22-45fd-ac25-c66bf67ebaf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load(\"./finish_model.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4c986be1-4c99-421b-ab96-696b5220111c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "28f4c6e0-5d34-48d6-a333-2ba2ebe53597",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Mem. usage decreased to 820.89 Mb (65.7% reduction),time spend:0.20 min\n"
     ]
    }
   ],
   "source": [
    "test_data = load_test_datas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6238329a-a940-4fd4-872a-49942d1d6fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将测试数据组织为 DaTaloader\n",
    "test_loader = DataLoader(dataset=torch.utils.data.TensorDataset(torch.tensor(test_data.astype(float).to_numpy())), batch_size=4096)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f2aea087-54f8-449d-b334-d0cdfa55441f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict():\n",
    "    # 预测\n",
    "    watch_pred = []\n",
    "    share_pred = []\n",
    "    with torch.no_grad():\n",
    "        # Uses loader to fetch one mini-batch for testing\n",
    "        for x_test in test_loader:\n",
    "            # Again, sends data to same device as model\n",
    "            x_test = x_test[0]\n",
    "            x_test = x_test.to(device)\n",
    "\n",
    "            model.eval()\n",
    "            # Makes predictions\n",
    "            yhat = model(x_test.float())\n",
    "#             print(yhat)\n",
    "#             print(yhat.shape)\n",
    "            yhat = yhat.squeeze(2)\n",
    "            yhat = yhat.view(-1, 2)\n",
    "#             print(yhat)\n",
    "            yhat_watch = yhat[:, 0]\n",
    "#             print(yhat_watch.shape)\n",
    "            yhat_share = yhat[:, 1]\n",
    "#             print(yhat_share.shape)\n",
    "            \n",
    "            \n",
    "            # save\n",
    "            watch_pred.append(yhat_watch)\n",
    "            share_pred.append(yhat_share)\n",
    "    return watch_pred, share_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f7cc4d72-57b9-4737-aa49-e523bf7b72f6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "wh_pred, sh_pred = predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7bd44fb6-c625-4795-9b71-b7caa91b3f4e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "wh_preds = torch.cat(wh_pred, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "af2657df-696f-49a9-b6ea-1a6f7d32932e",
   "metadata": {},
   "outputs": [],
   "source": [
    "wh_preds = wh_preds.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f9079785-1b1e-4c23-a10f-bc919e74f758",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = np.rint(wh_preds.numpy() * 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e6f03530-b241-4396-88a3-b1e6da600f73",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({0.0: 2769536,\n",
       "         10.0: 28528,\n",
       "         9.0: 5948,\n",
       "         6.0: 2319,\n",
       "         7.0: 2549,\n",
       "         4.0: 4785,\n",
       "         8.0: 3463,\n",
       "         2.0: 912,\n",
       "         5.0: 2075,\n",
       "         1.0: 1344,\n",
       "         3.0: 721})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e2e90f8c-dffd-4622-94d5-b311f641f353",
   "metadata": {},
   "outputs": [],
   "source": [
    "sh_preds = torch.cat(sh_pred, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "76ed830c-c6a4-4f9b-97bd-1af7df0d900e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sh_preds = np.rint(sh_preds.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dd12f4cb-c4b3-430f-854b-3ce6c883e402",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({0.0: 2764068, 1.0: 58112})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(sh_preds)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
