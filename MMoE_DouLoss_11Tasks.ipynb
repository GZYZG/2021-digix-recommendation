{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "gkNWKaPTpxiB",
    "outputId": "2a91c159-0121-4ca0-866f-618514ce5e60",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type='text/css'>\n",
       ".datatable table.frame { margin-bottom: 0; }\n",
       ".datatable table.frame thead { border-bottom: none; }\n",
       ".datatable table.frame tr.coltypes td {  color: #FFFFFF;  line-height: 6px;  padding: 0 0.5em;}\n",
       ".datatable .bool    { background: #DDDD99; }\n",
       ".datatable .object  { background: #565656; }\n",
       ".datatable .int     { background: #5D9E5D; }\n",
       ".datatable .float   { background: #4040CC; }\n",
       ".datatable .str     { background: #CC4040; }\n",
       ".datatable .row_index {  background: var(--jp-border-color3);  border-right: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  font-size: 9px;}\n",
       ".datatable .frame tr.coltypes .row_index {  background: var(--jp-border-color0);}\n",
       ".datatable th:nth-child(2) { padding-left: 12px; }\n",
       ".datatable .hellipsis {  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .vellipsis {  background: var(--jp-layout-color0);  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .na {  color: var(--jp-cell-editor-border-color);  font-size: 80%;}\n",
       ".datatable .footer { font-size: 9px; }\n",
       ".datatable .frame_dimensions {  background: var(--jp-border-color3);  border-top: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  display: inline-block;  opacity: 0.6;  padding: 1px 10px 1px 5px;}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'cuda:0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import datatable as dt\n",
    "import time\n",
    "from collections import Counter\n",
    "from sklearn import preprocessing\n",
    "import logging\n",
    "import sys\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "SEED = 42\n",
    "BATCH_SIZE = 1024\n",
    "\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_f = './train_log.txt'\n",
    "logging.basicConfig(filename= out_f , level=logging.INFO, filemode='a',\n",
    "                        format='[%(asctime)s.%(msecs)03d] %(message)s', datefmt='%H:%M:%S')\n",
    "logging.getLogger().addHandler(logging.StreamHandler(sys.stdout))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 节约内存的一个标配函数\n",
    "def reduce_mem(df):\n",
    "    starttime = time.time()\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if pd.isnull(c_min) or pd.isnull(c_max):\n",
    "                continue\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)\n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('-- Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction),time spend:{:2.2f} min'.format(end_mem,\n",
    "                                                                                                           100*(start_mem-end_mem)/start_mem,\n",
    "                                                                                                           (time.time()-starttime)/60))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def confusion_matrix(label, predict, n):\n",
    "    \"\"\"\n",
    "    计算混淆矩阵\n",
    "    :param label: 标签，np.array类型。形状可以是(n_sample,) 或者 (n_sample, n_classes)，当为第二种形状时可以表示多标签分类的情况\n",
    "    :param predict: 预测值，与 `label` 同理\n",
    "    :param n: 类别数目\n",
    "    :return: 混淆矩阵，np.array类型。shape 为 (n, n)。$cm_{ij}$表示真实标签为 $i$，预测标签为 $j$ 的样本个数\n",
    "    \"\"\"\n",
    "    k = (label >= 0) & (label < n)\n",
    "    # bincount()函数用于统计数组内每个非负整数的个数\n",
    "    # 详见 https://docs.scipy.org/doc/numpy/reference/generated/numpy.bincount.html\n",
    "    return np.bincount(n * label[k].astype(int) + predict[k], minlength=n ** 2).reshape(n, n)\n",
    "\n",
    "\n",
    "def auc(y, p, classes):\n",
    "    \"\"\"\n",
    "    给定真实标签和预测标签，计算每个类别的auc值。实际只算出了roc曲线上一个点，即一个(fpr, tpr)，再并上(0, 0)和(1, 1)来计算auc\n",
    "    :param y: 标签，np.array类型\n",
    "    :param p: 预测标签，np.array类型\n",
    "    :param classes: 类别，list-like，表示有哪些类别\n",
    "    \"\"\"\n",
    "    p = p.cpu()\n",
    "    all_aucs = np.zeros(len(classes))\n",
    "    for i, c in enumerate(classes):\n",
    "        _y = np.zeros_like(y)\n",
    "        _y[y==c] = 1\n",
    "        _y[y!=c] = 0\n",
    "        _p = np.zeros_like(p)\n",
    "        _p[p==c] = 1\n",
    "        _p[p!=c] = 0\n",
    "#         print(_y, _p)\n",
    "        cm = confusion_matrix(_y, _p, 2)\n",
    "#         print(cm)\n",
    "        tpr = (cm[0, 0] / (cm[0, 0] + cm[0, 1])) if (cm[0, 0] + cm[0, 1]) != 0 else 0\n",
    "        fpr = (cm[1, 0] / (cm[1, 0] + cm[1, 1])) if (cm[1, 0] + cm[1, 1]) != 0 else 0\n",
    "        tpr = [0, tpr, 1]\n",
    "        fpr = [0, fpr, 1]\n",
    "        auc = metrics.auc(fpr, tpr)\n",
    "        all_aucs[i] = auc\n",
    "        if _y.sum() == 0 or _p.sum() == 0:\n",
    "            all_aucs[i] = 0\n",
    "    return all_aucs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'cpu'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-44a54370d78b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mclasses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mall_aucs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mauc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mweighted_auc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mall_aucs\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-47507055a435>\u001b[0m in \u001b[0;36mauc\u001b[0;34m(y, p, classes)\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mparam\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0m类别\u001b[0m\u001b[0;31m，\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlike\u001b[0m\u001b[0;31m，\u001b[0m\u001b[0m表示有哪些类别\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \"\"\"\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m     \u001b[0mall_aucs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclasses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclasses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'cpu'"
     ]
    }
   ],
   "source": [
    "y = np.random.randint(0, 10, 100)\n",
    "p = np.random.randint(0, 10, 100)\n",
    "\n",
    "classes = list(range(10))\n",
    "weights = np.arange(0, 1, 0.1)\n",
    "all_aucs = auc(y, p, classes)\n",
    "\n",
    "weighted_auc = (all_aucs * weights).sum()\n",
    "print(f\"{all_aucs}\\n{weighted_auc}\")\n",
    "\n",
    "classes = list(range(2))\n",
    "y = np.array([0, 0, 1, 1])\n",
    "p = np.array([0, 1, 0, 1])\n",
    "all_aucs = auc(y, p, classes)\n",
    "\n",
    "print(f\"{all_aucs}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 导入数据集"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 处理训练数据\n",
    "可在此做一些预处理：\n",
    "- 从用户历史行为数据中筛掉在视频特征中没出现过的video_id\n",
    "- 删除多余的列\n",
    "- 调整列的顺序\n",
    "- 改变列的数据类型\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 加载训练数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Mem. usage decreased to 1928.41 Mb (65.5% reduction),time spend:0.46 min\n"
     ]
    }
   ],
   "source": [
    "df_train = reduce_mem(dt.fread(\"../完整版df_train.jay\").to_pandas())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train[df_train['is_watch'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 7353024 entries, 0 to 7353023\n",
      "Columns: 135 entries, user_id to da_4\n",
      "dtypes: float16(97), float32(6), int16(6), int32(4), int8(21), object(1)\n",
      "memory usage: 1.9+ GB\n"
     ]
    }
   ],
   "source": [
    "df_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 删除 video_name、is_watch、video_id、user_id 列\n",
    "df_train.drop(['video_name','is_watch','date', 'user_id', 'video_id'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['watch_label', 'is_share', 'is_work_day', 'u_avg_watch_label_1',\n",
       "       'u_sum_watch_times_1', 'u_sum_watch_overs_1', 'u_sum_quit_times_1',\n",
       "       'u_sum_skip_times_1', 'u_sum_comment_times_1', 'u_sum_collect_times_1',\n",
       "       ...\n",
       "       'class_5', 'class_6', 'class_7', 'class_8', 'class_9', 'da_0', 'da_1',\n",
       "       'da_2', 'da_3', 'da_4'],\n",
       "      dtype='object', length=130)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7353024, 130)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = df_train\n",
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((7353024,), (7353024,), (7353024, 128))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 准备数据\n",
    "watch_label = dataset.pop('watch_label').astype(np.uint8)\n",
    "is_share = dataset.pop('is_share').astype(np.uint8)\n",
    "watch_label.shape, is_share.shape, dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>is_work_day</th>\n",
       "      <th>u_avg_watch_label_1</th>\n",
       "      <th>u_sum_watch_times_1</th>\n",
       "      <th>u_sum_watch_overs_1</th>\n",
       "      <th>u_sum_quit_times_1</th>\n",
       "      <th>u_sum_skip_times_1</th>\n",
       "      <th>u_sum_comment_times_1</th>\n",
       "      <th>u_sum_collect_times_1</th>\n",
       "      <th>u_sum_share_times_1</th>\n",
       "      <th>u_sum_watch_time_1</th>\n",
       "      <th>...</th>\n",
       "      <th>class_5</th>\n",
       "      <th>class_6</th>\n",
       "      <th>class_7</th>\n",
       "      <th>class_8</th>\n",
       "      <th>class_9</th>\n",
       "      <th>da_0</th>\n",
       "      <th>da_1</th>\n",
       "      <th>da_2</th>\n",
       "      <th>da_3</th>\n",
       "      <th>da_4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.034393</td>\n",
       "      <td>0.034393</td>\n",
       "      <td>0.034393</td>\n",
       "      <td>0.233032</td>\n",
       "      <td>0.213745</td>\n",
       "      <td>0.174927</td>\n",
       "      <td>0.068787</td>\n",
       "      <td>0.248169</td>\n",
       "      <td>0.439453</td>\n",
       "      <td>0.068787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.041534</td>\n",
       "      <td>0.041534</td>\n",
       "      <td>0.041534</td>\n",
       "      <td>0.625977</td>\n",
       "      <td>0.041534</td>\n",
       "      <td>0.346680</td>\n",
       "      <td>0.083069</td>\n",
       "      <td>0.083069</td>\n",
       "      <td>0.404053</td>\n",
       "      <td>0.083069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.038147</td>\n",
       "      <td>0.038147</td>\n",
       "      <td>0.038147</td>\n",
       "      <td>0.038147</td>\n",
       "      <td>0.038147</td>\n",
       "      <td>0.076355</td>\n",
       "      <td>0.076355</td>\n",
       "      <td>0.694336</td>\n",
       "      <td>0.076416</td>\n",
       "      <td>0.076294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.043579</td>\n",
       "      <td>0.043579</td>\n",
       "      <td>0.043579</td>\n",
       "      <td>0.043579</td>\n",
       "      <td>0.043579</td>\n",
       "      <td>0.087341</td>\n",
       "      <td>0.087341</td>\n",
       "      <td>0.650879</td>\n",
       "      <td>0.087341</td>\n",
       "      <td>0.087158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.041779</td>\n",
       "      <td>0.350098</td>\n",
       "      <td>0.041779</td>\n",
       "      <td>0.041779</td>\n",
       "      <td>0.041779</td>\n",
       "      <td>0.324951</td>\n",
       "      <td>0.083557</td>\n",
       "      <td>0.083557</td>\n",
       "      <td>0.083557</td>\n",
       "      <td>0.424316</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 128 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   is_work_day  u_avg_watch_label_1  u_sum_watch_times_1  u_sum_watch_overs_1  \\\n",
       "0            1                  0.0                    0                    0   \n",
       "1            1                  0.0                    0                    0   \n",
       "2            1                  0.0                    0                    0   \n",
       "3            1                  0.0                    0                    0   \n",
       "4            1                  0.0                    0                    0   \n",
       "\n",
       "   u_sum_quit_times_1  u_sum_skip_times_1  u_sum_comment_times_1  \\\n",
       "0                   0                   0                      0   \n",
       "1                   0                   0                      0   \n",
       "2                   0                   0                      0   \n",
       "3                   0                   0                      0   \n",
       "4                   0                   0                      0   \n",
       "\n",
       "   u_sum_collect_times_1  u_sum_share_times_1  u_sum_watch_time_1  ...  \\\n",
       "0                      0                    0                 0.0  ...   \n",
       "1                      0                    0                 0.0  ...   \n",
       "2                      0                    0                 0.0  ...   \n",
       "3                      0                    0                 0.0  ...   \n",
       "4                      0                    0                 0.0  ...   \n",
       "\n",
       "    class_5   class_6   class_7   class_8   class_9      da_0      da_1  \\\n",
       "0  0.034393  0.034393  0.034393  0.233032  0.213745  0.174927  0.068787   \n",
       "1  0.041534  0.041534  0.041534  0.625977  0.041534  0.346680  0.083069   \n",
       "2  0.038147  0.038147  0.038147  0.038147  0.038147  0.076355  0.076355   \n",
       "3  0.043579  0.043579  0.043579  0.043579  0.043579  0.087341  0.087341   \n",
       "4  0.041779  0.350098  0.041779  0.041779  0.041779  0.324951  0.083557   \n",
       "\n",
       "       da_2      da_3      da_4  \n",
       "0  0.248169  0.439453  0.068787  \n",
       "1  0.083069  0.404053  0.083069  \n",
       "2  0.694336  0.076416  0.076294  \n",
       "3  0.650879  0.087341  0.087158  \n",
       "4  0.083557  0.083557  0.424316  \n",
       "\n",
       "[5 rows x 128 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 处理测试数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 加载测试数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Mem. usage decreased to 820.89 Mb (65.7% reduction),time spend:0.23 min\n"
     ]
    }
   ],
   "source": [
    "# 拼接好的测试数据集\n",
    "df_test = reduce_mem(dt.fread(\"../完整版df_test.jay\").to_pandas())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2822180 entries, 0 to 2822179\n",
      "Columns: 132 entries, user_id to da_4\n",
      "dtypes: float16(120), float32(3), int32(3), int8(1), object(5)\n",
      "memory usage: 820.9+ MB\n"
     ]
    }
   ],
   "source": [
    "df_test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 删除 video_name、 video_id、user_id、date 列\n",
    "df_test.drop(['video_name','user_id', 'video_id','date'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 填充缺失值\n",
    "df_test.fillna(value=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 查看train和test维度是否相同\n",
    "df_train.shape[1] == df_test.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 样本均衡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[      0 5176743]\n",
      " [      1  557421]\n",
      " [      2  314107]\n",
      " [      3  219188]\n",
      " [      4  172404]\n",
      " [      5  143001]\n",
      " [      6  125092]\n",
      " [      7  117749]\n",
      " [      8  138798]\n",
      " [      9  388521]]\n",
      "[[0.         0.70402912]\n",
      " [1.         0.0758084 ]\n",
      " [2.         0.04271807]\n",
      " [3.         0.02980923]\n",
      " [4.         0.02344668]\n",
      " [5.         0.01944792]\n",
      " [6.         0.01701232]\n",
      " [7.         0.01601368]\n",
      " [8.         0.01887632]\n",
      " [9.         0.05283826]]\n"
     ]
    }
   ],
   "source": [
    "items = list(Counter(watch_label).items())\n",
    "items.sort(key=lambda x: x[0])\n",
    "print(np.array(items))\n",
    "\n",
    "dist = np.array(items, dtype=np.float)\n",
    "dist[:, 1] = dist[:, 1] / watch_label.shape[0]\n",
    "print(dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "under_ss = np.array(items)\n",
    "under_ss_thresh = under_ss[3, 1]  # 设置每个类别样本数目的上限 [219188], 超过上限按上限计算\n",
    "under_ss[:, 1] = np.clip(under_ss[:, 1], a_min=None, a_max=under_ss_thresh)\n",
    "\n",
    "over_ss = under_ss.copy() \n",
    "over_ss_thresh = under_ss[2, 1]  # 设置每个类别样本数据的下限，此时under_ss为 219188, 低于下限按下限计算。\n",
    "over_ss[:, 1] = np.clip(over_ss[:, 1], a_min=over_ss_thresh, a_max=None)\n",
    "\n",
    "under_ss = dict(under_ss)\n",
    "over_ss = dict(over_ss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({0: 219188,\n",
       "  1: 219188,\n",
       "  2: 219188,\n",
       "  3: 219188,\n",
       "  4: 172404,\n",
       "  5: 143001,\n",
       "  6: 125092,\n",
       "  7: 117749,\n",
       "  8: 138798,\n",
       "  9: 219188},\n",
       " {0: 219188,\n",
       "  1: 219188,\n",
       "  2: 219188,\n",
       "  3: 219188,\n",
       "  4: 219188,\n",
       "  5: 219188,\n",
       "  6: 219188,\n",
       "  7: 219188,\n",
       "  8: 219188,\n",
       "  9: 219188})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "under_ss, over_ss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5176743,)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idxs = watch_label == 0\n",
    "idxs = idxs.replace(False, np.nan).dropna().index  # 保留watch_label=0的行索引\n",
    "idxs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4957555,), (219188,))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "left_idxs = np.random.choice(idxs, under_ss_thresh, replace=False)  # 选择一部分保留，注意replace参数，为True时会重复采样\n",
    "del_idxs = idxs.difference(left_idxs)\n",
    "del_idxs.shape, left_idxs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({0: 7338705, 1: 14319})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(watch_label)\n",
    "Counter(is_share)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2395469, 128), (2395469,), (2395469,))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resampled_data = np.delete(dataset.values, del_idxs, axis=0)\n",
    "resampled_wl = np.delete(watch_label.values, del_idxs, axis=0)\n",
    "resampled_sl = np.delete(is_share.values, del_idxs, axis=0)\n",
    "resampled_data.shape, resampled_wl.shape, resampled_sl.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({9: 388521,\n",
       "         7: 117749,\n",
       "         1: 557421,\n",
       "         0: 219188,\n",
       "         4: 172404,\n",
       "         2: 314107,\n",
       "         5: 143001,\n",
       "         6: 125092,\n",
       "         3: 219188,\n",
       "         8: 138798})"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(resampled_wl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2395469, 128), (2395469,), (2395469,))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 将采样后的数据重装回 DataFrame\n",
    "resampled_dataset = pd.DataFrame(resampled_data, columns=dataset.columns)\n",
    "watch_label_res = pd.Series(resampled_wl)\n",
    "share_label_res = pd.Series(resampled_sl)\n",
    "resampled_dataset.shape, watch_label_res.shape, share_label_res.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "Li6LLUtnpxiD"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7353024, 128)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7353024,)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "watch_label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7353024,)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "is_share.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_categorical(y, num_classes=None, dtype='float32'):\n",
    "    \"\"\"\n",
    "    From keras sorucecode: https://github.com/keras-team/keras/blob/master/keras/utils/np_utils.py#L9\n",
    "    \"\"\"\n",
    "\n",
    "    y = np.array(y, dtype='int')\n",
    "    input_shape = y.shape\n",
    "    if input_shape and input_shape[-1] == 1 and len(input_shape) > 1:\n",
    "        input_shape = tuple(input_shape[:-1])\n",
    "    y = y.ravel()\n",
    "    if not num_classes:\n",
    "        num_classes = np.max(y) + 1\n",
    "    n = y.shape[0]\n",
    "    categorical = np.zeros((n, num_classes), dtype=dtype)\n",
    "    categorical[np.arange(n), y] = 1\n",
    "    output_shape = input_shape + (num_classes,)\n",
    "    categorical = np.reshape(categorical, output_shape)\n",
    "    return categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(dataset, wt_label, sh_label, val_ratio=0.15):\n",
    "    # 将watch 转为one-hot\n",
    "    transformed_watch_label = to_categorical(wt_label, num_classes=10, dtype=int)\n",
    "    \n",
    "    # 将train划分为 train、validation. validation占20%。\n",
    "    validation_indices = dataset.sample(frac=val_ratio, replace=False, random_state=SEED).index\n",
    "    validation_data = dataset.iloc[validation_indices]\n",
    "    validation_label = [transformed_watch_label[validation_indices], sh_label[validation_indices]] #key: income, marital.\n",
    "\n",
    "    train_indices = list(set(dataset.index) - set(validation_indices))\n",
    "    train_data = dataset.iloc[train_indices]\n",
    "    train_label = [transformed_watch_label[train_indices], sh_label[train_indices]]\n",
    "    \n",
    "    return train_data, train_label, validation_data, validation_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, train_label, validation_data, validation_label = split_dataset(resampled_dataset, watch_label_res, share_label_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_QT = QuantileTransformer(output_distribution='uniform').fit_transform(train_data)\n",
    "validation_data_QT = QuantileTransformer(output_distribution='uniform').fit_transform(validation_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "YRThKUgIpxiG"
   },
   "outputs": [],
   "source": [
    "def getTensorDataset(my_x, my_y):\n",
    "    tensor_x = torch.tensor(my_x)\n",
    "    tensor_y = torch.tensor(my_y)\n",
    "    return torch.utils.data.TensorDataset(tensor_x, tensor_y)\n",
    "\n",
    "# 拼接两个label\n",
    "train_label_tmp = np.column_stack([train_label[0],train_label[1]])\n",
    "train_loader = DataLoader(dataset=getTensorDataset(train_data_QT, train_label_tmp), batch_size=BATCH_SIZE)\n",
    "\n",
    "validation_label_tmp = np.column_stack([validation_label[0], validation_label[1]])\n",
    "val_loader = DataLoader(dataset=getTensorDataset(validation_data_QT, validation_label_tmp), batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2036149, 11)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_label_tmp.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UfA2Z8vkpxiH"
   },
   "source": [
    "## 模型定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "Oy_S9zUUpxiH",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Expert(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_size):\n",
    "        super(Expert, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        # self.log_soft = nn.LogSoftmax(1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout(out)\n",
    "        out = self.fc2(out)\n",
    "        # out = self.log_soft(out)\n",
    "        return out\n",
    "    \n",
    "class Tower(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_size):\n",
    "        super(Tower, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.4)\n",
    "        # self.softmax = nn.Softmax(dim=1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout(out)\n",
    "        out = self.fc2(out)\n",
    "        # out = self.softmax(out)\n",
    "        out = self.sigmoid(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "C6BGy21KpxiI",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MMOE(torch.nn.Module):\n",
    "    def __init__(self, input_size, num_experts, experts_out, experts_hidden, towers_hidden, tasks):\n",
    "        super(MMOE, self).__init__()\n",
    "        # params\n",
    "        self.input_size = input_size\n",
    "        self.num_experts = num_experts\n",
    "        self.experts_out = experts_out\n",
    "        self.experts_hidden = experts_hidden\n",
    "        self.towers_hidden = towers_hidden\n",
    "        self.tasks = tasks\n",
    "        # row by row\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        # model\n",
    "        self.experts = nn.ModuleList([Expert(self.input_size, self.experts_out, self.experts_hidden) for i in range(self.num_experts)])\n",
    "        self.w_gates = nn.ParameterList([nn.Parameter(torch.randn(input_size, num_experts), requires_grad=True) for i in range(self.tasks)])\n",
    "        self.towers = nn.ModuleList([Tower(self.experts_out, 1, self.towers_hidden) for i in range(self.tasks)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        # get the experts output\n",
    "        expers_o = [e(x) for e in self.experts]\n",
    "        expers_o_tensor = torch.stack(expers_o)\n",
    "\n",
    "        # get the gates output\n",
    "        # x @ g 矩阵整体乘法。\n",
    "        gates_o = [self.softmax(x @ g) for g in self.w_gates]\n",
    "        \n",
    "        # multiply the output of the experts with the corresponding gates output\n",
    "        # res = gates_o[0].t().unsqueeze(2).expand(-1, -1, self.experts_out) * expers_o_tensor\n",
    "        # https://discuss.pytorch.org/t/element-wise-multiplication-of-the-last-dimension/79534\n",
    "        towers_input = [g.t().unsqueeze(2).expand(-1, -1, self.experts_out) * expers_o_tensor for g in gates_o]\n",
    "        towers_input = [torch.sum(ti, dim=0) for ti in towers_input]\n",
    "        \n",
    "        # get the final output from the towers\n",
    "        final_output = [t(ti) for t, ti in zip(self.towers, towers_input)]\n",
    "        \n",
    "        # get the output of the towers, and stack them\n",
    "        final_output = torch.stack(final_output, dim=1)\n",
    "#         print(final_output)\n",
    "        return final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "ebInrBYnpxiK",
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = MMOE(input_size=128, num_experts=16, experts_out=32, experts_hidden=32, towers_hidden=16, tasks=11)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型预览"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {
    "id": "LP7p88WoBl9U",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {
    "collapsed": true,
    "id": "5Xq4dMaq-hcJ",
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x:  torch.Size([2, 128])\n",
      "expers_o_tensor  torch.Size([12, 2, 32])\n",
      "towers_input :  11\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1                 [1024, 32]           4,128\n",
      "              ReLU-2                 [1024, 32]               0\n",
      "           Dropout-3                 [1024, 32]               0\n",
      "            Linear-4                 [1024, 32]           1,056\n",
      "            Expert-5                 [1024, 32]               0\n",
      "            Linear-6                 [1024, 32]           4,128\n",
      "              ReLU-7                 [1024, 32]               0\n",
      "           Dropout-8                 [1024, 32]               0\n",
      "            Linear-9                 [1024, 32]           1,056\n",
      "           Expert-10                 [1024, 32]               0\n",
      "           Linear-11                 [1024, 32]           4,128\n",
      "             ReLU-12                 [1024, 32]               0\n",
      "          Dropout-13                 [1024, 32]               0\n",
      "           Linear-14                 [1024, 32]           1,056\n",
      "           Expert-15                 [1024, 32]               0\n",
      "           Linear-16                 [1024, 32]           4,128\n",
      "             ReLU-17                 [1024, 32]               0\n",
      "          Dropout-18                 [1024, 32]               0\n",
      "           Linear-19                 [1024, 32]           1,056\n",
      "           Expert-20                 [1024, 32]               0\n",
      "           Linear-21                 [1024, 32]           4,128\n",
      "             ReLU-22                 [1024, 32]               0\n",
      "          Dropout-23                 [1024, 32]               0\n",
      "           Linear-24                 [1024, 32]           1,056\n",
      "           Expert-25                 [1024, 32]               0\n",
      "           Linear-26                 [1024, 32]           4,128\n",
      "             ReLU-27                 [1024, 32]               0\n",
      "          Dropout-28                 [1024, 32]               0\n",
      "           Linear-29                 [1024, 32]           1,056\n",
      "           Expert-30                 [1024, 32]               0\n",
      "           Linear-31                 [1024, 32]           4,128\n",
      "             ReLU-32                 [1024, 32]               0\n",
      "          Dropout-33                 [1024, 32]               0\n",
      "           Linear-34                 [1024, 32]           1,056\n",
      "           Expert-35                 [1024, 32]               0\n",
      "           Linear-36                 [1024, 32]           4,128\n",
      "             ReLU-37                 [1024, 32]               0\n",
      "          Dropout-38                 [1024, 32]               0\n",
      "           Linear-39                 [1024, 32]           1,056\n",
      "           Expert-40                 [1024, 32]               0\n",
      "           Linear-41                 [1024, 32]           4,128\n",
      "             ReLU-42                 [1024, 32]               0\n",
      "          Dropout-43                 [1024, 32]               0\n",
      "           Linear-44                 [1024, 32]           1,056\n",
      "           Expert-45                 [1024, 32]               0\n",
      "           Linear-46                 [1024, 32]           4,128\n",
      "             ReLU-47                 [1024, 32]               0\n",
      "          Dropout-48                 [1024, 32]               0\n",
      "           Linear-49                 [1024, 32]           1,056\n",
      "           Expert-50                 [1024, 32]               0\n",
      "           Linear-51                 [1024, 32]           4,128\n",
      "             ReLU-52                 [1024, 32]               0\n",
      "          Dropout-53                 [1024, 32]               0\n",
      "           Linear-54                 [1024, 32]           1,056\n",
      "           Expert-55                 [1024, 32]               0\n",
      "           Linear-56                 [1024, 32]           4,128\n",
      "             ReLU-57                 [1024, 32]               0\n",
      "          Dropout-58                 [1024, 32]               0\n",
      "           Linear-59                 [1024, 32]           1,056\n",
      "           Expert-60                 [1024, 32]               0\n",
      "          Softmax-61                 [1024, 12]               0\n",
      "          Softmax-62                 [1024, 12]               0\n",
      "          Softmax-63                 [1024, 12]               0\n",
      "          Softmax-64                 [1024, 12]               0\n",
      "          Softmax-65                 [1024, 12]               0\n",
      "          Softmax-66                 [1024, 12]               0\n",
      "          Softmax-67                 [1024, 12]               0\n",
      "          Softmax-68                 [1024, 12]               0\n",
      "          Softmax-69                 [1024, 12]               0\n",
      "          Softmax-70                 [1024, 12]               0\n",
      "          Softmax-71                 [1024, 12]               0\n",
      "           Linear-72                  [1024, 8]             264\n",
      "             ReLU-73                  [1024, 8]               0\n",
      "          Dropout-74                  [1024, 8]               0\n",
      "           Linear-75                  [1024, 1]               9\n",
      "          Sigmoid-76                  [1024, 1]               0\n",
      "            Tower-77                  [1024, 1]               0\n",
      "           Linear-78                  [1024, 8]             264\n",
      "             ReLU-79                  [1024, 8]               0\n",
      "          Dropout-80                  [1024, 8]               0\n",
      "           Linear-81                  [1024, 1]               9\n",
      "          Sigmoid-82                  [1024, 1]               0\n",
      "            Tower-83                  [1024, 1]               0\n",
      "           Linear-84                  [1024, 8]             264\n",
      "             ReLU-85                  [1024, 8]               0\n",
      "          Dropout-86                  [1024, 8]               0\n",
      "           Linear-87                  [1024, 1]               9\n",
      "          Sigmoid-88                  [1024, 1]               0\n",
      "            Tower-89                  [1024, 1]               0\n",
      "           Linear-90                  [1024, 8]             264\n",
      "             ReLU-91                  [1024, 8]               0\n",
      "          Dropout-92                  [1024, 8]               0\n",
      "           Linear-93                  [1024, 1]               9\n",
      "          Sigmoid-94                  [1024, 1]               0\n",
      "            Tower-95                  [1024, 1]               0\n",
      "           Linear-96                  [1024, 8]             264\n",
      "             ReLU-97                  [1024, 8]               0\n",
      "          Dropout-98                  [1024, 8]               0\n",
      "           Linear-99                  [1024, 1]               9\n",
      "         Sigmoid-100                  [1024, 1]               0\n",
      "           Tower-101                  [1024, 1]               0\n",
      "          Linear-102                  [1024, 8]             264\n",
      "            ReLU-103                  [1024, 8]               0\n",
      "         Dropout-104                  [1024, 8]               0\n",
      "          Linear-105                  [1024, 1]               9\n",
      "         Sigmoid-106                  [1024, 1]               0\n",
      "           Tower-107                  [1024, 1]               0\n",
      "          Linear-108                  [1024, 8]             264\n",
      "            ReLU-109                  [1024, 8]               0\n",
      "         Dropout-110                  [1024, 8]               0\n",
      "          Linear-111                  [1024, 1]               9\n",
      "         Sigmoid-112                  [1024, 1]               0\n",
      "           Tower-113                  [1024, 1]               0\n",
      "          Linear-114                  [1024, 8]             264\n",
      "            ReLU-115                  [1024, 8]               0\n",
      "         Dropout-116                  [1024, 8]               0\n",
      "          Linear-117                  [1024, 1]               9\n",
      "         Sigmoid-118                  [1024, 1]               0\n",
      "           Tower-119                  [1024, 1]               0\n",
      "          Linear-120                  [1024, 8]             264\n",
      "            ReLU-121                  [1024, 8]               0\n",
      "         Dropout-122                  [1024, 8]               0\n",
      "          Linear-123                  [1024, 1]               9\n",
      "         Sigmoid-124                  [1024, 1]               0\n",
      "           Tower-125                  [1024, 1]               0\n",
      "          Linear-126                  [1024, 8]             264\n",
      "            ReLU-127                  [1024, 8]               0\n",
      "         Dropout-128                  [1024, 8]               0\n",
      "          Linear-129                  [1024, 1]               9\n",
      "         Sigmoid-130                  [1024, 1]               0\n",
      "           Tower-131                  [1024, 1]               0\n",
      "          Linear-132                  [1024, 8]             264\n",
      "            ReLU-133                  [1024, 8]               0\n",
      "         Dropout-134                  [1024, 8]               0\n",
      "          Linear-135                  [1024, 1]               9\n",
      "         Sigmoid-136                  [1024, 1]               0\n",
      "           Tower-137                  [1024, 1]               0\n",
      "================================================================\n",
      "Total params: 65,211\n",
      "Trainable params: 65,211\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.50\n",
      "Forward/backward pass size (MB): 18.35\n",
      "Params size (MB): 0.25\n",
      "Estimated Total Size (MB): 19.10\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(model, input_size=[(128,)], batch_size=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_input = torch.Tensor(2,128).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = model(simple_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {
    "id": "lt5pS6hZGxix",
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.onnx.export(model, simple_input,'MMoE-DouLoss.onnx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B-bMI0qepxiK"
   },
   "source": [
    "## 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n",
    "    def __init__(self, patience=7, verbose=False, delta=0):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            patience (int): How long to wait after last time validation loss improved.\n",
    "                            上次验证集损失值改善后等待几个epoch\n",
    "                            Default: 7\n",
    "            verbose (bool): If True, prints a message for each validation loss improvement.\n",
    "                            如果是True，为每个验证集损失值改善打印一条信息\n",
    "                            Default: False\n",
    "            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n",
    "                            监测数量的最小变化，以符合改进的要求\n",
    "                            Default: 0\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = np.Inf\n",
    "        self.delta = delta\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "\n",
    "        score = -val_loss\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model):\n",
    "        '''\n",
    "        Saves model when validation loss decrease.\n",
    "        验证损失减少时保存模型。\n",
    "        '''\n",
    "        if self.verbose:\n",
    "            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
    "        # torch.save(model.state_dict(), 'checkpoint.pt')     # 这里会存储迄今最优模型的参数\n",
    "        torch.save(model, 'finish_model.pkl')                 # 这里会存储迄今最优的模型\n",
    "        self.val_loss_min = val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 删除所有的hanlder\n",
    "logger = logging.getLogger()\n",
    "while logger.hasHandlers():\n",
    "    logger.removeHandler(logger.handlers[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true,
    "id": "7-uiAEZRpxiK",
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 0/50\n",
      "[Batch: 100]train_loss: 2.10299 \n",
      "[Batch: 200]train_loss: 1.44665 \n",
      "[Batch: 300]train_loss: 1.07268 \n",
      "[Batch: 400]train_loss: 1.03572 \n",
      "[Batch: 500]train_loss: 1.00667 \n",
      "[Batch: 600]train_loss: 1.02380 \n",
      "[Batch: 700]train_loss: 0.91250 \n",
      "[Batch: 800]train_loss: 0.92018 \n",
      "[Batch: 900]train_loss: 1.01560 \n",
      "[Batch: 1000]train_loss: 1.02357 \n",
      "[Batch: 1100]train_loss: 0.95266 \n",
      "[Batch: 1200]train_loss: 0.95931 \n",
      "[Batch: 1300]train_loss: 0.96991 \n",
      "[Batch: 1400]train_loss: 1.01141 \n",
      "[Batch: 1500]train_loss: 1.00747 \n",
      "[Batch: 1600]train_loss: 0.95760 \n",
      "[Batch: 1700]train_loss: 0.95917 \n",
      "[Batch: 1800]train_loss: 0.92834 \n",
      "[Batch: 1900]train_loss: 0.96006 \n",
      "[ 0/50] train_loss: 1.09956 valid_loss: 0.89268\n",
      "Validation loss decreased (inf --> 0.892677).  Saving model ...\n",
      "\n",
      "Epoch: 1/50\n",
      "[Batch: 100]train_loss: 0.93487 \n",
      "[Batch: 200]train_loss: 0.96556 \n",
      "[Batch: 300]train_loss: 0.95880 \n",
      "[Batch: 400]train_loss: 0.95723 \n",
      "[Batch: 500]train_loss: 0.94836 \n",
      "[Batch: 600]train_loss: 0.99470 \n",
      "[Batch: 700]train_loss: 0.87242 \n",
      "[Batch: 800]train_loss: 0.88297 \n",
      "[Batch: 900]train_loss: 0.98686 \n",
      "[Batch: 1000]train_loss: 0.97392 \n",
      "[Batch: 1100]train_loss: 0.93614 \n",
      "[Batch: 1200]train_loss: 0.92301 \n",
      "[Batch: 1300]train_loss: 0.95239 \n",
      "[Batch: 1400]train_loss: 0.97663 \n",
      "[Batch: 1500]train_loss: 0.98020 \n",
      "[Batch: 1600]train_loss: 0.94167 \n",
      "[Batch: 1700]train_loss: 0.90932 \n",
      "[Batch: 1800]train_loss: 0.89372 \n",
      "[Batch: 1900]train_loss: 0.93745 \n",
      "[ 1/50] train_loss: 0.94538 valid_loss: 0.88241\n",
      "Validation loss decreased (0.892677 --> 0.882414).  Saving model ...\n",
      "\n",
      "Epoch: 2/50\n",
      "[Batch: 100]train_loss: 0.91948 \n",
      "[Batch: 200]train_loss: 0.94175 \n",
      "[Batch: 300]train_loss: 0.93800 \n",
      "[Batch: 400]train_loss: 0.92600 \n",
      "[Batch: 500]train_loss: 0.93359 \n",
      "[Batch: 600]train_loss: 0.97143 \n",
      "[Batch: 700]train_loss: 0.85433 \n",
      "[Batch: 800]train_loss: 0.86608 \n",
      "[Batch: 900]train_loss: 0.96717 \n",
      "[Batch: 1000]train_loss: 0.95299 \n",
      "[Batch: 1100]train_loss: 0.92725 \n",
      "[Batch: 1200]train_loss: 0.90976 \n",
      "[Batch: 1300]train_loss: 0.93623 \n",
      "[Batch: 1400]train_loss: 0.97117 \n",
      "[Batch: 1500]train_loss: 0.95802 \n",
      "[Batch: 1600]train_loss: 0.93453 \n",
      "[Batch: 1700]train_loss: 0.91176 \n",
      "[Batch: 1800]train_loss: 0.88627 \n",
      "[Batch: 1900]train_loss: 0.91848 \n",
      "[ 2/50] train_loss: 0.93198 valid_loss: 0.87911\n",
      "Validation loss decreased (0.882414 --> 0.879110).  Saving model ...\n",
      "\n",
      "Epoch: 3/50\n",
      "[Batch: 100]train_loss: 0.90094 \n",
      "[Batch: 200]train_loss: 0.92717 \n",
      "[Batch: 300]train_loss: 0.92704 \n",
      "[Batch: 400]train_loss: 0.92833 \n",
      "[Batch: 500]train_loss: 0.92533 \n",
      "[Batch: 600]train_loss: 0.95969 \n",
      "[Batch: 700]train_loss: 0.85001 \n",
      "[Batch: 800]train_loss: 0.86046 \n",
      "[Batch: 900]train_loss: 0.95591 \n",
      "[Batch: 1000]train_loss: 0.95462 \n",
      "[Batch: 1100]train_loss: 0.91676 \n",
      "[Batch: 1200]train_loss: 0.89804 \n",
      "[Batch: 1300]train_loss: 0.92397 \n",
      "[Batch: 1400]train_loss: 0.97012 \n",
      "[Batch: 1500]train_loss: 0.94904 \n",
      "[Batch: 1600]train_loss: 0.91599 \n",
      "[Batch: 1700]train_loss: 0.90121 \n",
      "[Batch: 1800]train_loss: 0.88291 \n",
      "[Batch: 1900]train_loss: 0.91593 \n",
      "[ 3/50] train_loss: 0.92187 valid_loss: 0.87733\n",
      "Validation loss decreased (0.879110 --> 0.877327).  Saving model ...\n",
      "\n",
      "Epoch: 4/50\n",
      "[Batch: 100]train_loss: 0.89615 \n",
      "[Batch: 200]train_loss: 0.93678 \n",
      "[Batch: 300]train_loss: 0.91656 \n",
      "[Batch: 400]train_loss: 0.91160 \n",
      "[Batch: 500]train_loss: 0.91396 \n",
      "[Batch: 600]train_loss: 0.95580 \n",
      "[Batch: 700]train_loss: 0.83432 \n",
      "[Batch: 800]train_loss: 0.84699 \n",
      "[Batch: 900]train_loss: 0.95172 \n",
      "[Batch: 1000]train_loss: 0.94340 \n",
      "[Batch: 1100]train_loss: 0.90529 \n",
      "[Batch: 1200]train_loss: 0.90385 \n",
      "[Batch: 1300]train_loss: 0.91805 \n",
      "[Batch: 1400]train_loss: 0.96120 \n",
      "[Batch: 1500]train_loss: 0.93962 \n",
      "[Batch: 1600]train_loss: 0.91099 \n",
      "[Batch: 1700]train_loss: 0.89288 \n",
      "[Batch: 1800]train_loss: 0.87318 \n",
      "[Batch: 1900]train_loss: 0.90673 \n",
      "[ 4/50] train_loss: 0.91417 valid_loss: 0.87614\n",
      "Validation loss decreased (0.877327 --> 0.876143).  Saving model ...\n",
      "\n",
      "Epoch: 5/50\n",
      "[Batch: 100]train_loss: 0.88745 \n",
      "[Batch: 200]train_loss: 0.92245 \n",
      "[Batch: 300]train_loss: 0.91196 \n",
      "[Batch: 400]train_loss: 0.91142 \n",
      "[Batch: 500]train_loss: 0.90639 \n",
      "[Batch: 600]train_loss: 0.94268 \n",
      "[Batch: 700]train_loss: 0.83577 \n",
      "[Batch: 800]train_loss: 0.83006 \n",
      "[Batch: 900]train_loss: 0.94011 \n",
      "[Batch: 1000]train_loss: 0.93527 \n",
      "[Batch: 1100]train_loss: 0.89396 \n",
      "[Batch: 1200]train_loss: 0.89360 \n",
      "[Batch: 1300]train_loss: 0.92100 \n",
      "[Batch: 1400]train_loss: 0.95748 \n",
      "[Batch: 1500]train_loss: 0.94360 \n",
      "[Batch: 1600]train_loss: 0.91120 \n",
      "[Batch: 1700]train_loss: 0.88185 \n",
      "[Batch: 1800]train_loss: 0.86832 \n",
      "[Batch: 1900]train_loss: 0.90424 \n",
      "[ 5/50] train_loss: 0.90739 valid_loss: 0.87527\n",
      "Validation loss decreased (0.876143 --> 0.875270).  Saving model ...\n",
      "\n",
      "Epoch: 6/50\n",
      "[Batch: 100]train_loss: 0.88483 \n",
      "[Batch: 200]train_loss: 0.91614 \n",
      "[Batch: 300]train_loss: 0.90278 \n",
      "[Batch: 400]train_loss: 0.89979 \n",
      "[Batch: 500]train_loss: 0.90247 \n",
      "[Batch: 600]train_loss: 0.93967 \n",
      "[Batch: 700]train_loss: 0.82362 \n",
      "[Batch: 800]train_loss: 0.83867 \n",
      "[Batch: 900]train_loss: 0.93824 \n",
      "[Batch: 1000]train_loss: 0.92554 \n",
      "[Batch: 1100]train_loss: 0.89621 \n",
      "[Batch: 1200]train_loss: 0.89307 \n",
      "[Batch: 1300]train_loss: 0.90853 \n",
      "[Batch: 1400]train_loss: 0.95158 \n",
      "[Batch: 1500]train_loss: 0.93934 \n",
      "[Batch: 1600]train_loss: 0.90294 \n",
      "[Batch: 1700]train_loss: 0.87029 \n",
      "[Batch: 1800]train_loss: 0.86484 \n",
      "[Batch: 1900]train_loss: 0.90651 \n",
      "[ 6/50] train_loss: 0.90172 valid_loss: 0.87458\n",
      "Validation loss decreased (0.875270 --> 0.874583).  Saving model ...\n",
      "\n",
      "Epoch: 7/50\n",
      "[Batch: 100]train_loss: 0.86977 \n",
      "[Batch: 200]train_loss: 0.91090 \n",
      "[Batch: 300]train_loss: 0.90535 \n",
      "[Batch: 400]train_loss: 0.89685 \n",
      "[Batch: 500]train_loss: 0.90438 \n",
      "[Batch: 600]train_loss: 0.93699 \n",
      "[Batch: 700]train_loss: 0.81276 \n",
      "[Batch: 800]train_loss: 0.83468 \n",
      "[Batch: 900]train_loss: 0.93860 \n",
      "[Batch: 1000]train_loss: 0.92357 \n",
      "[Batch: 1100]train_loss: 0.89351 \n",
      "[Batch: 1200]train_loss: 0.88708 \n",
      "[Batch: 1300]train_loss: 0.90338 \n",
      "[Batch: 1400]train_loss: 0.94411 \n",
      "[Batch: 1500]train_loss: 0.93681 \n",
      "[Batch: 1600]train_loss: 0.90399 \n",
      "[Batch: 1700]train_loss: 0.87644 \n",
      "[Batch: 1800]train_loss: 0.86734 \n",
      "[Batch: 1900]train_loss: 0.89509 \n",
      "[ 7/50] train_loss: 0.89709 valid_loss: 0.87405\n",
      "Validation loss decreased (0.874583 --> 0.874048).  Saving model ...\n",
      "\n",
      "Epoch: 8/50\n",
      "[Batch: 100]train_loss: 0.87447 \n",
      "[Batch: 200]train_loss: 0.90332 \n",
      "[Batch: 300]train_loss: 0.90433 \n",
      "[Batch: 400]train_loss: 0.88852 \n",
      "[Batch: 500]train_loss: 0.89287 \n",
      "[Batch: 600]train_loss: 0.92821 \n",
      "[Batch: 700]train_loss: 0.80559 \n",
      "[Batch: 800]train_loss: 0.82786 \n",
      "[Batch: 900]train_loss: 0.93250 \n",
      "[Batch: 1000]train_loss: 0.91402 \n",
      "[Batch: 1100]train_loss: 0.89245 \n",
      "[Batch: 1200]train_loss: 0.87534 \n",
      "[Batch: 1300]train_loss: 0.90273 \n",
      "[Batch: 1400]train_loss: 0.93606 \n",
      "[Batch: 1500]train_loss: 0.92484 \n",
      "[Batch: 1600]train_loss: 0.89207 \n",
      "[Batch: 1700]train_loss: 0.86878 \n",
      "[Batch: 1800]train_loss: 0.85211 \n",
      "[Batch: 1900]train_loss: 0.89263 \n",
      "[ 8/50] train_loss: 0.89325 valid_loss: 0.87350\n",
      "Validation loss decreased (0.874048 --> 0.873503).  Saving model ...\n",
      "\n",
      "Epoch: 9/50\n",
      "[Batch: 100]train_loss: 0.86574 \n",
      "[Batch: 200]train_loss: 0.90007 \n",
      "[Batch: 300]train_loss: 0.89308 \n",
      "[Batch: 400]train_loss: 0.89684 \n",
      "[Batch: 500]train_loss: 0.88610 \n",
      "[Batch: 600]train_loss: 0.92984 \n",
      "[Batch: 700]train_loss: 0.80687 \n",
      "[Batch: 800]train_loss: 0.82630 \n",
      "[Batch: 900]train_loss: 0.92546 \n",
      "[Batch: 1000]train_loss: 0.91878 \n",
      "[Batch: 1100]train_loss: 0.88911 \n",
      "[Batch: 1200]train_loss: 0.87741 \n",
      "[Batch: 1300]train_loss: 0.89201 \n",
      "[Batch: 1400]train_loss: 0.94447 \n",
      "[Batch: 1500]train_loss: 0.93250 \n",
      "[Batch: 1600]train_loss: 0.88802 \n",
      "[Batch: 1700]train_loss: 0.86138 \n",
      "[Batch: 1800]train_loss: 0.85120 \n",
      "[Batch: 1900]train_loss: 0.89137 \n",
      "[ 9/50] train_loss: 0.88992 valid_loss: 0.87317\n",
      "Validation loss decreased (0.873503 --> 0.873169).  Saving model ...\n",
      "\n",
      "Epoch: 10/50\n",
      "[Batch: 100]train_loss: 0.86702 \n",
      "[Batch: 200]train_loss: 0.89799 \n",
      "[Batch: 300]train_loss: 0.89270 \n",
      "[Batch: 400]train_loss: 0.88695 \n",
      "[Batch: 500]train_loss: 0.89469 \n",
      "[Batch: 600]train_loss: 0.93522 \n",
      "[Batch: 700]train_loss: 0.80520 \n",
      "[Batch: 800]train_loss: 0.82952 \n",
      "[Batch: 900]train_loss: 0.92607 \n",
      "[Batch: 1000]train_loss: 0.91537 \n",
      "[Batch: 1100]train_loss: 0.88426 \n",
      "[Batch: 1200]train_loss: 0.87181 \n",
      "[Batch: 1300]train_loss: 0.89419 \n",
      "[Batch: 1400]train_loss: 0.93811 \n",
      "[Batch: 1500]train_loss: 0.92884 \n",
      "[Batch: 1600]train_loss: 0.88739 \n",
      "[Batch: 1700]train_loss: 0.85807 \n",
      "[Batch: 1800]train_loss: 0.84812 \n",
      "[Batch: 1900]train_loss: 0.88526 \n",
      "[10/50] train_loss: 0.88719 valid_loss: 0.87279\n",
      "Validation loss decreased (0.873169 --> 0.872793).  Saving model ...\n",
      "\n",
      "Epoch: 11/50\n",
      "[Batch: 100]train_loss: 0.86495 \n",
      "[Batch: 200]train_loss: 0.89326 \n",
      "[Batch: 300]train_loss: 0.89411 \n",
      "[Batch: 400]train_loss: 0.88395 \n",
      "[Batch: 500]train_loss: 0.88737 \n",
      "[Batch: 600]train_loss: 0.92665 \n",
      "[Batch: 700]train_loss: 0.80865 \n",
      "[Batch: 800]train_loss: 0.82437 \n",
      "[Batch: 900]train_loss: 0.91989 \n",
      "[Batch: 1000]train_loss: 0.91039 \n",
      "[Batch: 1100]train_loss: 0.88117 \n",
      "[Batch: 1200]train_loss: 0.86738 \n",
      "[Batch: 1300]train_loss: 0.89498 \n",
      "[Batch: 1400]train_loss: 0.93555 \n",
      "[Batch: 1500]train_loss: 0.92434 \n",
      "[Batch: 1600]train_loss: 0.89214 \n",
      "[Batch: 1700]train_loss: 0.85566 \n",
      "[Batch: 1800]train_loss: 0.84652 \n",
      "[Batch: 1900]train_loss: 0.88627 \n",
      "[11/50] train_loss: 0.88470 valid_loss: 0.87243\n",
      "Validation loss decreased (0.872793 --> 0.872431).  Saving model ...\n",
      "\n",
      "Epoch: 12/50\n",
      "[Batch: 100]train_loss: 0.86114 \n",
      "[Batch: 200]train_loss: 0.89501 \n",
      "[Batch: 300]train_loss: 0.88365 \n",
      "[Batch: 400]train_loss: 0.88058 \n",
      "[Batch: 500]train_loss: 0.88511 \n",
      "[Batch: 600]train_loss: 0.92108 \n",
      "[Batch: 700]train_loss: 0.80037 \n",
      "[Batch: 800]train_loss: 0.82218 \n",
      "[Batch: 900]train_loss: 0.91693 \n",
      "[Batch: 1000]train_loss: 0.91226 \n",
      "[Batch: 1100]train_loss: 0.87903 \n",
      "[Batch: 1200]train_loss: 0.87107 \n",
      "[Batch: 1300]train_loss: 0.88313 \n",
      "[Batch: 1400]train_loss: 0.93407 \n",
      "[Batch: 1500]train_loss: 0.92758 \n",
      "[Batch: 1600]train_loss: 0.88837 \n",
      "[Batch: 1700]train_loss: 0.85228 \n",
      "[Batch: 1800]train_loss: 0.84620 \n",
      "[Batch: 1900]train_loss: 0.88151 \n",
      "[12/50] train_loss: 0.88279 valid_loss: 0.87204\n",
      "Validation loss decreased (0.872431 --> 0.872036).  Saving model ...\n",
      "\n",
      "Epoch: 13/50\n",
      "[Batch: 100]train_loss: 0.85708 \n",
      "[Batch: 200]train_loss: 0.89694 \n",
      "[Batch: 300]train_loss: 0.88402 \n",
      "[Batch: 400]train_loss: 0.88180 \n",
      "[Batch: 500]train_loss: 0.88372 \n",
      "[Batch: 600]train_loss: 0.92203 \n",
      "[Batch: 700]train_loss: 0.79689 \n",
      "[Batch: 800]train_loss: 0.82337 \n",
      "[Batch: 900]train_loss: 0.92001 \n",
      "[Batch: 1000]train_loss: 0.91089 \n",
      "[Batch: 1100]train_loss: 0.87681 \n",
      "[Batch: 1200]train_loss: 0.86838 \n",
      "[Batch: 1300]train_loss: 0.88654 \n",
      "[Batch: 1400]train_loss: 0.92999 \n",
      "[Batch: 1500]train_loss: 0.92033 \n",
      "[Batch: 1600]train_loss: 0.88461 \n",
      "[Batch: 1700]train_loss: 0.86037 \n",
      "[Batch: 1800]train_loss: 0.84480 \n",
      "[Batch: 1900]train_loss: 0.87874 \n",
      "[13/50] train_loss: 0.88123 valid_loss: 0.87179\n",
      "Validation loss decreased (0.872036 --> 0.871788).  Saving model ...\n",
      "\n",
      "Epoch: 14/50\n",
      "[Batch: 100]train_loss: 0.86029 \n",
      "[Batch: 200]train_loss: 0.88488 \n",
      "[Batch: 300]train_loss: 0.88607 \n",
      "[Batch: 400]train_loss: 0.87959 \n",
      "[Batch: 500]train_loss: 0.87970 \n",
      "[Batch: 600]train_loss: 0.92096 \n",
      "[Batch: 700]train_loss: 0.80124 \n",
      "[Batch: 800]train_loss: 0.82377 \n",
      "[Batch: 900]train_loss: 0.91881 \n",
      "[Batch: 1000]train_loss: 0.90716 \n",
      "[Batch: 1100]train_loss: 0.87696 \n",
      "[Batch: 1200]train_loss: 0.86803 \n",
      "[Batch: 1300]train_loss: 0.88599 \n",
      "[Batch: 1400]train_loss: 0.93256 \n",
      "[Batch: 1500]train_loss: 0.91878 \n",
      "[Batch: 1600]train_loss: 0.88426 \n",
      "[Batch: 1700]train_loss: 0.85838 \n",
      "[Batch: 1800]train_loss: 0.84584 \n",
      "[Batch: 1900]train_loss: 0.88143 \n",
      "[14/50] train_loss: 0.87983 valid_loss: 0.87153\n",
      "Validation loss decreased (0.871788 --> 0.871532).  Saving model ...\n",
      "\n",
      "Epoch: 15/50\n",
      "[Batch: 100]train_loss: 0.85360 \n",
      "[Batch: 200]train_loss: 0.88956 \n",
      "[Batch: 300]train_loss: 0.88258 \n",
      "[Batch: 400]train_loss: 0.87846 \n",
      "[Batch: 500]train_loss: 0.88088 \n",
      "[Batch: 600]train_loss: 0.91626 \n",
      "[Batch: 700]train_loss: 0.79599 \n",
      "[Batch: 800]train_loss: 0.81742 \n",
      "[Batch: 900]train_loss: 0.91430 \n",
      "[Batch: 1000]train_loss: 0.90649 \n",
      "[Batch: 1100]train_loss: 0.87534 \n",
      "[Batch: 1200]train_loss: 0.86560 \n",
      "[Batch: 1300]train_loss: 0.88910 \n",
      "[Batch: 1400]train_loss: 0.92706 \n",
      "[Batch: 1500]train_loss: 0.91886 \n",
      "[Batch: 1600]train_loss: 0.88130 \n",
      "[Batch: 1700]train_loss: 0.85535 \n",
      "[Batch: 1800]train_loss: 0.83990 \n",
      "[Batch: 1900]train_loss: 0.88323 \n",
      "[15/50] train_loss: 0.87856 valid_loss: 0.87127\n",
      "Validation loss decreased (0.871532 --> 0.871269).  Saving model ...\n",
      "\n",
      "Epoch: 16/50\n",
      "[Batch: 100]train_loss: 0.85789 \n",
      "[Batch: 200]train_loss: 0.88930 \n",
      "[Batch: 300]train_loss: 0.87831 \n",
      "[Batch: 400]train_loss: 0.87718 \n",
      "[Batch: 500]train_loss: 0.88134 \n",
      "[Batch: 600]train_loss: 0.91781 \n",
      "[Batch: 700]train_loss: 0.79562 \n",
      "[Batch: 800]train_loss: 0.81820 \n",
      "[Batch: 900]train_loss: 0.91684 \n",
      "[Batch: 1000]train_loss: 0.90355 \n",
      "[Batch: 1100]train_loss: 0.87040 \n",
      "[Batch: 1200]train_loss: 0.86518 \n",
      "[Batch: 1300]train_loss: 0.88545 \n",
      "[Batch: 1400]train_loss: 0.92420 \n",
      "[Batch: 1500]train_loss: 0.91818 \n",
      "[Batch: 1600]train_loss: 0.88072 \n",
      "[Batch: 1700]train_loss: 0.85199 \n",
      "[Batch: 1800]train_loss: 0.83591 \n",
      "[Batch: 1900]train_loss: 0.87814 \n",
      "[16/50] train_loss: 0.87767 valid_loss: 0.87106\n",
      "Validation loss decreased (0.871269 --> 0.871056).  Saving model ...\n",
      "\n",
      "Epoch: 17/50\n",
      "[Batch: 100]train_loss: 0.85802 \n",
      "[Batch: 200]train_loss: 0.88514 \n",
      "[Batch: 300]train_loss: 0.88434 \n",
      "[Batch: 400]train_loss: 0.87586 \n",
      "[Batch: 500]train_loss: 0.87685 \n",
      "[Batch: 600]train_loss: 0.91678 \n",
      "[Batch: 700]train_loss: 0.79413 \n",
      "[Batch: 800]train_loss: 0.81054 \n",
      "[Batch: 900]train_loss: 0.91309 \n",
      "[Batch: 1000]train_loss: 0.90462 \n",
      "[Batch: 1100]train_loss: 0.87520 \n",
      "[Batch: 1200]train_loss: 0.85999 \n",
      "[Batch: 1300]train_loss: 0.88174 \n",
      "[Batch: 1400]train_loss: 0.92671 \n",
      "[Batch: 1500]train_loss: 0.91739 \n",
      "[Batch: 1600]train_loss: 0.87606 \n",
      "[Batch: 1700]train_loss: 0.84882 \n",
      "[Batch: 1800]train_loss: 0.83871 \n",
      "[Batch: 1900]train_loss: 0.87757 \n",
      "[17/50] train_loss: 0.87666 valid_loss: 0.87081\n",
      "Validation loss decreased (0.871056 --> 0.870810).  Saving model ...\n",
      "\n",
      "Epoch: 18/50\n",
      "[Batch: 100]train_loss: 0.84985 \n",
      "[Batch: 200]train_loss: 0.88967 \n",
      "[Batch: 300]train_loss: 0.88435 \n",
      "[Batch: 400]train_loss: 0.87541 \n",
      "[Batch: 500]train_loss: 0.87828 \n",
      "[Batch: 600]train_loss: 0.91798 \n",
      "[Batch: 700]train_loss: 0.79340 \n",
      "[Batch: 800]train_loss: 0.82003 \n",
      "[Batch: 900]train_loss: 0.91306 \n",
      "[Batch: 1000]train_loss: 0.90002 \n",
      "[Batch: 1100]train_loss: 0.87144 \n",
      "[Batch: 1200]train_loss: 0.86434 \n",
      "[Batch: 1300]train_loss: 0.88426 \n",
      "[Batch: 1400]train_loss: 0.92677 \n",
      "[Batch: 1500]train_loss: 0.91725 \n",
      "[Batch: 1600]train_loss: 0.87429 \n",
      "[Batch: 1700]train_loss: 0.85306 \n",
      "[Batch: 1800]train_loss: 0.83806 \n",
      "[Batch: 1900]train_loss: 0.87393 \n",
      "[18/50] train_loss: 0.87597 valid_loss: 0.87056\n",
      "Validation loss decreased (0.870810 --> 0.870562).  Saving model ...\n",
      "\n",
      "Epoch: 19/50\n",
      "[Batch: 100]train_loss: 0.85037 \n",
      "[Batch: 200]train_loss: 0.88084 \n",
      "[Batch: 300]train_loss: 0.87682 \n",
      "[Batch: 400]train_loss: 0.87332 \n",
      "[Batch: 500]train_loss: 0.87680 \n",
      "[Batch: 600]train_loss: 0.91438 \n",
      "[Batch: 700]train_loss: 0.79591 \n",
      "[Batch: 800]train_loss: 0.81422 \n",
      "[Batch: 900]train_loss: 0.91389 \n",
      "[Batch: 1000]train_loss: 0.90120 \n",
      "[Batch: 1100]train_loss: 0.86785 \n",
      "[Batch: 1200]train_loss: 0.86105 \n",
      "[Batch: 1300]train_loss: 0.88302 \n",
      "[Batch: 1400]train_loss: 0.92864 \n",
      "[Batch: 1500]train_loss: 0.91476 \n",
      "[Batch: 1600]train_loss: 0.87783 \n",
      "[Batch: 1700]train_loss: 0.85135 \n",
      "[Batch: 1800]train_loss: 0.84046 \n",
      "[Batch: 1900]train_loss: 0.87315 \n",
      "[19/50] train_loss: 0.87542 valid_loss: 0.87044\n",
      "Validation loss decreased (0.870562 --> 0.870437).  Saving model ...\n",
      "\n",
      "Epoch: 20/50\n",
      "[Batch: 100]train_loss: 0.84834 \n",
      "[Batch: 200]train_loss: 0.88386 \n",
      "[Batch: 300]train_loss: 0.87807 \n",
      "[Batch: 400]train_loss: 0.87193 \n",
      "[Batch: 500]train_loss: 0.87750 \n",
      "[Batch: 600]train_loss: 0.91538 \n",
      "[Batch: 700]train_loss: 0.78863 \n",
      "[Batch: 800]train_loss: 0.81583 \n",
      "[Batch: 900]train_loss: 0.91249 \n",
      "[Batch: 1000]train_loss: 0.90208 \n",
      "[Batch: 1100]train_loss: 0.87468 \n",
      "[Batch: 1200]train_loss: 0.85964 \n",
      "[Batch: 1300]train_loss: 0.87997 \n",
      "[Batch: 1400]train_loss: 0.92622 \n",
      "[Batch: 1500]train_loss: 0.91803 \n",
      "[Batch: 1600]train_loss: 0.87874 \n",
      "[Batch: 1700]train_loss: 0.84853 \n",
      "[Batch: 1800]train_loss: 0.83498 \n",
      "[Batch: 1900]train_loss: 0.87392 \n",
      "[20/50] train_loss: 0.87473 valid_loss: 0.87024\n",
      "Validation loss decreased (0.870437 --> 0.870236).  Saving model ...\n",
      "\n",
      "Epoch: 21/50\n",
      "[Batch: 100]train_loss: 0.85242 \n",
      "[Batch: 200]train_loss: 0.89000 \n",
      "[Batch: 300]train_loss: 0.88089 \n",
      "[Batch: 400]train_loss: 0.87222 \n",
      "[Batch: 500]train_loss: 0.87848 \n",
      "[Batch: 600]train_loss: 0.91181 \n",
      "[Batch: 700]train_loss: 0.78951 \n",
      "[Batch: 800]train_loss: 0.80707 \n",
      "[Batch: 900]train_loss: 0.91313 \n",
      "[Batch: 1000]train_loss: 0.90072 \n",
      "[Batch: 1100]train_loss: 0.87125 \n",
      "[Batch: 1200]train_loss: 0.86150 \n",
      "[Batch: 1300]train_loss: 0.88126 \n",
      "[Batch: 1400]train_loss: 0.92339 \n",
      "[Batch: 1500]train_loss: 0.91296 \n",
      "[Batch: 1600]train_loss: 0.87556 \n",
      "[Batch: 1700]train_loss: 0.84529 \n",
      "[Batch: 1800]train_loss: 0.84172 \n",
      "[Batch: 1900]train_loss: 0.87289 \n",
      "[21/50] train_loss: 0.87433 valid_loss: 0.86998\n",
      "Validation loss decreased (0.870236 --> 0.869979).  Saving model ...\n",
      "\n",
      "Epoch: 22/50\n",
      "[Batch: 100]train_loss: 0.85147 \n",
      "[Batch: 200]train_loss: 0.88755 \n",
      "[Batch: 300]train_loss: 0.87804 \n",
      "[Batch: 400]train_loss: 0.87288 \n",
      "[Batch: 500]train_loss: 0.87542 \n",
      "[Batch: 600]train_loss: 0.91285 \n",
      "[Batch: 700]train_loss: 0.78685 \n",
      "[Batch: 800]train_loss: 0.81392 \n",
      "[Batch: 900]train_loss: 0.91433 \n",
      "[Batch: 1000]train_loss: 0.89743 \n",
      "[Batch: 1100]train_loss: 0.86944 \n",
      "[Batch: 1200]train_loss: 0.86434 \n",
      "[Batch: 1300]train_loss: 0.87917 \n",
      "[Batch: 1400]train_loss: 0.92540 \n",
      "[Batch: 1500]train_loss: 0.91419 \n",
      "[Batch: 1600]train_loss: 0.87379 \n",
      "[Batch: 1700]train_loss: 0.84506 \n",
      "[Batch: 1800]train_loss: 0.83681 \n",
      "[Batch: 1900]train_loss: 0.87489 \n",
      "[22/50] train_loss: 0.87386 valid_loss: 0.86981\n",
      "Validation loss decreased (0.869979 --> 0.869811).  Saving model ...\n",
      "\n",
      "Epoch: 23/50\n",
      "[Batch: 100]train_loss: 0.84991 \n",
      "[Batch: 200]train_loss: 0.88074 \n",
      "[Batch: 300]train_loss: 0.87602 \n",
      "[Batch: 400]train_loss: 0.87022 \n",
      "[Batch: 500]train_loss: 0.87295 \n",
      "[Batch: 600]train_loss: 0.91339 \n",
      "[Batch: 700]train_loss: 0.79020 \n",
      "[Batch: 800]train_loss: 0.81238 \n",
      "[Batch: 900]train_loss: 0.90929 \n",
      "[Batch: 1000]train_loss: 0.90046 \n",
      "[Batch: 1100]train_loss: 0.87045 \n",
      "[Batch: 1200]train_loss: 0.85930 \n",
      "[Batch: 1300]train_loss: 0.88321 \n",
      "[Batch: 1400]train_loss: 0.92592 \n",
      "[Batch: 1500]train_loss: 0.91393 \n",
      "[Batch: 1600]train_loss: 0.87807 \n",
      "[Batch: 1700]train_loss: 0.85127 \n",
      "[Batch: 1800]train_loss: 0.83709 \n",
      "[Batch: 1900]train_loss: 0.87465 \n",
      "[23/50] train_loss: 0.87365 valid_loss: 0.86971\n",
      "Validation loss decreased (0.869811 --> 0.869710).  Saving model ...\n",
      "\n",
      "Epoch: 24/50\n",
      "[Batch: 100]train_loss: 0.84848 \n",
      "[Batch: 200]train_loss: 0.88423 \n",
      "[Batch: 300]train_loss: 0.87634 \n",
      "[Batch: 400]train_loss: 0.86743 \n",
      "[Batch: 500]train_loss: 0.87766 \n",
      "[Batch: 600]train_loss: 0.91163 \n",
      "[Batch: 700]train_loss: 0.78838 \n",
      "[Batch: 800]train_loss: 0.80974 \n",
      "[Batch: 900]train_loss: 0.91606 \n",
      "[Batch: 1000]train_loss: 0.89623 \n",
      "[Batch: 1100]train_loss: 0.86970 \n",
      "[Batch: 1200]train_loss: 0.85926 \n",
      "[Batch: 1300]train_loss: 0.87979 \n",
      "[Batch: 1400]train_loss: 0.92945 \n",
      "[Batch: 1500]train_loss: 0.91207 \n",
      "[Batch: 1600]train_loss: 0.87596 \n",
      "[Batch: 1700]train_loss: 0.84569 \n",
      "[Batch: 1800]train_loss: 0.84085 \n",
      "[Batch: 1900]train_loss: 0.87071 \n",
      "[24/50] train_loss: 0.87321 valid_loss: 0.86957\n",
      "Validation loss decreased (0.869710 --> 0.869573).  Saving model ...\n",
      "\n",
      "Epoch: 25/50\n",
      "[Batch: 100]train_loss: 0.84895 \n",
      "[Batch: 200]train_loss: 0.88360 \n",
      "[Batch: 300]train_loss: 0.87555 \n",
      "[Batch: 400]train_loss: 0.86935 \n",
      "[Batch: 500]train_loss: 0.87264 \n",
      "[Batch: 600]train_loss: 0.91096 \n",
      "[Batch: 700]train_loss: 0.79044 \n",
      "[Batch: 800]train_loss: 0.81071 \n",
      "[Batch: 900]train_loss: 0.91068 \n",
      "[Batch: 1000]train_loss: 0.89573 \n",
      "[Batch: 1100]train_loss: 0.86978 \n",
      "[Batch: 1200]train_loss: 0.86113 \n",
      "[Batch: 1300]train_loss: 0.88479 \n",
      "[Batch: 1400]train_loss: 0.92556 \n",
      "[Batch: 1500]train_loss: 0.91342 \n",
      "[Batch: 1600]train_loss: 0.87375 \n",
      "[Batch: 1700]train_loss: 0.84518 \n",
      "[Batch: 1800]train_loss: 0.83535 \n",
      "[Batch: 1900]train_loss: 0.87206 \n",
      "[25/50] train_loss: 0.87285 valid_loss: 0.86940\n",
      "Validation loss decreased (0.869573 --> 0.869398).  Saving model ...\n",
      "\n",
      "Epoch: 26/50\n",
      "[Batch: 100]train_loss: 0.84901 \n",
      "[Batch: 200]train_loss: 0.88392 \n",
      "[Batch: 300]train_loss: 0.87720 \n",
      "[Batch: 400]train_loss: 0.86909 \n",
      "[Batch: 500]train_loss: 0.87569 \n",
      "[Batch: 600]train_loss: 0.91121 \n",
      "[Batch: 700]train_loss: 0.78778 \n",
      "[Batch: 800]train_loss: 0.80990 \n",
      "[Batch: 900]train_loss: 0.91331 \n",
      "[Batch: 1000]train_loss: 0.89594 \n",
      "[Batch: 1100]train_loss: 0.86496 \n",
      "[Batch: 1200]train_loss: 0.85776 \n",
      "[Batch: 1300]train_loss: 0.88002 \n",
      "[Batch: 1400]train_loss: 0.92692 \n",
      "[Batch: 1500]train_loss: 0.91343 \n",
      "[Batch: 1600]train_loss: 0.87679 \n",
      "[Batch: 1700]train_loss: 0.84934 \n",
      "[Batch: 1800]train_loss: 0.83485 \n",
      "[Batch: 1900]train_loss: 0.87592 \n",
      "[26/50] train_loss: 0.87261 valid_loss: 0.86921\n",
      "Validation loss decreased (0.869398 --> 0.869215).  Saving model ...\n",
      "\n",
      "Epoch: 27/50\n",
      "[Batch: 100]train_loss: 0.84931 \n",
      "[Batch: 200]train_loss: 0.87940 \n",
      "[Batch: 300]train_loss: 0.87897 \n",
      "[Batch: 400]train_loss: 0.86916 \n",
      "[Batch: 500]train_loss: 0.87474 \n",
      "[Batch: 600]train_loss: 0.91134 \n",
      "[Batch: 700]train_loss: 0.78709 \n",
      "[Batch: 800]train_loss: 0.81218 \n",
      "[Batch: 900]train_loss: 0.91112 \n",
      "[Batch: 1000]train_loss: 0.90165 \n",
      "[Batch: 1100]train_loss: 0.86481 \n",
      "[Batch: 1200]train_loss: 0.85902 \n",
      "[Batch: 1300]train_loss: 0.88041 \n",
      "[Batch: 1400]train_loss: 0.92137 \n",
      "[Batch: 1500]train_loss: 0.91135 \n",
      "[Batch: 1600]train_loss: 0.87171 \n",
      "[Batch: 1700]train_loss: 0.84947 \n",
      "[Batch: 1800]train_loss: 0.83553 \n",
      "[Batch: 1900]train_loss: 0.87301 \n",
      "[27/50] train_loss: 0.87247 valid_loss: 0.86905\n",
      "Validation loss decreased (0.869215 --> 0.869046).  Saving model ...\n",
      "\n",
      "Epoch: 28/50\n",
      "[Batch: 100]train_loss: 0.84709 \n",
      "[Batch: 200]train_loss: 0.88170 \n",
      "[Batch: 300]train_loss: 0.87488 \n",
      "[Batch: 400]train_loss: 0.87106 \n",
      "[Batch: 500]train_loss: 0.87451 \n",
      "[Batch: 600]train_loss: 0.91229 \n",
      "[Batch: 700]train_loss: 0.78695 \n",
      "[Batch: 800]train_loss: 0.81075 \n",
      "[Batch: 900]train_loss: 0.91046 \n",
      "[Batch: 1000]train_loss: 0.89886 \n",
      "[Batch: 1100]train_loss: 0.87002 \n",
      "[Batch: 1200]train_loss: 0.85875 \n",
      "[Batch: 1300]train_loss: 0.88110 \n",
      "[Batch: 1400]train_loss: 0.92693 \n",
      "[Batch: 1500]train_loss: 0.91314 \n",
      "[Batch: 1600]train_loss: 0.87163 \n",
      "[Batch: 1700]train_loss: 0.84437 \n",
      "[Batch: 1800]train_loss: 0.83488 \n",
      "[Batch: 1900]train_loss: 0.87366 \n",
      "[28/50] train_loss: 0.87218 valid_loss: 0.86899\n",
      "Validation loss decreased (0.869046 --> 0.868990).  Saving model ...\n",
      "\n",
      "Epoch: 29/50\n",
      "[Batch: 100]train_loss: 0.84927 \n",
      "[Batch: 200]train_loss: 0.87819 \n",
      "[Batch: 300]train_loss: 0.87370 \n",
      "[Batch: 400]train_loss: 0.87108 \n",
      "[Batch: 500]train_loss: 0.87460 \n",
      "[Batch: 600]train_loss: 0.91177 \n",
      "[Batch: 700]train_loss: 0.78440 \n",
      "[Batch: 800]train_loss: 0.80899 \n",
      "[Batch: 900]train_loss: 0.90961 \n",
      "[Batch: 1000]train_loss: 0.89558 \n",
      "[Batch: 1100]train_loss: 0.86769 \n",
      "[Batch: 1200]train_loss: 0.85651 \n",
      "[Batch: 1300]train_loss: 0.88036 \n",
      "[Batch: 1400]train_loss: 0.92562 \n",
      "[Batch: 1500]train_loss: 0.91514 \n",
      "[Batch: 1600]train_loss: 0.87259 \n",
      "[Batch: 1700]train_loss: 0.84207 \n",
      "[Batch: 1800]train_loss: 0.83459 \n",
      "[Batch: 1900]train_loss: 0.87222 \n",
      "[29/50] train_loss: 0.87194 valid_loss: 0.86892\n",
      "Validation loss decreased (0.868990 --> 0.868922).  Saving model ...\n",
      "\n",
      "Epoch: 30/50\n",
      "[Batch: 100]train_loss: 0.84604 \n",
      "[Batch: 200]train_loss: 0.87815 \n",
      "[Batch: 300]train_loss: 0.87312 \n",
      "[Batch: 400]train_loss: 0.86672 \n",
      "[Batch: 500]train_loss: 0.87390 \n",
      "[Batch: 600]train_loss: 0.91188 \n",
      "[Batch: 700]train_loss: 0.78532 \n",
      "[Batch: 800]train_loss: 0.81241 \n",
      "[Batch: 900]train_loss: 0.91295 \n",
      "[Batch: 1000]train_loss: 0.89790 \n",
      "[Batch: 1100]train_loss: 0.86782 \n",
      "[Batch: 1200]train_loss: 0.86075 \n",
      "[Batch: 1300]train_loss: 0.88277 \n",
      "[Batch: 1400]train_loss: 0.92538 \n",
      "[Batch: 1500]train_loss: 0.91166 \n",
      "[Batch: 1600]train_loss: 0.87997 \n",
      "[Batch: 1700]train_loss: 0.84585 \n",
      "[Batch: 1800]train_loss: 0.83462 \n",
      "[Batch: 1900]train_loss: 0.87332 \n",
      "[30/50] train_loss: 0.87185 valid_loss: 0.86874\n",
      "Validation loss decreased (0.868922 --> 0.868740).  Saving model ...\n",
      "\n",
      "Epoch: 31/50\n",
      "[Batch: 100]train_loss: 0.84744 \n",
      "[Batch: 200]train_loss: 0.87886 \n",
      "[Batch: 300]train_loss: 0.87475 \n",
      "[Batch: 400]train_loss: 0.86834 \n",
      "[Batch: 500]train_loss: 0.87137 \n",
      "[Batch: 600]train_loss: 0.91071 \n",
      "[Batch: 700]train_loss: 0.78577 \n",
      "[Batch: 800]train_loss: 0.81159 \n",
      "[Batch: 900]train_loss: 0.91136 \n",
      "[Batch: 1000]train_loss: 0.89506 \n",
      "[Batch: 1100]train_loss: 0.87083 \n",
      "[Batch: 1200]train_loss: 0.86044 \n",
      "[Batch: 1300]train_loss: 0.87973 \n",
      "[Batch: 1400]train_loss: 0.92658 \n",
      "[Batch: 1500]train_loss: 0.91158 \n",
      "[Batch: 1600]train_loss: 0.87513 \n",
      "[Batch: 1700]train_loss: 0.84453 \n",
      "[Batch: 1800]train_loss: 0.83492 \n",
      "[Batch: 1900]train_loss: 0.87070 \n",
      "[31/50] train_loss: 0.87156 valid_loss: 0.86865\n",
      "Validation loss decreased (0.868740 --> 0.868654).  Saving model ...\n",
      "\n",
      "Epoch: 32/50\n",
      "[Batch: 100]train_loss: 0.84728 \n",
      "[Batch: 200]train_loss: 0.88420 \n",
      "[Batch: 300]train_loss: 0.87598 \n",
      "[Batch: 400]train_loss: 0.86925 \n",
      "[Batch: 500]train_loss: 0.87544 \n",
      "[Batch: 600]train_loss: 0.91114 \n",
      "[Batch: 700]train_loss: 0.78585 \n",
      "[Batch: 800]train_loss: 0.81208 \n",
      "[Batch: 900]train_loss: 0.90734 \n",
      "[Batch: 1000]train_loss: 0.89921 \n",
      "[Batch: 1100]train_loss: 0.86900 \n",
      "[Batch: 1200]train_loss: 0.85931 \n",
      "[Batch: 1300]train_loss: 0.87932 \n",
      "[Batch: 1400]train_loss: 0.92503 \n",
      "[Batch: 1500]train_loss: 0.90820 \n",
      "[Batch: 1600]train_loss: 0.87305 \n",
      "[Batch: 1700]train_loss: 0.84816 \n",
      "[Batch: 1800]train_loss: 0.83195 \n",
      "[Batch: 1900]train_loss: 0.87220 \n",
      "[32/50] train_loss: 0.87143 valid_loss: 0.86861\n",
      "Validation loss decreased (0.868654 --> 0.868613).  Saving model ...\n",
      "\n",
      "Epoch: 33/50\n",
      "[Batch: 100]train_loss: 0.84826 \n",
      "[Batch: 200]train_loss: 0.88118 \n",
      "[Batch: 300]train_loss: 0.87810 \n",
      "[Batch: 400]train_loss: 0.86914 \n",
      "[Batch: 500]train_loss: 0.87598 \n",
      "[Batch: 600]train_loss: 0.91422 \n",
      "[Batch: 700]train_loss: 0.78925 \n",
      "[Batch: 800]train_loss: 0.80914 \n",
      "[Batch: 900]train_loss: 0.90982 \n",
      "[Batch: 1000]train_loss: 0.89591 \n",
      "[Batch: 1100]train_loss: 0.86860 \n",
      "[Batch: 1200]train_loss: 0.86119 \n",
      "[Batch: 1300]train_loss: 0.88124 \n",
      "[Batch: 1400]train_loss: 0.92639 \n",
      "[Batch: 1500]train_loss: 0.91138 \n",
      "[Batch: 1600]train_loss: 0.87434 \n",
      "[Batch: 1700]train_loss: 0.84397 \n",
      "[Batch: 1800]train_loss: 0.83747 \n",
      "[Batch: 1900]train_loss: 0.87075 \n",
      "[33/50] train_loss: 0.87131 valid_loss: 0.86842\n",
      "Validation loss decreased (0.868613 --> 0.868422).  Saving model ...\n",
      "\n",
      "Epoch: 34/50\n",
      "[Batch: 100]train_loss: 0.85056 \n",
      "[Batch: 200]train_loss: 0.88032 \n",
      "[Batch: 300]train_loss: 0.87866 \n",
      "[Batch: 400]train_loss: 0.86970 \n",
      "[Batch: 500]train_loss: 0.87260 \n",
      "[Batch: 600]train_loss: 0.91068 \n",
      "[Batch: 700]train_loss: 0.78648 \n",
      "[Batch: 800]train_loss: 0.80600 \n",
      "[Batch: 900]train_loss: 0.91184 \n",
      "[Batch: 1000]train_loss: 0.89499 \n",
      "[Batch: 1100]train_loss: 0.86860 \n",
      "[Batch: 1200]train_loss: 0.85885 \n",
      "[Batch: 1300]train_loss: 0.88208 \n",
      "[Batch: 1400]train_loss: 0.92493 \n",
      "[Batch: 1500]train_loss: 0.91224 \n",
      "[Batch: 1600]train_loss: 0.87344 \n",
      "[Batch: 1700]train_loss: 0.84590 \n",
      "[Batch: 1800]train_loss: 0.83725 \n",
      "[Batch: 1900]train_loss: 0.86877 \n",
      "[34/50] train_loss: 0.87118 valid_loss: 0.86836\n",
      "Validation loss decreased (0.868422 --> 0.868358).  Saving model ...\n",
      "\n",
      "Epoch: 35/50\n",
      "[Batch: 100]train_loss: 0.84778 \n",
      "[Batch: 200]train_loss: 0.88032 \n",
      "[Batch: 300]train_loss: 0.87409 \n",
      "[Batch: 400]train_loss: 0.86902 \n",
      "[Batch: 500]train_loss: 0.87852 \n",
      "[Batch: 600]train_loss: 0.91039 \n",
      "[Batch: 700]train_loss: 0.78618 \n",
      "[Batch: 800]train_loss: 0.81017 \n",
      "[Batch: 900]train_loss: 0.91104 \n",
      "[Batch: 1000]train_loss: 0.89458 \n",
      "[Batch: 1100]train_loss: 0.86703 \n",
      "[Batch: 1200]train_loss: 0.85692 \n",
      "[Batch: 1300]train_loss: 0.88063 \n",
      "[Batch: 1400]train_loss: 0.92462 \n",
      "[Batch: 1500]train_loss: 0.91068 \n",
      "[Batch: 1600]train_loss: 0.87284 \n",
      "[Batch: 1700]train_loss: 0.84662 \n",
      "[Batch: 1800]train_loss: 0.83645 \n",
      "[Batch: 1900]train_loss: 0.87030 \n",
      "[35/50] train_loss: 0.87102 valid_loss: 0.86838\n",
      "EarlyStopping counter: 1 out of 10\n",
      "\n",
      "Epoch: 36/50\n",
      "[Batch: 100]train_loss: 0.85020 \n",
      "[Batch: 200]train_loss: 0.87820 \n",
      "[Batch: 300]train_loss: 0.87321 \n",
      "[Batch: 400]train_loss: 0.87066 \n",
      "[Batch: 500]train_loss: 0.87625 \n",
      "[Batch: 600]train_loss: 0.90975 \n",
      "[Batch: 700]train_loss: 0.78770 \n",
      "[Batch: 800]train_loss: 0.80531 \n",
      "[Batch: 900]train_loss: 0.91155 \n",
      "[Batch: 1000]train_loss: 0.89582 \n",
      "[Batch: 1100]train_loss: 0.86803 \n",
      "[Batch: 1200]train_loss: 0.85590 \n",
      "[Batch: 1300]train_loss: 0.88076 \n",
      "[Batch: 1400]train_loss: 0.92653 \n",
      "[Batch: 1500]train_loss: 0.91231 \n",
      "[Batch: 1600]train_loss: 0.87048 \n",
      "[Batch: 1700]train_loss: 0.84592 \n",
      "[Batch: 1800]train_loss: 0.83573 \n",
      "[Batch: 1900]train_loss: 0.87140 \n",
      "[36/50] train_loss: 0.87091 valid_loss: 0.86821\n",
      "Validation loss decreased (0.868358 --> 0.868212).  Saving model ...\n",
      "\n",
      "Epoch: 37/50\n",
      "[Batch: 100]train_loss: 0.84840 \n",
      "[Batch: 200]train_loss: 0.87952 \n",
      "[Batch: 300]train_loss: 0.87601 \n",
      "[Batch: 400]train_loss: 0.86865 \n",
      "[Batch: 500]train_loss: 0.87395 \n",
      "[Batch: 600]train_loss: 0.90933 \n",
      "[Batch: 700]train_loss: 0.78414 \n",
      "[Batch: 800]train_loss: 0.80555 \n",
      "[Batch: 900]train_loss: 0.91175 \n",
      "[Batch: 1000]train_loss: 0.89657 \n",
      "[Batch: 1100]train_loss: 0.86652 \n",
      "[Batch: 1200]train_loss: 0.85818 \n",
      "[Batch: 1300]train_loss: 0.88036 \n",
      "[Batch: 1400]train_loss: 0.92196 \n",
      "[Batch: 1500]train_loss: 0.90973 \n",
      "[Batch: 1600]train_loss: 0.87668 \n",
      "[Batch: 1700]train_loss: 0.84332 \n",
      "[Batch: 1800]train_loss: 0.83492 \n",
      "[Batch: 1900]train_loss: 0.87197 \n",
      "[37/50] train_loss: 0.87080 valid_loss: 0.86816\n",
      "Validation loss decreased (0.868212 --> 0.868164).  Saving model ...\n",
      "\n",
      "Epoch: 38/50\n",
      "[Batch: 100]train_loss: 0.84653 \n",
      "[Batch: 200]train_loss: 0.88201 \n",
      "[Batch: 300]train_loss: 0.87328 \n",
      "[Batch: 400]train_loss: 0.87109 \n",
      "[Batch: 500]train_loss: 0.87555 \n",
      "[Batch: 600]train_loss: 0.91034 \n",
      "[Batch: 700]train_loss: 0.78364 \n",
      "[Batch: 800]train_loss: 0.80636 \n",
      "[Batch: 900]train_loss: 0.91016 \n",
      "[Batch: 1000]train_loss: 0.89681 \n",
      "[Batch: 1100]train_loss: 0.86852 \n",
      "[Batch: 1200]train_loss: 0.85368 \n",
      "[Batch: 1300]train_loss: 0.88096 \n",
      "[Batch: 1400]train_loss: 0.92194 \n",
      "[Batch: 1500]train_loss: 0.90794 \n",
      "[Batch: 1600]train_loss: 0.87243 \n",
      "[Batch: 1700]train_loss: 0.84346 \n",
      "[Batch: 1800]train_loss: 0.83331 \n",
      "[Batch: 1900]train_loss: 0.87112 \n",
      "[38/50] train_loss: 0.87071 valid_loss: 0.86815\n",
      "Validation loss decreased (0.868164 --> 0.868151).  Saving model ...\n",
      "\n",
      "Epoch: 39/50\n",
      "[Batch: 100]train_loss: 0.84723 \n",
      "[Batch: 200]train_loss: 0.87956 \n",
      "[Batch: 300]train_loss: 0.87445 \n",
      "[Batch: 400]train_loss: 0.87310 \n",
      "[Batch: 500]train_loss: 0.87446 \n",
      "[Batch: 600]train_loss: 0.90831 \n",
      "[Batch: 700]train_loss: 0.78690 \n",
      "[Batch: 800]train_loss: 0.80909 \n",
      "[Batch: 900]train_loss: 0.90836 \n",
      "[Batch: 1000]train_loss: 0.89371 \n",
      "[Batch: 1100]train_loss: 0.86480 \n",
      "[Batch: 1200]train_loss: 0.85346 \n",
      "[Batch: 1300]train_loss: 0.87601 \n",
      "[Batch: 1400]train_loss: 0.92087 \n",
      "[Batch: 1500]train_loss: 0.90990 \n",
      "[Batch: 1600]train_loss: 0.87611 \n",
      "[Batch: 1700]train_loss: 0.84402 \n",
      "[Batch: 1800]train_loss: 0.83185 \n",
      "[Batch: 1900]train_loss: 0.87212 \n",
      "[39/50] train_loss: 0.87064 valid_loss: 0.86817\n",
      "EarlyStopping counter: 1 out of 10\n",
      "\n",
      "Epoch: 40/50\n",
      "[Batch: 100]train_loss: 0.84829 \n",
      "[Batch: 200]train_loss: 0.87689 \n",
      "[Batch: 300]train_loss: 0.87822 \n",
      "[Batch: 400]train_loss: 0.86878 \n",
      "[Batch: 500]train_loss: 0.87555 \n",
      "[Batch: 600]train_loss: 0.90957 \n",
      "[Batch: 700]train_loss: 0.78657 \n",
      "[Batch: 800]train_loss: 0.80543 \n",
      "[Batch: 900]train_loss: 0.90942 \n",
      "[Batch: 1000]train_loss: 0.89247 \n",
      "[Batch: 1100]train_loss: 0.86473 \n",
      "[Batch: 1200]train_loss: 0.85506 \n",
      "[Batch: 1300]train_loss: 0.88115 \n",
      "[Batch: 1400]train_loss: 0.92666 \n",
      "[Batch: 1500]train_loss: 0.91001 \n",
      "[Batch: 1600]train_loss: 0.87285 \n",
      "[Batch: 1700]train_loss: 0.84514 \n",
      "[Batch: 1800]train_loss: 0.83788 \n",
      "[Batch: 1900]train_loss: 0.87153 \n",
      "[40/50] train_loss: 0.87052 valid_loss: 0.86797\n",
      "Validation loss decreased (0.868151 --> 0.867966).  Saving model ...\n",
      "\n",
      "Epoch: 41/50\n",
      "[Batch: 100]train_loss: 0.84966 \n",
      "[Batch: 200]train_loss: 0.87809 \n",
      "[Batch: 300]train_loss: 0.87392 \n",
      "[Batch: 400]train_loss: 0.86945 \n",
      "[Batch: 500]train_loss: 0.87152 \n",
      "[Batch: 600]train_loss: 0.90814 \n",
      "[Batch: 700]train_loss: 0.78290 \n",
      "[Batch: 800]train_loss: 0.81345 \n",
      "[Batch: 900]train_loss: 0.90933 \n",
      "[Batch: 1000]train_loss: 0.89554 \n",
      "[Batch: 1100]train_loss: 0.86648 \n",
      "[Batch: 1200]train_loss: 0.85861 \n",
      "[Batch: 1300]train_loss: 0.87832 \n",
      "[Batch: 1400]train_loss: 0.92265 \n",
      "[Batch: 1500]train_loss: 0.91339 \n",
      "[Batch: 1600]train_loss: 0.86911 \n",
      "[Batch: 1700]train_loss: 0.84582 \n",
      "[Batch: 1800]train_loss: 0.83720 \n",
      "[Batch: 1900]train_loss: 0.87191 \n",
      "[41/50] train_loss: 0.87042 valid_loss: 0.86798\n",
      "EarlyStopping counter: 1 out of 10\n",
      "\n",
      "Epoch: 42/50\n",
      "[Batch: 100]train_loss: 0.84751 \n",
      "[Batch: 200]train_loss: 0.88022 \n",
      "[Batch: 300]train_loss: 0.87428 \n",
      "[Batch: 400]train_loss: 0.87002 \n",
      "[Batch: 500]train_loss: 0.87370 \n",
      "[Batch: 600]train_loss: 0.90946 \n",
      "[Batch: 700]train_loss: 0.78438 \n",
      "[Batch: 800]train_loss: 0.80992 \n",
      "[Batch: 900]train_loss: 0.91060 \n",
      "[Batch: 1000]train_loss: 0.89671 \n",
      "[Batch: 1100]train_loss: 0.86546 \n",
      "[Batch: 1200]train_loss: 0.85842 \n",
      "[Batch: 1300]train_loss: 0.87862 \n",
      "[Batch: 1400]train_loss: 0.92129 \n",
      "[Batch: 1500]train_loss: 0.91098 \n",
      "[Batch: 1600]train_loss: 0.87545 \n",
      "[Batch: 1700]train_loss: 0.84375 \n",
      "[Batch: 1800]train_loss: 0.83444 \n",
      "[Batch: 1900]train_loss: 0.87237 \n",
      "[42/50] train_loss: 0.87035 valid_loss: 0.86794\n",
      "Validation loss decreased (0.867966 --> 0.867943).  Saving model ...\n",
      "\n",
      "Epoch: 43/50\n",
      "[Batch: 100]train_loss: 0.84426 \n",
      "[Batch: 200]train_loss: 0.87884 \n",
      "[Batch: 300]train_loss: 0.87410 \n",
      "[Batch: 400]train_loss: 0.86751 \n",
      "[Batch: 500]train_loss: 0.87519 \n",
      "[Batch: 600]train_loss: 0.90677 \n",
      "[Batch: 700]train_loss: 0.78581 \n",
      "[Batch: 800]train_loss: 0.80865 \n",
      "[Batch: 900]train_loss: 0.90712 \n",
      "[Batch: 1000]train_loss: 0.89541 \n",
      "[Batch: 1100]train_loss: 0.86829 \n",
      "[Batch: 1200]train_loss: 0.85629 \n",
      "[Batch: 1300]train_loss: 0.87710 \n",
      "[Batch: 1400]train_loss: 0.91942 \n",
      "[Batch: 1500]train_loss: 0.90544 \n",
      "[Batch: 1600]train_loss: 0.87310 \n",
      "[Batch: 1700]train_loss: 0.84321 \n",
      "[Batch: 1800]train_loss: 0.83047 \n",
      "[Batch: 1900]train_loss: 0.86927 \n",
      "[43/50] train_loss: 0.87033 valid_loss: 0.86786\n",
      "Validation loss decreased (0.867943 --> 0.867863).  Saving model ...\n",
      "\n",
      "Epoch: 44/50\n",
      "[Batch: 100]train_loss: 0.84744 \n",
      "[Batch: 200]train_loss: 0.88049 \n",
      "[Batch: 300]train_loss: 0.87325 \n",
      "[Batch: 400]train_loss: 0.86875 \n",
      "[Batch: 500]train_loss: 0.87673 \n",
      "[Batch: 600]train_loss: 0.90944 \n",
      "[Batch: 700]train_loss: 0.78524 \n",
      "[Batch: 800]train_loss: 0.80457 \n",
      "[Batch: 900]train_loss: 0.91150 \n",
      "[Batch: 1000]train_loss: 0.89618 \n",
      "[Batch: 1100]train_loss: 0.86907 \n",
      "[Batch: 1200]train_loss: 0.85509 \n",
      "[Batch: 1300]train_loss: 0.87831 \n",
      "[Batch: 1400]train_loss: 0.92448 \n",
      "[Batch: 1500]train_loss: 0.90950 \n",
      "[Batch: 1600]train_loss: 0.87222 \n",
      "[Batch: 1700]train_loss: 0.84268 \n",
      "[Batch: 1800]train_loss: 0.83497 \n",
      "[Batch: 1900]train_loss: 0.87109 \n",
      "[44/50] train_loss: 0.87016 valid_loss: 0.86779\n",
      "Validation loss decreased (0.867863 --> 0.867794).  Saving model ...\n",
      "\n",
      "Epoch: 45/50\n",
      "[Batch: 100]train_loss: 0.84406 \n",
      "[Batch: 200]train_loss: 0.87665 \n",
      "[Batch: 300]train_loss: 0.87413 \n",
      "[Batch: 400]train_loss: 0.86671 \n",
      "[Batch: 500]train_loss: 0.87623 \n",
      "[Batch: 600]train_loss: 0.90801 \n",
      "[Batch: 700]train_loss: 0.78672 \n",
      "[Batch: 800]train_loss: 0.80695 \n",
      "[Batch: 900]train_loss: 0.91016 \n",
      "[Batch: 1000]train_loss: 0.89734 \n",
      "[Batch: 1100]train_loss: 0.86875 \n",
      "[Batch: 1200]train_loss: 0.85582 \n",
      "[Batch: 1300]train_loss: 0.88112 \n",
      "[Batch: 1400]train_loss: 0.92515 \n",
      "[Batch: 1500]train_loss: 0.91021 \n",
      "[Batch: 1600]train_loss: 0.87415 \n",
      "[Batch: 1700]train_loss: 0.84500 \n",
      "[Batch: 1800]train_loss: 0.83628 \n",
      "[Batch: 1900]train_loss: 0.87244 \n",
      "[45/50] train_loss: 0.87005 valid_loss: 0.86770\n",
      "Validation loss decreased (0.867794 --> 0.867703).  Saving model ...\n",
      "\n",
      "Epoch: 46/50\n",
      "[Batch: 100]train_loss: 0.85086 \n",
      "[Batch: 200]train_loss: 0.87354 \n",
      "[Batch: 300]train_loss: 0.87293 \n",
      "[Batch: 400]train_loss: 0.86710 \n",
      "[Batch: 500]train_loss: 0.87200 \n",
      "[Batch: 600]train_loss: 0.90640 \n",
      "[Batch: 700]train_loss: 0.78421 \n",
      "[Batch: 800]train_loss: 0.80834 \n",
      "[Batch: 900]train_loss: 0.91007 \n",
      "[Batch: 1000]train_loss: 0.89717 \n",
      "[Batch: 1100]train_loss: 0.86624 \n",
      "[Batch: 1200]train_loss: 0.85433 \n",
      "[Batch: 1300]train_loss: 0.87561 \n",
      "[Batch: 1400]train_loss: 0.92025 \n",
      "[Batch: 1500]train_loss: 0.90873 \n",
      "[Batch: 1600]train_loss: 0.87579 \n",
      "[Batch: 1700]train_loss: 0.84441 \n",
      "[Batch: 1800]train_loss: 0.83589 \n",
      "[Batch: 1900]train_loss: 0.86796 \n",
      "[46/50] train_loss: 0.86997 valid_loss: 0.86774\n",
      "EarlyStopping counter: 1 out of 10\n",
      "\n",
      "Epoch: 47/50\n",
      "[Batch: 100]train_loss: 0.84975 \n",
      "[Batch: 200]train_loss: 0.87639 \n",
      "[Batch: 300]train_loss: 0.87490 \n",
      "[Batch: 400]train_loss: 0.86495 \n",
      "[Batch: 500]train_loss: 0.87262 \n",
      "[Batch: 600]train_loss: 0.90924 \n",
      "[Batch: 700]train_loss: 0.78485 \n",
      "[Batch: 800]train_loss: 0.80488 \n",
      "[Batch: 900]train_loss: 0.90738 \n",
      "[Batch: 1000]train_loss: 0.89469 \n",
      "[Batch: 1100]train_loss: 0.86465 \n",
      "[Batch: 1200]train_loss: 0.85252 \n",
      "[Batch: 1300]train_loss: 0.88152 \n",
      "[Batch: 1400]train_loss: 0.92392 \n",
      "[Batch: 1500]train_loss: 0.90576 \n",
      "[Batch: 1600]train_loss: 0.87202 \n",
      "[Batch: 1700]train_loss: 0.84500 \n",
      "[Batch: 1800]train_loss: 0.83709 \n",
      "[Batch: 1900]train_loss: 0.87396 \n",
      "[47/50] train_loss: 0.86994 valid_loss: 0.86777\n",
      "EarlyStopping counter: 2 out of 10\n",
      "\n",
      "Epoch: 48/50\n",
      "[Batch: 100]train_loss: 0.84579 \n",
      "[Batch: 200]train_loss: 0.87551 \n",
      "[Batch: 300]train_loss: 0.87307 \n",
      "[Batch: 400]train_loss: 0.86851 \n",
      "[Batch: 500]train_loss: 0.87687 \n",
      "[Batch: 600]train_loss: 0.90946 \n",
      "[Batch: 700]train_loss: 0.78284 \n",
      "[Batch: 800]train_loss: 0.81057 \n",
      "[Batch: 900]train_loss: 0.90879 \n",
      "[Batch: 1000]train_loss: 0.89472 \n",
      "[Batch: 1100]train_loss: 0.86688 \n",
      "[Batch: 1200]train_loss: 0.85306 \n",
      "[Batch: 1300]train_loss: 0.88091 \n",
      "[Batch: 1400]train_loss: 0.92438 \n",
      "[Batch: 1500]train_loss: 0.90980 \n",
      "[Batch: 1600]train_loss: 0.87702 \n",
      "[Batch: 1700]train_loss: 0.84572 \n",
      "[Batch: 1800]train_loss: 0.83414 \n",
      "[Batch: 1900]train_loss: 0.86979 \n",
      "[48/50] train_loss: 0.86983 valid_loss: 0.86761\n",
      "Validation loss decreased (0.867703 --> 0.867609).  Saving model ...\n",
      "\n",
      "Epoch: 49/50\n",
      "[Batch: 100]train_loss: 0.84608 \n",
      "[Batch: 200]train_loss: 0.87908 \n",
      "[Batch: 300]train_loss: 0.87395 \n",
      "[Batch: 400]train_loss: 0.87196 \n",
      "[Batch: 500]train_loss: 0.87361 \n",
      "[Batch: 600]train_loss: 0.90978 \n",
      "[Batch: 700]train_loss: 0.78446 \n",
      "[Batch: 800]train_loss: 0.81068 \n",
      "[Batch: 900]train_loss: 0.91101 \n",
      "[Batch: 1000]train_loss: 0.89312 \n",
      "[Batch: 1100]train_loss: 0.86640 \n",
      "[Batch: 1200]train_loss: 0.85629 \n",
      "[Batch: 1300]train_loss: 0.87760 \n",
      "[Batch: 1400]train_loss: 0.92286 \n",
      "[Batch: 1500]train_loss: 0.91020 \n",
      "[Batch: 1600]train_loss: 0.87045 \n",
      "[Batch: 1700]train_loss: 0.84460 \n",
      "[Batch: 1800]train_loss: 0.83177 \n",
      "[Batch: 1900]train_loss: 0.86704 \n",
      "[49/50] train_loss: 0.86978 valid_loss: 0.86773\n",
      "EarlyStopping counter: 1 out of 10\n",
      "loss:  0.8849390251310463\n",
      "val_loss:  0.8709960808849061\n"
     ]
    }
   ],
   "source": [
    "# Sets hyper-parameters\n",
    "lr = 1e-4\n",
    "n_epochs = 50\n",
    "tasks = 11\n",
    "patience = 10\n",
    "# BATCH_SIZE=4096\n",
    "\n",
    "# # Defines loss function and optimizer\n",
    "# loss_fn_watch = nn.CrossEntropyLoss(reduction='mean')\n",
    "loss_fn = nn.BCELoss(reduction='mean')\n",
    "early_stopping = EarlyStopping(patience, verbose=True)\n",
    "# loss_fn_watch = nn.MSELoss(reduction='mean')\n",
    "# loss_fn_share = nn.BCELoss(reduction='mean')\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "loss_weight = [0, 0.07, 0.14, 0.21, 0.28, 0.35, 0.42, 0.49, 0.56, 0.63, 0.3]\n",
    "\n",
    "losses = []\n",
    "val_losses = []\n",
    "\n",
    "watch_auc = []\n",
    "share_auc = []\n",
    "sum_auc = []\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()\n",
    "    \n",
    "    # Uses loader to fetch one mini-batch for training\n",
    "    epoch_loss = []\n",
    "    c = 0\n",
    "    print(\"\\nEpoch: {}/{}\".format(epoch, n_epochs)) \n",
    "    for x_batch, y_batch in train_loader:\n",
    "        \n",
    "        # NOW, sends the mini-batch data to the device\n",
    "        # so it matches location of the MODEL\n",
    "        x_batch = x_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "        \n",
    "        # One stpe of training\n",
    "        yhat = model(x_batch.float())\n",
    "        \n",
    "        loss = 0\n",
    "        for i in range(tasks):\n",
    "            loss += loss_fn(yhat[:,i].float(), y_batch[:, i].view(-1, 1).float()) * loss_weight[i]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss.append(loss.item())\n",
    "        c += 1\n",
    "        if c % 100 == 0:     \n",
    "            mesg = f'[Batch: {c}]' + \\\n",
    "                     f'train_loss: {np.mean(loss.item()):.5f} '\n",
    "            logging.info(mesg)\n",
    "#             print(mesg)\n",
    "            \n",
    "    losses.append(np.mean(epoch_loss))\n",
    "        \n",
    "    # After finishing training steps for all mini-batches,\n",
    "    # it is time for evaluation!\n",
    "        \n",
    "    # We tell PyTorch to NOT use autograd...\n",
    "    with torch.no_grad():\n",
    "        # Uses loader to fetch one mini-batch for validation\n",
    "        epoch_loss = []\n",
    "        epoch_watch_auc = []\n",
    "        epoch_share_auc = []\n",
    "        epoch_sum_auc = []\n",
    "        for x_val, y_val in val_loader:\n",
    "            # Again, sends data to same device as model\n",
    "            x_val = x_val.to(device)\n",
    "            y_val = y_val.to(device)\n",
    "    \n",
    "            model.eval()\n",
    "            # Makes predictions\n",
    "            yhat = model(x_val.float()) # len=11, 一组batch的预测值\n",
    "            \n",
    "            # Computes validation loss\n",
    "            loss = 0\n",
    "            for i in range(tasks):\n",
    "                loss += loss_fn(yhat[:, i].float(), y_val[:, i].view(-1, 1).float())  * loss_weight[i]\n",
    "            epoch_loss.append(loss.item())\n",
    "            \n",
    "            \n",
    "            # ********** AUC *************\n",
    "            # label preds [0,1,2,..\n",
    "#             y_v = y_val.cpu() \n",
    "#             for i in range(len(y_v)):\n",
    "#                 print(\"yhat : \", yh.shape)\n",
    "#                 print(\"y_val: \", y_v.shape)\n",
    "#                 epoch_watch_auc.append(auc(y_v[i][:10], yh[:10], np.arange(10)))\n",
    "#                 epoch_share_auc.append(auc(y_v[i][-1], yh[-1][i], [0,1]))\n",
    "#                 epoch_sum_auc.append(watch_auc * 0.7 + share_auc * 0.3)\n",
    "             \n",
    "        \n",
    "#     watch_auc.append(np.mean(epoch_watch_auc))\n",
    "#     share_auc.append(np.mean(epoch_share_auc))\n",
    "#     sum_auc.append(np.mean(epoch_sum_auc))\n",
    "            # ********** AUC *************\n",
    "\n",
    "    val_losses.append(np.mean(epoch_loss))\n",
    "          \n",
    "    epoch_len = len(str(n_epochs))\n",
    "    mesg = f'[{epoch:>{epoch_len}}/{n_epochs:>{epoch_len}}] ' + \\\n",
    "                     f'train_loss: {losses[-1]:.5f} ' + \\\n",
    "                     f'valid_loss: {val_losses[-1]:.5f}'\n",
    "    logging.info(mesg)\n",
    "    \n",
    "    # ************* Early Stopping ****************\n",
    "    # early_stopping needs the validation loss to check if it has decresed, \n",
    "        # and if it has, it will make a checkpoint of the current model\n",
    "    early_stopping(val_losses[-1], model)\n",
    "\n",
    "    if early_stopping.early_stop:\n",
    "        print(\"Early stopping\")\n",
    "        break\n",
    "        \n",
    "# print(model.state_dict())\n",
    "print(\"loss: \", np.mean(losses))\n",
    "print(\"val_loss: \", np.mean(val_losses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "vmujL-9DpxiL",
    "outputId": "a99cc787-ede6-4c30-d54b-614ea34e8fd4"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "epochs = range(1, len(losses)+1)\n",
    "plt.plot(epochs, losses, 'g', label='Training loss')\n",
    "plt.plot(epochs, val_losses, 'b', label='validation loss')\n",
    "plt.title('Training and Validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将测试数据组织为 DaTaloader\n",
    "test_loader = DataLoader(dataset=torch.utils.data.TensorDataset(torch.tensor(df_test.astype(float).to_numpy())), batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 预测\n",
    "watch_pred = []\n",
    "share_pred = []\n",
    "with torch.no_grad():\n",
    "    # Uses loader to fetch one mini-batch for testing\n",
    "    for x_test in test_loader:\n",
    "        # Again, sends data to same device as model\n",
    "        x_test = x_test[0]\n",
    "        x_test = x_test.to(device)\n",
    "\n",
    "        model.eval()\n",
    "        # Makes predictions\n",
    "        yhat = model(x_test.float())\n",
    "        yhat = yhat.squeeze(0)\n",
    "        yhat = yhat.view(-1, 11)\n",
    "        \n",
    "        yhat_watch = yhat[:, :10]\n",
    "        yhat_share = yhat[:, 10:]\n",
    "\n",
    "        # save\n",
    "        watch_pred.append(yhat_watch)\n",
    "        share_pred.append(yhat_share)\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 处理结果，还原为真实label\n",
    "watch_preds = torch.cat(watch_pred, dim=0)\n",
    "watch_preds = torch.detach(watch_preds).cpu()\n",
    "watch_preds_label = np.argmax(watch_preds,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[9.9966e-01, 1.4845e-04, 1.1525e-20,  ..., 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00],\n",
      "        [0.0000e+00, 1.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00],\n",
      "        [6.6977e-09, 1.0000e+00, 1.5977e-13,  ..., 1.9123e-02, 0.0000e+00,\n",
      "         0.0000e+00],\n",
      "        ...,\n",
      "        [5.6924e-28, 1.0000e+00, 7.6839e-13,  ..., 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00],\n",
      "        [1.5459e-23, 1.0000e+00, 1.8623e-08,  ..., 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00],\n",
      "        [9.9805e-01, 1.6117e-02, 5.2910e-19,  ..., 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00]])\n",
      "torch.Size([2822180])\n"
     ]
    }
   ],
   "source": [
    "print(watch_preds)\n",
    "print(watch_preds_label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(9)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.max(watch_preds_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({0: 1056016,\n",
       "         1: 1318304,\n",
       "         5: 138918,\n",
       "         8: 51916,\n",
       "         2: 143903,\n",
       "         7: 55418,\n",
       "         9: 29370,\n",
       "         4: 18130,\n",
       "         3: 6180,\n",
       "         6: 4025})"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(watch_preds_label.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 656,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.max(\n",
       "values=tensor(0),\n",
       "indices=tensor(0))"
      ]
     },
     "execution_count": 656,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# torch.max(watch_preds_label, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "share_preds = torch.cat(share_pred, dim=0)\n",
    "share_preds = torch.detach(share_preds).cpu()\n",
    "share_preds_label = np.rint(share_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({0.0: 1397260, 1.0: 1424920})"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(share_preds_label.squeeze(1).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv(\"../../submission-5fold-xgb.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission['watch_label'] = watch_preds_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission['is_share'] = share_preds_label.squeeze(1).numpy().astype('int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>video_id</th>\n",
       "      <th>watch_label</th>\n",
       "      <th>is_share</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1688013</td>\n",
       "      <td>32645</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4502598</td>\n",
       "      <td>41270</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5585629</td>\n",
       "      <td>16345</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1635520</td>\n",
       "      <td>28149</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4160191</td>\n",
       "      <td>40554</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2822175</th>\n",
       "      <td>5019057</td>\n",
       "      <td>18766</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2822176</th>\n",
       "      <td>5019057</td>\n",
       "      <td>12968</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2822177</th>\n",
       "      <td>4255762</td>\n",
       "      <td>21794</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2822178</th>\n",
       "      <td>171497</td>\n",
       "      <td>21578</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2822179</th>\n",
       "      <td>5642580</td>\n",
       "      <td>28914</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2822180 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         user_id  video_id  watch_label  is_share\n",
       "0        1688013     32645            0         0\n",
       "1        4502598     41270            1         0\n",
       "2        5585629     16345            1         1\n",
       "3        1635520     28149            0         0\n",
       "4        4160191     40554            1         1\n",
       "...          ...       ...          ...       ...\n",
       "2822175  5019057     18766            1         0\n",
       "2822176  5019057     12968            1         0\n",
       "2822177  4255762     21794            1         1\n",
       "2822178   171497     21578            1         1\n",
       "2822179  5642580     28914            0         0\n",
       "\n",
       "[2822180 rows x 4 columns]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('./MMoE_11tasks.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZybNEldCpxiM"
   },
   "source": [
    "### Testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Utuc-SE2pxiM"
   },
   "outputs": [],
   "source": [
    "test_label_tmp = np.column_stack((np.argmax(test_label[0], axis=1), np.argmax(test_label[1], axis=1)))\n",
    "test_loader = DataLoader(dataset=getTensorDataset(test_data.to_numpy(), test_label_tmp), batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f_Sn3evgpxiN",
    "outputId": "b13a4bcc-3547-42d0-80f1-588bf41ea6d0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5134528784119353\n"
     ]
    }
   ],
   "source": [
    "t1_pred = []\n",
    "t2_pred = []\n",
    "t1_target = []\n",
    "t2_target = []\n",
    "\n",
    "# We tell PyTorch to NOT use autograd...\n",
    "with torch.no_grad():\n",
    "    # Uses loader to fetch one mini-batch for testing\n",
    "    epoch_loss = []\n",
    "    for x_test, y_test in test_loader:\n",
    "        # Again, sends data to same device as model\n",
    "        x_test = x_test.to(device)\n",
    "        y_test = y_test.to(device)\n",
    "\n",
    "        model.eval()\n",
    "        # Makes predictions\n",
    "        yhat = model(x_test)\n",
    "\n",
    "        y_test_t1, y_test_t2 = y_test[:, 0], y_test[:, 1]\n",
    "        yhat_t1, yhat_t2 = yhat[0], yhat[1]\n",
    "\n",
    "        loss_t1 = loss_fn(yhat_t1, y_test_t1.view(-1, 1))\n",
    "        loss_t2 = loss_fn(yhat_t2, y_test_t2.view(-1, 1))\n",
    "        loss = loss_t1 + loss_t2\n",
    "        \n",
    "        # predict\n",
    "        t1_hat = yhat_t1.view(-1) > 0.5\n",
    "        t2_hat = yhat_t2.view(-1) > 0.5\n",
    "        \n",
    "        # save\n",
    "        t1_pred.append(t1_hat)\n",
    "        t2_pred.append(t2_hat)\n",
    "        t1_target.append(y_test_t1)\n",
    "        t2_target.append(y_test_t2)\n",
    "        \n",
    "\n",
    "        epoch_loss.append(loss.item())\n",
    "print(np.mean(epoch_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Uiu2JFtipxiN"
   },
   "outputs": [],
   "source": [
    "t1_pred = torch.cat(t1_pred)\n",
    "t2_pred = torch.cat(t2_pred)\n",
    "t1_target = torch.cat(t1_target)\n",
    "t2_target = torch.cat(t2_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8tcrBVJopxiN",
    "outputId": "38dc4a47-7d48-4e54-beff-0150dbacc6e6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[46694,   103],\n",
       "       [ 2486,   598]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "confusion_matrix(t1_target.cpu().numpy(), t1_pred.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l2vsn31ZpxiO",
    "outputId": "7f402b0b-7bde-4751-d233-c5c0b859283f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.95      1.00      0.97     46797\n",
      "         1.0       0.85      0.19      0.32      3084\n",
      "\n",
      "    accuracy                           0.95     49881\n",
      "   macro avg       0.90      0.60      0.64     49881\n",
      "weighted avg       0.94      0.95      0.93     49881\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(t1_target.cpu().numpy(), t1_pred.cpu().numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eDmGiBUIpxiO",
    "outputId": "38cd49c0-640f-4718-b37c-6ba77e199157"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.91      0.89      0.90     28284\n",
      "         1.0       0.86      0.89      0.87     21597\n",
      "\n",
      "    accuracy                           0.89     49881\n",
      "   macro avg       0.89      0.89      0.89     49881\n",
      "weighted avg       0.89      0.89      0.89     49881\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(t2_target.cpu().numpy(), t2_pred.cpu().numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AgxjvpF9pxiP"
   },
   "source": [
    "### Testing example for the loss function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fBSvc0V6pxiP",
    "outputId": "431d5aca-bae4-4094-9dec-e7fb47389797"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.5821)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = nn.CrossEntropyLoss()\n",
    "input = torch.tensor([[[0.1, 0.5], [0.1, 0.5]], [[0.1, 0.5], [0.3, 0.5]], [[0.1, 0.5], [1.0, 1.0]]])\n",
    "target = torch.tensor([[0, 0], [1, 1], [1, 1]])\n",
    "output = loss(input, target)\n",
    "output"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "MMoE-DouLoss.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
